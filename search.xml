<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2020-03-03-计量和监控管理服务Telemetry]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E8%AE%A1%E9%87%8F%E5%92%8C%E7%9B%91%E6%8E%A7%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Telemetry%2F</url>
    <content type="text"><![CDATA[概述Telemetry是OpenStack的计量与监控服务，用来获取和保存计量与监控的各种测量值，并根据测量值进行报警。同时这些保存下来的测量值也可以被第三方系统获取，用来做更进一步的分析、处理或展示。早期OpenStack的计量和监控服务由Ceilometer独自完成，将原始计量数据存储在MongoDB中，性能方面可以说是极烂，社区的活跃度也低，基本到了快被放弃的边缘。随着Gnocchi的出现，Gnocchi的出现使计量服务不论是在性能还是空间消耗上都有了质的飞跃，功不可没，从而挽救了Ceilometer这个项目。后续，OpenStack社区受自身架构的启发，针对计量和监控这块对Ceilometer项目做了拆分，Ceilometer专门为为OpenStack环境提供一个获取和保存各种测量值的统一框架，明确目标，同时将其告警功能拆分到aodh项目，采集数据存储到gnocchi时间序列数据库，事件相关的服务拆分到panko项目，相关数据仍存储在MongoDB中。因此，现在OpenStack的计量和监控项目就是一个“小帐篷”（“大帐篷”就是OpenStack自身），包括了：Ceilometer、Aodh、Gnocchi和Panko，统称为Telemetry。 Gnocchi是Telemetry项目的中的“灵魂”，是一个多租户时间序列，计量和资源数据库。提供了REST API接口来创建和操作数据，用于超大规模计量数据的存储，同时向操作者和用户提供对度量和资源信息的访问。Gnocchi需要与Ceilometer对接传递计量数据，而Ceilometer作为一个框架，处于核心位置，它像一个漏斗一样，能把OpenStack内部发生的几乎所有的事件都收集起来，为计费和监控以及其它服务提供数据支撑。Ceilometer服务及其剥离出来的服务由四个组件构成，四个组件分工明确，各司其职，使得计量系统结构清晰明了。 Gnocchi：时间序列数据库，保存计量数据。 Panko：事件数据库，保存事件数据。 Ceilometer：数据采集服务，采集资源使用量相关数据，并推送到Gnocchi，采集操作事件数据，并推送到Panko。 Aodh：告警服务，基于计量和事件数据提供告警通知功能。 Ceilometer架构Ceilometer框架整体采用了高度可扩展性的设计思想，其逻辑架构如下图所示，Ceilometer通过通知代理和轮询代理获取测量值，经流水线的发布者发布给收集器或外部系统。收集器继而将收到的测量值保存到数据库中，同时，外部系统也可以通过Ceilometer API将测量值送达Ceilometer数据库。Ceilometer告警由告警评估器触发，发送给告警通知器，同时调用Ceilometer API，最终将告警保存到Ceilometer数据库中。此外，告警通知器可以将告警发送给外部系统，外部系统也可以通过Ceilometer API新建告警发送给Ceilometer。 Ceilometer可以通过以下3种方式获取测量值数据，其数据采集机制有通知、轮询和REST ful API方式3种： 通知：所有的OpenStack服务都会在执行了某种操作或者状态变化时发送通知消息到oslo-messaging（OpenStack整体的消息队列框架），一些消息中包含了计量所需要的数据，这部分消息会被Ceilometer的ceilometer-agent-notification服务组件处理，并转化为samples。通知数据采集方式是被动地采集计量数据。 轮询：Ceilometer中的服务组件根据配置定期主动地通过OpenStack服务的API或者其他辅助工具（如Hypervisors）去远端或本地的不同服务实体中获取所需要的计量数据；Ceilometer的轮询机制通过3种类型的代理实现，即ceilometer-agent-central、ceilometer-agent-compute和ceilometer-agent-ipmi服务组件。每种代理使用不同的轮询插件（pollster）从不同的命名空间来收集数据。 REST ful API：用户可以通过调用RESTful API直接把利用其他方式获取的任意测量数据送达给Ceilometer。 Ceilometer通过以上3种方法获取到测量值数据后，会把它转化为符合某种标准格式的数据采样（Sample）通过内部总线发送给Notification agent。然后Notification Agent根据用户定义的Pipeline来对数据采样进行转换（Transform）和发布（Publish）。如果根据Pipeline的定义，这个数据采样（Sample）最后被发布给Collector的话，Collector会把这个数据采样保存在数据库中。Ceilometer的计量数据经过数据采集（agent）、数据处理（流水线数据转换及发布）、数据存储（collector）几个步骤，各个步骤有着各自的处理流程。 Ceilometer计量数据采集和转换发布机制Ceilometer的各个服务中，负责计量数据采集的服务组件有4个，分别是agent-notification、agent-central、agent-compute、agent-ipmi。其采集数据的方式各有不同，主要分为Poll和Push两种方式，如下图所示，Poll即轮询方式，主动采集数据，Push即通知方式，被动获取数据。 agent-notification服务采用Push方式被动获取数据，即通过监听OpenStack通知总线（Notification Bus）上的通知消息来获取数据。OpenStack中各个模块在执行了某种操作或者状态变化时都会推送通知信息到oslo-messaging消息框架。agent-notification通过访问这个消息队列服务框架，获取相关通知信息，并进一步转化为采样数据的格式。agent-notification服务的运行流程如下： Step1：解析Pipeline配置文件得到Pipeline定义。 Step2：调用stevedore库，载入所有的notification listener插件。 Step3：对每一个notification listener插件通过oslo.messaging库构造其对应的oslo.messaging库的notification listener对象，并且启动此对象监听通知消息。 Step4：当通知总线上有某个notification listener插件所感兴趣的通知到达时，所对应的notification listener插件就会被agent-notification调用，根据此通知消息构造出采样值Sample，然后根据Pipeline中的定义将此采样值转换和发布出去。 agent-central服务可以部署在任何节点上，通常部署在控制节点上，它用来和远程的各种不同的实体和服务进行通信，获取不同的测量值。agent-central主要通过调用各OpenStack服务的REST API来获取OpenStack服务的各种信息，以及通过SNMP来获取Hardware资源的信息，通过kwapi来收集设备能耗数据。 agent-compute服务需要部署在运行nova-compute服务的计算节点上，主要用来收集计算节点上的虚拟机实例的计量数据，在每一个计算节点上都要运行这个服务组件。agent-compute主要是用来和Hypervisor进行通信，通过调用Hypervisor的API来获取相关测量值，需要定期Poll轮询收集信息。 agent-ipmi服务需要部署在支持IPMI的计算节点上，该计算节点需要安装ipmitool工具。agent-ipmi服务通过ipmitool收集本机IPMI传感器（Sensor）的数据，以及Intel Node Manager的数据。 agent-central、agent-compute、agent-ipmi都属于Polling代理，Polling代理的作用是根据Pipeline的定义，周期性地调用不同的Pollster插件去轮询获得Pipeline中定义的测量值，再根据Pipeline的定义，对这些采样值进行转换和发布。各种不同的Polling代理的运行流程基本类似，如下： Step1：调用stevedore库，获取属于本Agent的所有Pollster插件。 Step2：创建PartitionCoordinator类实例对象，加入一个partition group，并创建一个定时器用来周期性地发送心跳消息。 Step3：解析Pipeline配置文件得到Pipeline的定义，并根据解析的结果创建一个或多个不同的PollingTask和所对应的定时器。由于所有采样频率相同的Pipeline都会在同一个PollingTask里处理，所以每个PollingTask都由某个特定频率的定时器驱动，在某一个线程中被执行。 Step4：由定时器驱动的PollingTask会周期性地调用其所包含的各种Pollster，由这些Pollster获取测量数据，然后根据Pipeline的定义，把获取的测量取样值交给Transformer转换后，再由Publisher发布。 Ceilometer的计量数据处理采用了Pipeline机制。Ceilometer适用于不同应用场景下的测量数据，采样频率可能有不同的要求，例如：用于计费的数据采用频率比较低，一般为10~30min，而用来监控的数据采用的频率就会比较高，一般会达到1~10s。除此之外，对于测量数据的发布方式，不同应用场景也有不同的要求，对于计费数据要求数据采样值不能丢失，而对于用来监控的数据则要求不那么严格。为此，Ceilometer引入了Pipeline的概念来解决采样频率和发布方式的问题。Pipeline由源（Source）和目标（Sink）两部分组成。源中定义了需要测量哪些数据、数据的采样频率、在哪些端点上进行数据采样，以及这些数据的目标。目标中定义了获得的数据要经过哪些Transformer进行数据转换，并且最终交由哪些Publisher发布。Ceilometer中同时允许有多个Pipeline，每个Pipeline都有自己的源和目标，这就解决了不同采样频率、不同发布方式的问题。Ceilometer的数据处理流程如下图所示： Transformer可以针对一个或者多个同一种类的数据采样值进行各种不同的操作，例如改变单位、聚合计算等，最终转换成一个或者多个其他不同种类的测量数据。通过Transformer转换后的数据最终会交由Pipeline定义中的Publisher进行数据发布，不同的Publisher发布的数据可以有不同的数据接收者，可以是Ceilometer的collector服务，也可以是外部系统。 Ceilometer获得的测量数据通过Pipeline发布后，需要有一个数据接收者获得这些数据并且保存下来，以便对数据进行进一步的处理。Ceilometer的collector服务就是用来接收这些测量数据的，并最终持久化到存储介质中。collector服务可以配置一个或多个dispatcher，对于每一个collector所接收到的采样数据，collector会调用所有配置的dispatcher，由这些dispatcher来决定如何处理数据。 Ceilometer支持4种不同的数据库后台：MongoDB、MySQL、PostgreSQL和Hbase。不同的数据库后台，所支持的功能和性能有所不同，比如采用MongoDB作为后台数据库，除了一般的最大值、最小值、平均值、累加总和、总数之外，还支持求标准方差和求基数两个操作。虽然Ceilometer的默认后台数据库是MySQL，但是历史上一般建议在实际生产环境中采用MongoDB为后台数据库。由于性能等方面的原因，目前Telemetry社区建议使用Gnocchi代替上面所说的数据库来保存采样数据。 aodh、panko和gnocchi在OpenStack Liberty版本中，为了适应更灵活的部署方案，Telemetry社区把Ceilometer项目中和报警相关的功能剥离出来，成立了一个新的项目Aodh，主要是提供基于Ceilometer所获取的测量值或者Event事件进行报警的功能。Aodh的基本体系结构主要由以下几种服务构成，每种服务都是可水平扩展（scale out）的。 API：主要提供面向用户的RESTful API接口服务。 Alarm Evaluator：用来周期性的检查除了event类型之外其他警告器（Alarm）相关的告警条件是否满足。 Alarm Listener：根据消息总线（Notification Bus）上面的Event事件消息，来检查相对应的event类型的警告器（Alarm）告警条件是否满足。 Alarm Notifier：当警告器的告警条件满足时，执行用户定义的动作。 对于每一种警告器的状态，在新建或者修改警告器的时候，都可以为其设置不同的报警动作。当Alarm Evaluator周期性检查警告器状态时，或者当Alarm Listener接收到相关的Event事件并进行检查后，如果发现当前警告器状态有对应的报警动作，那么它会通过Alarm Notifier服务来调用相应的报警动作。 在OpenStack Newton版本中，Telemetry社区把Ceilometer关于Event事件部分的API移到了Panko项目中。目前，Event事件的产生和保存仍旧是由Ceilometer负责，但是对于从后台数据库中读取Event事件的API service 被移到了Panko项目中（Ceilometer API中与Event相关的API目前还保留着）。Panko主要接收来自Ceilometer的事件数据，并提供查询接口。其主要组件只有一个：panko-api:，用于提供事件数据的插入和查询接口。 Gnocchi接收来自Ceilometer的原始计量数据，进行聚合运算后保存到持久化后端。Gnocchi中保存与资源使用量相关的计量数据，为了提高检索效率，额外将计量数据的元数据信息单独存储。另外由于从原始输入数据到最终存储的聚合数据，需要进行大量计算，为了缓冲输入与处理间的速率，引入缓存后端。因此Gnocchi中涉及三种存储后端： 索引后端：存储计量对象和采集项的基础属性，比如对象类型（虚拟机、硬盘、网络）、原始资源uuid等。索引数据量不大，一半用MySQL。 聚合数据后端：存储经过聚合计算的计量数据，比如cpu使用率的平均值、最大值、最小值等。推荐用Ceph，可以支持多实例共享数据。 传入数据后端：保存来自Ceilometer的原始计量数据。默认与聚合后端一致，推荐使用Redis。 gnocchi的主要组件如下： gnocchi-api：提供数据传入接口，接收原始计量数据，并将它们保存到传入数据后端。同时提供聚合计量数据的查询接口，从聚合数据后端读取计量数据返回给用户。 gnocchi-metricd：从传入数据后端读取原始计量数据，进行聚合计算，然后将聚合数据保存到聚合数据后端。 Telemetry操作实战步骤1：执行以下指令，查看归档策略 1openstack metric archive-policy list 归档策略指定计量数据的聚合计算方式，包括聚合方法（mean、min、max、sum、std、count），和计量数据的统计时间粒度、数据点数。根据时间粒度和统计点数可以确定计量的时间跨度，根据一个数据点的大小在0.05 bytes~8.04 bytes，就可以确定一个metric需要使用的存储空间大小。Gnocchi内置了bool、low、medium、high四种归档策略。 步骤2：执行以下指令，查看归档策略规则： 1openstack metric archive-policy-rule list 归档策略的作用单位是计量项（metric），上述默认规则将所有metric关联到low策略，metric使用通配符匹配。 步骤3：执行以下指令，查看资源列表： 1openstack metric resource list --limit 10 资源就是对于OpenStack各个项目中的逻辑资源，比如实例、端口、镜像、卷等。 步骤4：执行以下指令，查看metric列表： 1openstack metric metric list --limit 10 Metric是资源统计的基本单位，一个资源会有多个metrics，比如实例资源有cpu_util、memory.usage、disk.root.size等。 步骤5：执行以下指令，查看metric列表： 1openstack metric measures show 00b09a5c-3c7f-445e-b885-872db5d80b2b Measures就是Gnocchi中最终保存的计量数据，即每个metric的数据点。 步骤6：执行以下指令，创建告警： 1openstack alarm create --name cpu_high --type gnocchi_resources_threshold --description 'Instance Running HOT' --metric cpu_util --threshold 65 --comparison-operator ge --aggregation-method mean --granularity 300 --resource-id 07b49533-64fd-4e53-a6c7-9824c03e043a --resource-type instance --alarm-action 'log://' --ok-action 'log://' 这里设置触发器状态变为alarm和ok时都执行log动作，即记录到aodh-notifier日志中。可以将log://替换为外部告警接口，触发邮件、短信等通知，或者heat的扩容接口，实现服务自动扩容。 —————————————————————– 结束 ——————————————————————————]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-资源编排管理服务Heat]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E8%B5%84%E6%BA%90%E7%BC%96%E6%8E%92%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Heat%2F</url>
    <content type="text"><![CDATA[概述Heat是OpenStack项目中实现资源编排的子项目，它的目的是帮助用户实现在OpenStack云环境下手工资源创建、配置、应用部署、弹性伸缩等过程的自动化，从而提高OpenStack应用的效率，比如华为云Stack6.5.1版本的Deploy就是通过Heat的编排模版完成FusionSphere OpenStack的部署，华为的FusionSphere OpenStack部署和配置的复杂度只要实践过都知道，那么如果不依靠编排去自动部署而是手动部署，那种酸爽可想而知。因此，Heat这种编排类的系统或软件出现，就解决了这种复杂系统重复部署效率低下的问题。由于OpenStack缘起于AWZ Cloud，因此Heat至今仍保留着 AWZ Cloud的编排REST API请求接口，并在功能上继承AWZ的Formation（现在甚至逐渐在超越）。 对于Heat的功能和实现，简单来说就是用户可以预先定义一个规定格式的任务模版，任务模版中定义了一连串的相关任务（例如用什么配置开几台虚拟机，然后在其中一台中安装一个mysql服务，设定相关数据库属性，然后再配置几台虚拟机安装web服务群集等等），然后将模版交由Heat执行，就会按一定的顺序执行heat模版中定义的一连串任务。因此，Heat向开发人员和系统管理员提供了一种简便地创建和管理一批相关资源的方法，并通过有序且可预测的方式对其进行资源配置和更新。我们介绍Heat服务也是重点介绍如何使用Heat的标准示例模板或自己创建模板来设定业务资源以及业务程序运行时所需的所有相关依赖项或运行时参数。 Heat的架构Heat项目总体上可以看作由Heat模板文件和Heat执行引擎两部分。其中Heat模板定义了资源部署流程（资源可以是虚拟机实例，也可以是网络、IP、镜像、用户），而Heat执行引擎通过OpenStack标准的API与其他组件交互协作，不仅实现了云环境中资源依赖关系处理、资源的初始化、资源自动部署等针对资源的基本操作，还能实现弹性伸缩、负载均衡等服务配置操作。下图就是Heat的逻辑架构，Heat的逻辑架构主要由Heat-Client、Heat-API、Heat-Engine以及Service-Client（本质上是Plugin）组成。 Heat-Client：是Heat的命令行工具，提供Heat-API的命令行访问模式，安装Heat的Dashboard后，Horizon也可以作为Heat-Client，但是支持的功能较少。 Heat-api：为Heat-engine提供OpenStack风格的Rest查询接口，通过消息队列与Heat-engine交互。同时，Heat也支持AWS Cloud的REST API请求，通过Heat-api-cfn接收。 Heat-engine：Heat的核心，实现自动化部署的主要工作，包括定义模板、解析模板、资源依赖分析、资源部署调用等，以及提供相应的回调接口。 Service-Client：严格来说不属于Heat的组件，主要在Heat-Engine中通过插件方式实现OpenStack的其他服务的Client，来调用其他服务创建资源。 这里需要重点提一下Heat-api和Heat-Engine两个组件。Heat API包括Heat-api和Heat-api-cfn，Heat-api主要接收OpenStack自身的对HOT（Heat Orchestration Template）模版的API编排请求，为Heat-Engine提供操作OpenStack自身的资源类型的接口，如OS:: XX:: XXXX格式的资源。Heat-api-cfn主要接收AWS对Formation模版的API编排请求，为Heat-Engine提供操作AWS CLoud的资源类型接口，如AWS:XX:XXXX格式的资源。两种云OS的操作类型，在OpenStack的Heat中均支持，且能进行相关查询，如下图示。Heat API的调用授权管理遵循OpenStack中的标准，采用Keystone进行统一管理。 Heat-Engine作为Heat服务的核心，实现模版的定义、解析、资源依赖分析和部署调用，是整个Heat服务的“灵魂”。整体上如下图所示，Heat-Engine分为三层：模版解析层、资源依赖分析层和资源实现层。 模版解析层：最上面那层，处理Heat-api层面的请求，就是根据模板和输入参数来创建Stack，这里的Stack其实就是各种模版实现逻辑以及模版之间的嵌套关系，包含计算、存储、网络、镜像、裸机等包含各种资源的集合和约束关系。Heat-Engine首要工作就是是完成模板格式校验，此功能由Heat-Engine的cfn和hot组件完成，分别对应AWS和HOT类型的两种模板格式。 资源依赖分析层：解析Stack里各种资源的依赖关系以及模版和嵌套模版的关系，通过Heat-Engine的Resource模块中的stack组件完成的，具体为检测模板定义的ref和deponds_on属性的有效性和关系型，将所有资源形成一个有效的资源关系图。 资源实现层：根据解析出来的资源的依赖关系，依次调用各种服务客户端来创建各种资源。Heat模板中的每一种资源都会对应一个有效的资源类型，引擎通过一个唯一的资源类型名称找到对应的资源实现，这个映射关系由资源的接入机制实现，具体为：扫描指定目录下的所有类，如果发现其中包含了资源类型实现映射函数resource_mapping（），并且指向一个有效的实现，则建立相应的映射关系，为后续的栈调度执行打下基础。 Heat-Engine除了上面分析的三层逻辑，还有内部重要的功能子组件，那就是调度引擎。它为模板的执行建立了一套完整的流程引擎，包括资源创建状态监听，资源创建状态回调通知等。采用Stack的方式存储流程定义，提供与其他组件的同异步交互控制机制，并负责Stack生命周期状态的管理，在出现异常时，还能进行相应的回滚操作，并控制资源的依赖关系，保证回滚操作的完成。那么既然调度引擎负责各种资源调度，必然少不了与OpenStack的其他服务进行交互，它们之间就是通过Service-Client进行交互。但是，一个服务或业务的编排各类资源的实现并不简单的串行流程，资源实现根据模版中的定义不仅有依赖关系，甚至可以并行完成。比如，A资源的实现需要依赖B资源创建完成（虚拟机实例的拉起需要网络资源首先完成虚拟交换机创建），或者A资源和B资源可以同步进行（某虚拟机实例需要两个网卡，那么网络资源编排时可以同时拉起两个虚拟交换机），而这些工作就是由调度引擎利用内部的OpenStack其他服务模拟Client通过消息队列AMQP向Nova、Cinder、Neutron等各个服务发起的，他们之间是一种RPC.CAST调用机制。既然是异步调用，那么Heat-Engine内部还需要有接收这些异步回调结果的模块来确认结果，并根据资源创建结果以及模版的定义决定下一步操作流程。 Heat中的Stack资源编排项目Heat的主要目标，就是充分利用OpenStack中强大的自动化资源使用功能。通过Stack这一概念的引入，终端用户通过很简单的方式就可简单明了地组织自己的应用，而无须经历痛苦折磨的学习过程和各种“救火”经历。无论使用哪种格式的模板栈，终端用户所关心的，只是如何部署应用，并将整个部署过程看成一个独立单元。Heat使用的是由YAML语言定义的HOT（Heat Orchestration Template）格式模板，允许用户使用代码对OpenStack资源进行自动分配。HOT文件中的文本化描述语言充当了用户与应用环境之间的桥梁，OpenStack中资源栈的构建过程，就是从HOT文件中置备完整应用环境的过程。典型的HOT文件代码结构如下所示： heat_template_version：这里指定了HOT文件所使用的模板语法版本，HOT模版起始标准版本是2013-05-23，也就是说HOT模版诞生在2013-05-23这个时间点，在这前都是AWS的版本编号。通过Heat的命令行工具或Dashboad可以查看当前Heat支持的版本类型，如下图为Dashboard上的查询结果，不同版本的模版虽然语法上保持向前兼容，但是功能上有些许变化，各版本支持的具体功能可以查看官网https://docs.openstack.org/heat/latest/template_guide/hot_spec.html#hot-spec。 description：此处是关于该模板的详细描述，可选参数，可以不填写任何值，但是必须保留该字段，并且字段最后冒号”: “后面有一个空格（YAML语言格式）。 parameters：此处声明输入列表，每一个参数都有给定的名称、类型、描述和默认值，且默认值是可选的。参数部分可以包括任何信息，例如一个特定的镜像或者用户指定的网络ID。parameters的使用示例如下，定义两个参数image_id和flavor，都是String类型，各自的标签（label）分别为Image ID和Instance Type，这里的标签（label）在resources模块中，可以通过内部函数get_param()直接引用。 resources：这里的资源可以看成是Heat需要创建或者在其操作中需要修改的对象，HOT文件中的resources代码段就是定义不同组件的地方，例如，resource_name为virtual_web的资源具有OS::Nova::Server的属性，这就指定了资源类型为Nova计算实例。资源还可被子属性列表扩展限定，例如，可以为virtual_web实例资源指定所使用的镜像、资源模板和私有网络等资源。resources的使用示例如下，资源名称为my_instance，资源类型为OS: :Nova: :Server，资源使用的道具有镜像image和虚拟机实例类型flavor。 outputs：在将资源栈部署到Heat engine后，可以将其属性全部导出。outputs的使用示例如下，输出虚拟机实例的ip地址instance_ip，输出值为通过内置函数get_attr()获取虚拟机实例的第一个IP地址。 经过以上的分析，我们可以写出如下的批量拉起虚拟机实例的简单模板。该模板用于批量拉起三个虚拟机实例instance，定义了三个输入参数，分别是：image、flavor和internal_network，在create stack时通过–parameter Image ID=XXX，–parameter Instance Type=XXX和– parameter Internal_net=XXX指定。同时，定义了三个输出参数，分别输出三个虚拟机实例的首个网卡地址（在我们的模板中就定义一个虚拟机网卡，也可以定义多个），资源类型只使用了OS: :Nova: :Server。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687heat_template_version: 2018-08-31description: Simple template to deploy multiple compute instanceparameters: image_id: type: string label: Image ID description: Image to be used for compute instance flavor: type: string label: Instance Type description: Type of instance (flavor) to be used in_net: type: string label: Internal_net description: IP address of instance (Internal_net) to be usedresources: instance01: type: OS::Nova::Server properties: image: &#123; get_param: image_id &#125; flavor: &#123; get_param: flavor &#125; networks: &#123; get_param: in_net &#125; instance02: type: OS::Nova::Server properties: image: &#123; get_param: image_id &#125; flavor: &#123; get_param: flavor &#125; networks: &#123; get_param: in_net &#125; instance03: type: OS::Nova::Server properties: image: &#123; get_param: image_id &#125; flavor: &#123; get_param: flavor &#125; networks: &#123; get_param: in_net &#125;outputs: instance01_ip: description: The IP address of the deployed instance value: &#123; get_attr: [instance01, first_address] &#125; instance02_ip: description: The IP address of the deployed instance value: &#123; get_attr: [instance02, first_address] &#125; instance03_ip: description: The IP address of the deployed instance value: &#123; get_attr: [instance03, first_address] &#125; 除了上面单一模版自动化编排的示例外，在复杂编排的场景下，模板也可以实现嵌套，类似软件模块化开发的思想。可以先定义一个父模版Stack，作为进入其余子模板Stack的入口文件。应用部署所需的全部资源均在子模板Stack文件中描述，子栈之间的参数传递可由父栈进行管理，父栈将某个子栈创建后的输出作为另一个子栈的输入，例如将网络栈创建后的输出作为应用栈的输入。 Heat的编排场景Heat 采用了业界流行使用的模板方式来设计或者定义编排，用户只需要打开文本编辑器，编写一段基于 Key-Value 的模板，就能够方便地得到想要的编排。为了方便用户的使用，Heat 提供了大量的模板例子，可以参考https://docs.openstack.org/heat/latest/template_guide/hot_guide.html。大多数时候用户只需要选择想要的编排，通过拷贝-粘贴的方式来完成模板的编写。 如上图，Heat 从四个方面来支持编排。 基础架构资源编排也就是OpenStack自己提供的基础架构资源，包括计算，网络和存储等资源。通过编排这些资源，用户就可以得到最基本的VM。在编排VM的过程中，用户可以提供一些简单的脚本，以便对VM做一些简单的配置，如下示例图中红框部分。 应用资源的编排也就是用户可以通过Heat提供的Software Configuration和Software Deployment等对VM进行复杂的配置，比如安装软件、配置软件。Heat 提供了多种资源类型来支持对于软件配置和部署的编排，如下所列： OS::Heat::CloudConfig： VM引导程序启动时的配置，由 OS::Nova::Server 引用 OS::Heat::SoftwareConfig：描述软件配置 OS::Heat::SoftwareDeployment：执行软件部署 OS::Heat::SoftwareDeploymentGroup：对一组VM执行软件部署 OS::Heat::SoftwareComponent：针对软件的不同生命周期部分，对应描述软件配置 OS::Heat::StructuredConfig：和OS::Heat::SoftwareConfig类似，但是用Map来表述配置 OS::Heat::StructuredDeployment：执行OS::Heat::StructuredConfig对应的配置 OS::Heat::StructuredDeploymentsGroup：对一组VM执行 OS::Heat::StructuredConfig对应的配置 其中最常用的是OS::Heat::SoftwareConfig和OS::Heat::SoftwareDeployment，编排流程如下图所示。 高级功能编排也就是如果用户有一些高级的功能需求，比如需要一组能够根据负荷自动伸缩的VM组，或者需要一组负载均衡的VM，Heat提供了AutoScaling 和Load Balance等进行支持。如果要用户自己单独编程来完成这些功能，所花费的时间和编写的代码都是不菲的。现在通过Heat，只需要一段长度的Template，就可以实现这些复杂的应用。Heat提供自动伸缩组OS::Heat::AutoScalingGroup和伸缩策略OS::Heat::ScalingPolicy，结合基于Ceilometer的OS::Ceilometer::Alarm实现了可以根据各种条件，比如负载，进行资源自动伸缩的功能。 Heat对负载均衡的编排是由一组不同的资源类型来实现的。资源类型包括： OS::Neutron::Pool：定义资源池，一般可以由VM组成。 OS::Neutron::PoolMember：定义资源池的成员。 OS::Neutron::HealthMonitor：定义健康监视器，根据自定的协议，比如TCP来监控资源的状态，并提供给OS::Neutron::Pool来调整请求分发等。 OS::Neutron::LoadBalancer：关联资源池以定义整个负载均衡。 第三方工具继承编排也就是如果用户的应用足够复杂，或者说用户的应用已经有了一些基于流行配置管理工具的部署，比如说已经基于Chef有了Cookbook，那么可以通过集成Chef来复用这些Cookbook，这样就能够节省大量的开发时间或者是迁移时间。Heat在基OS::Heat::SoftwareConfig和OS::Heat::SoftwareDeployment的协同使用上，提供了对Chef、Puppet、Ansible等流行配置管理工具的支持。 Heat的编排实战Heat的编排实战涉及面较广，由于篇幅所限且也无法大量凭空臆造业务场景举例，因此这里着重从不同种类HOT模版编写入手通过一些简单实例熟悉Heat的基础功能用法和编排效果验证。Heat要想用好，必须有一定编程基础，建议大家首先熟悉HOT模板的语法格式和内置函数，可以参考OpenStack官方网站手册，里面有大量的模版实例可供参考。 步骤1：执行以下命令，安装python-heatclient，等待安装完成 1pip install python-heatclient 步骤2：执行以下命令，查看当前可用的镜像。 1openstack image list 步骤3：执行以下命令，查看当前可用的规格 1openstack flavor list 步骤4：执行以下命令，查看当前可用的密钥对 1openstack keypair list 步骤5：执行以下命令，创建“demo-template.yaml”，作为HOT模板。（注：每个“：”后都要有空格，即使“：”后没有字符也要有空格） 其中，resources.server中的image、flavor、key_name为环境中可用的镜像、规格和密钥对。 步骤9：执行以下命令，查看当前可用网络 1openstack network list 步骤10：执行以下命令，使用HOT模板“demo-template.yaml”，创建堆栈“Stack_demo”，网络即为上面的网络ID，过几秒后有如下输出 1openstack stack create -t demo-template.yaml --parameter "NetID=ce3368e7-cd00-4395-91f6-b83b6f38bc9f" Stack-Demo 步骤11：等待几分钟后，执行以下命令，查看堆栈的创建过程。 1openstack stack event list Stack-Demo 步骤12：执行以下命令，查看堆栈列表，堆栈“Stack_demo”的状态变为“CREATE_COMPLETE”表示堆栈创建完成。 步骤13：执行以下命令，查看堆栈的详细信息，图中表明已创建一个虚拟机实例，虚拟机server_name=Stack-Demo-server-o2fur3jgvsis，ip=10.0.11.105 也可以执行以下命令，查看堆栈创建完成后输出的虚拟机实例名称和IP地址。 步骤14：执行以下命令，查看虚拟机实例列表进行确认，并查看虚拟机实例的状态是否为“Active” 思考：1、Heat能解决什么问题？ 2、Heat中有哪些常用概念？ 3、如何查看Heat中的支持的版本号，内置函数以及资源类型？ 4、Heat能应用到哪些场景？每一种场景需要的资源类型有哪些？ 5、HOT模版中parameter的参数用于什么场景？在创建Stack时，通过什么方式导入？ 6、当前OpenStack Rocky版本中HOT模版中resources模块共有多少种？请列举出常用的resources模块（不少于5种），并说明其资源类型。 7、Heat中的stack本质是什么？起什么作用？ 8、删除stack后，所创建的虚拟机实例是否会被删除？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-对象存储管理服务Swift]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Swift%2F</url>
    <content type="text"><![CDATA[概述Swift 最初是由Rackspace公司开发的高可用分布式对象存储服务，并于2010年贡献给OpenStack开源社区作为其最初的核心子项目之一，为其Nova子项目提供虚机镜像存储服务。Swift构筑在比较便宜的标准硬件存储基础设施之上，无需采用 RAID（磁盘冗余阵列），通过在软件层面引入一致性散列技术和数据冗余性，牺牲一定程度的数据一致性来达到高可用性和可伸缩性，支持多租户模式、容器和对象读写操作，适合解决互联网的应用场景下非结构化数据存储问题。Swift在OpenStack系统中不依赖于任何服务，可以独立部署为其他系统提供分布式对象存储服务。而在OpenStack的应用中，其Proxy Server往往由Keystone节点兼任，由Keystone来完成服务访问的安全认证。 Swift是业务提供时，使用普通的服务器来构建冗余的、可扩展的分布式对象存储集群，存储容量可达PB级。通过统一REST API进行友好访问，不仅易于扩展，且无中心数据库，避免单点故障或单点性能瓶颈。Swift主要通过Account、Container和Object三个表单结构来完成存储对象的存储、查询、获取和上传等功能，通过数据存储的多副本机制实现数据的高可用。其中，Object是最小存储表单，代表实际存储的数据，三个表单的对应关系为为：Account 1:m Container 1:m Object。 Swift的架构和组件Swift的架构是一种完全对称、面向资源的分布式系统架构，所有组件都可扩展，避免因单点失效而影响整个系统运转。通信方式采用非阻塞式 I/O 模式，提高了系统吞吐和响应能力，如下图所示。系统架构整体上采用分层的理念设计，共分为两层：访问层和存储层。Controller的Ring以上部分属于访问层，接收外部REST API的访问，实现负载均衡和访问安全验证，并定位对象数据的存储的位置。后端的Server部分属于存储层，用来分别存储不同的对象数据：Account、Container和Object。 上图Swift的架构中的一些关键组件服务需要说明，包括： 代理服务（Proxy Server）：对外提供对象服务 API，会根据环（Ring）的信息来查找服务地址并转发用户请求至相应的账户、容器或者对象服务；由于采用无状态的 REST请求协议，可以进行横向扩展来均衡负载。 认证服务（Authentication Server）：验证访问用户的身份信息，并获得一个对象访问令牌（Token），在一定的时间内会一直有效。同时验证访问令牌的有效性并缓存下来，直到令牌过期为止。 缓存服务（Cache Server）：缓存的内容包括对象服务令牌，账户和容器的存在信息，但不会缓存对象本身的数据；缓存服务可采用Memcached集群，Swift 会使用一致性哈希算法来分配缓存地址。 账户服务（Account Server）：提供账户元数据和统计信息，并维护所含容器列表的服务，每个账户的信息被存储在一个 SQLite 数据库中。 容器服务（Container Server）：提供容器元数据和统计信息，并维护所含对象列表的服务，每个容器的信息也存储在一个 SQLite 数据库中。 对象服务（Object Server）：提供对象元数据和内容服务，每个对象的内容会以文件的形式存储在文件系统中，元数据会作为文件属性来存储，建议采用支持扩展属性的 XFS 文件系统。 复制服务（Replicator）：会检测本地分区副本和远程副本是否一致，具体是通过对比哈希文件和高级水印来完成，发现不一致时会采用推式（Push）更新远程副本，例如对象复制服务会使用远程文件拷贝工具 rsync 来同步，另外一个任务是确保被标记删除的对象从文件系统中移除。 更新服务（Updater）：当对象由于高负载的原因而无法立即更新时，任务将会被序列化到在本地文件系统中进行排队，以便服务恢复后进行异步更新。例如成功创建对象后容器服务器没有及时更新对象列表，这个时候容器的更新操作就会进入排队中，更新服务会在系统恢复正常后扫描队列并进行相应的更新处理。 审计服务（Auditor）：检查对象，容器和账户的完整性，如果发现比特级的错误，文件将被隔离，并复制其他的副本以覆盖本地损坏的副本，其他类型的错误会被记录到日志中。 账户清理服务（Account Reaper）：移除被标记为删除的账户，删除其所包含的所有容器和对象。 Swift中对象的存储URL示例格式为：https://swift.example.com/v1/account/container/object，整个存储URL有两个基本部分：**集群位置**和**存储位置**。比如示例URL中的swift.example.com/v1/表示集群位置， /account/container/object表示存储位置。而对象的存储位置中主要包括三层： /account：帐户存储位置，唯一命名的存储区域，其中包含帐户本身的元数据（描述性信息）以及帐户中的容器列表。必须说明一点，Swift中的帐户不是用户身份，而是一个存储区域。 /account/container：容器存储位置，是帐户内的用户自定义的存储区域，类似我们PC操作系统中的文件夹，其中包含容器本身和容器中的对象列表的元数据。 /account/container/object：对象存储位置，存储了数据对象及其元数据的位置。 Swift中的数据结构原理一致性哈希算法Swift中最重要的算法就是一致性哈希（Consistent Hashing），它是Swift实现海量数据存储，并能实现数据均衡度和可扩展性兼容的保证，可以不过分的认为一致性哈希算法是所有分布式存储的灵魂，不仅Swift中有它的影子，在开源分布式存储Ceph以及华为分布式存储FusionStorage中同样有它的影子。在面对海量级别的对象，需要存放在成千上万台服务器和硬盘设备上，首先要解决寻址问题，即如何将对象均匀地分布到这些设备地址上。这也是Swift中的一致性哈希首先要解决的问题，算法的基本思路是：通过计算可将对象均匀分布到虚拟空间的虚拟节点上，在增加或删除节点时可大大减少需移动的数据量；虚拟空间大小通常采用 2 的 n 次幂，便于进行高效的移位操作；然后通过独特的数据结构 Ring（环）再将虚拟节点映射到实际的物理存储设备上，完成寻址过程。 一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环（其实就是一个线性队列），如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希空间环如下图（1），整个空间按顺时针方向组织，0和2^32-1在零点中方向重合。下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将4台服务器使用ip地址哈希后在环空间的位置如下图（2）。接下来定位数据访问到相应服务器的算法：将数据key使用相同的函数Hash计算出哈希值，并映射此数据在环上的位置，从此位置沿环顺时针旋转，第一台遇到的服务器就是其应该存储到的服务器。例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下图（3），根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。现假设Node C不幸宕机，从图（3）中可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。如果在系统中增加一台服务器Node X，如下图（4）所示： 此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向旋转遇到的第一台服务器）之间数据，其它数据也不会受到影响。因此，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。在实际生产部署时，往往通过虚拟节点代替物理节点，并增加虚拟节点到物理节点的映射来确保即使服务器数量很少也能做到数据均匀分布存储。即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。这样即使在只有2台物理节点的场景下，可以为每台服务器计算3个虚拟节点，结果分别为“Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点。同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上，这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，比如华为的FusionStorage的虚拟节点数为固定值3600。 数据一致性模型（Consistency Model）Swift中数据一致性策略放弃严格一致性，而采用最终一致性模型（Eventual Consistency），来达到高可用性和无限水平扩展能力。为了实现这一目标，Swift采用 Quorum仲裁协议(Quorum 有法定投票人数的含义)，定义如下： N：数据的副本总数；W：写操作被确认接受的副本数量；R：读操作的副本数量； 强一致性：R+W&gt;N，以保证对副本的读写操作会产生交集，从而保证可以读取到最新版本；如果 W=N，R=1，则需要全部更新，适合大量读少量写操作场景下的强一致性；如果 R=N，W=1，则只更新一个副本，通过读取全部副本来得到最新版本，适合大量写少量读场景下的强一致性。 弱一致性：R+W&lt;=N，如果读写操作的副本集合不产生交集，就可能会读到脏数据，适合对一致性要求比较低的场景。 Swift提供的对象存储服务面向的都是读写都比较频繁的场景，所以采用了比较折中的策略，即写操作需要满足至少一半以上成功 W &gt;N/2，再保证读操作与写操作的副本集合至少产生一个交集，即 R+W&gt;N。Swift默认配置是N=3，W=2&gt;N/2，R=1或2，即每个对象会存在 3 个副本，这些副本会尽量被存储在不同区域的节点上，W=2 表示至少需要更新 2 个副本才算写成功；当 R=1 时意味着某一个读操作成功便立刻返回，此种情况下可能会读取到旧版本（弱一致性模型）；当 R=2 时，需要通过在读操作请求头中增加 x-newest=true 参数来同时读取 2 个副本的元数据信息，然后比较时间戳来确定哪个是最新版本（强一致性模型）；如果数据出现了不一致，后台服务进程会在一定时间窗口内通过检测和复制协议来完成数据同步，从而保证达到最终一致性。 环（Ring）的数据结构环（Ring）是为了将虚拟节点（分区）映射到一组物理存储设备上，并提供一定的冗余度而设计的，其数据结构由以下信息组成： 存储设备列表、设备信息：包括唯一标识号（id）、区域号（zone）、权重（weight）、IP 地址（ip）、端口（port）、设备名称（device）、元数据（meta）。 分区到设备映射关系（replica2part2dev_id数组) 计算分区号的位移(part_shift整数) 在Swift中主要有三种环（Ring）：Account Ring、Container Ring 、Object Ring，其分别对应Swift的三种数据模型：Account、Container和Object。三种数据模型的层次关系如下图，共设三层逻辑结构：Account/Container/Object（即账户/容器/对象)，每层节点数均没有限制，可以任意扩展。 环（Ring）的数据结构实现原理如下图所示： 层次结构 account/container/object 作为键，使用 MD5 散列算法得到一个散列值，对该散列值的前 4 个字节进行右移操作得到分区索引号，移动位数由上面的 part_shift设置指定；按照分区索引号在分区到设备映射表（replica2part2dev_id）里查找该对象所在分区的对应的所有设备编号，这些设备会被尽量选择部署在不同区域（zone）内，区域只是个抽象概念，它可以是某台机器，某个机架，甚至数据中心的机群，以提供最高级别的冗余性，建议至少部署 5 个区域；权重参数是个相对值，可以来根据磁盘的大小来调节，权重越大表示可分配的空间越多，可部署更多的分区。 Swift中存储的实现Swift中的数据存储过程当收到一个需要保存的object的PUT请求时，Proxy server 会执行如下的操作流程： 1）首先根据其完整的对象路径（/account[/container[/object]]）计算其哈希值，哈希值的长度取决于集群中分区的总数（虚拟节点的总数）。 2）将哈希值的开头N个字符映射为数目同replica值的若干partition ID。 3）根据 partition ID 确定某个数据服务的 IP 和 port。 4）依次尝试连接这些服务的端口，如果有一半的服务无法连接，则拒绝该请求。 5）尝试创建对象，存储服务会将对象以文件形式保存到某个磁盘上。（Object server 在完成文件存储后会异步地调用 container service 去更新container数据库） 6）在3份副本拷贝中有两份被成功写入后， Proxy Server就会向客户端返回成功。 当Proxy server收到一个获取object的GET请求时，它所执行的操作前四步同前面的PUT请求，主要功能就是确定存放所有replica的所有磁盘，而在第五步时，Proxy Server会排序这些存储节点，尝试连接第一个，如果成功，则将二进制数据返回客户端；如果不成功，则尝试下一个，直到成功或者都失败。 总体来说该过程简单直接，这也符合Swift的总体设计风格。在上面的数据结构原理部分，我们介绍了Swift中数据的映射机制和具体操作。那么在集群中的每一台存储节点上，Swift是如何实现Account、Container、Object的具体存储呢？ Swift中存储实现分析在Storage node上运行着Linux系统并使用了XFS文件系统，逻辑上使用一致性哈希算法将固定总数的partition映射到每个Storage node上，每个Data也使用同样的哈希算法映射到Partition上，其层次结构如下图所示： 以我们的一台storage node computer1为例，该device的文件路径挂载到/srv/node/sdc，目录结构如下所示： root@computer1:/srv/node/sdc# ls accounts async_pending containers objects quarantined tmp 其中accounts、containers、objects分别是账号、容器、对象的存储目录，async_pending是异步待更新目录，quarantined是隔离目录，tmp是临时目录。 在objects目录下存放的是各个partition目录，其中每个partition目录是由若干个suffix_path名的目录和一个hashes.pkl文件组成，suffix_path目录下是由object的hash_path名构成的目录，在hash_path目录下存放了关于object的数据和元数据，object存储目录的层次结构如下图所示。 其中，hashes.pkl是存放在每个partition中的一个2进制pickle化文件。例如： root@computer1:/srv/node/sdc/objects/100000# ls 8bd hashes.pkl 通过ipython读取这个pkl文件返回结果如下红色字体所示，其中‘8bd’是suffix_dir，而9e99c8eedaa3197a63f685dd92a5b4b8则是该partition下数据的md5哈希值。 12345678910111213In [1]: with open('hashes.pkl', 'rb') as fp: ...: import pickle ...: hashes = pickle.load(fp) ...: ...: In [2]: hashesOut[2]: &#123;'8bd': '9e99c8eedaa3197a63f685dd92a5b4b8'&#125; object的存储路径由object server进程内部称为DiskFile类初始化时产生，过程如下： 1）由文件所属的account、container和object名称产生’/account/container/object’格式的字符串，和HASH_PATH_SUFFIX组成新的字符串，调用hash_path函数，生成md5 hash值name_hash。其中HASH_PATH_SUFFIX作为salt来增加安全性，HASH_PATH_SUFFIX值存放在/etc/swift/swift.conf中。 2）调用storage_directory函数，传入DATADIR, partition, hash_path参数生成DATADIR/partition/name_path[-3:]/name_path格式字符串。 3）连结path/devcie/storage_directory(DATADIR, partition,name_ hash)生成数据存储路径datadir。 4）调用normalize_timestamp函数生成“16位.5位”的时间戳+扩展名的格式生成对象名称。 例如，某object的存储路径为：/srv/node/sdc/objects/19892/ab1/136d0ab88371e25e16663fbd2ef42ab1/1320050752.09979.data，其中每个目录分别表示： Object的数据存放在后缀为.data的文件中，它的metadata存放在以后缀为.meta的文件中，将被删除的Object以一个0字节后缀为.ts的文件存放。 在accounts目录下存放的是各个partition，而每个partition目录是由若干个suffix_path目录组成，suffix_path目录下是由account的hash名构成的目录，在hash目录下存放了关于account的sqlite db，account存储目录的层次结构如下图所示。 account使用AccountController类来生成path，其过程与object类似，唯一的不同之处在于，account的db命名调用hash_path(account)来生成，而不是使用时间戳的形式。例如，某account的db存储路径为：/srv/node/sdc/accounts/20443/ac8/c7a5e0f94b23b79345b6036209f9cac8/ c7a5e0f94b23b79345b6036209f9cac8.db，其中每个目录分别表示： 在account的db文件中，包含了account_stat、container、incoming_sync 、outgoing_sync 4张表。 表account_stat：记录关于account的信息，如名称、创建时间、container数统计等等。比如，其中account表示account名称，created_at表示创建时间，put_timestamp表示put request的时间戳，delete_timestamp表示delete request的时间戳，container_count为countainer的计数，object_count为object的计数，bytes_used表示已使用的字节数，hash表示db文件的hash值，id表示统一标识符，status表示account是否被标记为删除，status_changed_at表示状态修改时间，metadata表示account的元数据。 表container：记录关于container的信息等等。比如，其中ROWID字段表示自增的主键，name字段表示container的名称，put_timestamp和delete_timestamp分别表示container的put和delete的时间戳，object_count表示container内的object数， bytes_used 表示已使用的空间，deleted表示container是否标记为删除。 表incoming_sync：记录到来的同步数据项等等。比如，其中remote_id字段表示远程节点的id，sync_point字段表示上一次更新所在的行位置，updated_at字段表示更新时间。 表outgoing_sync：表示推送出的同步数据项等等。比如，其中，remote_id字段表示远程节点的id，sync_point字段表示上一次更新所在的行位置，updated_at字段表示更新时间。 Container目录结构和生成过程与Account类似，Container的db中共有5张表，其中incoming_sync和outgoing_sync的schema与Account中的相同。其他3张表分别为container_stat、object、sqlite_sequence，结构与account中的表类似，具体不再赘述。 tmp目录作为account/container/object server向partition目录内写入数据前的临时目录。例如，在client向服务端上传某一文件，object server调用DiskFile类的mkstemp方法在创建路径为path/device/tmp的目录。在数据上传完成之后，调用put()方法，将数据移动到相应路径。 async_pending目录是本地server在与remote server建立http连接或者发送数据时超时导致更新失败时，将把文件放入该目录。这种情况经常发生在系统故障或者是高负荷的情况下。如果更新失败，本次更新被加入队列，然后由Updater继续处理这些失败的更新工作。例如，假设一个container server处于高负荷下，此时一个新的对象被加入到系统。当Proxy成功地响应Client的请求时，这个对象将变为直接可访问的。但是container服务器并没有更新对象列表，本次更新将进入队列等待延后的更新。所以，container列表不可能马上就包含这个新对象。随后Updater使用object_sweep扫描device上的async pendings目录，遍历每一个prefix目录并执行升级。一旦完成升级，则移除pending目录下的文件(实际上，是通过调用renamer函数将文件移动到object相应的目录下)。account和container的db pending文件并不会独立地存在于async_pending目录下，它们的pending文件会与其db文件在一个目录下存放。但是，account与container的db与object两者的pending文件处理方式不同：db的pending文件在更新完其中的一项数据之后，删除pending文件中的相应的数据项，而object的数据在更新完成之后，移动pending文件到目标目录。 quarantined目录是Auditor进程会在本地服务器上每隔一段时间就扫面一次磁盘来检测account、container、object的完整性。一旦发现不完整的数据，该文件就会被隔离，该目录就称为quarantined目录。为了限制Auditor消耗过多的系统资源，其默认扫描间隔是30秒，每秒最大的扫描文件数为20，最高速率为10Mb/s。obj auditor使用AuditorWorker类的object_audit方法来检查文件的完整性，该方法封装了obj server的DiskFile类，该类有一个_handle_close_quarantine方法，用来检测文件是否需要被隔离，如果发现损坏，则直接将文件移动到隔离目录下。随后replicator从其他replica那拷贝新的文件来替换，最后Server计算文件的hash值是否正确。而account使用AccountAuditor类的account_audit方法，container使用ContainerAuditor类的container_audit方法对目录下的db文件进行检查。 Swift存储服务部署和实战正常来说，Swift服务部署至少需要3节点，但是我们实验环境首先机器配置原因，因此在控制节点新增两块硬盘/dev/sdc、/dev/sdd作为对象存储的后端存储设备，也就不考虑对象存储的性能和平衡性了。同时，Swift服务部署成功后，需要将Glance的镜像存储后端由本地文件系统改为对象存储，同时新增cinder-backup服务，其后端也对接对象存储Swift，以此来模拟实际生产环境。 控制节点部署预置环境1）创建swift用户 1openstack user create --domain default --password-prompt swift 2）给swift用户添加admin角色 1openstack role add --project service --user swift admin 3）创建swift服务实体 1openstack service create --name swift --description "OpenStack Object Storage" object-store 4）创建对象存储服务API端点： 12345openstack endpoint create --region RegionOne object-store public http://rocky-controller:8080/v1/AUTH_%\(project_id\)sopenstack endpoint create --region RegionOne object-store internal http://rocky-controller:8080/v1/AUTH_%\(project_id\)sopenstack endpoint create --region RegionOne object-store admin http://rocky-controller:8080/v1 5）安装软件包 1yum -y install openstack-swift-proxy python-swiftclient python-keystoneclient python-keystonemiddleware memcached -y 6）从对象存储源仓库获取代理服务配置文件 1curl -o /etc/swift/proxy-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/proxy-server.conf-sample 7）编辑/etc/swift/proxy-server.conf文件并完成以下操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243#配置绑定端口，用户和配置目录：[DEFAULT]bind_port = 8080user = swiftswift_dir = /etc/swift#在该[pipeline:main]部分中，删除tempurl和 tempauth模块并添加authtoken和keystoneauth 模块[pipeline:main]pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server#在该[app:proxy-server]部分中，启用自动帐户创建：[app:proxy-server]use = egg:swift#proxyaccount_autocreate = True#在该[filter:keystoneauth]部分中，配置操作员角色：[filter:keystoneauth]use = egg:swift#keystoneauthoperator_roles = admin,user #在该[filter:authtoken]部分中，配置身份服务访问(注释掉或删除该[filter:authtoken] 部分中的任何其他选项)：[filter:authtoken]paste.filter_factory = keystonemiddleware.auth_token:filter_factorywww_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = swiftpassword = swiftdelay_auth_decision = True#在该[filter:cache]部分中，配置memcached位置：[filter:cache]use = egg:swift#memcachememcache_servers = rocky-controller:11211 存储节点部署预置环境（控制节点兼任）1）安装支持实用程序包： 1yum -y install xfsprogs rsync 2）格式化/dev/sdc和/dev/sdd设备为XFS文件系统： 3）创建挂载点目录结构： 12mkdir -p /srv/node/sdcmkdir -p /srv/node/sdd 4）编辑/etc/fstab文件并将以下内容添加到其中，设置开机自动挂载 12echo "/dev/sdc /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2" &gt;&gt; /etc/fstabecho "/dev/sdd /srv/node/sdd xfs noatime,nodiratime,nobarrier,logbufs=8 0 2" &gt;&gt; /etc/fstab 5）挂载安装设备 1mount -a 6）创建或编辑/etc/rsyncd.conf文件以包含以下内容： 12345678910111213141516171819202122232425uid = swiftgid = swiftlog file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidaddress = 10.28.101.81 #存储节点上管理网络的IP地址[account]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/account.lock[container]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/container.lock [object]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/object.lock 7）启动rsyncd服务并将其配置为在系统引导时启动 1systemctl enable rsyncd.service &amp;&amp; systemctl start rsyncd.service &amp;&amp; systemctl status rsyncd.service 8）安装软件包 1yum -y install openstack-swift-account openstack-swift-container openstack-swift-object 9）从对象存储源仓库获取记帐，容器和对象服务配置文件： 12345curl -o /etc/swift/account-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/account-server.conf-samplecurl -o /etc/swift/container-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/container-server.conf-samplecurl -o /etc/swift/object-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/object-server.conf-sample 10）编辑/etc/swift/account-server.conf 文件并完成以下操作： 1234567891011121314151617#在[DEFAULT]节中，配置绑定IP地址，绑定端口，用户，配置目录和挂载点目录：[DEFAULT]bind_ip = 10.28.101.81 #&lt;===为存储节点上管理网络的IP地址bind_port = 6202user = swiftswift_dir = /etc/swiftdevices = /srv/nodemount_check = True#在该[pipeline:main]部分中，启用相应的模块：[pipeline:main]pipeline = healthcheck recon account-server# 在该[filter:recon]部分中，配置recon（米）缓存目录：[filter:recon]use = egg:swift#reconrecon_cache_path = /var/cache/swift 11）/etc/swift/container-server.conf文件并完成以下操作： 1234567891011121314151617#在[DEFAULT]节中，配置绑定IP地址，绑定端口，用户，配置目录和挂载点目录：[DEFAULT]bind_ip = 10.28.101.81 #&lt;===为存储节点上管理网络的IP地址bind_port = 6201user = swiftswift_dir = /etc/swiftdevices = /srv/nodemount_check = True#在该[pipeline:main]部分中，启用相应的模块：[pipeline:main]pipeline = healthcheck recon container-server #在该[filter:recon]部分中，配置recon（米）缓存目录：[filter:recon]use = egg:swift#reconrecon_cache_path = /var/cache/swift 12）编辑/etc/swift/object-server.conf文件并完成以下操作： 123456789101112131415161718#在[DEFAULT]节中，配置绑定IP地址，绑定端口，用户，配置目录和挂载点目录：[DEFAULT]bind_ip = 10.28.101.81 #&lt;===为存储节点上管理网络的IP地址bind_port = 6200user = swiftswift_dir = /etc/swiftdevices = /srv/nodemount_check = True#在该[pipeline:main]部分中，启用相应的模块：[pipeline:main]pipeline = healthcheck recon object-serve#在该[filter:recon]部分中，配置recon（米）缓存和锁定目录：[filter:recon]use = egg:swift#reconrecon_cache_path = /var/cache/swiftrecon_lock_path = /var/lock 13）确保安装点目录结构的正确所有权： 12chown -R swift:swift /srv/nodesudo restorecon -R /srv 14） 创建recon目录并确保其正确拥有： 123mkdir -p /var/cache/swiftchown -R root:swift /var/cache/swiftchmod -R 775 /var/cache/swift 控制节点上创建Ring1）切换到/etc/swift目录 1cd /etc/swift/ 2）创建基本account.builder，container.builder和object.builder文件： 123swift-ring-builder account.builder create 10 2 1swift-ring-builder container.builder create 10 2 1swift-ring-builder object.builder create 10 2 1 3）将每个存储节点添加到环中 123456swift-ring-builder account.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6202 --device sdc --weight 100swift-ring-builder account.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6202 --device sdd --weight 100swift-ring-builder container.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6201 --device sdc --weight 100swift-ring-builder container.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6201 --device sdd --weight 100swift-ring-builder object.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6200 --device sdc --weight 100swift-ring-builder object.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6200 --device sdd --weight 100 4）验证环内容 123swift-ring-builder account.builderswift-ring-builder container.builderswift-ring-builder object.builder 5）重新平衡环 123swift-ring-builder account.builder rebalanceswift-ring-builder container.builder rebalanceswift-ring-builder object.builder rebalance 6）分配环的配置文件到每个swift的存储节点，将生成的副本account.ring.gz，container.ring.gz以及 object.ring.gz文件复制到/etc/swift每个存储节点和运行代理服务的任何其他节点上目录。（我们环境控制节点兼任swift存储节点） 7）从Object Storage源仓库获取swift.conf配置文件 1curl -o /etc/swift/swift.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/swift.conf-sample 8）编辑/etc/swift/swift.conf 文件并完成以下操作 123456789#在该[swift-hash]部分中，为您的环境配置哈希路径前缀和后缀。[swift-hash]swift_hash_path_suffix = PASSWORDswift_hash_path_prefix = PASSWORD #&lt;===此处PASSWORD可以换成其他唯一值#在该[storage-policy:0]部分中，配置默认存储策略：[storage-policy:0]name = Policy-0default = yes 9） 将swift.conf文件复制到/etc/swift每个存储节点上的目录以及运行代理服务的任何其他节点。 10）在所有节点上（控制节点和存储节点），确保配置目录的正确权限： 1chown -R root:swift /etc/swift 11）在控制器节点和运行代理服务的任何其他节点上，启动对象存储代理服务（包括其依赖项）并将其配置为在系统引导时启动： 1systemctl enable openstack-swift-proxy.service memcached.service &amp;&amp; systemctl start openstack-swift-proxy.service memcached.service 12）在存储节点上，启动对象存储服务并将其配置为在系统引导时启动： 1234567891011121314151617systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.servicesystemctl start openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.servicesystemctl status openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.servicesystemctl enable openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.servicesystemctl start openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.servicesystemctl status openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.servicesystemctl enable openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.servicesystemctl start openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.servicesystemctl status openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.service glance后端配置为swift1）修改glance-api.conf的[glance_store]部分如下： 2）重启glancea服务 1systemctl restart openstack-glance-* 3）验证： 12# 上传镜像cirrosopenstack image create --public --disk-format qcow2 --container-format bare --file cirros-0.4.0-x86_64-disk.img cirros_swift # 查看swift的容器，如下： # 查看glance-img容器下对象数据 # 与新创建的cirros_img的id进行对比 # 在Horizon中验证 配置cinder-backup使用swift作为后端1）安装cinder-backup组件 1yum install openstack-cinder -y 2）编辑/etc/cinder/cinder.conf文件，完成如下配置：（注释掉的是NFS的backup后端） 3）重启cinder相关服务 1systemctl restart openstack-cinder-* 4）验证： # 查看当前cinder服务 # 创建一个卷swift_test_volume01 # 创建该卷的备份卷swift_test_backup_volume01 # 查看当前swift容器 # 删除源卷，从备份卷恢复 思考：1、swift中ring对应华为分布式存储FusionStorage原理中哪个算法？其作用是什么？开源ceph实现同样功能是否也采用一致性hash算法？ 2、swift中的zone对应分布式存储的哪个进程？作用是什么？ 3、一致性hash中的vNode在华为FusionStorage和中兴Ceph中分别取值为多少？其所起的作用是什么？ 4、swift中强一致性和弱一致性的区别是什么？华为FusionStorage和中兴Ceph分别是采用哪种数据一致性规则？ 5、华为FusionStorage是否源自开源Ceph？请从软件架构的角度分析，并说出理由？ 6、一致性hash与余数hash的区别是什么？为什么在对象存储和分布式存储中普遍采用一致性hash？ 7、请分别说出对象存储中account、container和object的作用和各个角色之间关系？以及数据在对象存储中的存取路径？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-块存储管理服务Cinder]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E5%9D%97%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Cinder%2F</url>
    <content type="text"><![CDATA[概述OpenStack早期版本是使用nova-volume为云平台提供持久性块存储服务器的。从Folsom版本后，就把作为Nova组成部分的nova-volume分离了出来，形成了独立的Cinder组件。Cinder本身并不直接提供块存储设备实际的管理和服务，而是在虚拟机和具体的存储设备之间引入一个抽象的“逻辑存储卷”。Cinder与Neutron类似也是通过Plugins-Agent的方式通过添加了不同厂家的DRIVE来整合多种厂家的后端存储设备，并通过提供统一的API接口的方式为云平台提供持久性的块设备存储服务，类似于Amazon的EBS（Elastic Block Storage）。Cinder服务的实现在OpenStack众多服务中，只依赖Keystone服务提供认证。可能有些人觉得Cinder提供的Volume作为云主机的云磁盘，因此Cinder与Nova也有依赖关系。其实，这是一种错误的映像，Cinder-Volume创建的“逻辑存储卷”不仅可以用于云主机的云磁盘，也可以用于其他场景，其创建卷的过程与Nova创建的云主机的状态并没有直接关联。或者换个角度来看，Nova创建的云主机也可以不用挂载Cinder创建的Volume而正常运行。 在OpenStack系统中，共有4中存储类型，分别是：临时存储、块存储、对象存储和文件系统存储，负责实现其功能服务的也不相同。下表就是OpenStack系统中，各种存储类型在访问方式、客户端、管理服务、生命周期、容量和典型应用场景方面的对比。 上面的4种存储类型总体上分为临时性存储和持久性存储两大类，除了Nova管理的临时存储外，其余都是持久性存储。所谓临时存储（Ephemeral Storage），就是指如果只部署了Nova服务，默认分配给虚拟机实例instance的系统磁盘为计算节点的本地磁盘空间，以文件形式存放在本地磁盘中，随着虚拟机实例instance终止，该存储空间也会被释放，虚拟机实例instance的数据无法持久性保存。而持久性存储（Persistent Storage），是指持久化存储设备的生命周期独立于任何其他系统资源和设备，无论虚拟机实例instance的运行状态如何，是否被终止，其上存储的数据一直可用。 在OpenStack支持的持久化存储中，块存储操作对象是磁盘或卷（Volume），将其直接挂在到主机，一般用于主机的直接存储空间和数据库应用，DAS和SAN设备都可以提供块存储功能，甚至NFS作为Cinder存储后端时，也可以当做块存储设备来使用（OpenStack的文件存储管理服务是Manila，Manila也可以使用NFS作为存储后端，这时NFS被当做文件系统存储使用，但是NFS挂载为Cinder的后端存储时，只能当做块存储使用）。总结起来而言：块存储就是通过SAN或iSCSI等存储协议将存储设备端的卷（Volume）挂载到虚拟机上并进行分区和格式化，然后挂载到本地文件系统使用的存储实现方式。 Cinder的架构在OpenStack中，块存储服务Cinder为Nova项目所实现的虚拟机实例提供了数据持久性的存储服务。此外，块存储还提供了Volumes管理的基础架构，同时还负责Volumes的快照和类型管理。从功能层面来看，Cinder以插件架构的形式为各种存储后端提供了统一API访问接口的抽象层实现，使得存储客户端可以通过统一的API访问不同的存储资源，而不用担心底层各式各样的存储驱动。Cinder提供的块存储通常以存储卷的形式挂载到虚拟机后才能使用，目前一个Volume同时只能挂载到一个虚拟机，但是不同的时刻可以挂载到不同的虚拟机，因此Cinder块存储与AWS的EBS不同，不能像EBS一样提供共享存储解决方案。除了挂载到虚拟机作为块存储使用外，用户还可以将系统镜像写入块存储并从加载有镜像的Volume启动系统（SAN BOOT）。 Cinder的逻辑架构上图，除了cinder-client外，其余均是Cinder服务的核心组件，共有4个：cinder-api、cinder-scheduler、cinder-volume和cinder-backup。而cinder-client其实就是封装了Cinder提供的REST接口的CLI工具。 cinder-api：对外提供REST API，对操作需求进行解析，对API进行路由并寻找相应的处理方法，包含卷的CRUD，快照的CRUD、备份、Volume Type的管理、卷的挂载/卸载等操作。Cinder API本质上其实是一个WSGI App，启动Cinder API服务相当于启动一个名为osapi_volume的WSGI服务去监听Client的HTTP请求。OpenStack定义了两种类型的Cinder资源，包括核心资源（Core Resoure）和扩展资源（Extension Resoure）。而核心资源及其API路由器分为V1及V2两个版本，分别放在V1和V2两个目录下，其中API路由器（目录中的router.py）负责把HTTP请求分发到其管理的核心资源中去；V1的核心资源则包括卷（Volume）、卷类型（Volume Type）、快照（Snapshot）的操作管理，比如创建和删除一个卷，或为某个卷做一个快照等；V2的核心资源增加了Qos、limit及备份的操作管理（H版本以后）。 cinder-scheduler：负责收集后端存储Backends上报的容量、能力信息。根据指定的算法完成卷到cinder-volume的映射。在Folsom版本中，cinder-scheduler服务只是实现了简单调度（Simple Scheduler）算法和随机调度（Chancer Scheduler）算法。简单调度算法就是获取活动的卷服务节点，按剩余容量从小到大排列，选择剩余容量最多的Host节点；随机调度算法就是在满足条件的节点中随机挑选出一个Host节点。在G版本后有了类似nova-scheduler的基于过滤和权重的新调度策略FilterScheduler。 cinder-volume：多节点部署，使用不同的配置文件、接入不同的backend设备，由各存储厂商插入driver代码与设备交互完成设备容量和能力信息收集、卷操作。每个存储节点都会运行一个cinder-volume服务，若干个这样的存储节点联合起来可以构成一个存储资源池。 cinder-backup：实现卷的数据备份到其他介质。目前支持的有以Ceph、Swift和IBM TSM（Tivoli Storage Manager）为后端存储的备份存储系统，其中默认的是采用Swift备份的存储系统。与Cinder-volume类似，Cinder-backup也通过Driver插件架构的形式与不同的存储备份后端交互。 cinder-db：提供存储卷、快照、备份、service等数据，支持Mysql、PG、MSSQL等SQL数据库。 Cinder服务除了上面的逻辑架构外，还有部署架构。在实际生产中，为了实现数据持久化存储的高可用功能，避免数据丢失风险，往往要对Cider服务的关键组件以及相关数据库、消息队列进行HA。基本思路如下图：Cinder服务的关键组件可以部署在一个节点也可以分布式部署在多个节点，icinder-api，cinder-scheduler和cinder-volume均采用采用active/1active模式部署。利用HAProxy实现的负载均衡器实现API请求到多个cinder-api的分发，避免单个cinder-api故障影响块存储服务提供；cinder-scheduler利用RabbitMQ实现负载均衡模式，向多个cinder-scheduler节点分发任务，同时利用RabbitMQ收取cinder-volume上报的能力信息，调度时，scheduler通过在DB中预留资源从而保证数据一致性。多个cinder-volumen同时上报同一个backend的容量和能力信息，并同时接受请求进行处理。而数据库DB往往采用主备或集群部署，实现数据库的HA。 Cinder-Scheduler的过滤和加权机制在G版本后cinder-scheduler与nova-scheduler类似，有了基于过滤和权重的新调度策略FilterScheduler，其工作原理如下图所示，首先使用一个指定的过滤器过滤后得到符合条件的cinder-volume节点，然后对列表中cinder-volume的节点计算权重并进行排序，获得最佳的一个Host节点。 上图中，scheduler负责收集后端上报的容量、能力信息，根据设定的算法完成卷到指定cinder-volume的调度。其中关键环节就是第二步根据后端的能力进行筛选，其筛选的依据主要来自三个方面： 根据Driver定期上报后端的能力和状态进行筛选； 根据管理员admin创建的卷类型进行筛选； 创建卷时，根据用户指定的卷类型进行筛选； cinder-scheduler服务提供了调度重试机制，在通过排序挑选出最优的Host节点后，将创建卷的消息发送给该节点，并由该节点的cinder-volume服务来处理。如果该节点的cinder-volume服务在处理过程中由于某些条件不满足而导致返回错误结果，cinder-scheduler将选择次优Host节点重试。最大可重试次数默认是3次，并且可以设置。 cinder-scheduler实现了多种Filter和Weighter的算法。所有的Filter实现都在cinder/scheduler/filters目录中，所有的Weighter的实现在cinder/scheduler/weighters目录中。主要的过滤器功能如下： Availability Zone Filter：在指定的Zone中选择Host。 Retry Filter：在没有重试过的Host中重新进行调度。 JsonFilter：允许应用以Json的形式写出比较复杂的查询Host的表达式。 CapabilitiesFilter：在一个由各种类型存储组合（LVM、Ceph等）的存储系统中选择某种存储类型。 CapacityFilter：容量过滤，只选择那些剩余容量大于要创建的卷的大小的Host。 Cinder-Volume的插件类型Cinder-volume服务通过插件式（Plugins）的Driver与各种后端Volume Providers进行交互，从而屏蔽了后端各种开源或商业的存储实现方式，并对外提供统一的Cinder API接口。Cinder项目在架构上与Neutron类似，均是采用可插拔的插件来兼容各个厂商的存储设备和驱动，从而使得各个存储厂商均可参与Cinder项目并提供自己的存储插件供用户使用，而用户也可以自主选择适合自己的存储产品，并只需在Cinder的配置文件中进行适当的存储Driver设置即可。在Cinder中，Cinder-volume为不同的Volume后端（Volume Providers，通常为不同厂商的存储设备）提供了统一的Driver接口，而Volume Provider只需实现这些接口即可。 从实现上来看，Cinder的插件可以分为基于软件实现的插件和基于硬件的插件。基于软件实现的插件又可以分为基于文件系统的插件（开源Gluster FS、NFS、Ceph RBD和IBM的GPFS等插件）和基于块存储的插件（LVM插件）。基于硬件实现的插件主要是各个厂商针对自己的存储设备所实现的插件，这些插件主要是基于FC（Fiber Channel）和iSCSI协议来实现的，如IBM的XIV、V7000、DS8000和System Flash等存储设备插件，以及EMC的VNX、VMAX等存储设备插件，当然还有很多如Dell、Netapp、HPE和华为等存储厂商所提供的插件。对于用户而言，部署Cinder块存储服务的首要步骤便是选择使用哪种Cinder存储后端插件，是采用完全基于开源软件实现的插件还是采用存储设备厂商提供的特定插件，需要根据用户自己的预算和特定的存储环境进行选择。通常LVM插件适用于各种厂商存储设备，但是其I/O性能相对较差，因此比较适合轻负载应用系统。而针对特定厂商存储设备的FC插件通常具有更好的I/O性能和较低的I/O延迟，但是需要采购特定厂商的存储设备，因此比较适合I/O要求较高的生产环境系统。除了基于本地LVM的Cinder插件和Vendors厂商插件外，日立（Hitachi）的Mitsuhiro团队还提出了一种集合LVM和FC的共享LVM插件。在我们的实验环境中，主要使用了LVM和NFS两种后端存储，因此这里主要介绍这两种开源的软件插件，其他插件请参考服务商提供的产品手册。而后续我们会将实验环境的后端块存储改造为ceph-rbd，至于Cinder对接Ceph实战会在OpenStack运维实践中专门介绍，这里不再赘述。 LVM插件实现基于软件实现的Cinder插件主要以开源软件为主，其中使用最为普遍的是LVM插件、NFS插件和Ceph RBD插件。下图中，在LVM插件架构中，Volumes以VG的LV形式被创建，即用户在配置Cinder块存储和创建Volumes之前，需要在存储节点上创建卷组VG，并在配置文件中指定使用此VG进行Volumes创建源。此外，计算节点与存储节点之间的存储网络通过iSCSI协议实现，运行虚拟机的计算节点作为iSCSI的发起端（iSCSI Initiator），而运行Cinder-volume的存储节点作为iSCSI的目标端（iSCSI Target），用户通过Cinder-API创建的每个Volume对应的都是存储节点中VG上的一个逻辑卷LV，当用户发起Attach（或Detach）操作时，存储节点上的Volumes通过iSCSI协议自动挂载到计算节点虚拟机上（或从计算节点虚拟机上卸载）。 在基于LVM的Cinder创建架构中，Volume操作完全由LVM来实现，此外，全部计算节点虚拟机上的I/O通过软件iSCSI协议集中传输到存储节点，因此采用LVM插件实现的Cinder块存储比较适合轻负载I/O应用场景，而且由于全部虚拟机的读写I/O通过软件iSCSI协议汇聚至存储节点，这很容易造成存储节点的I/O性能瓶颈。不过使用LVM插件的优势在于无须特定存储设备和对应插件的支持，任何存储设备都可以映射到存储节点Linux系统中，并通过LVM插件架构实现Cinder块存储服务。 NFS插件实现Cinder默认使用的存储后端驱动是LVM，而LVM是一种基于块存储实现的Cinder插件，在基于软件实现的插件中，除了LVM外，还有很多基于文件系统的插件，如Gluster FS、GPFS和NFS等插件。其中，NFS是一种开源实现的文件系统，其配置实现和使用非常简单，这里主要介绍基于NFS插件实现的Cinder块存储服务，至于NFS服务端和客户端如何配置，请参见博客《Linux开源网络文件系统NFS》。 从本质上而言，基于NFS的Cinder块存储服务并不允许虚拟机像LVM驱动一样访问块级别的存储设备。相反，NFS驱动在NFS共享目录上创建对应Volume的文件，并通过计算节点将文件映射为块设备供虚拟机使用。基于文件系统的块存储服务通常采用控制流与数据流分离的设计，即用户通过控制节点对运行Cinder-volume的存储节点进行Volume相关的控制操作，而虚拟机对存储设备的访问并不经过存储节点，而是通过计算节点后直接进入Volume Provider，这里Volume Provider就是NFS服务器。由于Cinder屏蔽了不同存储后端的差异，并对外提供统一的API访问，因此基于NFS驱动的Cinder块存储服务与基于LVM的Cinder块存储服务在Volume操作上并无差异。 Multi-Backends实现对于公有云而言，多后端存储实现的优势在于可为用户提供更灵活的块存储使用选择，而对于私有云建设而言，多后端存储的实现允许采用不同的厂商存储插件，以集成不同的存储设备或存储解决方案，从而节省存储设备上再投资成本。 在OpenStack中，Cinder项目也实现了多后端存储功能（Storage Multi-Backends），通过并行运行多个存储后端插件和驱动。实际使用时，可以通过指定存储类型的形式来指定Volume应该由哪个运行中的存储后端提供。通过Cinder的Multi-Backends功能，可以配置多个后端存储解决方案以供相同的OpenStack计算资源使用。针对配置的每一个存储后端，Cinder会启动一个对应的Cinder-volume服务（服务名称格式为“hostname@backend_name”）。理论上，可以自由组合不同的存储后端插件，如同时实现LVM后端和Ceph RBD后端，或者IBM存储后端和EMC存储后端。为了简单起见，我们在实验环境中实现LVM和NFS驱动的多后端组合，部署仍然采取控制节点加存储节点模式，存储多后端插件（LVM和NFS Plugins）由存储节点加载，并由控制节点统一管理。Cinder多后端存储的实现与单独存储后端的实现类似，不同之处在于，前者需要在存储节点的/etc/cinder/cinder.conf配置文件中同时配置多个存储后端的实现部分，并在控制节点中为每个存储后端创建对应的存储类型，在多后端场景中进行Volume操作时，需要通过指定存储类型来使用特定的存储后端。 Cinder的卷操作流程Cinder的核心功能就是对卷的管理，可以对卷、卷的类型、卷的快照、卷备份进行处理。在Cinder提供的卷有关的上述操作中，创建、扩展、删除、Attach和Detach是最核心的功能，也是块存储工作过程中最常使用的功能。其他如快照创建和基于镜像的Volume创建也是比较重要的功能。但是，我们这里主要讨论实际生产中最常使用3个流程：卷的创建、挂载和卸载。理解了这3个流程，卷的其他操作流程的理解也就水到渠成了。 创卷流程在使用Cinder提供的“逻辑磁盘卷”之前，我们首先要完成卷的创建，尤其在使用多后端存储的生产环境下，创建卷时还需要特意指定卷的类型。如下图所示，就是一个典型的生产环境下，多后端存储的创卷流程。 Step1：用户下发创卷请求，请求API中携带创建卷的相关关键字段，如{“name”:”test”,”size”:”100”,”type”:”IPSAN”}； Step2：cinder-api校验用户是否具有权限，以及做一些基本校验，quota预占等操作，并通过消息队列的cast异步返回卷信息（生成卷id）。 Step3：cinder-api将创卷消息投递到cinder-scheduler消息队列中； Step4：cinder-scheduler从自己的消息队列中订阅并消费创卷消息，根据各个cinder-volume定期上报的后端存储能力以及卷信息，选择一个主机进行创卷。由于在创卷请求中，指定了后端存储类型type为IPSAN，因此cinder-scheduler通过CapabilitiesFilter过滤器找出所有安装IPSAN插件的后端存储节点，再根据创卷请求中size值，筛选出符合要求的节点。 Step5：cinder-scheduler调度到主机后，将消息投递到相应的cinder-volume队列中，上图中表示创卷请求消息被投递到IPSAN后端的cinder-volume队列。 Step6：cinder-volume从自己的消息队列中消费创卷消息，调用driver的接口进行创卷，最后更新数据库。 Step7：cinder-volume调用相应的driver插件。 Step8：Drive插件r将发送相应的创卷命令到实际的存储设备。 挂载卷的流程挂载卷/卸载卷是通过Nova和Cinder的配合最终将远端的卷连接到虚拟机所在的Host节点上，并最终通过虚拟机管理程序映射到内部的虚拟机中。挂载卷的流程如下： Step1：客户端发起虚拟机实例instance挂载卷的API请求到nova-api，nova-api发起异步调用cast，将挂卷请求通过attach_volume()函数发给nova-compute，nova-compute通过volume_bdm()函数向nova-api返回块设备的映射信息，并通过reserver_block_device_name()函数保存块设备的基础信息，然后通过REST API向cinder-api发起获取卷信息请求，利用get()函数传递主机的信息，如hostname, iSCSI initiator name, FC WWPNs。 Step2：Cinder API根据传递的卷信息，利用db_volume_get()函数去cinder-db中查询该卷归属的volume详细信息，将请求发给cinder-volume。 Step3：Cinder Volume通过创建卷时保存的host信息找到对应的Cinder Driver。 Step4：Cinder Driver通知存储允许该主机访问该卷。 Step5：Cinder-Driver返回该存储的连接信息（如iSCSI iqn，portal，FC Target WWPN，NFS path等）到nova-compute。 Step6：Nova-compute调用check_volume()函数针对于不同存储类型进行主机识别磁盘的代码（ Cinder 提供了brick模块用于参考）实现识别磁盘或者文件设备，成功后调用reserve_volume()函数向cinder-api发起更新卷挂载等预占操作信息，cinder-api完成数据库卷的状态更新。 Step7：Nova-compute向cinder-api通知进行卷的挂载，此操作分两步进行。首先，nova-compute发起卷的初始化连接信息，cinder-api向cinder-volume发起异步调用cast的初始化连接信息，cinder-volume收到请求后向cinder-driver发起初始化连接信息，完成后，cinder-driver返回初始化连接成功响应，并携带相关卷的iqn等信息给nova-compute。 Step8：然后，nova-compute将卷的及主机的设备信息传递给hypervisor来实现虚拟机挂载磁盘，成功后nova-compute通知cinder-api完成卷的数据库状态更新。 卸载卷的流程卸载卷的流程与挂在卷类似，区别如下： 1）在nova-compute向cinder-api发起begin_detach_volume()请求后，nova-compute首先向hypervisor发起disconnect_volume()的请求，待hypervisor完成卷的连接删除后，nova-compute向cinder-api发起disconnect_volume()请求。 2）cinder-api将disconnect_volume()请求发给cinder-volume，cinder-volume根据卷的信息找到cinder-driver完成卷的连接删除，cinder-driver删除成功后返回none信息给nova-compute。 3）nova-compute收到cinder返回的卸载卷响应None后，nova-compute向cinder-api发起detach()请求，完成卷在cinder-db中的状态更新。 Cinder的操作实战由于我们的实验环境采用LVM和NFS多后端方式，因此在创建卷前需要首先创建存储类型。 步骤1：输入以下命令，创建lvm和nfs的存储类型 12cinder type-create lvmcinder type-create nfs 步骤2：输入以下命令，将存储类型绑定到对应的存储后端 12cinder type-key lvm set volume_backend_name=kklvm01cinder type-key nfs set volume_backend_name=kknfs01 步骤3：输入以下命令，在lvm存储后端创建大小为2GB，名称为lvm-volume1的Volume 1cinder create --volume-type lvm --name lvm-volume1 2 步骤4：输入以下命令，在nfs存储后端创建大小为2GB，名称为nfs-volume1的Volume 1cinder create --volume-type nfs --name nfs-volume1 2 步骤5：输入以下命令，查看当前系统的卷列表信息，并在Horizon中查看 1cinder list 步骤6：在NFS后端的挂载目录下，我们可以看见有一个volume-前缀的文件，后面跟的就是Volume的ID 步骤7：执行以下命令，在NFS后端创建一个启动卷boot-nfs-vol-cirros，大小1G，可用分区nova，卷类型nfs，卷来源Cirros 1cinder create --volume-type nfs --name boot-nfs-vol-cirros --image Cirros --availability-zone nova 1 步骤8：执行以下命令，利用启动卷boot-nfs-vol-cirros拉起一个虚拟机实例nfs-boot-instance01 1openstack server create --flavor Flavor_Web --volume 3c5432ca-9969-405d-9dad-d416660945f2 nfs-boot-instance01 步骤9：执行以下命令，将lvm-volume1卷挂载到虚拟机实例nfs-boot-instance01上，作为一个数据盘/dev/vdb 1openstack server add volume nfs-boot-instance01 lvm-volume1 步骤10：执行以下命令，将lvm-volume1卷从虚拟机实例nfs-boot-instance01上卸载掉 1openstack server remove volume nfs-boot-instance01 lvm-volume1 步骤11：执行以下命令，创建卷lvm-volume1的快照snap-lvm-volume1 1cinder snapshot-create --name snap-lvm-volume1 lvm-volume1 步骤12：执行以下命令，将卷lvm-volume1的扩容到4G 1cinder extend lvm-volume1 4 思考：1、OpenStack中有哪几种类型的存储？各自的管理服务和应用场景是什么？ 2、块存储服务Cinder主要包含哪些组件？各组件的功能是什么？从创建卷的场景，通过时序图描述各组件的交互过程？ 3、在高可用场景下，cinder-scheduler的消息分发依靠什么组件完成负载均衡功能？cinder-scheduler获取存储节点容量等信息，依靠哪种机制获取？（PUSH或PUT） 4、块存储服务Cinder实现多后端存储时，首先需要完成什么配置才能使用多后端存储（从LVM和NFS两种类型的后端存储进行阐述），在使用多后端时，必须依赖什么来创建卷？ 5、已挂载的卷是否支持创建快照，若支持，应该使用什么命令？ 6、要想从卷发放虚拟机实例，则卷必须设置哪种卷属性？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-网络管理服务Neutron]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Neutron%2F</url>
    <content type="text"><![CDATA[概述Neutron便是OpenStack云计算中的网络服务项目，其源自早期的Nova-network网络组件。Nova-network从Nova项目中独立出来之后，社区成立了针对网络功能虚拟化的Quantum项目。但由于商标侵权的原因，在Havana版本后，Quantum项目更名为现在的Neutron项目。Neutron是OpenStack的核心项目之一，虽然是OpenStack核心项目中成熟相对较晚的项目，但是Neutron项目在社区的热度很高。直到目前，在最新发行的OpenStack版本中，Neutron是新增功能和问题修复最多的核心项目。OpenStack的网络服务由Neutron项目提供，Neutron允许OpenStack用户创建和管理网络对象，如网络、子网、端口和路由等，而这些网络对象正是其他OpenStack服务正常运行所需的网络资源。Neutron项目中实现的各种插件使得用户可以选择不同的网络设备和软件，并为OpenStack的网络架构和部署提供极大的灵活性。此外，Neutron提供了API Server以供用户进行云计算网络的定义和配置，而Neutron灵活多样的插件使得用户可以借助各种网络技术来增强自己的云计算网络能力。Neutron还提供了用以配置、管理各种网络服务的API，如L3转发、NAT、负载均衡、防火墙和VPN等高级网络服务。因此，我们大致可以画出Neutron在OpenStack上下文架构（增加个Heat云编排服务），如下图所示，Neutron已经成为OpenStack三大核心（计算、存储、网络）之一，对外提供Naas（Network as a Service）服务。 目前为止，Neutron支持的特性如上表所示，Neutron支持的这些特性，涵盖了2~7层的各种服务。除了基本的、必须支持的二层、三层服务外，4~7层支持的服务有：LBaaS（负载均衡服务）、FWaaS（防火墙服务）、VPNaaS（VPN服务）、Metering（网络计量服务）、DNSaaS（DNS服务）等。Neutron在大规模高性能层面，还支持L2POP、BGP、DVR、VRRP等特性。Neutron项目与OpenStack其他项目（Cinder除外）不同的地方在于，其基本全是通过Plugin-Agent方式实现各类网络技术的软件化呈现，虽然不如Nova项目复杂，但确是OpenStack中最灵活、最难理解的一个项目，尤其是针对没有网络基础新手，可以说直接让您懵逼也不为过。因此，要学好Nuetron的除了重点理解各类资源概念外，还需要宏观的去理解Neutron的不同表现形成。 Neutron的应用方式Neutron的应用本质上分为两大类：基于OpenStack的应用和基于SDN的应用。前者是在云的场景下，与OpenStack其他部件配合，为用户提供基于云的服务，此时Neutron只需要提供租户网络隔离和二层转发功能，使用户的云服务器能够进行网络互联互通。后者是在SDN场景下，作为SDN Controller或与SDN Controller（如ODL、OVN)一起，为用户提供NaaS服务。 基于OpenStack的应用—从Provider Network Service到Self-Network Service基于OpenStack的应用，就是原生的云应用，Neutron作为OpenStack中的网络实现部件，为用户提供网络服务，其本质上是一种Provider Network Service类型。在Provider网络中，由于二层网络直接接入物理设备，二层网络之间的通信也由物理设备进行转发，因此Provider网络在实现过程中无须L3服务，控制节点只需部署API Server、ML2核心插件及DHCP和Linux bridging代理即可。计算节点中的实例通过虚拟二层交换机直接接入物理网络，因此计算节点只需部署如Open vSwitch或Linux bridging等代理软件即可。先来看一个简单的例子，如下图左边所示，是一个单平面的租户共享网络，共享网络VLAN/FLAT就是Neutron网络实现控制方式。 上图左边的含义为：租户可以创建许多虚机，但是这些虚机只能属于一个Network。不同租户共享这同一个网络，而且这个网络只能是VLAN网络或FLAT网络，网络类型必须单一实现。同时，在这个架构中，是通过外部物理网络的路由器实现网络包的转发功能，Neutron并不提供虚拟路由转发功能。而且因为各租户属于同一网络，也不支持IP地址重叠功能。其实，这个网络实现对应物理网络实现就是如上图右边所示，由一个交换机+一个路由器来完成。 如果网络都这么简单该多好，各种标准组织、厂家等没必要为各自的网络实现方式满世界BB，维护和开发人员也没必要花大力气去学习网络，整个世界都清净了。但是世界是复杂的，网络也是复杂的。就算只是和OpenStack其他服务为用户云服务，Neutron的简单网络实现也不是上面那么简单，而是下面这个样子： 租户A独占一个VLAN网络，地址段为10.0.0.0/24，同时租户A的虚机要求能够外部网络互联互通（包括和租户B的虚机互联互通）。 租户B占用2个隧道网络，一个VxLAN网络，地址段也为10.0.0.0/24，另一个是GRE网络，地址段为10.0.1.0/24。而且，租户B这两个网络的虚机也要求和外部网络互联互通（包括和租户A网络内的虚机互联互通）。 租户A独占一个虚拟路由器，租户B独占一个虚拟路由器，且租户A网络地址段和租户B私有网络（1）地址段重叠，也就是要实现网络资源隔离功能。 租户A和租户B与外部网络互联互通时，要支持SNAT/DNAT功能。 可以看出，即使是基于OpenStack的云服务应用，在多租户场景下，Neutron也从最初始低端交换机提供角色发展成为支持各种协议、融合交换路由转发一体的，支持多租户隔离的网络实现角色，即Self-Network Service。上面的模型对应到物理网络实现就是如下的架构： 因此，Neutron基于OpenStack的应用，网络的实现一般都是Host内的虚拟交换机、子网和端口模型，3层-7层的路由等功能可以通过物理设备实现也可通过虚拟设备实现，取决于不同网络部署类型（Provider or Self）。 基于SDN的应用不同的人心中，就有不同SDN实现方式。因此，这也是目前SDN亟待完成标准制定和冻结的需求所在。即使是业内公认的北向采用RESTful协议，大家都号称自己支持并采用的就是RESTful方式，但是对接起来兼容性仍然感人。既然要谈SDN的应用，我们首先看下什么是SDN？这里我们采用设备商和运营商共同认可的方式来介绍。通过下图的左边，大家可以对SDN有个初步感性认识。 SDN主要将网络的控制平面与数据转发平面进行分离，采用集中控制替代现有的分布式控制，并通过开放的可编程的接口实现“软件定义”的网络架构。SDN是IT化的网络，是“软件主导一切”的趋势从IT产业向网络领域延伸的标志性技术，其核心就是网络的“软化”。SDN的标准架构就是俗称的“三层两接口”，其实这种架构并不是SDN所独有，在现有网络中如VoLTE网络的核心网IMS也是一个标准的“三层两接口”架构，其目的就是实现转控分离。别忘了，IMS可是转发、控制和业务三层完全分离的一个架构体系。SDN的核心特点是将实体设备作为基础资源，抽象出NOS（网络操作系统），隐藏底层物理细节并向上层提供统一的管理和编程接口。以NOS为平台，开发的应用程序可以实现通过软件定义的网络拓扑、资源分配和处理流程及机制等。SDN架构的特点如下： 分离网络的控制功能和转发功能。 使得网络变得可编程控制。 底层的设备抽象成虚拟网络设备。 南向支持OpenFlow协议。 接下来，我们来看下Neutron作为SDN应用场景下逻辑架构是什么？如上图右边所示，Neutron具备SDN网络的4个特点。在Neutron基于SDN的应用场景中，所有SDN控制器都挂接在Netron之下，这不仅是因为业界期望Neutron能够成为统一的北向接口，也是源于Neutron的可扩展能力。我们再来看看Neutron内部组件实现方式是什么？先来放一张较复杂图，大家不要被吓着，重点看我图中标注出来的（1）、（2）、（3）三个地方，这些标注出来的地方就是Neutron的可扩展能力，其余部分会在后文中逐步介绍。 支持Vendor Plugins（设备商/供应商插件）扩展。这是对Core API（Network、Subnet和Port三个核心资源的业务API)的扩展。原生的Neutron，实现Core API的载体是ML2插件，这是一种Driver方式，实现多种协议的协同，支持网络多种方式实现和共存。因此，作为设备商不会去扩展Core API，因为Core API正是运营商所追求的统一API接口模式。但是，Neutron原生的ML2管理的是虚拟交换机，如果需要管理设备商自己的交换机，则需要设备商自己扩展。因此，Neutron在设计时，就保留了Vendor Plugins的位置，便于不同设备商开发自己的插件并实现对接自己的SDN控制器。 支持Service Plugins扩展。与第(1)点一样，不过它所对应的其他业务API（Neutron称之为Extension API，如上图中标注位置（3）点）。它的功能是：如果设备商觉得Neutron原生的Service Plugins不合适或者功能缺失，设备商可以扩展自己的的插件，然后对接自己的SDN控制器。比如与ODL对接时，此处的LBaaS，FWaaS、VPNaaS和L3 Service均需要替换。 支持Extension API扩展。这一点可以说将Neutron的可扩展能力放大到极致。所谓挂一漏万、百密一疏，Neutron也不能保证自己的原生定义的业务API就能100%满足业界需求，所以Neutron也允许其他组织（包括运营商、设备商）对它的业务API进行扩展，并同时在标注点（2）完成对应插件的扩展，这样就能满足各种业务需求。 上面列举的3个可扩展点只是为了说明Neutron具备的优势，并不是它的全部，它的全部包括但不限于上图中列举出所有功能。也就是说，厂商或运营商可以根据自己的需要对Neutron进行扩展，如果不扩展，Neutron仍然可以提供完备的网络实现方案。为了和SDN网络的“三层两接口”架构进行对应，让大家更好的理解“Neutron就是SDN”这个说法，我们再将上面的进行抽象，就得出下面SDN应用场景典型逻辑架构图。 Neutron对外提供的CLI和Web UI接口，其用户主要是“人”；对外提供的REST ful API接口，其用户主要是OpenStack之内的其他服务（如Nova等）以及OpenStack之外的其他系统（如MANO等）。Neutron管理的网元，主要以“软”网元为主（即虚拟网络功能）。这些“软”网元，有三种来源： Linux原生（内核模块）的网络功能，比如LinuxBridge，LinuxRouter等。 开源的网络功能，比如OVS等。 厂商提供的闭源商用产品。 Neutron管理的网元，也可能涉及一些“硬”网元，比如采用隧道网络进行主机之间互通时，往往各个机柜中TOR交换机（如5300\9300等）就是隧道网络的VTEP（隧道终结点），此时这些TOR交换机也在Neutron的管理范围内。再比如数据内部的DCGW，Firewall、LB这些3-7层的硬件网络设备，也是Nuetron的管理对象。这就是Neutron作为SDN应用时承担的管理角色。 Neutron的逻辑架构Neutron服务的架构和OpenStack其他服务一样，是一种分布式架构，各子服务服务通过消息队列来完成异步通信，从而对外提供网络服务。如下图所示，主要包括neutron-server、neutron-agent、neutron-metadata、消息队列MQ几部分。 Neutron Server：包含neutron-api和neutron-plugin两部分。neutron-api对外提供统一的REST ful API接口，接收和响应外部其他用户API请求。neutron-plugin接收来自neutron-api的请求，建立并维护网络的逻辑状态，同时调用响应agent来实现各种网络服务，包括core-plugin和service-plugin两部分。 Nuetron-agent：处理 Plugin 的请求，负责在 network provider （linuxbridgde、openvswitch、物理网络交换机等）上真正实现各种网络功能。核心的neutron-agent包括：neutron-ml2-agent（实现2层网络的功能，比如通过虚拟交换机linuxbridge或openstackvswitch实现2层转发功能）、neutron-dhcp-agent（实现DHCP服务）、neutron-L3-agent（实现3层路由转发功能）。 Neutron-metadata：上面图中未画出，在后面节点服务分布图中相应位置。用来和nova-metadata服务通信，获取虚机的网络元数据。因为VM在启动时需要访问 nova-metadata-api 服务获取 metadata 和 userdata，这些 data 是该VM的定制化信息，比如 hostname, ip， public key 等。但VM 启动时并没有 ip，那如何通过网络访问到 nova-metadata-api 服务呢？答案就是 neutron-metadata-agent 让VM 能够通过 dhcp-agent 或者 l3-agent 与 nova-metadata-api 通信。 Neutron-ML2-agent：响应来自ML2-plugin的调用，实现2层网络功能。由于ML2-plugin属于Core-plugin，用来控制实现Network、Subnet、Port等核心网络功能，因此ML2-agent负责完成这些核心网络功能的具体实现，概括起来有两种实现方式，主要基于软件虚拟的linuxbridge（官方推荐）或OVS（生产环境普遍采用）和基于专有硬件的物理交换机VLAN网络或VxLAN网络。 Neutron-L3-agent：L3 Agent在Provider类型的网络中并不是必须的，而在Self-Service网络中却是必须的。L3 Agent服务主要在外网访问租户网络中的虚拟机时提供L3 Route功能和DNAT功能，其也需要访问消息队列，其路由功能默认通过 IPtables 实现。 Neutron-dhcp-agent：DHCP Agent服务（neutron-dhcp-agent）主要为租户网络提供DHCP服务，DHCP Agent对于Neutron支持的全部网络Plugins都是相同的，即DHCP代理并不依赖用户选用的Plugin。DHCP Agent服务也需要访问消息队列以便同网络Plugins进行交互。 AMQP-Server：Neutron Server，Plugin 和 Agent 之间通过 Messaging Queue 通信和调用。 Neutron-DATABASE：存放 OpenStack 的网络状态信息，包括 Network, Subnet, Port, Router 等。 在Neutron中，除了上述介绍的DHCP Agent和L3 Agent，还有很多提供高级服务的插件代理，如VPNaaS代理、FWaaS代理和LBaaS代理等，而这些代理通过基于消息队列的RPC与Neutron的API Server、Plugins进行交互。Neutron之所以能够处理各种网络技术和协议，主要因为Neutron包含了实现各种网络技术的插件。从代码层面而言，插件是“可插拔（Pluggable）”的Python类和函数，这些Python类在Neutron API响应请求时被触发，同时插件在Neutron Server服务启动时加载，加载完成后插件被当成Neutron Server的一部分而运行。在Neutron中，API是Pluggable的，因此用户可以实现自己的Plugin并将其“插入”Neutron API中使用，通常不同插件实现的Neutron API是不同的。用户对插件的实现可以是完整性（Monolithic）的也可以是模块式（Moduler）的。Monolithic意味着用户需要实现对网络进行直接或间接控制的全部核心技术，由于其实现过程较为复杂，Monolithic形式的插件正被逐步淘汰，取而代之的是Moduler插件，其中最为成功的便是Moduler Layer2，即ML2插件。ML2插件解耦了对不同网络驱动的调用，它将驱动分为两个部分，即TyperDriver和MechanismDriver。在Neutron网络中，TyperDriver代表不同的网络隔离类型（Segmentation Type），如Flat、Local、VLAN、GRE和VxLAN，TyperDriver负责维护特定类型网络所需的状态信息、执行Provider网络验证和租户网络分配等工作，而MechanismDriver主要负责提取TyperDriver所建立的信息并确保将其正确应用到用户启用的特定网络机制（Networking Mechanism）中。尽管ML2插件是个单一整体的插件，但是ML2插件支持多种TyperDriver和MechanismDriver，并且不同的MechanismDriver所支持的TyperDriver种类不一样，如LinuxBridge和OpevSwitch两种MechanismDriver都支持Flat、VLAN、VXLAN和GRE这4种TyperDriver，而L2 population仅支持VXLAN和GRE这2种TyperDriver，这意味着在启动L2 population时，用户不能使用Flat和VLAN进行租户网络的隔离。ML2插件各类型MechanismDriver与TypeDriver的矩阵管理如下： 在Neutron中，Neutron Server服务扮演的是网络控制中心的角色，而实际上与网络相关的命令和配置主要在网络节点和各个计算节点上执行，位于这些节点上的Agents便是与网络相关命令的实际执行者，而Agents可以划分为L2 Agents与非L2 Agents两大类，如OpenvSwitch agent、LinuxBridge Agent、SRIOV nic switch Agent和Hyper-V Agent等属于L2 Agents。L2 Agents主要负责处理OpenStack中的二层网络通信，是Neutron网络中最为核心的部分。非L2 Agents主要包括L3 Agent、DHCP Agent和Metadata Agent等。Agents所获取的消息或指令来自消息队列总线，并且由Neutron Server或Plugins发出。由于Agents负责网络的具体实现，因此Agents与特定的网络技术、Plugins密切相关，如使用OpenvSwitch Agent则意味着通过OpenvSwitch技术来实现虚拟网络，并且其对应的插件是OpenvSwitch Plugin（被ML2插件取代）。大致上Neutron实现虚拟网络的实现过程就是：Neutron Server接收到客户端的API请求后，触发ML2插件进行请求处理，这里假设ML2插件已经加载了OVS Mechanism Driver（即OpenvSwitch插件），于是ML2插件将请求转发给OVS驱动，OVS驱动收到请求后使用其中的可用信息创建RPC消息，RPC消息以Cast形式投递到计算节点上特定的OVS Agent，计算节点上的OVS Agent接收到消息后便开始配置本地OpenvSwitch交换机实例。 这里有一个业界常使用的概念—参考实现，即Mechanism Driver与L2 Agent的一种组合，如Open vSwitch这个Mechanism Driver与Open vSwitch Agent这个代理的组合被称为OpenvSwitch参考实现，用Open vSwitch&amp;Open vSwitch Agent表示。不同的参考实现支持不同的非L2 Agent，如参考实现MacVTap&amp;MacVTap Agent不支持L3 Agent和DHCP Agent，这意味着如果用户在ML2插件配置时选择了使用MacVTap这个Mechanism Driver，并在计算节点/网络节点部署了MacVTap Agent，则该Neutron网络中不能使用L3服务和DHCP服务。不同参考实现支持的非L2 Agent的对应关系如下表所示。 在Neutron中，L3 Service是个非常关键的服务，其主要提供虚拟机不同L2子网之间的L3路由，以及虚拟机与外网之间的SNAT和基于Floating IP的DNAT功能，如果OpenStack网络中未部署L3服务，则用户只能部署基于Provider的网络，而无法实现真正的云计算Self-Service网络。L3 Service主要由L3 Service插件及其API和对应的L3 Agent组成，从OpenStack的Juno版本开始，L3 Agent也可以部署在计算节点上以实现分布式虚拟路由（Distributed Vritual Router，DVR）功能。通常情况下，租户可以通过L3 Plugin提供的API创建虚拟Router和Floating IP，并通过Nova的API将Floating IP绑定到实例上，从而实现租户内网与外网的通信。 Neutron各组件在各节点的部署方案一般有两种：网络节点和控制节点合一部署，网络节点单独部署。在我们的实验环境采用的是前者，从逻辑功能上划分，其实两者都一样，Nuetron各组件服务在不同角色的节点上实际部署如下图所示。 控制节点：部署 neutron server 、消息队列MQ、数据库NuetronDB等服务。其作用就是接收网络实现请求，建立网络逻辑模型，保存网络逻辑模型到NuetronDB，然后通过消息队列MQ调度网络节点和计算节点的各种agent完成网络的具体实现。简易流程如下： 计算节点：部署 core plugin 的agent，调用network provider来完成二层网络的具体实现。agent一直监听Nuetron MQ的消息，一旦发现有来自Plugin的网络建立请求，根据请求中网络实现需求，调用本节点的Network provider来实现网络功能。比如在我们的例子中采用linuxbridge来实现网络，那么计算节点的linuxbridge-agent就会调用本节点linux OS原生的linuxbridge来建立虚拟二层交换机。简易流程如下： 网络节点：部署的服务包括：ML2- agent 和 service plugin 的 agent。其中，ML2-agent用来在网络节点完成具体二层网络实现，用来确保各计算节点虚机VM的数据流量通过网络节点的ML2-agent来互通，即网络节点为整个网络中数据流量汇聚点。而各种Service plugin的agent用来实现各种3层服务。在我们的例子中用到了DHCP三层服务，具体由DHCP-agent调用linux操作系统的dnsmasq来具体实现DHCP服务，同时默认实现DNS三层服务，其简易流程如下： 通过上面对各节点上Neutron各组家部署方案的了解，我们应该抽象并总结出Neutron在OpenStack的各个功能节点中所处的角色和功能：控制节点上的Neutron-Server并不实现具体的网络功能，它只是对各种虚拟网元和实体网元做管理配合的工作。控制节点的Neutron-Server进程（子服务）接收REST ful API外部请求，调用内部的Plugins通过RPC与agent进行交互，Neutron-server内的Puligins进程与各个agent进行共同完成网络的管理任务；计算节点上的neutron-ml2-agent通过调用本节点操作系统原生linuxbridge或OVS完成各种2层网络的具体实现，并完成虚机的虚拟网卡绑定。即用户创建的租户网络请求信息到达计算节点后，ml2-agent通过各种Bridge来实现2层虚拟交换机，或调用物理交换机来实现2层功能，同时将Port通过tap设备绑定到VM上，提供VM的MAC和IP，即实现虚拟网卡功能；网络节点上的neutron-ml2-agent建立各种bridge（OVS也是bridge）实现一个二层网络的汇聚转发网桥，各计算节点中的虚拟机互通或访问外部网络时，数据包首先发到这个二层汇聚转发网桥上，同时这个网桥会连接到物理网络中的一个设备（如交换机、路由器），通过这个设备再到达物理网络的网关，其上的DHCP-agent，采用dnsmasq进程提供DNS、DHCP和TFTP服务，一个网络对应一个DHCP，不同网络的DHCP通过namespace实现隔离。L3-agent，利用linux操作系统内核实现虚拟router功能，同时还提供SNAT/DNAT服务。 Neutron的资源模型在OpenStack的官网https://developer.openstack.org/api-ref/network/v2/，对其公开的REST ful API做了相关说明。这些RESTful API背后就是Neutron的网络资源模型。Neutron把它管理的对象统统称为资源，比如Network、Subnet、Port等，表面看与传统网络中概念一样，但是由于Neutron管理的范围（DC内）和对象的特点（Host内部虚机VM）等原因，与传统网络的概念并不完全相同，甚至有些令人困惑。Neutron管理的核心资源包括：Network、Subent、Port和Router。 Network：是一个隔离的二层广播域。Neutron支持多种类型的network，包括local，flat，VLAN，VxLAN和GRE。其中，VxLAN和GRE属于隧道网络，local、vlan和flat属于非隧道网络。从全局看，Network是Neutron资源模型的“根”，Subnet隶属于它，Port也要隶属于它。在Network中引申出两个重要概念：运营商网络(provider network)和物理网络（provider:physical_network）。运营商网络（provider network）是相对于租户（project）创建的网络，即租户网络而言。租户创建的租户网络属于内网网络，和外部网络之间是隔离的，如果该租户创建的虚机VM有访问外部网络的需求，此时neutron就必须通过一个网络来映射这个外部网络，实现这个功能的就是运营商网络（provider network），即运营商网络就是运营商的某个物理网络在OpenStack的延伸。而物理网络（provider：physical_network)主要指为了满足租户（project）创建的虚机VM访问外网的需求（这里的外网不一定是internet，也可能是企业内部其他物理网络），需要创建一个运营商网络（provider network）来对外网进行映射，而这个映射是如何实现的呢？答案就是通过物理主机Host的物理网卡来完成，通过这物理网卡实现运营商网络和外部网络的对接。这个物理网卡就是neutron管理范围内的物理网络（如果Neutron作为SDN控制器，该物理网络好包括DC内部的物理数通网络设备)，这个概念在创建运营商网络类型为VLAN和FLAT是尤其明显，因为创建这两种网络类型时，其配置文件中明确要求对应物理网卡名称。但是当创建的运营商网络为隧道网络时，却不需要指定物理网卡。这又是为什么呢？答案就要从隧道网络的报文说起，如下图所示，隧道性网络离开主机的报文，外面有一层隧道Header的，这个Header包括隧道的源IP和目的IP。只要有了目的IP，主机Host的IP协议栈就会找到合适的网卡将报文转发出去。 因此，当运营商网络类型为VxLAN或GRE等隧道网络时，是通过对应物理网卡的IP找到物理网卡转发报文，本质上还是需要占用一个物理网卡，只是配置上无需指定物理网卡名称，只需完成隧道网络对应VTEP IP配置即可。这里面还隐含一个概念，就是隧道网络的物理网卡可以复用，即当我们的创建多个隧道网络类型的运营商网络时，可以通过同一个VTEP IP对应的物理网卡转发数据包。那么VLAN网络类型和FLAT网络类型运营商网络指定了物理网卡名称，是否也可以复用？答案是，VLAN网络可以通过在物理网卡上起子接口来实现复用，但是主机Host的物理网卡要配置成Trunk模式。这个不难理解，和实际物理网络交换机的实现方式一致。但FLAT网络一个网络必须独占一个物理网卡，这也是flat网络在实际生产中应用不多的原因，因为成本太大。 Subnet：是一个IPv4或者IPv6地址段，其数据模型只包含两个字段cidr和ip_version。虚拟机VM的IP从subnet中分配。每个subnet需要定义IP地址的范围和掩码。network与subnet是1对多关系，即一个subnet只能属于某 network，一个network可以有多个subnet，这些subnet可以是不同的IP段，但不能重叠。表面看，Subnet只是代表纯逻辑资源，是一批IP地址的集合，但实际上每个IP背后都代表一个实体，如VM。那么就会产生以下两个问题：虚机VM的IP地址如何分配？虚机VM的DNS是什么？因此，Subnet除了cidr和ip_version等纯逻辑资源外，还隐含了IP核心网络服务的内容，即俗称的DDI服务（DHCP\DNS\IPAM）。当创建网络时，勾选启用DHCP服务，其对应配置文件中的enabel_dhcp=true，此时Neutron会自动创建一个DHCP Server，该DHCP Server被操作系统OS创建在namespace中，通过veth pair与Bridge网桥相连。DHCP可以配置为一个IP地址池，如果没配置，就默认采用subnet的cidr字段作为标准地址池。当创建网络时，勾选启用DHCP服务，同时也默认启用DNS服务，Neutron会根据虚机的主机名和绑定Port信息完成主机名和IP地址解析功能。有了DNS、有了DHCP，这个还不够。实际的组网中，一般还有一个IPAM（IP Address Management，IP地址管理）功能，数据模型中与其相关的字段是Subnetpool_id，其与DHCP数据模型的allocation_pools重复，也就意味着IPAM就是从subnet设定的地址池中管理虚机VM的IP地址分配。 Port：可以看做虚拟交换机上的一个端口。port上定义了MAC地址和IP地址，当VM的虚拟网卡VIF（Virtual Interface） 绑定到port时，port会将MAC和IP分配给VIF。其实这个port其实就是linux操作系统的一个tap设备。而tap又是什么呢？它其实是Linux原生的虚拟网络设备，在linux中所指的“设备”并不是我们实际生产或生活中常见路由器或交换机这类设备，它其实本质上往往是一个数据结构、内核模块或设备驱动这样含义。在linux中，tap和tun往往是会被并列讨论，tap位于二层，tun位于三层，详见博客《Linux原生网络虚拟化实践》一文。 Router：如果说Port是Neutron资源模型的“灵魂”，那么Router就是Neutron资源模型的“发动机“，它承担着路由转发功能。Router的资源模型可以简单抽象为三部分：端口、路由表、路由协议处理单元。如下图所示： Router并没有使用某个字段表示端口，而是提供2个API以增加/删除端口，如下所示，非常好理解。 Add interface to router /v2.0/routers/{router_id}/add_router_interface —-增加端口 Remove interface from router /v2.0/routers/{router_id}/remove_router_interface —-删除端口 理论上，Router只要有了路由表以及对应的端口信息就可以进行路由转发，但是对于外部网络（Neutron管理范围之外的网络）路由转发，尤其公网internet，Router还用了一个特殊字段external_gateway_info（外部网关信息）来表示。这又是什么意思？我们通过一个例子来理解，如下图所示： 位于Nuetron管理网络的内部虚机VM，IP地址为10.10.10.10，它要访问位于公网（外部网络）的网站www.openstack .org，IP地址为104.20.110.33，需要经过公网的路由器RouterB才能到达。RouterB的Port2直接与Neutron网络节点的RouterA的Port1相连（中间经过Bridge相连）。这个RouterB就是真正意义上的外部网关，RouterB的接口Port2的IP地址120.192.0.1就是Neutron网络的外部网关IP。但是RouterB根本不在Neutron的管理范围内（RouterA才属于），而且Neutron也不需要管理它。从路由转发的角度讲，它只需要在RouterA中建一个路由表项即可，如下： 不过从RouterA的角度来看，不仅仅是增加一个路由表项那么简单（后面网络云SDN实现源码讲解时会详细阐述）。于是，Neutron提出了external_gateway_info这个模型，它由network_id，enable_snat，external_fixed_ips等几个字段组成。对应上图，其数据结构如下： 12345678910“external_gateway_info":&#123; "enable_snat": true， “external_fixed_ips": [ &#123; "ip_address": "120.192.0.6" "subnet_id": "b7832312223-ceb8-40ad-8b81---a332dd999dse" &#125;,], "network_id": "ae3405f12-aa7d-4b87-abdd-50fccaadef453"&#125; 其中，ip_address就是RouterA的Port1的地址，subnet_id中对应的subnet的gateway_ip就是RouterB中的Port2地址。所以，external_gateway_info其实隐含了Neutron的管理理念： Neutron只能管理自己的网络。 Neutron不需要管理外部网络，只需知道外部网络网关IP即可。而它获取外部网关IP的方式就是通过subnet_id间接获取到其gateway_ip得到。 当enable_snat=true时，SNAT真正生效的地方在RouterA的port1上。 如上面例子，当我们创建外部网关信息（external_gateway_info)的时候，Neutron会自动在Router上增加一个相应的路由表项，这个路由称为默认静态路由。同时，当我们在Router上增加一个端口时，这个端口背后的subnet的所有进出流量都要经过这个端口转发，这其实也是一种路由转发模式，称为直连路由。但是，无论是增加外部网关信息还是增加端口，其产生的路由表项均不会在Router的路由表routers中增加相应的表项。那这个routers到底有什么用？答案是，这个routers中只体现静态路由信息，它与默认路由一样，都是通往外部网络的。不过，静态路由的外部网络一般指的是私网，而默认静态路由的外部网络一般指的是公网，同时在external_gateway_info中并没有包含目的网段，也就是说除了直连路由（由链路层协议自动发现，不体现在路由表中），静态路由（体现在路由表中，需手工配置）外，到达其他所有目的地都走这条默认静态路由转发。默认静态路由、静态路由和直连路由的路由表转发逻辑如下： Neutron网络资源的隔离机制OpenStack是一个多租户的云操作系统，其中提供网络实现Neutron自然也需要提供多租户的服务。所以，多租户之间资源隔离是Neutron必须要支持的特性。在Neutron的资源模型中，老版本有一个tenant_id的字段，而从Newton版本开始，又引入了project_id，在官方文档解释中，这两个都是：The ID of the project。因此，不论新老版本，这两个字段都表示一个意思：租户资源隔离。租户隔离，顾名思义就是为了资源隔离，但其更深层次的含义是多租户资源共享！！！—-这句话怎么理解?我们慢慢往下看。 Neutron下面租户资源隔离的含义从租户的视角，或者从需求的角度来看，租户资源隔离有三种含义：管理面资源隔离、数据面资源隔离和故障面资源隔离。管理面资源隔离是指管理权限的隔离，如下图所示，两个网络Network VIN 100和Network VNI 200都是Neutron的管理范围，但是VNI 100属于租户A，VNI 200属于租户B，这就意味租户A的网络对租户B而言是不可见的，租户A也就无法管理（CRUD）租户B的网络，反之亦然。 数据面资源隔离是指数据转发的隔离。不同租户的网络不仅在管理上面隔离，而且在数据转发层面还可以“重复”。比如，租户A建立一个私网网段10.0.10.0/24，租户B同样可以建立10.0.10.0/24这个网段，这两个网段不仅不会冲突，还可以互通（通过3层转发），因此，数据面资源隔离的目的正是为了资源共享。故障面的资源隔离就是指一个租户网络出问题了，不能影响另一个租户网络，但是这是相对的。比如上面那个图，如果租户A和租户B同属于一个计算节点host1，当租户A网络的分布式虚拟路由器DVR-A出问题了，自然不会影响租户B的网络。但是，当租户A和租户B在网路节点的互通路由器router_global出问题了，那么自然两个网络都会受影响。再比如，该云系统在都在西咸数据中心A机房部署，那么这个机房的动力、线路等出问题，部署在该机房A的所有租户业务都会中断。因此，故障面的租户资源隔离，只是针对租户本身独占的资源而言，而对于多租户共享资源是无法做到也无需刻意去做的（就算是土豪动，也不能做到每个租户所有资源都独占，那属于败家！）。因此，在多租户共享层面资源只能尽量做到容错，比如高可用集群的架构。其实，不光Neutron的故障资源隔离是这样，所有OpenStack其他服务的故障隔离都是这种特点。 Neutron的租户资源隔离实现模型管理面的租户资源隔离实现模型：对于Neutron而言就是控制节点资源隔离实现。OpenStack的控制节点实现模型如下图所示，对于管理面而言，租户资源隔离一般涉及几个层面：硬件/操作系统层面、应用程序层面、数据库层面。 Neutron在这几个层面的隔离方案如下： 硬件/操作系统层面：不隔离。管理面（控制节点）都部署在一个Host，多个租户共享一个Host、一个操作系统，因此无法隔离，只能依靠集群高可用实现容灾保障。 应用程序层面：不隔离。原因同上，管理面的各个服务，多租户共享，因此无法隔离，也只能依靠集群高可用在做容灾保障。 数据库层面：轻微隔离。OpenStack的各个服务（包括Nuetron）针对多个租户，在数据库层面采用共享数据库，共享表的方式进行资源共享，只是通过表中字段(project_id）来区分不同租户。因此是一种轻微隔离方案。除了这种方案，数据库层面还可以采用独立数据库或共享数据库，独立表的方式来做隔离。 通过上面的分析，在OpenStack这种云操作系统中，为了尽量保证服务的高可用特性，针对控制节点而言，生产环境必然也必须采用高可用集群的方式部署，其目的就是保证业务的连续性。 数据面的租户资源隔离实现模型：为清楚描述，这里我们采用OVS的实现方案，LinuxBridge的实现方案由于SecurityGroups和网桥brq在一起，会呈现独占和共享一体的画面，对于新手容易混淆。Nuetron的计算节点和网络节点都涉及数据转发，所以这两个节点也都涉及数据转发的租户隔离机制。计算节点的实现模型如下，br-ethx/br-tun、br-int分别只有一个实例，这个属于：用“多租户共享”的方案，来实现多租户隔离。比如br-int、br-ethx通过VLAN来隔离来自各个租户网络的数据流量，br-tun通过相应的tunnel ID，即VNI来隔离多租户的网络流量。 qbr和VM一一对应，这个属于：“单租户独占”的方案，来实现多租户隔离。qbr由于绑定了安全组，它在原生的数据面租户隔离的基础上又多加了一层“安全层”来保证租户资源隔离。原生的数据面转发（br-tun/br-ehtx、br-int）负责“正常行为”的租户隔离，而安全技术（qbr)负责“异常行为”（非法访问）的租户隔离。Router/DVR跟租户相对应，而且每个Router/DVR运行在一个namespace中，这个属于：“单租户独占”（用namespace隔离）的方案，来实现多租户隔离的目的。这里的namespace是linux操作系统虚拟网络的一个重要概念。最早的linux系统的许多资源都是共享的，比如进程ID资源，而namespace的出现就是将这些资源进行隔离，详见博客《Linux原生网络虚拟化实践一文》。单纯从网络角度来说，一个namespace提供了一份独立的网络协议栈（网络设备接口、IPv4、IPv6、路由、防火墙规则和套接字socket等）。一个设备（linux device）只能位于一个namespace中，不同namespace的设备可以利用veth pair进行桥接。而网络节点的实现模型如下图所示，网络节点中，br-ethx/br-tun、br-int、br-ex分别只有一个实例，这是属于：“多租户共享”的方案，实现了多租户隔离的目的。 Router跟租户对应，而且每个Router运行在一个namespace中，这属于：“单租户独占”的方案，实现了多租户隔离的目的。Router/DVR不仅保证了租户间网络资源不能互相访问外，还解决了逻辑资源（IP地址）冲突的问题。 故障面的租户资源隔离实现模型：通过前面的介绍，Neutron在数据面和控制面的租户资源隔离方案分为两类：资源单租户独占（比如Router等）和资源多租户共享（比如br-int等）。在故障资源隔离层面，对资源共享方案，没有任何故障层面的租户隔离能力，一旦一个部件发生故障，所有与其关联的租户都要受到影响。而对于资源独占层面，具有一定故障层面隔离能力，比如一个租户的Router发生故障，不会影响其他租户。以数据面的实现方案为例，故障面的租户隔离度与资源共享度的关系如下图所示。 但是，资源独占的粒度是有限的，也仅仅是在Router这样的层面才能做到租户资源独占，稍微大一点的粒度，比如主机Host，都不能保证资源独占。这不是技术问题，这是云服务的商业本质决定的。因此，高可用集群部署方案必不可少。用资源共享的方式来实现租户隔离，在正常情况下没有任何问题，但是如果发生故障，资源共享方案要想做到故障层面的租户隔离，这是不可能的。如果非要做成独占方式来隔离故障那是败家行为，土豪动也伤不起。 通过上面的介绍，就回到我们开头那句话：“租户隔离，顾名思义就是为了资源隔离，但其更深层次的含义是多租户资源共享！！！”。隔离的其实都是租户使用的逻辑资源，其目的就是为了对底层资源能够做到复用，最大限度使用底层资源，这是针对控制面和数据转发而言。而针对故障面，由于隔离的真实目的是共享这个原因，正所谓，一荣俱荣，一损俱损，此时不仅要考虑逻辑资源隔离的问题，更多的是考虑资源容错的问题，也就是我们常说的高可用集群，这其实也是资源复用的一种方式，只是复用的资源对象是集群这个整体而已。 采用OVS参考实现的Neutron网络数据转发模型我们在前面提到过，Neutron网络管理服务的类型分为Provider Network Service和Self Network Service两种，同时在实际部署时，Neutron各组件分别部署在控制节点、网络节点和计算节点3种不同角色的逻辑功能节点上。因此，针对不同网络管理服务类型，不同的TypeDriver下的不同的参考实现方案，部署在计算节点的虚拟机实例instance的数据包转发模型并不相同。这里，我们主要分析两种网络管理服务类型下，VLAN和VxLAN两种TypeDriver下的OVS参考实现解决方案的数据包转发路径，理解了OVS参考实现解决方案，LinuxBridge就是个菜。 Provider Network Service下的OVS参考实现数据包转发模型Provider网络是一种仅实现二层网络虚拟化，不提供三层路由和更高层的VPN、LoadBlancer、Firewall等高级功能的Neutron网络类型，相对而言，Provider是一种半虚拟化的网络。在Provider中，三层以上的功能不被虚拟化，而是借助物理网络设备来实现。在Provider网络中，由于二层网络直接接入物理设备，二层网络之间的通信也由物理设备进行转发，因此Provider网络在实现过程中无须L3服务，控制节点只需部署API Server、ML2核心插件及DHCP和Linux bridging代理即可。计算节点中的实例通过虚拟二层交换机直接接入物理网络，因此计算节点只需部署如Open vSwitch或Linux bridging等代理软件即可，Provider网络的节点服务布局如下图所示。 Provider网络拓扑架构很简单，只需在控制节点和计算节点中分别规划一块物理网卡，并将其接入物理网络即可。如果采用的是VLAN网络，则将交换机接口配置为Trunk模式，节点只需一块物理网卡便可通过多个VLAN ID来实现不同网络的隔离。如果采用的是Flat网络，由于Flat网络没有Tagging，如要配置多个Flat网络则需要节点提供同样数量的物理网卡。需要注意一点：Provider网络不支持VxLAN的TypeDriver。在Provider网络中，虚拟机实例instance直接接入Provider网络（外部网络，如192.168.101.0/24），因此也不存在私有网络和虚拟路由设备的概念，同时也无须Floating IP，即实例虚拟接口上获取的就是外网IP。外部网络可以直接访问位于Provider网络上的虚拟机实例，而访问控制由计算节点上的防火墙规则来实现。Provider物理和虚拟网络必须属于同一个网段（192.168.101.0/24），只是网络被分为物理实现和虚拟实现，即节点内部为虚拟网络，而节点外部为物理设备网络。相对Self-Service网络，Provider网络的拓扑架构和通信过程都很简单，在故障排查中也相对容易，并且Provider网络中节点之间和二层网络之间的通信都由物理设备负责，因此Provider的稳定性和性能比起全虚拟化实现的Self-Service网络来说要高很多也更容易被理解和实现。Provider网络的拓扑架构如下图所示。 Provider南北网络数据流分析：在Provider网络架构中，物理网络设备负责处理Provider网路（Provider Network）与外网网络（External Network）之间的路由和其他网络服务。 上图中，如果位于计算节点上的实例Instance发送一个数据包到外网中的主机上，则计算节点将产生如下的操作：（外网主机对Provider网络中的实例访问过程与实例对外网主机的访问过程正好相反） 实例Tap接口将数据转发到节点内部的LinuxBridge网桥qbr上，由于目标主机位于其他网络，实例instance发出的数据包中包含目标主机的MAC地址； 数据进入qbr后，其上的安全组规则对数据包进行状态跟踪和防火墙处理。之后qbr将数据转发到OpenvSwitch的聚合网桥br-int上，并为数据包添加与Provider Network对应的内部VLAN标记（Internal tag）。然后br-int将数据转发到OpenvSwitch的ProviderBridge外部网桥br-ex上，br-ex用实际的外部VLAN ID替换掉br-int添加的内部VLAN ID，然后通过计算节点的Provider网络接口将数据包转发到物理网络设备上。 数据包进入物理网络设备后，首先由交换机处理Provider Network与路由之间的外部VLAN tag操作，剥离掉外部VLAN tag，将数据包转发给路由器； 路由器查询路由表，将来自Provider Network的数据包路由到外部网络，交换机再次处理路由器与外网之间的VLAN tag操作，添加外部VLAN ID，最后交换机将数据包转发到外网。 Provider东西数据流分析：Provider网络东西数据流分为两种类型，即位于相同网段中的实例通信和不同网段之间的实例通信。二者的区别在于，不同Provider网段之间的实例通信需要具备三层路由功能的物理网络设备进行路由转发，而相同Provider网络中的实例通信只需二层物理交换机进行转发。 上图中，当Compute Node1中的实例Instance1向Compute Node2中的实例Instance2发送数据包时，Compute Node1中将会发生如下的数据传递操作： 首先Instance1的tap接口将数据包转发到LinuxBridge qbr，转发的数据包中包含了目标地址的MAC地址。 qbr中的安全组规则对数据包进行防火墙相关的操作，之后qbr将数据包转发到OpenvSwitch聚合网桥br-int，并为数据包添加属于Provider Network1的内部VLAN ID，然后br-int将数据包转发到OpenvSwitch的ProviderBridge br-ex，br-ex使用Provider Network1的外部VLAN ID替换br-int添加的内部VLAN ID，然后将数据包通过Compute Node1节点的Provider网络接口转发到物理网络设备中。 物理网络设备接收到Compute Node1转发来的数据包后，交换机处理Provider Network1与路由之间的VLAN tag操作（剥离VLAN ID），将数据包转发到路由器。 路由将来自Provider Network1的数据包转发到Provider Network2，交换机再次处理路由与Provider Network2之间的VLAN tag操作（添加VLAN ID），之后交换机将数据包转发到Compute Node2中。 Compute Node2接收到物理网络设备发送的数据包后，位于其上的Provider网络接口将数据包转发到OpenvSwitch的ProviderBridge br-ex，然后br-ex将数据包转发到OpenvSwitch的集成网桥br-int，br-int使用Provider Network2的内部VLAN ID替换掉Provider Network2的外部VLAN ID，并将数据包转发到LinuxBridge qbr，qbr使用安全组规则对网络数据包进行过滤。 最后，qbr通过tap接口将数据包转发给Instance2。至此，Compute Node1上的Instance1发出的数据包成功到达Compute Node2上的Instance2中。 当Compute Node1中的Instance1与Compute Node2中的Instance2位于相同的Provider网络时，Instance1对Instance2的访问过程与上面过程类似，但是由于相同网段通信不需路由转发，因此此时的网络设备无须具备三层路由功能，只需二层交换功能即可。 Self Network Service下的OVS参考实现数据包转发模型Self-Service网络又称租户网络，在实现Self-Service网络之前，管理员必须已经实现Provider网络，因此在Self-Service网络中，用户也可以直接使用Provider网络，并将实例直接接入Provider物理网络而不是租户私有云网络。也可以认为Self-Service网络是对Provider网络的扩展和增强实现。与Provider网络相比，Self-Service最大的不同在于租户可以按需创建自己的私有网络，并且网络中需要提供L3服务以实现网络的东西和南北数据流。此外，从主流的Self-Service网络与Provider网络的部署模型上看，Self-Service网络的Neutron服务组件通常分布在控制节点、网络节点和计算节点上，而Provider网络并不需要网络节点。如下图所示，在最简单的三节点Self-Service网络部署中，为了Project网络既可以使用VLAN类型，也可以使用GRE/VxLAN类型，控制节点至少需要一个网络接口（管理网络），网络节点至少需要四个网络接口（管理网络、隧道网络、VLAN网络和外部网络），计算节点至少需要三个网络接口（管理网络、隧道网络和VLAN网络）。需要指出的是，计算节点和网络节点上并非同时需要VLAN网络和Tunnel网络。根据Project网络类型，用户也可以只部署VLAN网络或Tunnel网络。如Project网络为GRE/VxLAN类型，则仅需要Tunnel网络，如果Project网络为VLAN，则仅需要VLAN网络。 Self-Service网络南北数据流分析：在Self-Service网络中，南北数据通信分为两种情况，即Fixed IP形式的南北网络通信和Floating IP形式的南北网络通信。对于仅有Fixed IP而没有为其绑定Floating IP的实例，网络节点的Router负责Project网络与External网络的通信路由。 上图中，假设Compute Node1中的instance向External网络中的主机发送数据包，则该数据包在Compute Node1节点内部的处理流程如下： 实例instance转发包含目标主机MAC地址的数据包到LinuxBridge网桥qbr。 LinuxBridge网桥qbr对数据包采用安全组规则进行过滤处理。 LinuxBridge网桥qbr将安全规则过滤后的数据转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int为数据包添加Project网络的内部VLAN ID。 如果Project网络是GRE/VxLAN类型，则转到步骤6。 如果Project网络是VLAN类型，则OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitch VLAN网桥br-vlan。 OpenvSwitchVLAN网桥br-vlan使用Project网络的外部VALN ID替换掉br-int添加的Project网络内部VLAN ID。 OpenvSwitchVLAN网桥br-vlan通过VALN接口将数据转发到网络节点。 如果Project网络是GRE/VxLAN类型，则直接进入此步骤。 OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitch隧道网桥br-tun。 OpenvSwitch隧道网桥br-tun将数据封装到VxLAN/GRE隧道中，并为封装后的数据包添加用以识别不同Project网络的VNI。 OpenvSwitch隧道网桥br-tun通过隧道网络接口将数据转发到网络节点。 数据包到达网络节点后，在网络节点内部的处理流程如下： 如果Project网络是GRE/VxLAN类型，则转到步骤2。 网络节点的VLAN网络接口将数据包转发到OpenvSwitchVLAN网桥br-vlan。 OpenvSwitchVLAN网桥br-vlan将数据包转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int使用Project网络的内部VLAN ID替换掉Project的外部VLAN ID。 如果Project网络是GRE/VxLAN类型，则直接进入此步骤。 网络节点隧道网络接口将数据转发到OpenvSwitch隧道网桥br-tun。 OpenvSwitch隧道网桥br-tun解封经过GRE/VxLAN协议封装的数据包，并为解封后的数据添加Project网络内部tag。 OpenvSwitch隧道网桥br-tun将数据包转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int将数据转发到qrouter命名空间的qr接口，qr接口事先配置了Project网络的网关地址。 qrouter命名空间中的iptables服务使用qg接口上的IP作为Source IP对数据包进行SNAT操作，qg接口事先配置了External网络的网关地址。 Router通过qg接口转发数据包到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int转发数据包到OpenvSwitch外部网桥br-ex。 OpenvSwitch外部网桥br-int通过网络节点上的外部网络接口将数据包转发到External网络。 Self-Service网络东西数据流分析：Project网络内部的实例之间的通信称为东西网络通信，东西网络通信通常分为两种，即相同Project网络内部的实例通信和不同Project网络中的实例通信。此外，对于东西网络通信，实例是否绑定Floating IP并不影响通信方式。 上图中，表示两个不同Project网络的东西流量模型，假设位于Compute Node1且Project网络为Network1的实例instance1要与位于Compute Node2且Project网络为Network2的实例instance1通信，则instance1发出的数据在Compute Node1上的处理过程如下： instance1的tap接口将包含目标MAC地址的数据转发到LinuxBridge网桥qbr中。 LinuxBridge网桥qbr按照安全组规则对数据进行过滤处理。 LinuxBridge网桥qbr中将数据转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int为数据包添加Project Network1的内部tag。 如果Project网络是VLAN类型，则进入此步骤，否则转到步骤6。 OpenvSwitch集成网桥br-int将数据转发到OpenvSwitch VLAN网桥br-vlan。 OpenvSwitch VLAN网桥br-vlan使用Project Network1的外部VLAN ID替换掉内部tag。 OpenvSwitch VLAN网桥br-vlan通过Compute Node1的VLAN网络接口将数据转发到网络节点。 如果Project网络为GRE/VxLAN类型，则直接接入此步骤。 OpenvSwitch集成网桥br-int将数据转发到OpenvSwitch隧道网桥br-tun。 OpenvSwitch隧道网桥br-tun将数据封装到VxLAN/GRE隧道中，并为封装后的数据包添加用以识别不同Project网络的隧道标记tag。 OpenvSwitch隧道网桥br-tun通过Compute Node1的隧道网络接口将数据转发到网络节点。 数据进入网络节点之后，在网络节点中的处理过程如下： 如果Project网络是VLAN类型，则进入此步骤，否则转到步骤2。 网络节点的VLAN网络接口将数据包转发到OpenvSwitchVLAN网桥br-vlan。 OpenvSwitchVLAN网桥br-vlan将数据包转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int使用Project网络的内部tag替换掉Project的实际VLAN ID。 如果Project网络是GRE/VxLAN类型，则直接进入此步骤。 网络节点隧道网络接口将数据转发到OpenvSwitch隧道网桥br-tun。 OpenvSwitch隧道网桥br-tun解封经过GRE/VxLAN协议封装的数据包，并为解封后的数据添加Project网络内部tag。 OpenvSwitch隧道网桥br-tun将数据包转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int将数据转发到qrouter命名空间中的qr-1接口。qr-1接口已配置有Project Network1的网关IP。 qrouter命名空间将数据包路由到qr-2接口，qr-2接口已配置有Project Network2的网关IP。 qrouter命名空间将数据包转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int为数据包添加Project Network2的内部tag。 如果Project网络是GRE/VxLAN类型，则转到步骤8。如果是VLAN类型，则进入此步骤。 OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitchVLAN网桥br-vlan。 OpenvSwitchVLAN网桥br-vlan使用Project网络实际VLAN ID替换掉Project网络内部tag。 OpenvSwitchVLAN网桥br-vlan通过网络节点VLAN接口将数据转发到Compute Node2计算节点。 如果Project网络是GRE/VxLAN类型，则进入此步骤。 OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitch隧道网桥br-tun。 OpenvSwitch隧道网桥br-tun将数据封装到VxLAN/GRE隧道中，并为封装后的数据包添加用以识别不同Project网络的隧道标记tag。 OpenvSwitch隧道网桥br-tun将数据包通过网络节点上的隧道网络接口转发到Compute Node2计算节点。 数据进入Compute Node2计算节点后，在Compute Node2中的处理过程如下： 如果Project网络是VLAN类型，则直接进入此步骤，如果是GRE/VxLAN网络，则转到步骤2。 计算节点的VLAN接口将数据包转发到OpenvSwitchVLAN网桥br-vlan。 OpenvSwitchVLAN网桥br-vlan将数据包转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int使用Project Network2的内部tag替换掉实际VLAN ID。 如果Project网络是GRE/VxLAN类型，则直接进入此步骤。 计算节点上的隧道接口将数据转发到OpenvSwitch隧道网桥br-tun。 OpenvSwitch隧道网桥br-tun解封经过GRE/VxLAN协议封装的数据包，并为解封后的数据添加Project Network2络内部tag。 OpenvSwitch隧道网桥br-tun将数据包转发到OpenvSwitch集成网桥br-int。 OpenvSwitch集成网桥br-int转发数据到LinuxBridge网桥qbr。 LinuxBridge网桥qbr根据安全组规则过滤数据包。 LinuxBridge网桥qbr将数据转发到Compute Node2计算节点实例instance2的tap接口。 相比不同Project网络中的实例通信，相同Project网络内部实例通信要简单很多，因为相同Project网络内部实例通信无须进行路由，而是完全由OpenvSwitch Agent进行数据交换处理，因此也不需要网络节点参与。 Neutron的不同网络类型实现实战Local网络local network的特点是不会与宿主机的任何物理网卡相连，也不关联任何的VLAN ID。对于每个local netwrok，ML2 linux-bridge会创建一个bridge，instance的tap 设备会连接到bridge。位于同一个local network的instance会连接到相同的bridge，这样instance之间就可以通信了。因为bridge没有与物理网卡连接，所以instance 无法与宿主机之外的网络通信。 同时因为每个local network有自己的bridge，bridge之间是没有连通的，所以两个local network之间也不能通信，即使它们位于同一宿主机上的同一网段。以下就是通过linuxbridge方式实现的一个local网络的拓扑逻辑图。 步骤1：首先通过web页面创建一个本地网络local_net1，local_net1的子网地址段cidr为10.10.10.0/24，如下图 由于创建子网subnet时，我们勾选了起用DHCP服务（默认勾选，如想手动配置地址，可不选），因此子网subnet创建成功后，默认起用了一个DHCP端口，如下： 此时，我们看下网络节点的底层发生 了什么变化？如下图所示，在网络节点执行brctl show，如下显示 我们发现，在网络节点的底层网络上，多了一个网桥brqea861547-ce，同时其上挂载了一个tap设备tapb8dca9e6-ec。通过命名我们应该可以联想到网桥brqea861547-ce就是我们创建的local_net1，后面ID就是local_network ID的前11位“短ID”，而tap设备tapb8dca9e6-ec就是DHCP Server在root namespace中接口（这是个veth pair设备，另一头连接的是DHCP Server的namespace接口），同样tap后面的字符串ID就是DHCP端口ID的前11位“短ID”。 步骤2：我们再创建一个local_net2，和local_net1的创建方法一样，如下图所示。同样，我们可以设置local_net2的子网subnet的cidr为10.10.10.0/24，实现资源隔离和复用。 同样，我们在网络节点上查看一下底层网络有什么变化？如下图所示： 我们发现网络节点的底层网络多了一个网桥brq05b09958-c0，同时其上也挂载了一个tap设备tap436a2ae5-ac，通过ID我们知道其分别对应local_net2和其上DHCP Server的veth pair接口。 步骤3：此时，我们创建两个虚机VM1和VM2，其网络分配为local_net1，如下图所示 可以看出虚机VM1和VM2创建成功，DHCP分配的IP地址分别为10.10.10.13和10.10.10.16，此时，我们通过VNC或SSH登录到虚机VM2中，尝试ping虚机VM1，可以ping通。如下图，与我们预期一样。 步骤4：我们再创建一个虚机VM3，为其分配的网络local_net2，创建成功后DHCP为其分配的IP地址为10.10.10.11 同样，我们登录到虚机VM3中，尝试ping虚机VM1和VM2，如下图所示，ping不通，完全符合我们的预期。 flat网络flat network 是不带tag的网络，就是一个大二层网络。要求宿主机的物理网卡直接虚拟网络相连，这意味着：每个flat network都会独占一个物理网卡。如下图，ens34 桥接到 brqXXX，为 instance 提供 flat 网络。如果需要创建多个 flat network，就得准备多个物理网卡。 步骤1：创建一个flat网络llb_flat，标签为default，子网段cidr为30.10.20.0/24，DHCP Server的IP地址为30.10.20.10.如下所示： 步骤2：我们看下底层网络有啥变化。如下所示 对于ovs bridge “br-ex”和其上桥接的port“ens35”我们应该不会感到意外，这是前面配置的结果。然而除此之外，br-int和br-ex分别多了一个port“int-br-ex”和“phy-br-ex”，而且这两个port都是“patch”类型，同时通过“peer”指向对方。上面的配置描述了这样一个事实：br-int与br-ex这两个网桥通过int-br-ex和phy-br-ex连接在一起了。如下图所示 此时网络节点的逻辑拓扑如下： 通过前面的分析我们可知veth pair和patch port都可以连接网桥，使用的时候如何选择呢？答案是patch port 是ovs bridge自己特有的port类型，只能在ovs中使用。如果是连接两个ovs bridge，优先使用patch port，因为性能更好。 步骤3：此时我们创建两个虚机FLAT_VM1和FLAT_VM2，为其配置为FLAT网络模式，如下，FLAT-VM1和FLAT_VM2通过DHCP拿到的地址分别为30.10.20.13和30.10.20.16。 我们看到instance在启动过程中能够从Neutron的DHCP服务获得IP，Neutron提供DHCP服务的组件是DHCP agent。DHCP agent在网络节点运行上，通过dnsmasq实现DHCP功能。DHCP agent的配置文件位于/etc/neutron/dhcp_agent.ini。当创建network并在subnet上enable DHCP时，网络节点上的DHCP agent会启动一个dnsmasq进程为该network提供DHCP服务。dnsmasq是一个提供DHCP和DNS服务的开源软件。dnsmasq与network是一对一关系，一个dnsmasq进程可以为同一netowrk中所有enable了DHCP的subnet提供服务。DHCP agent会为每个network创建一个目录/etc/data/neutron/dhcp/，用于存放该network的dnsmasq配置文件。Neutron通过namespace为每个network提供独立的DHCP和路由服务，从而允许租户创建重叠的网络。如果没有namespace，网络就不能重叠，这样就失去了很多灵活性。在创建虚拟机实例instance时，Neutron会为其分配一个port，里面包含了MAC和IP地址信息。这些信息会同步更新到dnsmasq的host文件。同时nova-compute会设置虚机实例instance的VIF的MAC地址。一切准备就绪，虚拟机实例instance获取IP的过程如下： Step1：虚拟机实例instance开机启动，发出DHCPDISCOVER广播，该广播消息在整个Network中都可以被收到。 Step2：广播到达veth interface的tap设备，然后传送给veth pair的另一端ns设备。dnsmasq在它上面监听，dnsmasq检查其 host 文件，发现有对应项，于是dnsmasq以DHCPOFFER消息将IP、子网掩码、地址租用期限等信息发送给虚拟机实例instance。 Step3：虚拟机实例instance发送DHCPREQUEST消息确认接受此DHCPOFFER。 Step4：dnsmasq发送确认消息DHCPACK，整个过程结束。 vlan网络vlan network是带tag的网络，是实际应用最广泛的网络类型。在Open vSwitch实现方式下，不同vlan instance的虚拟网卡都接到聚合网桥br-int上。这一点与 linux bridge非常不同，linux bridge是不同vlan接到不同的网桥上。下面，我们将采用实践部署的方式先完成网络创建，然后通过底层分析，最后画出我们的逻辑拓扑图。 步骤1：首先我们创建一个VLAN网络，VLAN ID为10，如下VLAN的label是flat（在ml2.conf.ini文件中配置，名字可以随意起，也可以配置多个，之间用逗号“,”隔开），子网cidr为172.18.10.0/24，DHCP Server的端口和IP地址为172.18.10.10 我们在网络节点的br-int上发现多了一个tap设备，这个tap设备就是DHCP Server在root空间veth interface。此时网络节点的拓扑如下图右边所示 步骤2：现在创建一个虚拟机实例instance，为其配置网络的为上面VLAN10，同时这个虚拟机实例instance被创建在计算节点computer1上，IP地址为172.18.10.15，发现在VLAN10的网络上了多了一个端口 在计算节点computer1上看下底层网络变化？发现网桥上不仅有个tap设备（虚机虚拟网卡VIF），还多个qvb设备，如下图左边上部，这其实是个veth pair设备的veth interface，同时在计算节点的br-int找到了该veth pair设备另一端接口qvo，如下图左边下部，此时计算节点computer1上的网络拓扑如下图右边。 步骤3：再创建一个虚机VM2，该虚机被调度到了计算节点computer2上，IP地址为172.18.10.18。 在计算节点computer2上查看底层网络变化，如下： 发现其上创建的qbr网桥与计算节点computer1的网桥并不是同一个。这一点就是OVS网络与linuxbridge网络不同点，在linuxbridge网络中，同一个网络对应一个网桥brq，也就是说网桥brq与网络network是一一对应的关系。但是在OVS网络中，一个网桥qbr其实对应的一个虚机，而网络network对应的是br-int这个网桥，但不是一一对应，而是m:1的关系。 步骤4：再创建一个vlan11的网络，方法同VLAN10，此时网络节点的拓扑如下： 步骤5：再创建一个虚机VM3，为其分配VLAN11的网络，其IP地址为172.18.11.18，该虚机被调度到计算节点computer1上 此时计算节点compute1的拓扑如下，VM3和VM1虽然同属于一个Host，甚至共享同一个br-int，但他们分属于不同VLAN网络，因此二层上应该是隔离的。下面我们就来分析下OVS网络下的VLAN隔离机制。 与Linux Bridge driver不同，Open vSwitch driver并不通过eth1.100, eth1.101等VLAN interface来隔离不同的VLAN。所有的instance都连接到同一个网桥br-int，Open vSwitch通过 flow rule（流规则）来指定如何对进出br-int 的数据进行转发，进而实现vlan之间的隔离。具体来说：当数据进出br-int时，flow rule可以修改、添加或者剥掉数据包的VLAN tag，Neutron负责创建这些flow rule并将它们配置到br-int，br-ens34等Open vSwitch上。 下面就来研究一下当前的flow rule。查看flow rule的命令是ovs-ofctl dump-flow ，首先查看计算节点computer1的br-ens34的flow rule，如下，每一个cookie就是一条flow rule，下图中br-ens34上配置了四条rule，每条rule有不少属性，其中比较重要的属性有：priority、in_port、dl_vlan和actions。 priority：rule 的优先级，值越大优先级越高，Open vSwitch会按照优先级从高到低应用规则。 in_port：inbound端口编号，每个port 在Open vSwitch中会有一个内部的编号。可以通过命令ovs-ofctl show 查看port编号，如下图br-ens34：ens34编号为1；phy-br-ens34编号为2。 dl_vlan：数据包原始的 VLAN ID。 actions：对数据包进行的操作。 br-ens34跟VLAN相关的flow rule比较重要的有两条，下面我们来详细分析。清晰起见，我们只保留重要的信息，如下： priority=4,in_port=2,dl_vlan=2 actions=mod_vlan_vid:10,NORMAL priority=4,in_port=2,dl_vlan=3 actions=mod_vlan_vid:11,NORMAL 第一条的含义是：从br-ens34的端口phy-br-ens34（in_port=2）接收进来的包，如果VLAN ID是2（dl_vlan=2），那么需要将VLAN ID改为10（actions=mod_vlan_vid:10） 从上面的网络结构我们可知，phy-br-ens34连接的是br-int，phy-br-ens34的inbound包实际上就是虚机VM通过br-int发送给外部网络的数据。那么怎么理解将VLAN ID 2改为VLAN ID 10呢？请看下面计算节点computer1的ovs-vsctl show的输出： br-int通过tag隔离不同的port，这个tag可以看成内部的VLAN ID。从qvo2cc37622-be（对应VM1，vlan10）进入的数据包会被打上内部VLAN ID= 2的VLAN tag。因为br-int中的VLAN ID跟物理网络中的VLAN ID并不相同，所以当br-ens34接收到br-int发来的数据包时，需要对VLAN进行转换。Neutron负责维护内外VLAN ID的对应关系，并将转换规则配置在flow rule中。理解了br-ens34的flow rule，我们再来分析br-int的flow rule。如下： 最关键的是下面两条： priority=3,inport=2,dl_vlan=10 actions=mod_vlan_vid:2,NORMAL priority=3,inport=2,dl_vlan=11 actions=mod_vlan_vid:3,NORMAL port 2为int-br-ens34，那么这两条规则的含义就应该是： 从物理网卡接收进来的数据包，如果 VLAN 为 10，则改为内部 VLAN 2。 从物理网卡接收进来的数据包，如果 VLAN 为 11，则将为内部 VLAN 3。 简单的说，数据包在物理网络中通过VLAN 10和VLAN 11隔离，在计算节点OVS br-int中则是通过内部VLAN 2和VLAN 3隔离。也就是说通过OVS实现的VLAN网络存在内外VLAN转换的机制，这一点同样适用于后面的VxLAN网络，同时也是和Linubridge实现的VLAN的不同点之一。OVS实现下不同VLAN之间通过router三层互通和前面linuxbridge实现基本一样，唯一区别是Router的接口不再与qbr通过veth pair相连，而改为和br-int相连。 VxLAN网络前面讨论了local, flat, vlan这几类网络，OpenStack还支持vxlan和gre这两种overlay network。overlay network是指建立在其他网络之上的网络。overlay network中的节点可以看作通过虚拟（或逻辑）链路连接起来的。overlay network在底层可能由若干物理节点组成，但是不需要关心这些底层实现。例如P2P网络就是overlay network。vxlan和gre都是基于隧道技术实现的，它们也都是overlay network。linux bridge只支持vxlan，不支持gre；open vswitch两者都支持。vxlan与gre实现非常类似，而且vxlan用得较多，所以只讨论vxlan网络的Neutron实现。VxLAN即Virtual eXtensible Local Area Network，正如名字所描述的，VxLAN提供与VLAN 相同的以太网二层服务，但拥有更强的扩展性和灵活性，且支持16777216个二层网段。关于VxLAN的技术原理请参考博客《服务器外部交换网络虚拟化》一文。Neutron服务重启后，通过ovs-vsctl show查看网络配置：可以发现br-tun和br-int上各有一个path 接口指向对方，如下图左边，此时网络节点/计算节点的拓扑如下图右边所示： 步骤1：创建一个VIN=30的VxLAN网络，子网cidr为10.10.30.0/24，如下 步骤2：此时网络节点的底层网络发生如下变化，如下图左边所示，在br-int网桥上多了一个tap设备，同时该tap设备与DHCP Server的namespace ID一致，这个tap设备是个veth pair设备，一头连接在br-int网桥，一头连接namespace中DHCP Server。此时，网络节点的逻辑拓扑如下图右边所示。 步骤3：在VXLAN30的网络上创建2个VM，虚机VM1被调度到计算节点computer01上，IP地址为10.10.30.15。虚机VM2被调度到计算节点computer02上，IP地址为10.10.30.13。 步骤4：分别在计算节点computer01和computer02上查看下底层网络的变化，如下图左边所示，两个计算节点各多了一个qbr网桥，每个qbr网桥上挂载了虚机虚拟网卡VIF和qvb设备。此时，网络节点和计算节点的逻辑拓扑如下图右边所示。 步骤5：在computer01上有2个VxLAN端口：1个是vxlan-ac124b0c，另一个是vxlan-ac124b0e，如下图上半部分。同时，在网络节点的network01的底层网络也是存在两个VxLAN网络：vxlan-ac124b0d和vxlan-ac124b0e，如下图下半部分。 此时，网络逻辑拓扑如下，表示当计算节点computer01和computer02上的VM通过VxLAN互访时，vxlan-ac124b0c完成。当计算节点computer01和computer02上的VM要与外部网络互访时，分别通过vxlan-ac124b0d和vxlan-ac124b0e完成。 步骤6：分析VxLAN网络数据包的转发规则，首先查看br-int 的 flow rule，如下： br-int的rule看上去虽然多，其实逻辑很简单，br-int被当作一个二层交换机，其重要的rule 是下面这条，含义为根据 vlan 和 mac 进行转发。 cookie=0x965a57f99ba197f2, duration=406.721s, table=0, n_packets=51, n_bytes=3570, idle_age=119, priority=0 actions=NORMAL 然后，查看br-tun的flow rule，如下，这才是真正处理 VXLAN 数据包的 rule。 其具体处理流程如下，下图各方块中的数字对应 rule 中 table 的序号。 （1）编号为0的方块（table 0）对应下面4条rule： cookie=0x9455c3750d53ad75, duration=602.160s, table=0, n_packets=79, n_bytes=5500, idle_age=12, priority=1,in_port=1 actions=resubmit(,2) cookie=0x9455c3750d53ad75, duration=420.913s, table=0, n_packets=0, n_bytes=0, idle_age=420, priority=1,in_port=2 actions=resubmit(,4) cookie=0x9455c3750d53ad75, duration=420.908s, table=0, n_packets=0, n_bytes=0, idle_age=420, priority=1,in_port=3 actions=resubmit(,4) cookie=0x9455c3750d53ad75, duration=602.159s, table=0, n_packets=0, n_bytes=0, idle_age=602, priority=0 actions=drop 结合如下 port 编号，table 0 flow rule 的含义为： 从 port 1（patch-int）进来的包，扔给table 2 处理：actions=resubmit(,2) 从 port 2（vxlan-ac124b0e）进来的包，扔给 table 4 处理：actions=resubmit(,4) 从 port 3 （vxlan-ac124b0d）进来的包，扔给table 4处理：actions=resubmit(,4) 即第一条rule处理来自内部br-int（这上面挂载着所有的网络服务，包括路由、DHCP 等）的数据；第二条、第三条rule处理来自外部VxLAN隧道的数据。 （2）编号为4的方块（table 4）对应下面2条rule： cookie=0x9455c3750d53ad75, duration=422.298s, table=4, n_packets=0, n_bytes=0, idle_age=422, priority=1,tun_id=0x64 actions=mod_vlan_vid:1,resubmit(,10) cookie=0x9455c3750d53ad75, duration=602.155s, table=4, n_packets=0, n_bytes=0, idle_age=602, priority=0 actions=drop 其含义为：如果数据包的 VXLAN tunnel ID 为 100（tun_id=0x64），action 是添加内部 VLAN ID 1（tag=1），然后扔给 table 10 去学习。而对于其他tun_id的数据包则直接丢弃。 （3）编号为10的方块（table 10）对应下面1条rule： cookie=0x9455c3750d53ad75, duration=602.154s, table=10, n_packets=0, n_bytes=0, idle_age=602, priority=1actions=learn(table=20,hard_timeout=300,priority=1,cookie=0x9455c3750d53ad75,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0-&gt;NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]-&gt;NXM_NX_TUN_ID[],output:OXM_OF_IN_PORT[]),output:1 其含义为：学习外部（从tunnel）进来的包，往 table 20 中添加对返程包的正常转发规则，然后从 port 1（patch-int）扔给 br-int。 （4）编号为2的方块（table 2）对应下面2条rule： cookie=0x9455c3750d53ad75, duration=602.158s, table=2, n_packets=0, n_bytes=0, idle_age=602, priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20) cookie=0x9455c3750d53ad75, duration=602.157s, table=2, n_packets=79, n_bytes=5500, idle_age=12, priority=0,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,22) 其含义为：br-int 发过来数据如果是单播包，扔给 table 20 处理：resubmit(,20)；br-int 发过来数据如果是多播或广播包，扔 table 22 处理：resubmit(,22)。 （5）编号为20的方块（table 20）对应下面3条rule： cookie=0x9455c3750d53ad75, duration=323.953s, table=20, n_packets=0, n_bytes=0, idle_age=420, priority=2,dl_vlan=1,dl_dst=fa:16:3e:36:14:b3 actions=strip_vlan,load:0x64-&gt;NXM_NX_TUN_ID[],output:2 cookie=0x9455c3750d53ad75, duration=323.950s, table=20, n_packets=0, n_bytes=0, idle_age=420, priority=2,dl_vlan=1,dl_dst=fa:16:3e:83:ec:c7 actions=strip_vlan,load:0x64-&gt;NXM_NX_TUN_ID[],output:3 cookie=0x9455c3750d53ad75, duration=602.153s, table=20, n_packets=0, n_bytes=0, idle_age=602, priority=0 actions=resubmit(,22) 其含义为：第一条规则就是 table 10 学习来的结果，即数据包的返程规则。内部 VLAN 号为 1（tag=1），目标 MAC 是 fa:16:3e:36:14:b3（VM1）的数据包，即发送给 VM1的包（VM1在computer01计算节点），action 是去掉 VLAN 号，添加 VXLAN tunnel ID 100(十六进制 0x64)，并从 port 2 (tunnel 端口 vxlan-ac124b0e) 发出。第二条规则也是table 10 学习来的结果，也是数据包的返程规则。内部 VLAN 号为 1（tag=1），目标 MAC 是 fa:16:3e:83:ec:c7（VM2）的数据包，即发送给 VM2的包（VM2在computer02计算节点），action 是去掉 VLAN 号，添加 VXLAN tunnel ID 100(十六进制 0x64)，并从 port 3(tunnel 端口 vxlan-ac124b0d) 发出。第三条规则是对于没学习到规则的数据包，则扔给 table 22 处理。 （6）编号为22的方块（table 22）对应下面2条rule： cookie=0x9455c3750d53ad75, duration=323.951s, table=22, n_packets=11, n_bytes=866, idle_age=318, priority=1,dl_vlan=1 actions=strip_vlan,load:0x64-&gt;NXM_NX_TUN_ID[],output:2,output:3 cookie=0x9455c3750d53ad75, duration=602.152s, table=22, n_packets=68, n_bytes=4634, idle_age=12, priority=0 actions=drop 其含义为：如果数据包的内部 VLAN 号为 1（tag=1），action 是去掉 VLAN 号，添加 VXLAN tunnel ID 100(十六进制 0x64)，并从port 2 (tunnel 端口 vxlan-ac124b0e) 或port 3 (tunnel 端口 vxlan-ac124b0d) 。 SNAT、Floating IP不同VLAN网络之间是二层隔离的，如果两个不同VLAN网络要互通，就需要建立一个路由器，此时路由器会增加2个Port，分别作为这两个VLAN网络的网关，然后通过直连路由表项完成两个不同VLAN网络之间的三层互通。在数据模型部分的Router数据模型讨论中，每个虚拟Router会有3种路由表项：直连路由、静态路由，默认静态路由。而默认静态路由表项的产生，就是当我们给Router设置外部网关时，它会自动增加一个Port，而这个Port就是用来对外部网络进行映射。也就是说，在虚机角度来看，外部网络的网关就是这个Port。因此，当内部网络的数据包经过这个Port时，该Port会对数据包的源地址进行替换，也就是我们常说的SNAT（Source NAT）。而Floating IP的作用正好相反，当外部网络的主机需要访问云主机时，此时目的地址为云主机在Router上的Floating IP，当数据包到达Router时，需要将Floating IP替换为云主机的内部IP，此过程就是DNAT。floating IP 提供静态NAT功能，建立外网IP与VM租户网络IP的一对一映射，其配置在router提供网关的外网interface上的，而非VM 中。router会根据通信的方向修改数据包的源或者目的地址，即SNAT/DNAT。 步骤1：建立一个外部网络ext_net1，该网络cidr为10.10.10.0/24，默认网关10.10.10.1。 同时，我们的宿主机Vnet2对应的地址段也是10.10.10.0/24，而虚拟网卡Vnet2的IP地址就是10.10.10.1，如下所示。也就是说，建立这个外部网络ext_net1就是Neutron中建立的一个物理网络的映射。 步骤2：给路由器Router设置一个外部网关，网关映射配置为ext_net1，此时ext_net1的DHCP Server会给Router的这个Port分配一个IP地址，我们这里分配的IP地址为：10.10.10.19。如下： 此时我们的底层网络网桥上会增加一个tap设备tap099f513f-e9，这是一个veth interface，用来连接namespace for router的中另一个veth interface，目的就是将Router的外部网关Port连接到外部网络ext_net1。如下，在Router的namespace中，用来连接外部网络的Port命名为：qg-xxxxx，而用来连接内部网络的Port命名为：gr-xxxx。 步骤3：登录到路由器Router中，查看其路由表情况发现：存在一条默认路由，网关为10.10.10.1（Vnet2地址），接口为qg_099，含义就是内网任意云服务器(VM)访问任意外部网络（0.0.0.0），数据包从qg-099转发出去，到达网关10.10.10.1。同时，存在一条直连路由，含义是当这个外部网络映射ext_net1也有云服务器时，其他任意网络云服务器访问这些云服务器的数据包从接口qg-099接口转发。 步骤4：登录虚机VM进行验证，VM的IP地址为（172.18.15.12）。发现可以ping通路由器RouterA的Port1（10.10.10.19），同时也能ping通我们宿主机网卡Vnet2（10.10.10.1）。进行trace发现，路由只经过2跳就到达外部网关RouterB的Port2，如下： 步骤5：当数据包从 RouterA连接外网的接口 qg-099f513f-e9 发出的时候，会做一次 Source NAT，即将包的源地址修改为 RouterA的接口Port1的IP地址 10.10.10.19，这样就能够保证目的端能够将应答的包发回给 RouterA，然后再转发回源端 VM。可以通过 iptables 命令查看Router的 SNAT 的规则，如下： 此时当VLAN11的VM（172.18.11.12） Ping 10.10.10.1 时，可用通过 tcpdump 分别观察 RouterA和VM的 icmp 数据包来验证 SNAT 的行为。如下： RouterA的Port0端口（qr-1b1706db-b0）的ICMP数据包： RouterA的PortA端口（qg-099f513f-e9）的ICMP数据包 因此，SNAT 让 VM 能够直接访问外网，但外网还不能直接访问VM。因为VM没有外网 IP。这里 “直接访问 VM” 是指通信连接由外网发起，例如从外网 SSH VM，这个问题可以通过 floating IP 解决。 步骤6：给虚机VM（172.18.15.19）绑定一个floating IP（10.10.10.12），如下所示： 步骤7：查看Router的interface变化发现，在qg-099f513f-e9接口上增加了一个IP地址（10.10.10.12），如下： 查看 router 的 NAT 规则，iptables 增加了两条处理 floating IP 的规则：1）当 router 接收到从外网发来的包，如果目的地址是 floating IP 10.10.10.12，将目的地址修改为 VM的 IP 172.18.15.19。这样外网的包就能送达到 VM。2）当VM发送数据到外网，源地址 172.18.15.19 将被修改为 floating IP 10.10.10.12。 步骤8：在我的实验环境中，10.10.10.1 是宿主机网卡Vnet2的地址，现在让它PING VM。 能够 PING 通。同时，我们任意一个主机节点ssh连接虚机VM，发现可以直接登录。如下： 思考：1、Nuetron的Nuetron-Server为什么与其他服务不一样，不起名为Neutron-api？ 2、请阐述Provider与Self网络的区别，请分析网络云采用哪种网络虚拟化解决方案，并给出理由？ 3、OVS agent与Linux Bridge agent在实现二层网络上区别是什么？为什么OVS agent实现的二层网络会包含一个Linux Bridge网桥？ 4、请分析直连路由与静态路由的区别？并分析在Neutron的vRoute解决方案中，直连路由和静态路由的内部转发机制（结合实际虚拟机实例instance的数据包流向分析） 5、在网络云大区华为OVS/EVS解决方案中，一个OVS网桥内部至少包含几个二层网桥？各网桥的类型和作用是什么？ 6、虚拟机的虚拟网卡vNIC，在Neutron的网络虚拟化解决方案中本质上是一个什么设备？它与Network上Port是否同一个概念？为什么？Network上的Port资源是否与虚拟机实例一一对应？ 7、在网络云数据中心内各物理网络平面的网关设置在哪里？不同物理平台互通通过什么设备实现（根据管理平面下发指令到业务虚机的场景阐述） 8、SNAT和DNAT分别用于什么场景？NAT技术是否属于安全领域的技术之一？ 9、在OVS的br-int网桥上不同VLAN的虚拟机实例通过什么机制进行隔离？（根据实际部署场景抓包分析） 10、在后续网络云资源池引入SDN控制器，其需要与Nuetron进行对接，则SDN控制作为Neutron的pulgin进行集成还是作为Neutron的agent进行对接？其API接口通过Neutron的什么API组件实现？其与neutron-server，neutron-agent是否通过消息队列进行交互？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-计算管理服务Nova]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E8%AE%A1%E7%AE%97%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Nova%2F</url>
    <content type="text"><![CDATA[概述Compute Service Nova是OpenStack最核心的服务，负责维护和管理云环境的计算资源。OpenStack作为IaaS的云操作系统，虚拟机生命周期管理就是通过Nova来实现的。因此，Nova是OpenStack云中的计算组织控制器，首次出现在OpenStack的Austin版本，也就是伴随着OpenStack的诞生就存在。提供大规模、可扩展、按需自助的计算资源服务，现在的版本同时支持管理裸机、虚拟机和容器。而在OpenStack早期的几个版本中计算、存储和网络均由Nova来提供，也就说不仅存在nova-compute组件，同时也有nova-volume和nova-network组件，后续随着项目拆分，nova-volume演变为现在Cinder服务，nova-network演变为现在Nuetron服务，而Nova自身则专注于计算服务，主要依赖Keystone提供认证服务，Glance提供镜像服务，Cinder提供块存储服务和Nuetron提供网络服务，其在OpenStack整个系统中位置如下，处于系统整体的核心地位。 Nova计算管理服务的具体作用主要包括以下几个方面： 支持OpenStack云中实例（VM Instances）生命周期的所有活动。 负责管理计算资源、网络、认证、所需可扩展性的平台。 Nova自身并没有提供任何虚拟化能力，它通过调用诸如libvirt之类的API和下层Hypervisors实现交互。 Nova 通过一个与Amazon Web Services（AWS）EC2 API兼容的web services API来对外提供服务。 Horizon或者其他系统可以通过调用Nova-API实现和计算服务的交互。 存在多个各司其职的服务（即守护进程）。 Nova的架构Nova项目最初的源代码由美国国家航空航天局（NASA）贡献，截至Ocata版本，Nova项目已发行了15个版本，也是社区所有项目中最为成熟和用户生产环境部署率最高的项目。在2010年OpenStack项目成立之初，Nova项目主要分为Nova-Compute、Nova-volume和Nova-network三大功能模块。在2012年9月OpenStack的Folsom版本发行时，社区才将Nova-volume和Nova-network独立出来分别构建了Cinder和Quantum项目（后因商标原因更名为Neutron项目）。在OpenStack的A至E版本中，OpenStack Nova项目的逻辑架构如下图所示，其中，除了Nova-Compute、Nova-volume和Nova-network三大功能模块之外，还有处理RESTful API请求的Nova-API模块、调度Nova-Compute的Nova-scheduler模块、用以模块信息交互的消息队列系统和配置及状态数据存储的数据库。而在早期的OpenStack版本中，仅有Nova、Swift和Glance三大项目，如果用户不准备使用对象存储Swift，则Nova和Glance项目即构成了早期的OpenStack云平台。 在OpenStack的Folsom版本发行后，Nova-volume和Nova-network被独立成为块存储Cinder项目和网络Neutron项目，而Nova自身的功能模块也被不断细分，除了Nova-Compute和Nova-API功能模块，以及消息队列和数据库之外，Nova项目还构建了Nova-cert、Nova-Conductor、Nova-consoleauth和nova-console等模块。块存储Cinder项目和网络服务Neutron独立后，OpenStack中三大核心功能计算、存储和网络项目之间的逻辑架构如下图所示， Nova由负责不同功能的服务进程所构成，其对外提供的服务接口为REST API，而各个内部组件之间通过RPC消息传递机制进行通信。Nova中提供API请求处理功能的模块是Nova-API，由API服务进程来处理数据库读写请求、向其他服务组件发送RPC消息的请求和生成RPC调用应答的请求等。在Nova中，RPC消息机制通过oslo.messaging库实现，这里的oslo.messaging库是对消息队列系统的顶层抽象。Nova中的大部分服务组件，除了Nova-Compute，都能以分布式的方式运行在多台服务器上，并且各服务组件之间会使用一个Manager进程来监听RPC消息。Nova-Compute组件的特别之处在于其作为一个独立进程运行在某个Nova-Compute管理下的Hypervisor上，从RPC消息机制而言，Nova-Compute进程通过专有队列进行消息订阅，而其他Nova组件则可以通过共享队列订阅消息。关于Nova与RPC消息机制之间的更多细节，可以参考前面《2、中间件服务—消息队列RabbitMQ》的介绍。 Nova在设计过程中使用了中心化数据库思想，即各个服务组件共同使用相同的数据库。不过，为了便于用户快速升级OpenStack版本，服务组件对数据库的访问经过了一个对象层，其主要作用在于解耦数据库与Nova组件之间的强关联，使得已经升级完成的服务组件仍然可以与运行在老版本下的Nova-Compute服务通信。为了实现这一功能，Nova采用了中心化的管理组件Nova-Conductor来代理Nova-Compute对数据库的RPC请求**。**在大规模Nova-Compute部署过程中，RPC消息机制是最可能的集群瓶颈。为了解决这一问题，Nova在水平扩展时采用了一种称为Cell的部署方式。Cell有父Cell和子Cell之分，并且不同的Cell内部使用不同的消息队列系统。 在实际的OpenStack系统运行过程中，计算服务Nova通过与认证授权服务Keystone交互从而实现身份权限的识别认证过程，并通过OpenStack的镜像服务Glance为实例提供系统镜像，而用户和云管理员则通过OpenStack的控制面板服务Horizon与Nova进行交互。此外，Nova计算资源的使用限额（Qoutas）以项目（Project）为单位进行限制，而镜像资源的访问则通过项目和用户来限制。OpenStack的计算服务Nova有如下组件构成： nova-api：Nova核心组件。负责接收和响应终端用户对计算资源发起的API调用请求，如WSGI APP的路由请求和授权相关请求。nova-api接收到请求后，通常将请求转发给Nova服务的其他组件，如nova-scheduler。nova-api除了支持OpenStack的API请求外，还支持Amazon的EC2 API请求，nova-api的内部逻辑如下图所示。 如上图，nova-api层提供了三种服务：支持OpenStack API调用的osapi_compute，支持Amazon API调用的ec2以及metadata元数据服务（主要提供虚拟机相关信息的获取，比如虚拟机内置了cloud-init组件后，启动时会从metadata服务获取虚拟机的名称等信息设置到虚拟机内部）。整个nova-api的架构类似洋葱结构一样，从外部到内核一层一层包裹着，中间的每层是一个filter对象，用于对消息的处理再加工（比如增加request_id）或者在消息真正被处理前做一些其他事情（比如记录access日志），最终内核就是app，真正消息的处理者。App会根据消息请求的资源和action等信息route到注册的controller执行，每个controller都处理一个资源的action，如创建，删除，更新，查询等。也可以定位其他的action，如对虚拟机的迁移等，即承担虚拟机生命周期管理的入口。 nova-conductor：Nova核心组件。nova-conductor主要起到nova-compute服务与数据库之间的交互承接作用，其在nova-compute的顶层实现了一个新的对象层以防止Nova-Compute直接访问数据库带来的安全风险。nova-conductor的内部逻辑如下图所示。 在实际运行中，nova-compute并不直接读写访问数据库，而是通过nova-conductor实现数据库访问。nova-conductor组件可以水平扩展到多个节点上同时运行，但是nova-conductor不能部署到运行nova-compute的计算节点上，否则将不能隔离nova-Compute对数据库的直接访问，从而不能真正起到降低数据安全风险的作用。除此之外，nova-conductor还负责Nova服务的复杂流程控制，比如创建、冷迁移、热迁移、虚拟机规格调整和虚拟机重建等，以及与其他组件心跳信息定时写入等功能。需要注意一点，nova-compute组件只有在nova-conductor组件正常启动之后才能启动，即nova-compute组件的启动依赖nova-conductor。 nova-scheduler：Nova核心组件。主要负责从队列中截取虚拟机实例创建请求，依据默认或者用户自定义设置的过滤算法（根据计算节点的CPU、内存和磁盘等参数过滤）从计算节点集群中选取某个节点，并将虚拟机实例创建请求转发到该计算节点上执行，即最终的虚拟机将运行在该计算节点上。采用的过滤算法也可以根据需求自定义并在nova.conf配置文件中指定，如在配置Host Aggregate功能时，通常就需要更改默认的Scheduler规则。nova-scheduler的内部逻辑如下图所示。 上图中，nova-scheduler选择计算节点主要采用过滤和权重两步进行。过滤就是通过过滤器选择符合条件的节点，权重就是通过权重对比选择最优的节点，默认的权重取计算节点的剩余内存大小，也可以自定义其他通用参数，比如CPU和磁盘空间等。如果在创建虚拟机时，指定主机或主机组创建，则权重这一步骤就会失效，根据过滤器中主机或主机组过滤规则直接选定某个主机进行创建。 nova-compute：Nova核心组件。主要功能是接收来自队列的请求，并执行一系列系统命令，如创建一个KVM虚拟机实例并在数据库中更新对应实例的状态等。nova-compute的内部逻辑如下图所示。 上图中，nova-compute组件实际上运行在计算节点上一个进程，其内部逻辑分为两个部分：Manager和Driver。Manager主要负责各类操作指令接收，并下发给Hypervisor去执行，同时通过resource_tracker进程从Hypervisor收取各节点的资源信息，并通过AMQP与OpenStack的其他服务进行交互，比如与Glance交互获取创建虚拟机时需要的镜像Image，与Cinder交互获取创建虚拟机时需要的虚拟磁盘Volume，与Neutron交互获取创建虚拟机时需要的IP和Port信息等等。Driver组件用于对接底层异构的Hypervisor，常见的Hypervisor API有支持KVM/QEMU虚拟化引擎的Libvirt API、支持XenServer/XCP虚拟化引擎的XenAPI和支持VMware虚拟化引擎的VMware API。OpenStack默认使用的是KVM虚拟化引擎，因此在OpenStack nova-compute中最常使用的还是Libvirt API。因此，nova-compute组件内部工作流程比较复杂，主要完成以下功能： 虚拟机生命周期操作的真正执行者worker，通过调用不同Hypervisor的Driver实现虚拟机的生命周期管理指令执行； 底层可以对接不同虚拟化平台，实现不同虚拟化解决方案的异构，不仅支持虚拟机，还支持裸机和容器； 内置周期性的任务调度，完成计算节点资源的刷新，并通过nova-conductor将计算节点资源状态写入nova-db，便于nova-scheduler在调度时查询nova-db获取最新的节点资源状态。 在各Hypervisor上以插件的方式植入resource_tracker资源管理模块，配合周期性的任务调度完成资源状态的统计； Nova服务的核心组件除了上述组件之外，还有一个nova-cert组件，该组件主要用于与Amazon EC2 API对接时启用，是一个服务器的守护进程，负责为基于X509认证的Nova Cert提供服务，通常用于对euca-bundle-image镜像生成X509证书。除此之外，还有其他一些便于虚拟机维护的支撑组件—虚拟机控制台服务，包括：nova-consoleauth、nova-novncproxy、nova-xvpvncproxy等，其主要功能如下： nova-consoleauth：虚拟机控制台服务。nova-consoleauth主要为虚拟机控制台连接提供认证授权服务。在运行VNC代理服务的OpenStack集群中，必须运行nova-consoleauth服务。一个nova-consoleauth实例可为多种类型的代理服务提供认证授权服务。需要注意的是，不要混淆nova-consoleauth与nova-console，后者是XenAPI风格的控制台服务，而目前多数VNC代理软件都已经不再使用nova-console。 nova-novncproxy：虚拟机控制台服务。nova-novncproxy主要提供对运行状态中的实例进行VNC连接访问的代理。其直接基于网页的novnc客户端连接，即支持基于Web网页的实例访问，是个非常方便的功能。 nova-xvpvncproxy：虚拟机控制台服务。nova-xvpvncproxy主要提供对运行状态中的实例进行VNC连接访问的代理，其仅支持特定于OpenStack的Java客户端发起的VNC连接。 Nova中的几个重要概念Nova服务是OpenStack所有服务中最复杂的一个组件，没有之一。不仅涉及虚拟机的一些重要概念，比如server、flavor，还包括主机服务器的一些概念，比如host、hypervisor，甚至还有一些数据中心DC规划方面的概念，比如Region、cell等等。因此，弄清楚这些概念是理解Nova服务的关键。 1）虚拟机实例方面的概念包括：server/instance、server metadata、flavor、quota、server group、bdm，具体如下： server/instance：Nova中最重要的数据对象，本质上就是虚拟机实例，是Nova管理提供的云服务资源。 server metadata：虚拟机实例的元数据信息，key-value格式，用于对虚拟机附加必要的描述等信息，比如虚拟机实例名、当前状态、部署位置等等。 flavor：虚拟机实例规格模版，用于定义一类虚拟机实例所占用的资源要求，比如2C8G40G，表示使用该flavor创建的虚拟机实例需要2vCPU，8G RAM，40G根磁盘。flavor只能由系统管理员admin创建，供普通用户/租户在创建虚拟机时调用。 quota：资源配额，用于指定租户最多能够使用的资源上限。 server group：虚拟机实例的亲和性/反亲和性组。同一个亲和性组的虚拟机在创建时会被调度到相同的物理主机上，而反亲和性组，顾名思义，在创建时，同一个反亲和性组的虚拟机实例会被调度不同的主机上。 bdm：Block Device Mapping，块存储设备映射，用于描述虚拟机实例拥有的存储设备信息。 2）主机服务器方面的概念包括：hypervisor/node、host**，具体如下： hypervisor/node：即安装虚拟化组件的物理主机，对于KVM、Xen等虚拟化解决方案，一个node即对应一个物理主机；对于VMware虚拟化解决方案，一个node对应同一个vCenter下的一个CLuster。 host：物理主机。对于KVM、Xen等虚拟化解决方案，一个host对应一个物理主机，同时对应一个node；对于VMWare的虚拟化解决方案，一个host对应一套vCenter，至少包含一个Cluster，一个Cluster至少包括一个物理主机。 3）数据中心DC规划方面的概念包括：Region、Cell、AZ和HA（Host Aggregate），具体如下： Region：是地理位置上隔离的数据中心区域，可以简单理解一个Region就代表一个数据中心DC。不同的Region是彼此独立的，即某个Region范围的人为或自然灾害并不会影响其他Region。Region的概念通常在公有云中出现，因为Region的多少是衡量一个公有云服务提供商运营能力的关键指标。而对于我们云化网络来说，要想实现自身业务的容灾和高可用设计，通常需要将同一个业务系统部署到不同的Region中，比如西安和汉中不同的DC中各部署一套，同时要借助负载均衡器才能实现容灾双活的功能。Region在实际部署中示意图如下所示： 在具体的实现过程中，不同Region通常共用相同的认证服务和控制面板服务，可以通过共享存储池进行数据复制同步来实现高可用。下图就是OpenStack官方推荐的私有云多区域部署架构，Region1和Region2共享Horizon、Keystone和Swift服务，其他OpenStack服务在各自Region中独立部署。所以，不同Region内部相同服务就会有不同的endpoint API，对不同Region内部服务的访问就需制定不同的API，在实际生产中，一般是通过负责均衡器实现。需要注意一点：在OpenStack的众多服务中，并不是所有服务都支持跨Region部署，除了Horizon、Keystone和Swift外，其余服务都被设计为在同一个Region运行。如对于计算资源的分区，OpenStack仅支持Cell部署、AZ和Host Aggregate划分，并不能跨Region；网络管理服务仅能管理相同广播域或者互联集中的网络资源；块存储管理服务也只能管理单个Region内相同存储网络内的存储资源。 Cell：Nova服务中的Cell功能模块在OpenStack的G版本提出，主要为了解决大规模Nova计算节点部署过程中可能带来的集群瓶颈问题。原来社区中有过大范围讨论，一致认为在计算节点数目超过500个时，便会遇到共享消息队列系统的性能问题。在早期的OpenStack版本中，nova cell的架构为V1版本，以树型结构为基础，由一个API-Cell（父Cell）与多个Child-Cell构成。其中，API-Cell只运行Nova-API服务，而每个Child-Cell运行除nova-api以外的全部Nova服务，且每个Child-Cell运行自己的消息队列、数据库及Nova-Cells服务。这种架构，用户请求需要经历两层调度实现，即顶层的API-Cell在多个Compute Cell之间调度和各个Compute Cell内部的主机调度，不同的Cell之间通过nova-Cells服务进行消息传递。有很多bug，且社区维护人员基本为0，属于冷门。如下图所示，其基本流程就是：在启用nova-cell v1的OpenStack集群中，用户创建虚机的请求首先到达顶层API Cell，然后通过API Cell的调度算法决定虚机由哪个Cell负责创建，当某个Compute Cell接收到来自API Cell的请求后，Compute Cell将把这个请求通过Nova-Cells服务传递到Cell中的nova-scheduler服务进行主机调度，Compute Cell中的nova-scheduler服务接收到请求后，再根据主机资源统计信息将虚机创建某一台服务器上。 在2016年上半年的Austin峰会上，Nova核心领导成员发布了Nova Cell v2版本，并在OpenStack的N版本及后续版本中默认启动。与V1版本不同，V2版本采用单层调度的思想。即用户请求只需要通过一层调度即可抵达最终的物理服务器。也就是说API Cell不仅在Compute Cell之间进行调度，还同时对选定的Compute Cell内部的主机进行调度。在每一个Compute Cell中，消息队列和数据库独立实现，同时Cell v2版本将数据库进行分离实现，即将整个OpenStack的全局信息保存在API Cell的数据库中，而虚机的相关资源数据保存在各个独立的Cell的数据库中。同时，在设计上还新增了Cell0模块，一旦API Cell对全部Compute Cell调度失败，则用户请求暂时被放在Cell0中。其基本原理和流程如下，api和cell有了明显的边界 ，api层面只需要数据库，不需要Message Queue，nova-api只创建和依赖 nova_api和nova_cell0两个数据库，nova-scheduler服务只需要在api层面上安装 ，cell不需要参数调度 。这样实现了一次调度就可以确定到具体在哪个cell的哪台机器上启动。各个cell中里面只需要安装nova-compute和nova-conductor服务以及其依赖的DB和MQ，nova-api根据nova-scheduler的调度结果会直接连接相关cell的MQ和DB,，所以不需要类似nova-cell 这样的额外子服务存在，性能上也会有及大的提升。 上面的nova-scheduler调度各个cell本质上是依赖placement 服务获取各个cell的资源使用情况从而实现调度。placement一个比较独立的REST API栈，主要为了追踪记录resources provider目录和resource使用情况。例如，resource provider可以是一个计算节点、共享存储池或是IP地址池。placement 服务追踪每种resource provider的服务目录，使用情况。这样就可以从placement API获取resource provider目录，并获取resource provider的资源使用情况。这里需要重点提一下nova_cell0数据库，从nova_cell0数据库与nova数据库的数据表对比发现（如下图），nova_cell0数据库的schema和nova是一样的，其存在的主要用途为：当VM调度失败时，VM的信息不属于任何一个cell时， 可以放到cell0上面 。因此，nova_cell0这里面的数据并不是太重要 。 Cell相关的数据库表都在nova_api里面，包括cell_mappings, host_mappings, instance_mappings。其表结构如下 ，cell_mappings表包含了cell的Database和Mesage Queue连接信息（transport_url和database_connection），用于和子cell 通讯。host_mappings是用于存储nova-scheduler可以调度的主机物理节点的信息。instance_mapping表里有所有instance id，这样在查询instance时，就可以从这个表里查到他所在的cell_id， 然后通过cell_mappings表就可以查询到对应cell的具体信息 。 这里其实也有一个坑，之前 nova-compute 启动起来，就可以直接使用了，但是cell v2之后就需要手动运行nova-manage cell_v2 discover_host，把host mapping映射到cell_mappings表里面 ，那台计算节点才会加入到调度中 。因此，在小型一些的环境上，推荐打开自动发现功能 ，就不用手动跑命令了。比如，我们在配置控制节点的nova.conf文件中，设置Scheduler的会话的discover_hosts_in_cells_interval = 300，表示每5分钟就进行计算节点扫描，同步cell_mappings数据库表。如下图，我们当前的环境规划了一个cell1，该cell下包含两个计算主机：rocky-controller和rocky-compute，host_mappings表与cell_mappings表的关联就是通过cell_id主键进行关联。同理，我们也可以通过虚拟机实例instance_id，查询到关联的cell_id，通过cell_id查询到cell的具体信息及内部主机节点的信息。 AZ：可用域（Availability Zone，AZ）是对计算资源的额另一种划分，在AWS的区域划分设计中，AZ是对Region的再次划分。按照AWS解释，划分AZ的主要目的是为了提高容灾和提供廉价的故障隔离服务。在OpenStack中，我们会启动nova cell功能，一般是将单个Region部署为compute cell，然后再到cell中去划分AZ。注意：AZ与cell不同，AZ中没有独立的数据库和消息队列，是一种物理资源的划分。多个AZ共享cell内部的消息队列和数据库服务。因此，OpenStack中的AZ主要起到故障隔离的作用，一般在数据中心规划时，按照动力、网络、存储等物理资源故障隔离的角度将多个物理主机划分为一个AZ，在部署OpenStack时，可以将一个AZ对应一个Cell，然后在数据库中创建虚拟资源与物理资源的映射关系。下面就是AZ的逻辑架构图。 HA：主机聚合（Host Aggregate），是OpenStack平台管理员用户进行主机划分的另一种方法，即管理员可以根据硬件资源的某一属性，将具有相同硬件属性的服务器归类划分的方式。其对普通用户是不可见的。这里举个例子说明下。比如运营商的DC中部署了不同类型的服务器，有些服务器具有强大的计算能力，有些服务器不仅有强大的计算能力其IO能力也很强大，而另一些服务器配置高速网卡具有强大的网络吞吐能力。这时，作为运营商的云平台管理员就可以对这些服务器进行主机聚合划分，比如将具有强大计算能力的服务器放在主机聚合A中，将既有强大计算能力同时具备强大IO能力的服务器放在主机聚合B中，将具备高速网卡的服务器放在主句聚合C中。此时，如果某企业用户想要建设一套高性能、高可用集群的私有云业务，而高性能、高可用集群通常有“胖节点”和“瘦节点”之分（“胖节点”既充当计算也充当IO转发的角色，“瘦节点”只充当计算的角色），这样运营商就可以从主机聚合A中选择创建虚机提供“胖节点”，从主机聚合B中选择创建虚机提供“瘦节点”，同时进行差异性的服务收费。除了上述计算能力、IO能力、网路吞吐能力可以用来做聚合参考外，服务器硬盘类型、内存大小等都可用来主机聚合的参考。同一个Host可以规划在多个HA中，但是必须属于同一个AZ。 在OpenStack的解决方案，上述数据中心规划方面的概念，对资源的使用范围从大到小的排序为：Region&gt;Cell&gt;AZ&gt;HA。 Nova中的主机调度策略在Nova的各个服务组件中，Nova-scheduler是个非常关键的组件，其主要作用便是为Nova的主机选取提供智能决策功能。当Nova客户端发起创建实例请求时，Nova-API会将请求转发到Nova-scheduler，由Nova-scheduler在运行Nova-compute的计算节点中选取用于创建符合请求虚拟机资源条件的宿主机。Nova-scheduler对宿主机的选取分为两个步骤：第一步从计算节点集群中选取符合请求虚拟机资源条件的全部节点，这一过程称为过滤（Filter）；第二步从符合创建请求虚拟机的计算节点中选取唯一最佳的计算节点，作为本次请求虚拟机的宿主机，这一过程称为加权（Weigh）。在Nova-scheduler的过滤阶段，Nova的Filter Scheduler根据配置文件nova.conf中设置的策略集合对OpenStack集群中全部计算节点进行过滤迭代，通过三个参数来配置，即scheduler_driver、scheduler_available_filters和scheduler_default_filters，这三个参数都在/etc/nova/nova.conf文件的DEFAULT段配置，其配置项默认值如下： scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler ,这里表示使用默认FilterScheduler驱动，使用/usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py文件中的FilterScheduler类，如下图所示。 scheduler_available_filters = nova.scheduler.filters.all_filters，这里表示可以使用的Filter，默认使用全部filter，所支持的filter在/usr/lib/python2.7/site-packages/nova/scheduler/filters/目录下，选择all_filters表示全部。该目录下的文件列表如下： scheduler_default_filters = RetryFilter, AvailabilityZoneFilter，这里显式制定要使用的过滤器filter，多个过滤器之间用逗号“,”隔开。 在上面的是三个配置项中，配置参数scheduler_driver用于设置使用的Scheduler，FilterScheduler是Nova-scheduler默认使用的Scheduler，此外，Nova-scheduler允许使用第三方的Sched-uler，只需设置scheduler_driver即可。配置参数scheduler_available_filters用于指定可以使用的过滤器，默认情况下Nova自带的全部过滤器均可被使用（nova.scheduler.filters.all_filters）。可用过滤器参数可以重复指定，如用户自己实现了一个过滤器myfilter.WarriorFilter，然后用户既想使用Nova自带的过滤器又想使用自建的过滤器，可以配置多行scheduler_available_filters实现。配置参数scheduler_default_filters用于指定使用的过滤器列表。在进行主机过滤时，主机会被此参数指定的列表过滤器顺序过滤一遍。在主机进入每个过滤器时，如果主机条件满足请求中的实例需求，则返回True，否则返回False。当返回False后，该主机的过滤流也就结束，不再进入后续过滤器继续过滤。Nova-Scheduler内建自带的过滤器主要包括以下几种： AggregateCoreFilter：以Aggregate为前缀的过滤器通常只在创建了Host Aggregate的情况下才会使用到。AggregateCoreFilter表示通过主机CPU核数来过滤每个Aggregate中的主机。主机可用CPU核心数通过每个主机集配置文件中的cpu_allocation_ratio值（默认为16）来计算。如果主机CPU核心数满足请求创建实例的vCPUS需求，则返回True。如果当前主机属于多个Host Aggregate，并且有多个cpu_allocation_ratio值，则使用cpu_allocation_ratio的最小值。cpu_allocation_ratio参数主要用于设置主机CPU的overcommit，如cpu_allocation_ratio=16，而主机vCPUs为8，则调度时Scheduler认为主机可用vCPU为128。 AggregateDiskFilter：表示通过主机磁盘使用率来过滤每个Aggregate中的主机。主机磁盘使用率会根据每个主机集配置文件中的disk_allocation_ratio（默认值为1，即按实际磁盘容量来调度）参数来计算，如果OpenStack集群中没有创建主机集，则disk_allocation_ratio便会成为全局性的参数。如果主机集中主机上的可用磁盘容量（单位为GB）满足请求虚拟机中的磁盘需求（镜像系统盘大小+临时磁盘大小），则返回True。 AggregateImagePropertiesIsolation：表示根据实例创建请求中的镜像属性来过滤主机集中的主机，通常用于将基于特定镜像的实例创建到与镜像匹配的特定主机上。镜像属性与主机匹配与否根据如下规则来判断：如果当前主机属于某个主机集，并且此主机集定义了相应的元数据来匹配某个镜像的属性，则认为该主机与镜像属性匹配，并且该主机将成为启动这个匹配镜像的候选主机；如果当前主机不属于任何主机集，则该主机可以启动任何镜像。 AggregateInstanceExtraSpecsFilter：表示匹配的是Nova中Flavor与主机集的元数据，通常需要为Flavor的属性aggregate_instance_extra_specs设置一个属性值。过滤器将会根据主机集的元数据与实例创建请求中Flavor的aggregate_instance_extra_specs属性值来判断主机是否符合要求。 AggregateIoOpsFilter：表示根据主机的IO负载来过滤每个主机集中的主机。主机IO负载根据参数max_io_ops_per_host值（默认为8）来决定，此过滤器会将IO负载过高的主机过滤掉。 AggregateNumInstancesFilter：表示根据主机已有实例数目来过滤每个主机集中的主机，过滤标准为主机集中允许的最大主机实例数目参数max_instances_per_host（默认50）值，当前运行实例数目超过此参数值的主机将会被过滤掉。 AggregateMultiTenancyIsolation：用于将特定某个租户或多个租户的实例创建到特定的主机集或可用域中。如果主机属于某个具有元数据且key为filter_tenant_id的主机集，则对应filter_tenant_id值的某个租户或多个租户发起的创建实例请求将会在该主机上创建实例。而如果主机不属于元数据key为filter_tenant_id的主机集，则所有组合都可以在该主机上创建实例。AggregateMultiTenancyIsolation过滤器并不会将具有元数据Key为filter_tenant_id的主机集与其他租户隔离，任何租户仍然可以在指定的主机集上创建实例。 AggregateRamFilter：表示通过主机可用内存RAM来过滤每个主机集中的主机，主机的可用内存根据主机集配置文件中ram_allocation_ratio值（默认为1.5）来计算。 AggregateTypeAffinityFilter：用于通过主机集的instance_type键来过滤主机，如果主机集没有设置Key为instance_type的元数据，或者将主机集中key为instance_type的元数据值设为实例创建请求中所包含的instance_type值，则请求中的instance_type的元数据值可以是单个字符串或者逗号分开的多个字符串，如m1.nano或m1.nano，m1.small。 AllHostsFilter：不会发生任何过滤操作，所有正常运行的计算节点都会通过该过滤器。 AvailabilityZoneFilter：是可用域AZ过滤器，当在请求中指定创建实例的AZ时，必须在Scheduler中使用该过滤器。 ComputeCapabilitiesFilter：与AggregateInstanceExtraSpecsFilter类似，也是通过设置Nova中主机类型的元数据来对主机进行过滤，不过ComputeCapabilitiesFilter过滤器中与Flavor的元数据匹配的对象不是主机集中的metadata，而是主机的Capabilities。Capabilities可以是主机的Architecture、Cores、Features、Model和Vendor等，在使用ComputeCapabi-litiesFilter过滤器时，Flavors的extra specs格式为“Capabilities：key：value”形式，“Capabilities”表示命名空间，“key：value”表示extra specs键值对。如extra specs为“Capabilities：architecture：x86”的Flavor在创建实例时，ComputeCapabilitiesFilter只会允许X86架构的主机通过，如果命名空间不是“Capabilities”，则Flavors的extra specs中命名空间字段会被忽略而仅提取“key：value”部分。在使用ComputeCapabilitiesFilter过滤器时，强烈建议不要省略命名空间（extra specs中第一个冒号前的字符串），否则将会与AggregateInstanceExtraSpecsFilter过滤器冲突（两个过滤器同时启用的情况下），同时建议将命名空间字符串设为Capabilities。 ComputeFilter：意味着正常运行nova-compute服务的计算节点才能被Nova-scheduler调度，通常而言，ComputeFilter是必须的过滤器。 CoreFilter：CoreFilter与AggregateCoreFilter过滤器类似，按主机CPU核数来过滤主机。如果未启用此过滤器，则Scheduler调度给实例的CPU核数会超出物理主机的实际CPU核数，即运行在实例上的虚拟CPU核数之和可以超出物理主机CPU核数。通过启用CoreFilter过滤器，并且设置超额使用比率参数cpu_allocation_ratio，限定实例可以超额使用的虚拟CPU核数。例如在启用CoreFilter的情况下，cpu_allocation_ratio设置为8，主机物理CPU核数为8，则该主机可供实例使用的全部vCPU核数为64。如果希望实例vCPU与主机物理CPU为1：1的分配使用关系，则可以设置cpu_allocation_ratio为1。 DifferentHostFilter：将请求实例创建在与请求所指定的实例主机不同的主机上，即指定新创建的实例不能位于某个或多个实例的宿主机上。如Nova中已经有两个实例，并且用户希望新创建的实例不能与这两个已有实例的宿主机相同，则可以启用DifferentHostFilter过滤器。 DiskFilter：与AggregateDiskFilter过滤器类似，即仅调度磁盘空间满足请求实例的root和临时存储空间的主机。在Nova中，主机磁盘可以超额使用，可以通过配置磁盘超额使用比率参数disk_allocation_ratio来限定调度器可见的最大虚拟主机磁盘容量。在配置文件nova.conf中，disk_allocation_ratio的默认值为1，即不允许主机磁盘空间被超额分配，在调度时以主机的实际可用磁盘容量为准。此外，需要注意的是，Scheduler提取的主机磁盘空间并不是hypervisor统计结果中free_disk_gb的值，而是disk_available_least的值。 RamFilter：RamFilter过滤器将根据主机是否有足够的可用RAM来进行过滤。如果未启用RamFilter过滤器，则Scheduler会从主机上提取超额的RAM，即分配给实例的RAM之和会大于主机物理RAM容量。在启用RamFilter后，可以通过配置RAM超额使用比率参数ram_allocation_ratio的值来限定该主机可用的最大虚拟内存值，ram_allocation_ratio的默认值为1.5，因此如果当前主机可用物理RAM为10GB，则Scheduler提取到的主机RAM为15GB。 JsonFilter：JsonFilter过滤器允许用户通过Json格式的Scheduler调度提示（hint）来自定义主机调度。JsonFilter过滤器允许的操作符有=、&gt;、&lt;、in、&lt;=、&gt;=、not、or和and，允许使用的变量有$free_ram_mb、$free_disk_mb、$total_usable_ram_mb、$vcpus_total和$vcpus_used。如希望将实例创建在主机可用RAM大于等于2GB的主机上，则在启用JsonFilter过滤器的情况下，使用如下实例创建语句：nova boot –image 827d564a-e636-4fc4-a376-d36f7ebe1747 –flavor 1 –hint query=’[“&gt;=”,”$free_ram_mb”,2048]’ VM01 RetryFilter：用于将之前已经调度过的主机过滤掉。此过滤器只有在主机过滤重复参数scheduler_max_attempts值大于1的情况下才有效。如A、B、C三台主机在某次过滤后均符合标准，并且A主机因为权重最优而被选为创建实例的主机，但是由于某些原因实例创建失败，同时参数scheduler_max_attempts值大于1，因此Scheduler将进行再次调度，为了避免再次失败，当启用RetryFilter过滤器时，A主机将不再参与调度。为了避免反复过滤失败主机，RetryFilter通常是默认过滤器列表中的第一个过滤器。 SameHostFilter：与DifferentHostFilter相反，即将请求实例创建在指定实例的宿主机上，使得要创建的实例与一个或多个已经存在的实例位于同一主机上。要使用此过滤器，客户端在创建实例时需要通过hint传入same_host键，键值为特定实例的UUID。 ServerGroupAffinityFilter：与GroupAffinityFiltre功能相同，而GroupAffinityFilter即将被淘汰，过滤器ServerGroupAffinityFilter的作用是将请求实例调度到指定的Server Group中（此处的Server表示虚拟机），即将实例尽量创建到同一台主机上。要使用此过滤器，用户必须使用“affinity”策略预先创建一个服务器组（实例组），并在创建实例时通过hint参数的group键值对指定该实例属于此服务器组，group键的值为服务器组的UUID。 ServerGroupAntiAffinityFilter：与ServerGroupAffinityFilter正好相反，以前的过滤器GroupAntiAffinityFilter即将被丢弃，ServerGroupAntiAffinityFilter表示将实例尽量不要创建在同一台主机上。要启用此过滤器，用户需要使用“anti-affinity”策略预先创建服务器组，并在创建实例时通过hint参数的group键值对指定该实例属于此服务器组，group键的值为服务器组的UUID。 SimpleCIDRAffinityFilter：是一个基于网络的过滤器，主要用于根据创建实例时指定的IP地址范围来过滤主机。要使用此过滤器，用户在创建实例时必须指定两个hint参数，hint参数的两个Key分别为build_near_host_ip和cidr，第一个键的值为有效的IP地址，第二个键的值为CIDR格式的掩码。 在经过SchedulerFilter中一系列的过滤器过滤之后，Nova-scheduler会选出符合请求实例资源的主机，但是如果有多台主机同时符合要求，则经过SchedulerFilter之后，仍然会得到一个主机列表。为了得到最终的实例创建主机，Nova-scheduler会给过滤后的主机列表中的计算节点进行“评分”，分数最高的主机将成为最终的实例创建主机，这个“评分”的过程即是所谓的权重计算。权重计算就是给有效主机列表中的主机进行权重赋值并选出最优权重值主机的过程。每个计算节点都可以通过不同的维度参数来进行评估，如内存、磁盘、CPU和IO负载，由于各个权重的单位不同，如内存单位为MB，磁盘单位为GB，CPU单位为核数等，因此不能对这些权重进行简单的线性相加来获取计算节点最终的权重值。同时，不同的权重对用户而言可能重要程度并不一样（如A用户认为内存比CPU重要，而B用户认为CPU比内存更重要）。为了解决这一问题，需要预先为每个权重定义关联的权重因子（multiplier），如果认为此权重要优于其他权重，则可以为其设置较大的权重因子，nova.conf配置文件中与权重相关的默认参数配置在DEFAULT会话中，如下： 1234567891011121314151617181920212223[DEFAULT]scheduler_host_subset_size = 1 #&lt;===按照排序后主机列表仅选取一台主机scheduler_weight_classes = nova.scheduler.weights.all_weighersram_weight_multiplier = 1.0 #&lt;===权重因子io_ops_weight_multiplier = 2.0 #&lt;===权重因子soft_affinity_weight_multiplier = 1.0 #&lt;===权重因子soft_anti_affinity_weight_multiplier = 1.0 #&lt;===权重因子[metrics]weight_multiplier = 1.0weight_setting = var_name1=1.0, var_name2=-1.0required = falseweight_of_unavailable = -10000.0 最后，各个权重与其权重因子的乘积之和便是最终的计算节点权重值。需要指出的是，各个权重在参与计算之前会事先进行标准化（normalize），因此，节点最终的权重计算公式为：weight = w1_multiplier * norm（w1） + w2_multiplier * norm（w2） + ……，这里的norm(w1)就是标准化的权重值，原始权重值的标准化公式为：ratio_weight =（raw_weight - min_weight）/（max_weight - min_weight）。现假设有host1、host2、host3、host4、host5和host6六台主机需要进行权重排序，并且有3个权重，分别为W1、W2和W3，而3个权重对应的权重因子分别为1、1、2。六台主机经过W1、W2和W3计算后得到的原始权重值分别如下： 主机名称 HOST-01 HOST-02 HOST-03 HOST-04 HOST-05 HOST-06 W1原始权重 110 20 10 80 50 90 W2原始权重 4 2 1 11 6 8 W3原始权重 5 25 10 15 5 20 根据标准化计算公式，六台主机针对W1、W2和W3原始权重值进行标准化计算后的结果分别如下： 主机名称 HOST-01 HOST-02 HOST-03 HOST-04 HOST-05 HOST-06 W1标准化权重 1 0.1 0 0.7 0.4 0.8 W2标准化权重 0.3 0.1 0 1 0.5 0.7 W3标准化权重 0 1 0.25 0.5 0 0.75 由于W1、W2和W3的权重因子分别为1、1、2，因此根据节点最终的权重计算公式，六台主机节点的最终权重值分别如下： 主机名称 HOST-01 HOST-02 HOST-03 HOST-04 HOST-05 HOST-06 weight 1.3 2.2 0.5 2.7 0.9 3.0 对主机列表中的主机依据各自权重值进行降序排列后，得到的排序结果为host6、host4、host2、host1、host5、host3，因此根据权重排序后的最优节点应该是host6节点。如果各个权重的权重因子为负数，则host3节点为最优节点。关于节点加权排序的过程描述，OpenStack的官方网站给出了形象图示，如下： 在实际使用中，FilterScheduler根据主机配置文件nova.conf中的scheduler_weight_classes配置参数对主机进行不同的加权计算，此参数的默认配置为：scheduler_weight_classes=nova.scheduler.weights.all_weighers，即默认使用全部Nova自带的Weigher，也可以自定义scheduler_weight_classes参数值。Nova自带的常用Weigher包括：RAMWeigher、DiskWeigher、MetricsWeigher、IoOpsWeigher、ServerGroupSoftAffinity-Weigher和ServerGroupSoftAntiAffinityWeigher，其具体含义如下： RAMWeigher：表示根据计算节点的可用RAM进行权重计算，剩余可用RAM最大的节点具有最优权重值。如果权重因子是负数，则剩余可用RAM最小的节点具有最优权重值。 DiskWeigher：表示根据计算节点的可用磁盘空间进行权重计算，剩余可用磁盘空间最大的节点具有最优权重值。如果权重因子是负数，则剩余可用磁盘空间最小的节点具有最优权重值。 MetricsWeigher：表示根据计算节点的各自定义指标进行权重计算，要成为计算权重的自定义指标参数需要在配置文件中指定，设置语法如下： metrics_weight_setting = var_name1=1.0, var_name2=-1.0 IoOpsWeigher：表示根据计算节点的工作负载来计算权重值，默认选择负载较轻的计算节点作为最优节点，如果权重因子是正数，则会认为负载较大的节点最优，即此时效果刚好也默认情况相反。 ServerGroupSoftAffinityWeigher：表示根据主机上运行在服务器组（Server Group）中的实例数目来计算节点权重值，实例将会创建到权重值最大的主机上。在使用该Weigher时，只有对应的权重因子设为正数才有效，因为负权重因子意味着不要将实例创建在同一台主机上，正好与目标结果相反。 ServerGroupSoftAntiAffinityWeigher：表示根据主机上运行在服务器组（Server Group）中的实例数目来计算节点权重值，其权重值为负数，实例将会创建到权重值最大的主机上（意味着服务器组中的实例数目最少）。在使用该Weigher时，只有对应的权重因子设为负数才有效，因为负权重因子意味着将实例尽量创建在同一台主机上，正好与目标结果相反。 Nova创建虚拟机实例流程分析在OpenStack中，创建实例主要有两种方式：Nova客户端命令行和Dashboard用户接口GUI界面。无论是哪种方式，其实例创建过程的后端调用流程本质上是相同的，都要经过Nova内部组件之间和Nova与其他服务项目之间的频繁交互才能完成实例的创建工作。正常情况下，要完成一个实例的创建，需要参与的OpenStack服务项目除了Nova之外，还有Dashboard、Keystone、Glance、Cinder和Neutron等项目，其中，Nova、Keystone、Glance和Neutron为必须项目，而Dashboard和Cinder为非必须。此外，在Nova内部，参与实例创建的组件包括nova-api、nova-conductor、nova-scheduler、nova-compute、nova-consoleauth、nova-novncproxy、nova-cert和nova-objectstore等，其中，nova-api、nova-conductor、nova-scheduler、nova-compute为必须使用到的组件。如果要使用虚拟网络控制台VNC与实例交互，则nova-consoleauth和nova-novncproxy是必须的；如果使用AWS风格API，则nova-cert和nova-objectstore是必须的。 在OpenStack中创建实例的大致流程为：用户通过Dashboard界面或命令行发起实例创建请求，Keystone从请求中获取用户相关信息并进行身份验证；验证通过后，用户获得认证Token，实例创建请求进入nova-api；在向Keystone验证用户Token有效后，nova-api将请求转入nova-scheduler；nova-scheduler进行实例创建目的主机的调度选择，目标主机选取完成后，请求转入nova-compute；nova-compute与nova-conductor交互以获取创建实例的信息，在成功获取实例信息后，nova-compute分别与Glance、Neutron和Cinder交互以获取镜像资源、网络资源和云存储资源；一切资源准备就绪后，nova-compute便通过Libvirt API与具体的Hypervisor交互并创建虚拟机。在创建虚拟机实例的过程中，各组件的交互流程如下图所示： Keystone的工作流程当Nova客户端发起实例创建请求时，Nova客户端便与Keystone进行交互以获取授权Token，如果Nova客户端通过Keystone的身份验证，则Keystone为其生成授权Token，并将此Token存储到后端数据库中。Keystone的后端数据库可以是关系型的SQL数据库，也可以是如Memcache这样的缓存系统。Keystone的身份验证过程可以在本地进行，也可以通过LDAP服务器进行身份验证。Nova客户端与Keystone交互进行身份验证和Token颁发（使用LDAP进行身份验证）流程如下： nova-api的工作流程Nova客户端在通过Keystone的验证并获取Token后，便向nova-api发出实例创建的请求，nova-api接收到请求后向Keystone验证Token的有效性，验证成功后，nova-api开始判断实例创建请求所携带的参数是否有效合法，如检查虚拟机名字是否符合命名规范，使用的虚拟机模板flavor_id在数据库中是否存在，使用的镜像image_uuid是否是有效的uuid格式等。检查instance、vCPU、RAM的数量是否超出了配置文件中的限制，通常每个Project拥有的资源都是有限的，如创建虚拟机的个数、vCPU个数、内存大小和volume个数等，这些限制是管理员通过与Project相关的Quota来设置的，如quota_instances、quota_cores、quota_ram、quota_volumes等。如果管理员未做设置更改，则默认情况下所有Project拥有相等的资源数量。此外，nova-api还会检查相关metadata的长度是否超过限制，inject_files的数目是否超过限制，一般情况下这些参数都为空，因此都能通过检查。实例请求中的网络、镜像和flavor也是nova-api主要的检查对象，其主要检查请求中的network是否存在且可用以及image和flavor是否存在且可用，同时还会检测请求中flavor的磁盘是否满足image需求等。当所有资源检查都通过后，nova-api便在nova数据库中初始化虚拟机相关的记录（intial entry），主要包括instance、block_device_mapping和quota等记录，并将instance记录中的vm_states字段设为building，task_state字段设为scheduling。之后，nova-api调用nova-conductor并将请求传递到消息队列（MQ）中。由于nova-api将请求以RPC cast的方式发送给nova-conductor，而cast（）方法发送的请求并不会返回消息，因此Nova客户端此时只会接收到请求已被接受的返回，但是具体的虚拟创建过程还在后台继续执行，而Nova客户端可以查到的虚拟机vmstate为building，task_state为scheduling。nova-api在实例创建过程的工作流程如下： nova-conductor工作流程由于nova-conductor提供了build_instances（）这个RPC方法，因此一直处于消息队列监听状态，一旦监听的队列有消息进入，nova-conductor便开始执行build_instances（）方法。nova-conductor还向nova-schduler发出RPC call调用，并要求其返回计算节点调度结果，在收到nova-scheduler的调度结果后，nova-conductor的build_instances（）方法将请求传递到nova-compute的消息队列中。nova-conductor在实例创建过程中的流程如下： nova-scheduler工作流程nova-scheduler的功能就是负责监听消息队列，并在获取消息队列的请求消息之后进行计算节点的调度选取，同时将调度结果传递给消息队列。为了获取创建实例的目标主机，nova-conductor会向nova-scheduler发起RPC call调用，并调用nova-scheduler的select_destin-ations（）方法；nova-scheduler在消息队列中接收到调用消息后，根据nova.conf配置文件中关于Filters和Weight的配置参数对计算节点主机列表进行过滤和加权调度，在这个过程中，nova-scheduler需要访问数据库以获取节点相关的信息；在获取信息后，nova-scheduler便开始进行节点调度，默认使用的调度驱动为FilterScheduler，调度完成之后，将节点调度结果传递到消息队列，以完成nova-conductor的RPC call调用过程。实际上，nova-scheduler从topic：scheduler消息队列获取调用信息后，开始执行select_destinations（）方法，由于Nova通常采用默认的FilterScheduler调度器，因此FilterScheduler的select_destinations（）和_scheduler（）方法被执行，_scheduler（）函数最终会调用获取主机信息的函数和各种节点过滤器以及加权方法以对计算节点进行过滤和计算权重值，并将最终调度选取的计算节点信息传递到消息队列，返回给nova-conductor。nova-scheduler在实例创建过程中的流程如下： nova-compute工作流程nova-compute是Nova服务项目中最复杂和最关键的组件，运行nova-compute的节点称为计算节点，通常nova-compute部署在独立的计算节点上，并与Nova项目的其他组件分开部署。在实例创建过程中，nova-compute随时监听topic：compute-computeN（computeN为计算节点名称）消息队列，在监听到nova-conductor传递的实例创建请求后，nova-compute开始启动内部工作流程以响应实例创建请求。在nova-api的工作流程中，请求中创建实例所需的镜像、网络和存储等资源已经做过有效性和可用性的检查，因此nova-compute将直接与Glance交互以获取镜像资源，与Cinder交互获取块存储资源。如果配置了Ceph块存储或对象存储服务，nova-compute还会与Ceph RADOS进行交互以访问Ceph存储集群。 在获取镜像和存储资源后，nova-compute还需与OpenStack的网络服务项目Neutron进行交互访问以获取网络资源。由于网络资源的有效性和可用性已经在nova-api工作流程中完成，这里主要是获取虚拟机的Fixed IP等网络资源。在准备好常见实例所需的一切资源后，nova-compute将通过Libvirt与对应的Hypervisor API进行交互，并通过Libvirt API接口进行虚拟机的创建工作。以上就是nova-compute的大致工作流程，实际上nova-compute接收到消息队列中由nova-conductor发起的RPC cast调用请求，该请求调用manager.py中的ComputeManager.build_and_run_instance（）方法，该方法继而调用ComputeManager类中的_build_and_run_instance（）方法以尝试创建实例；这里创建实例不一定成功，如果创建失败并且是因为Scheduler调度结果的原因，则会发起Re-scheduler操作以重新调度计算节点，并再次尝试创建实例。_ _进入_build_and_run_instance（）方法开始创建实例后，nova-compute便开始准备网络和存储等资源（这两个资源分别由_build_networks_for_instance（）和_prep_block_device（）方法来完成）。其中，在进行网络资源准备时，实例的vm_state被设为building，而task_state被设置为networking；在准备块存储资源时，实例的vm_state被设为building，而task_state被设置为blockdevicemapping。在实例所需资源均准备完成后，nova/virt/libvirt/driver.py的LibvirtDriver.spawn（）方法被调用，此时实例vm_state被设为building，而task_state被设置为spawning。 在spawn过程中，首先进行镜像加载，如果镜像较大，则此过程可能会花费较长时间，然后创建虚拟机的资源定义文件并通过Libvirt发起创建虚拟机的操作。虚拟机创建完成之后，spawn（）将调用_wait_for_boot（）方法以等待虚拟机的power_state状态变为running才返回，此时虚拟机的各个状态应该是，vm_state为active，power_state为running，task_state为none。因此，nova-compute在实例创建过程中的流程主要包括三个子流程，如下所示： 与Glance、Cinder交互获取镜像和云磁盘等资源（后端采用ceph分布式存储） 与Neutron交互获得网络资源 与libvirt对应的Hypervisor交互创建虚拟机实例（虚拟机磁盘由分布式存储ceph提供） Nova实例的状态机在虚拟机实例的创建和运行维护过程中，Nova使用三个变量来描述虚拟机当前的状态，分别为vm_state、power_state和task_state。其中，power_state代表虚拟机电源状态，其本质上反应的是Hypervisor的状态，其状态值遵循“由下至上（bottom-up）”的变更过程，即先是底层计算节点上Hypervisor状态变更，然后上层数据库对应值变更；vm_state反应的是基于API调用的一种稳定状态，其状态变更比较符合用户的逻辑思维，是一种由上至下（top-down）的API实现过程；task_state代表的是API调用过程中的过渡状态，反映了不同阶段所调用API对实例进行的操作。 1）power_state power_state是Nova程序调用特定虚拟机上的虚拟驱动所获取的状态。数据库中关于特定实例的power_state值仅是虚拟机最近一段时间的快照值，并不能真正反映当前虚拟机的实际power_state，虚拟机当前power_state的实际值位于Hypervisor中。如果出现可能影响到power_state的任务，则在此任务结束时，位于数据库中的power_state值就会被更新。其更新过程遵循“bottom-top”原则，即计算节点首先报告虚拟机power_state已经更新，然后再刷新数据库中的power_state字段值。power_state状态值衍生自Libvirt，其中BLOCKED状态值本质上代表的是RUNNING状态，所以不会记录。SHUTOFF被映射为SHUTDOWN状态，FAILED被映射为NOSTATE状态，因此power_state目前有RUNNING、SHUTDOWN和NOSTATE三种状态。 2）vm_state vm_state状态值是对虚拟机当前稳定状态的描述，而不是过渡状态。也就是说，如果针对特定虚拟机已经没有API继续调用，则应该用vm_state来描述此时虚拟机的稳定状态。例如，当vm_state状态值为ACTIVE，则表示虚拟机正常运行。再如SUSPENDING状态，其表示虚拟机正处于挂起过程中，并且在几秒之后会过渡到SUSPENDED状态，因此这是一种过渡状态，即SUSPENDING应该属于task_state的状态值。vm_state状态值更新的前提是针对此虚拟机有任务发生，并且任务成功完成，同时task_state被置为NOSTATE。如果没有API调用发生，则vm_state状态值绝对不会改变；如果对虚拟机的操作任务失败，但是成功回滚（rollback），则vm_state状态值也不会改变；如果不能回滚成功，则vm_state被置为ERROR状态。 vm_state状态与power_state状态之间没有必然的对应关系，即不能由vm_state的某一状态值推导出power_state的状态，反之亦然。当有针对虚拟机的操作任务正在执行时，power_state与vm_state的状态值很可能不协调：出现不协调的主要原因是vm_state仅代表虚拟机的稳定状态，而在任务执行中，虚拟机的状态一直在过渡，而且不能及时同步更新。如果没有任务正在执行，则vm_state与power_state不应该冲突，除非出现错误或任务失败，此时就需要具体问题具体分析，常见的不一致有以下几种： power_state为SHUTDOWN，而vm_state为ACTIVE：这种情况最可能的原因是虚拟机内部执行shutdonw命令时出现异常，解决这个问题最简单粗暴的方法就是手动调用stop（）API，之后，vm_state应该变为STOPPED。 power_state为RUNNING，而vm_state为HARD_DELETE：如果出现这种情况，则表明用户已经发出删除虚拟机的命令，但是执行过程出错，可以尝试再次删除虚拟机。 power_state为RUNNING，而vm_state为PAUSED：出现这个情况，表明虚拟机在执行pause（）之前出现了意外情况，这个问题的解决办法就比较多样，可以尝试将其强设为ERROR。 vm_state有多个状态值，不同状态值表示虚拟机当前处于不同的稳定状态。有些状态值彼此之间可以互相转换，如PAUSED与ACTIVE状态；而有些状态值之间没有任何直接联系，如PAUSED与STOPPED；没有联系的状态值之间只有借助第三个状态值才能转换，如ACTIVE状态值。vm_state状态机示意图如下： 3）task_state task_state代表的是过渡状态，并且与调用的API函数密切相关，其状态值描述了虚拟机当前正在执行的任务。只有对虚拟机发起了操作任务时，才会有task_state状态值出现，而当vm_state处于稳定值时，task_state通常为NOSTATE。简单来说，当虚拟机在执行任务时，task_state便会有反映操作任务的状态值，而如果没有执行任何任务，则task_state一定是NOSTATE。如下图所示： task_state的一个特殊任务状态是FORCE_DELETE（或HARD_DELETE）。正常情况下，任何时候用户都可以成功删除虚拟机，删除虚拟机后便可使用更多受Quota限制的资源，同时删除的虚拟机不会被计费，但是虚拟机删除任务可能会因为某些原因而失败，例如之前的任务堵塞或卡死而导致虚拟机删除失败，删除虚机时虚拟驱动卡死或计算节点因为网络/硬件不可用导致操作失败等。失败的虚拟机删除任务意味着task_state状态值不能成功过渡到NOSTATE，而虚拟机状态值vm_state更新的前提是task_state从具体的任务状态过渡到NOSTATE，因此，对于force_delete（）任务，不应该等到任务执行到计算节点并完成全部相关工作才更新vm_state的状态值为HARD_DELETE，而是应该发起force_delete（）任务之后便立即更新vm_state状态值。也就是说，force_delete（）是个纯粹的数据库操作任务，其对数据库中vm_state字段值刷新之后便结束，而相应的虚拟机清除工作随后才进行。 当任务被确认为虚拟机上唯一执行的任务时，task_state就会被设置为具体的状态值。虚拟机中每个正在执行的任务都会被分配一个与虚拟机相关的唯一task_id（UUID格式）。如果虚拟机已经有个task_id，则表明有任务正在执行，要在任务执行中途更新task_state，就必须确保虚机的task_id匹配当前的task_id，否则当前执行的任务会被抢占（目前只有force_delete任务能抢占）。通常在某个具体的任务执行期间，task_state的值绝对不能改变。当一个任务执行完成后，task_state被置为NOSTATE，而task_id被置为NONE。task_state的状态值名称通常是代表某个API方法动词的正在进行时（“ing”形式），如下表所示，因此，从task_state的状态值便可推出虚拟机当前正在执行什么任务。 vm_state task_state status active rebooting REBOOT reboot_pending REBOOT reboot_started REBOOT rebooting_hard HARD_REBOOT reboot_pending_hard HARD_REBOOT reboot_started_hard HARD_REBOOT rebuild_block_device_mapping REBUILD rebuilding REBUILD rebuild_spawning REBUILD migrating MIGRATING resize_prep RESIZE resize_migrating RESIZE resize_migrated RESIZE resize_finish RESIZE default ACTIVE vm_state task_state status stopped resize_prep RESIZE resize_migrating RESIZE resize_migrated RESIZE resize_finish RESIZE default SHUTOFF 对任一实例，不管是在实例创建过程中，还是创建完成后的维护操作中，vm_state、power_state和task_state总是同时存在，并共同决定了虚拟机的当前运行状态。下图为虚拟机创建过程中vm_state、power_state和task_state的状态值变更过程，可以看到，实例创建过程中，vm_state状态由Building变为Active，power_state则由NoState变为Running，经历状态值变更最多的是task_state，其经历了Scheduling、None、Networking、Block_Device_Mapping和Spawning等状态值的变更。 虚拟机之间的状态可以相互转换，上图就描述了这种状态转换关系。让虚拟机由一个或多个状态转换到另外一种或几种状态，需要对虚拟机发出相关的任务操作，或者说要让虚拟机最终处于某种状态，则需要对当前状态的虚拟机执行相应的操作命令。虚拟机当前状态与目标状态之间的对应关系可以是一对一或多对一，如当前为Paused状态的虚拟机通过unpause命令可以变为Active状态，当前是Active、Shutoff和Rescued状态的虚拟机都可以通过pause命令变为Paused状态，而对于某些如backup或snapshot等备份命令则不会改变虚拟机状态。 Nova虚拟机实例的迁移冷迁移—resize/migrateNova为虚拟机提供了资源升级（resize）和主机迁移（migrate）操作。从底层调用的API接口来看，resize与migrate本质上是相同的，不同之处在于resize需要提供新的资源配置flavor，并使用新的flavor参数在目标主机上重新启动实例，而migrate则无须flavor参数，或者可以认为当resize操作提供的flavor与原实例的flavor一致时，则resize操作就是migrate操作。由于resize/migrate操作在迁移过程中会关闭源主机上的实例，并在新的主机上重新启动实例，因此resize/migrate对实例的迁移并非实时在线，而是先关闭实例再以copy镜像的形式迁移，迁移完成之后再在新的主机上启动实例，resize/migrate迁移也称为“冷迁移”。在虚拟机实例冷迁移结束之前，无法对虚拟机进行其他操作。 resize/migrate迁移的优点在于其无须共享存储支持，而且允许用户重新自定义虚拟机资源和宿主机。例如，由于Nova调度或者物理设备的原因导致某台宿主机负载过重，而且其上的某些虚拟机比较关键，同时需要更多的物理资源，此时便可申请维护窗口，通过Nova的resize/migrate功能将此宿主机上的某些关键虚拟机迁移到硬件资源更强大的物理宿主机上。resize/migrate操作无须用户指定目标主机，而是由Nova-scheduler调度。resize操作允许源主机与目标主机相同，前提是当前主机的资源满足虚拟机resize后的资源需求，同时用户需要设置nova.conf配置文件中的allow_resize_to_same_host参数为true（默认为false），并重新启动Nova相关服务。而migrate操作则不允许scheduler将模板节点选取为源主机，如果scheduler调度的目标节点与源节点相同，则会引起re-scheduler操作（再次调度时源主机会因RetryFilter而被排除）。 对于resize/migrate操作，在迁移过程接近尾声，准备在目标主机启动实例和虚拟机状态为resized时，还有两个相关操作需要用户执行，即confirm_resize和revert_resize。confirm_resize操作表示用户接受此次实例迁移操作，revert_resize表示用户不接受（反悔）此次迁移操作。如果用户需要Nova自动确认迁移操作，则可以将resize_confirm_window参数设置为某个大于0的时间值（一般设置为1即可），当迁移完成并且虚拟机处于reszied的时间大于此参数值时，迁移操作将会被自动确认。要对虚拟机进行resize操作，需要修改计算节点上/etc/passwd中nova条目并为用户nova设置密码，配置可登录系统，同时需要在计算节点之间设置SSH免密互信机制。如果不需要进行虚拟机资源调整，只需进行纯粹的“冷迁移”migrate操作即可。migrate与resize类似，但是migrate操作无须指定flavor参数，仅仅是将实例迁移至其他主机，而不进行资源调整，也无需进行上述配置。 热迁移—live-migration为了进行服务非中断的实时维护，Nova提供了live-migration功能。相对于resize/migrate的“冷迁移”，live-migration称为“热迁移”，即实时在线迁移，其可以将实例由一个计算节点在线迁移到另一个计算节点，期间仅有非常短暂的访问延时。live-migration迁移按其实现方式可以划分为三种类型，即基于非共享存储的块迁移（block live migration）、基于共享存储的迁移（Shared storage-based live migration）和基于Volume后端的迁移（Volume-backed live migration）。其中，后两种是使用最多的live-migration方式，而块迁移在使用上一直存在很多问题，而且也不符合实时迁移的基本设计思想，故不推荐使用。块迁移在迁移过程中需要拷贝临时存储及镜像文件，对于节点之间的网络带宽要求极高，尤其是在临时存储很大的情况下，还有可能出现消耗很长时间却不能成功的情况，如当实例I/O负载很大时，如果应用程序对临时存储的写入速率快于块迁移对临时存储的拷贝速率，则块迁移将不能完成。 使用live-migration功能时，不管是基于共享存储还是Volume的迁移，都需要对nova.conf和libvirt.conf做相应修改，然后重启nova-compute和libvirtd服务，即需要完成一些迁移前的准备工作。准备工作完成之后，便可开始live-migration迁移—基于Volume后端的实例迁移其实就是对SAN BOOT形式的实例进行迁移，由于实例系统位于Volume而非临时磁盘上，因此无须共享存储，其迁移原理就是将Volume从源主机卸载，并重新挂载到目标主机的过程。与基于Volume后端的live-migration不同，其原理是需要计算节点之间共享实例镜像文件目录，通常是/var/lib/nova/instances，即每个计算节点都可以对共享的/var/lib/nova/instances目录进行读写，然后在共享目录创建实例镜像，这样每个计算节点都能读取该镜像。 上述的Nova实例的迁移配置和实战详情请参考本站的《OpenStack虚拟机实例迁移实战》一文。 Nova计算管理服务实战Hypervisor、主机聚合和可用分区管理步骤1：执行以下命令，查看OpenStack Hypervisor的列表 1openstack hypervisor list --long 步骤2：执行以下命令，查看OpenStack Hypervisor的列表 1openstack host list 步骤3：执行以下命令，创建主机聚合“Aggregate_kk” 1openstack aggregate create --zone nova Aggregate_kk 步骤4：执行以下命令，为主机聚合“Aggregate_kk”添加主机“rocky-compute” 1openstack aggregate add host Aggregate_kk rocky-compute 步骤5：执行以下命令，验证同一个主机是否可以加入不同的AZ 步骤6：执行以下命令，验证同一个主机是否可以加入不同的AZ 1openstack aggregate set --zone nova Aggregate_Test 步骤7：执行以下命令，从主机聚合“Aggregate_Test”移除主机“rocky-compute” 1openstack aggregate remove host Aggregate_Test rocky-compute 步骤8：执行以下命令，删除主机聚合“Aggregate_Test” 1openstack aggregate delete Aggregate_Test 虚拟机规格管理实战步骤1：执行以下命令，查看现有的虚拟机规格列表 1openstack flavor list 步骤2：执行以下命令，创建规则Flavor_Test ，vCPU=1，RAM=128M，ROOT DISK=1G，仅对项目kkproject可见 1openstack flavor create --vcpu 1 --ram 128 --disk 1 --private --project kkproject Flavor_Test 步骤3：执行以下命令，移除规格Flavor_Test仅对项目kkproject可见 1openstack flavor unset --project kkproject Flavor_Test 步骤4：执行以下命令，删除规格Flavor_Test 1openstack flavor delete Flavor_Test 步骤5：执行以下命令，创建规格Flavor_ubuntu，vCPU=2，RAM=2048M，ROOT DISK=20G，其他默认 1openstack flavor create --vcpu 2 --ram 2048 --disk 20 Flavor_ubuntu 秘钥对和虚拟机组的管理实战步骤1：执行以下命令，创建密钥对“KeyPair_kk01” 1openstack keypair create KeyPair_kk01 步骤2：执行以下命令，创建虚拟机组“ServerGroup_kk01”，策略设置为“affinity” 1openstack server group create --policy affinity ServerGroup_kk01 步骤3：执行以下命令，创建虚拟机组“ServerGroup_kk02”，策略设置为“anti-affinity” 1openstack server group create --policy anti-affinity ServerGroup_kk02 虚拟机实例实战步骤1：执行以下命令，创建虚拟机实例“instance_01”，要求配置为：AZ=nova，image=kk_img，flavor=Flavor_Web，KeyPair=KeyPair_kk01，Server Group=ServerGroup_kk01 1openstack server create --availability-zone nova --image kk_img --flavor Flavor_Web --key-name KeyPair_kk01 --hint group=e320072b-1a76-4b87-854b-53c40db3e265 instance_01 步骤2：执行以下命令，查看虚拟机实例“instance_01”的状态 1openstack server show instance_01 | grep status 步骤3：执行以下命令，软重启虚拟机实例“instance_01” 1openstack server reboot instance_01 步骤4：执行以下命令，硬重启虚拟机实例“instance_01” 1openstack server reboot --hard instance_01 步骤5：执行以下命令，关闭虚拟机实例“instance_01” 步骤6：执行以下命令，启动虚拟机实例“instance_01” 1openstack server start instance_01 步骤7：执行以下命令，锁定虚拟机实例“instance_01” 1openstack server lock instance_01 步骤8：执行以下命令，解锁虚拟机实例“instance_01” 1openstack server unlock instance_01 步骤9：执行以下命令，暂停虚拟机实例“instance_01” 1openstack server pause instance_01 步骤10：执行以下命令，恢复暂停的虚拟机实例“instance_01” 1openstack server unpause instance_01 步骤11：执行以下命令，挂起虚拟机实例“instance_01” 1openstack server suspend instance_01 步骤12：执行以下命令，恢复挂起的虚拟机实例“instance_01” 1openstack server resume instance_01 步骤13：执行以下命令，搁置虚拟机实例“instance_01” 1openstack server shelve instance_01 步骤14：执行以下命令，恢复搁置的虚拟机实例“instance_01” 1openstack server unshelve instance_01 步骤15：执行以下命令，创建虚拟机实例“instance_01”的快照“instance01_snap_01” 1openstack server image create --name instance01_snap_01 instance_01 步骤16：执行以下命令，查看镜像列表 步骤17：执行以下命令，创建一个新的规格“Flavor_Web_new”，将内存大小调整为256M，其余与规格“Flavor_Web”保持一致 1openstack flavor create --vcpu 1 --ram 256 --disk 1 Flavor_Web_New 步骤18：执行以下命令，将虚拟机实例“instance_01”的规格调整为新的规格“Flavor_Web_New” 1openstack server resize --flavor Flavor_Web_New instance_01 步骤19：执行以下命令，确认虚拟机实例“instance_01”的Resize调整 1openstack server resize --confirm instance_01 步骤20：执行以下命令，确认虚拟机实例“instance_01”的规格信息 1openstack server show instance_01 | grep flavor 步骤21：执行以下命令，从虚拟机快照恢复实例instance_01，并查看虚拟机实例的镜像信息 1openstack server rebuild --image instance01_snap_01 instance_01 思考1、请从nova的重要概念角度阐述虚拟机故障迁移的最大范围是多少？ 2、多个AZ可以通过一个nova-scheduler进行调度，那么在虚拟机故障时或部署虚拟机的计算节点主机故障时，虚拟机实例是否可以跨AZ迁移？请阐述理由？ 3、请阐述为什么一个主机Host只能属于一个AZ？ 4、请阐述nova cell的V1与V2版本的区别？并分析在V2版本中，nova_cell0S数据库的作用？在nova-api数据库中请分析与cell相关的数据表之间的挂链关系？ 5、请分析虚拟机实例在软重启和硬重启时内部流程的区别？并解释为什么虚拟机软重启和硬重启中状态status一直为active？ 6、请分析虚拟机实例暂停状态下pause和suspend的区别？关机状态下Poweroff和shelve的区别？ 7、请根据虚拟机实战的2-14步画出虚拟机实例的状态变更图，并标注出引起状态变化的相关操作指令？ 8、请分析虚拟化场景下和云场景下虚拟机快照的区别？ 9、请分析虚拟机实例resize过程中虚拟机的在线状态？并分析为什么在VERIFY_RESIZE状态下需要通过cofirm操作进行确认？确认后虚拟机状态的的变化是什么？同时，除了confirm操作外，还有回滚操作revert，那么revert后虚拟机实例状态变为ACTIVE前是否需要再次confirm？ 10、请分析为什么在你们的环境中虚拟机实例resize不成功？请从日志关联的角度分析产生错误请求400 Bad Request的原因？ 11、请从Ubuntu官网下载一个cloud镜像，并通过命令行拉起一个ubuntu虚拟机实例，并要求能够通过SSH的用户名密码方式登录。 12、请手动制作一个win7/win10的cloud镜像，并通过命令行拉起一个win7/win10的虚拟机实例，并要求该实例能够正常运行。 13、请阐述冷迁移resize和migrate的区别？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-镜像管理服务Glance]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Glance%2F</url>
    <content type="text"><![CDATA[概述Glance在OpenStack中主要为实例提供公共镜像服务能力以及镜像/虚拟机快照管理功能，属于OpenStack的核心组件之一，与Swift、Cinder并列OpenStack存储的三驾马车。在实际生产中，Glance本身并不负责大量数据的存储，对镜像的存储主要依赖于后端Swift，一般都是采用对象存储方式。如果后端对象存储采用Swift，则镜像存在Swift中；如果后端存储采用分布式存储ceph，则主要存储在ceph的对象存储rgw模块中。而在我们的实验环境中（考虑大家PC的配置问题），没有部署swift对象存储，因此镜像采用部署Glance服务的节点本地磁盘进行存储。 Glance首次出现在OpenStack的Bexar版本中，提供发现、注册和检索虚拟机镜像的功能，只依赖于OpenStack的Keystone服务，属于OpenStack八大核心组件之一，为OpenStack的核心项目之一。 Image的概念OpenStack 由 Glance 提供 Image 服务。要理解 Image Service 先得搞清楚什么是 Image 以及为什么要用 Image？在传统 IT 环境下，安装一个系统是要么从安装 CD 从头安装，要么用 Ghost 等克隆工具恢复。这两种方式有如下几个问题： 如果要安装的系统多了效率就很低、时间长，工作量大。 安装完还要进行手工配置，比如安装其他的软件，设置 IP 等 备份和恢复系统不灵活。 云环境下需要更高效的解决方案，这就是 Image。Image 是一个模板，里面包含了基本的操作系统和其他的软件。举例来说，有家公司需要为每位员工配置一套办公用的系统，一般需要一个 Win7 系统再加 MS office 软件。OpenStack 是这么玩的：先手工安装好这么一个虚机、然后对虚机执行 snapshot，这样就得到了一个 image。当有新员工入职需要办公环境时，立马启动一个或多个该 image 的 instance（虚机）就可以了。在这个过程中，第 1 步跟传统方式类似，需要手工操作和一定时间。但第 2、3 步非常快，全自动化，一般都是秒级别。而且 2、3 步可以循环做。比如公司新上了一套 OA 系统，每个员工的 PC 上都得有客户端软件。那么可以在某个员工的虚机中手工安装好 OA 客户端，然后执行 snapshot ，得到新的 image，以后就直接使用新 image 创建虚机就可以了。另外，snapshot 还有备份的作用，能够非常方便的恢复系统。各位在安装OpenStack时可以通过VMWare的快照（snapshot）来进行备份，每安装完一个服务就备份一次，这样即使后面服务安装出现失误等，且无法调整，就可以通过恢复快照的方式来解决，但是这样也存在一个占用本地磁盘较大的问题。就如下图所示： 镜像、实例和规格的关系： 用户可以从同一个镜像启动任意数量的实例。 每个启动的实例都是基于镜像的一个副本，实例上的任何修改都不会影响到镜像。 启动实例时，必须指定一个规格，实例按照规格使用资源。 在创建Glance服务管理的Image时，需要指定Image的容器格式container format和磁盘格式disk format，其含义具体如下： disk format：是虚拟机镜像文件的磁盘格式，不同的虚拟化厂商有不同的格式，用来标明虚拟机磁盘镜像包含的信息，可以设置的镜像磁盘格式有：raw，vhd ，vmdk，vdi， iso ，qcow2 ，aki ，ami ，ari等。 镜像磁盘格式 说明 raw 一种非结构化的镜像磁盘格式 vhd VMware，Xen，Microsoft，VirtualBox等使用的常见磁盘格式 vhdx vhd格式的增强版本，支持更大的磁盘容量和其他功能 vmdk 常见的磁盘镜像格式 vdi Virtual Box和QEMU支持的磁盘格式 iso 光盘（例如CDROM）的存档格式 qcow2 QEMU支持的磁盘格式，支持动态扩展和写时复制 aki Amazon Kernel Image ari Amazon Ramdisk Image ami Amazon Machine Image container format：是指虚拟机镜像是以一种什么样的文件格式存放，同时包含实际虚拟机有关的metadata格式。container format有以下几种：bare，ovf ，aki， ari ，ami等。 镜像容器格式 说明 ovf 开放式虚拟机磁盘格式， 由Vmware发起， 目前已被多种虚拟化设备支持。 bare 这表示镜像没有container或者元数据。 aki Amazon Kernel Image ari Amazon Ramdisk Image ami Amazon Machine Image 镜像不仅有存储的磁盘格式和容器格式，同时还有访问权限，主要有四种： public：公共的，可以被所有的tenant使用； private：私有的，只能被owner的tenant使用； shared：共享的，一种非公有的Image，可以被其owner共享给其他tenant使用； protected：受保护的，不能被非管理员以外的用户删除； Glance的架构Glance 使用C/S架构，提供REST API，用户可以通过API执行对服务的请求，其架构在OpenStack版本的演进中发生了一定的变化，主要有V1和V2两个版本。在老版本（V1）的Glance架构中，主要通过glance-api、glance-registry、glance-DB、image-store四个组件构成，glance-api和glance-registry同时提供WSGI接口，只是glance-api对外，glance-registry只能被内部调用。如下图所示，各个组件的功能如下： glance-api：接收REST ful的API请求，通过其他组件（glance-registry和image-store）来完成镜像的CRUD操作。 glance-registry：负责镜像的元数据管理，与glance-DB交互，完成存储或获取镜像的元数据。 glance-DB：通过关系型数据库组件提供镜像管理、控制数据及元数据的存储功能，在开源解决方案中采用MySQL数据库实现glance-DB，在华为的解决方案中采用GuassDB实现glance-DB。 image-store：本质上是一个存储接口层，可以异构对接不同的后端存储，将镜像数据通过该接口层持久化存储在后端存储中。 而在新版本（V2）中，Glance的架构发生了一定的变化，在源代码中仍然保留glance-registry函数的入口点（这也是为什么我们在配置glance的服务时，仍需配置glance-registry.conf的原因），但是实际实现时将glance-registry的功能整合到了glance-api中，通过在glance-api中提供一个registry共享层来实现镜像元数据的数据库存取。新版本OpenStack的Glance镜像管理服务的架构如下，镜像的元数据主要包括：镜像资源的访问策略Policy，镜像资源的规格、镜像资源的存储位置等信息。 glance-client：任何使用glance服务的应用程序，接收API请求，发起操作认证并调用glance-api。 glance-api：通过REST ful对外开放API接口提供镜像管理功能，并接收镜像管理的相关API操作请求。 glance-domain-controller：管理Glance的内部服务组件，分层实现特定的任务，如认证（Auth）、事件通知（Notifier）、策略控制（Policy）和数据库连接等。 registry-layer：实现glance-domain-controller与DAL之间的安全访问控制。 DAL（DataBase-Abstraction-Layer）：提供glance-domain-controller、registry-layer与glance-DB之间的统一API接口，实现glance-domain-controller、registry-layer对glance-DB的安全访问。 glance-DB：在所有组件之间共享，存放镜像的管理、配置数据及元数据。 Glance Store Drivers：就是image store，本质上是一个存储接口层，负责与外部异构存储后端或本地文件系统的交互，持久化存储镜像文件。 了解了V1和V2版本的glance架构，这里需要重点了解下Glance服务的缓存机制。Glance服务提供缓存机制主要是为了解决节点扩容后带来多个节点同时提供镜像管理服务带来的镜像服务性能问题。其原理就是通过glance-api提供缓存机制，就是在glance-api服务节点存放一份原始镜像的拷贝，本质上是为了实现api服务器的数量扩展，提高为同一个镜像提供服务的效率。这里需要说明一下，网上针对glance服务提供缓存的位置有两种说法：一是将镜像缓存到计算节点；二是将镜像缓存到glance-api节点。从官网的如下描述我理解是第二种方式-—This local image cache is transparent to the end user – in other words, the end user doesn’t know that the Glance API is streaming an image file from its local cache or from the actual backend storage system。但是，在创建虚拟机时，nova-compute会将镜像缓存拷贝一份到计算节点，便于后续再次创建同样镜像的虚拟机时可以利用本地存储的镜像完成（通过镜像ID识别是否同一镜像的虚拟机）。上述镜像的缓存对用户是不可见的，用户并不知道使用的是本地缓存的镜像还是后端存储的镜像。镜像的缓存服务开关、缓存大小和缓存周期可通过命令行配置，命令行操作缓存的指令如下： 1234567glance-cache-pruner --image_cache_max_size= * &lt;===控制cache总量的大小，周期性的运行glance-cache-cleaner &lt;===清理状态异常的cache文件glance-cache-manage --host=&lt;HOST&gt; queue-image &lt;IMAGE_ID&gt; &lt;===预取某些热门镜像到新增的api节点中glance-cache-manage --host=&lt;HOST&gt; delete-cached-image &lt;IMAGE_ID&gt; &lt;===手动删除image cache来释放空间 Glance的状态机镜像Image在虚拟机实例的全生命周期管理中，也是有各种不同的状态的。类似Nova服务管理虚拟机的各种状态，Cinder负责管理Volume的各种状态，Glance负责管理Image的各种状态。下图就是创建镜像时，Image的不同状态之间的转换图。 在Glance中有两种状态机，一种是镜像状态，另一种是任务状态。镜像状态表示镜像在数据库中的各种状态，任务状态表示操作镜像的各种任务的状态，两种状态结合的上图状态机流程图表示了镜像在不同任务的不同时期的一种状态形式，只有任务执行成功success时的active状态的镜像才能被正常使用。 任务状态包括pending、processor、success、failure四种，其具体含义如下： pending：表示任务被挂起 processor：表示任务正在执行中 success：表示任务执行成功 failure：表示任务失败 镜像状态包括queued、saving、uploadding、importing、active、deactive、killed、deleted、pending_delete九种，其具体含义如下： queued：已在glance-registry中保留镜像的标识符，但镜像数据未上传，镜像大小未初始化。 saving：镜像的原始数据正在上传到glance中。 uploading：对镜像调用了import data-put请求。 importing：导入镜像过程中，但镜像尚未就绪。 active：镜像创建完成，可以使用。 deactived：禁止任务非管理员用户访问镜像，即镜像去激活。 killed：镜像上传时出错，镜像不可用。 deleted：glance保留了镜像的信息，但不能被使用，一定时间后自动清理镜像信息。 pending_delete：类似deleted，此时glance尚未删除镜像的数据，处于该状态的镜像可以恢复。 例如上图中，当发起创建一个镜像任务Create image时，首先在glance-registry中保存镜像的标识符，但此时镜像文件还未上传，镜像大小还未初始化（Queued），此时任务状态为processor。下一步将初始化镜像的元数据并保存到数据库中，并开始上传镜像（saving），此时任务状态仍为processor。镜像上传如果成功，此时镜像状态转换为active，任务状态为success，镜像可以正常使用；镜像上传如果失败，此时镜像状态为killed，任务状态为failure。同时根据用户的操作镜像的状态和任务状态有不同的表现形式：用户如果选择重新上传，则镜像状态转为queued，任务状态转为processor，即重新开始上传；用户如果选择删除镜像，则镜像状态变为deleted，任务状态为success，表示镜像删除成功。同时，在镜像上传过程中，即saving态时，也可以选择删除镜像，此时镜像状态直接变为deleted，任务状态变为success时，表示镜像删除成功。同理，镜像的导入过程与创建过程类似，只是镜像状态不包含saving态，只有importing态，这里不再赘述。 OpenStack不同操作系统类型镜像的制作OpenStack中虚拟机使用的镜像并不是我们平时装系统使用的iso光盘文件，而是经过裁剪后的适合各个云主机使用公共模板img，且支持加入个性化化定制任务和软件。获取与OpenStack兼容的虚拟机映像的最简单方法是下载别人已经创建的虚拟机映像，最简单的Glance镜像制作方法是下载系统供应商官方发布的OpenStack镜像文件，官方发布的OpenStack大多数镜像预安装了cloud-init包，支持SSH密钥对登录和用户数据注入功能，下图表示了OpenStack官方镜像支持的主流操作系统类型。镜像的具体下载链接，请参考OpenStack社区网站：https://docs.openstack.org/image-guide/obtain-images.html。 但是，有时候官方下载的镜像的并不符合我们的实际需要，比如我们想要安装一些预置软件或脚本。一种简单的方法是将官方镜像进行改造，然后再生成新的模板文件，这种方法虽然简单，但是我们无法完全掌握镜像具体包含哪些工具和软件，存在一定的安全风险。另一种方法就是我们自己手动创建镜像，这样我们自己可以定义安装哪些软件和定制哪些脚本，并自行设定镜像生成的云主机的访问权限等等。下面就以创建一个ubuntu18.04镜像为例，简单说明一下手动创建镜像的流程，并介绍几种常用镜像制作工具。 Step1：使用virt-manager创建一个Ubuntu 18.04虚拟机并安装系统 Step2：登录虚拟机并安装cloud-init，sudo apt install cloud-init Step3：在虚拟机内部，停止虚拟机，sudo shutdown -h now（或init 0） Step4：预清理虚拟机，在KVM宿主机上执行virt-sysprep -d VM_ID Step5：释放虚拟机定义，在KVM宿主机上执行virsh undefine VM_ID Step6：制作镜像，在KVM宿主机上执行qemu-img create or qemu-img convert Step7：上传镜像，在OpenStack控制节点上执行 openstack image create 按照上面的步骤，就可以创建各种操作类型各类虚拟机，具体指令的差异由安装的镜像操作系统决定。需要注意一点，在制作windows操作系统类型虚拟机时，需要额外安装virtio驱动，但这不是OpenStack的原因引起的，而是KVM虚拟化解决方案要求的。virtio是一种IO的半虚拟化解决方案，在KVM虚拟机中一般要求磁盘IO使用virtio的半虚拟化驱动，有时网卡的IO也要求使用virtio的半虚拟化驱动，目的就是提高IO虚拟化后的性能。而windows操作系统自身并不支持virtio驱动，因此需要额外安装第三方驱动。 除了上面手工制作OpenStack的镜像外，开源社区还提供了一些镜像制作工具，用于快速批量制作镜像。常见的工具有以下几种： Diskimage-builder：自动化磁盘映像创建工具，可以制作Fedora，Red Hat Enterprise Linux，Ubuntu，Debian，CentOS和openSUSE镜像，示例：disk-image-create ubuntu VM Packer：使用Packer制作的镜像，可以适配到不同云平台，适合使用多个云平台的用户，一种兼具图形化和指令集的制作工具。 virt-builder：快速创建新虚拟机的工具，可以在几分钟或更短的时间内创建各种用于本地或云用途的虚拟机镜像，其本质上是利用一个已存在的虚拟机作为模板，快速创建一个新的虚拟机。 Glance镜像管理实战步骤1：执行以下命令，查看已上传的镜像列表，OpenStack上传镜像的默认保存位置在/var/lib/glance/images目录下，上传后保存的镜像文件名为镜像的id值 1ll /var/lib/glance/images/ 步骤2：执行以下命令，查看上传镜像的详细信息 1qemu-img info /var/lib/glance/images/3d1a7c9b-d954-4a52-bb80-bd2682972de3 步骤3：执行以下命令，创建镜像“kk_img”，镜像格式为“QCOW2”，镜像设置为“Private”和“Protected” 1openstack image create --disk-format qcow2 --container-format bare --min-disk 1 --min-ram 128 --private --protected --file ~/cirros-0.4.0-x86_64-disk.img kk_img 步骤4：执行以下命令，查看刚创建镜像“kk_img”的详细信息 1openstack image show kk_img 步骤5：执行以下命令，将镜像“kk_img”设置为public和unprotected状态 1openstack image set --public --unprotected kk_img 步骤6：执行以下命令，将镜像“kk_img”设置为共享状态 1openstack image set --shared kk_img 步骤7：执行以下命令，将镜像“kk_img”加入项目kkproject中，并查看镜像的状态信息 1openstack image add project cfa6d427-f6e2-48c0-91da-0365101da223 fa23a9822e8449239d8f9d7befbabc75 步骤9：执行以下命令，在普通用户kkutysllb登录情况，接收共享镜像“kk_img”，再次观察权限上的变化 1openstack image set --accept cfa6d427-f6e2-48c0-91da-0365101da223 步骤10：执行以下命令，查看vmdk格式的Ubuntu18.04镜像详细信息 1qemu-img info bionic-server-cloudimg-amd64.vmdk 步骤11：执行以下命令，将vmdk格式的Ubuntu18.04镜像转换为qcow2格式 1qemu-img convert -f vmdk -O qcow2 -c -p bionic-server-cloudimg-amd64.vmdk bionic-server-cloudimg-amd64.qcow2 步骤12：执行以下命令，上传bionic-server-cloudimg-amd64.qcow2镜像，命名为ubuntu-18.04-s，镜像状态设为public和unprotected 1openstack image create --disk-format qcow2 --container-format bare --min-disk 1 --min-ram 128 --public --unprotected --file ~/bionic-server-cloudimg-amd64.qcow2 ubuntu-18.04-s 步骤13：执行以下命令，将镜像ubuntu-18.04-s导出保存到本地磁盘 1openstack image save --file ubuntu-18.04-s.qcow2 ubuntu-18.04-s 思考：1、请分析镜像导入任务过程中，镜像状态变化过程及任务状态的变化过程？ 2、将激活状态的镜像去激活再次激活时，请分析镜像状态的变化过程及任务状态变化过程。那么将激活态的镜像直接删除，此时镜像状态和任务状态的变化流程如何？ 3、将镜像设置为private和protected时 ，有哪些限制？将镜像改为public和unprotected后，限制上有哪些变化？将镜像改为shared后，限制上有哪些变化？ 4、普通用户是否可以创建public状态的镜像？为什么？shared状态的镜像呢？又为什么？ 5、创建一个虚拟机实例时，实例是如何从镜像启动的（多磁盘场景下，即实例有vda、vdb、vdc三块虚拟磁盘，其中vda是系统盘，vdb是临时盘、vdc是数据盘）？删除虚拟机实例时，实例关联的惊险会怎么样？（答出glance、nova、cinder的大致交互流程即可）]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-认证管理服务Keystone]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E8%AE%A4%E8%AF%81%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Keystone%2F</url>
    <content type="text"><![CDATA[概述Keystone作为OpenStack中的一个独立的提供安全认证的模块，作为OpenStack的身份管理服务（Identity Service）主要负责OpenStack用户的身份认证、令牌管理、提供访问资源的服务目录，以及基于用户角色的访问控制。用户访问系统的用户名及密码是否正确、令牌的颁发、服务端点的注册，以及该用户是否具有访问特定资源的权限等，这些都离不开Keystone服务的参与。无论是公有云还是私有云，都会开放接口给众多的用户。为保证系统安全和用户的安全，就需要有完善的安全认证服务，无非包含几个方面：用户认证、服务认证和口令认证。Keystone提供的认证服务是双向的，既对用户进行合法性认证，对用户的权限进行了限制，又对提供给用户的服务进行认证，保证提供给用户的服务是安全可靠的。 在OpenStack的整体框架结构中，Keystone与Horizon类似，相当于一个服务总线与Nova、Glance、Horizon、Swift、Cinder及Neutron等其他核心服务全互联，其他核心服务通过Keystone来注册其服务的入口Endpoint，针对这些服务的任何调用都需要经过Keystone的身份认证，并获得服务的Endpoint来进行访问。 Keystone服务作为独立的身份认证模块，首次出现来OpenStack的Essex版本，提供身份认证、服务发现和分布式多用户授权，OpenID Connect，SAML和SQL。它不依赖于其他任何服务，相反其他服务的访问都需要依赖它来提供访问安全机制。 Keystone的基本概念作为 OpenStack 的基础支持服务，Keystone主要完成身份认证、令牌管理、服务管理、端点注册和访问控制5大功能，如下 总结起来，Keystone服务主要完成以下三件工作： 管理用户及其权限（Identity、Policy）； 维护 OpenStack Services 的 服务目录和入口（catalog、Endpoint）； Authentication（认证）和 Authorization（鉴权）（Identity、Token）； 以上三件工作彼此之间不是相互隔离独立的，反而是有紧密的联系。用户访问云服务时，首先提供用户凭证（Credentials），Keystone完成认证（Identity）后返回给用户一个临时身份凭证（Token），同时根据的用户的权限（Policy）返回用户可以访问的服务和权限（Endpoint、Catalog）。随后，用户就使用Keystone分配的临时身份凭证（Token）按照服务的入口（Endpoint）去访问相关的核心服务并进行相关的资源操作。各核心服务接收到用户的访问请求后，会使用用户的临时身份凭证（Token）去Keystone进行认证（Identity），同时根据权限列表（Policy）对用户的操作进行授权。通过后，才允许用户进行服务访问并进行相关资源操作。 要想理解Keystone服务的机制，首先要掌握Keystone的服务架构，并根据架构去归类理解Keystone服务的对象模型的各个概念，最后贯穿起来去拉通整个认证服务的流程。Keystone服务的架构如下： 从Keystone服务的整体架构来看，整个服务架构分为入口访问模块，核心服务模块、后端存储模块和扩展插件模块，其中： 入口访问模块：主要由两个子组件构成，即Keystone API和Middleware。Keystone API接收外部的REST访问请求，而Middleware模块是一个安全认证的中间件，主要用来缓存Token，减轻核心服务模块的认证压力。当用户首次访问时，会通过Keystone API去Middleware模块验证Token的签名，由于此时用户并没有被分配Token，所以Middleware模块会向核心服务模块Keystone Service的Identity子模块进行认证，认证通过后Token子模块会给用户分配一个Token并缓存到Middleware模块中（对Token进行签名），后续用户使用该Token认证时，Middleware模块会代替Identity模块完成认证（Token有效期内），从而减轻Keystone Service核心模块的访问压力。 核心服务模块：Keystone Service，由不同的子模块构成，不同子模块提供不同身份认证服务。比如：Identity子模块对用户凭证进行合法性认证，Token子模块用于给用户分配一个临时身份凭证，Assignment子模块用于分配用户的资源配额，Policy子模块用户指定用户访问资源的权限，Catalog子模块用户指定用户可以访问的服务目录等等。 后端存储模块：Keystone Backends，主要用来存储实现不同核心服务的数据，不同核心服务数据可以使用不同后端存储来存储。支持的后端存储有：LDAP、SQL、Template、Rules、KVS等。 扩展插件模块：Kesytone Plugins，针对不同的身份认证服务提供不同的扩展插件。比如基于密码的认证，通过Password插件完成，基于Token的认证基于Token插件完成等等。 了解Keystone服务的整体架构后，我们再来看看Keystone服务的对象模型。如下图所示，所谓的对象就是Keystone服务管理的对象，而对象模型就是指Keystone服务管理的几类对象的集合及其子集。其顶层的对象模型就是Service和Policy，而Service对象模型的子集则包括：Identity、Resource、Assignment、Token和Catalog等多个子对象模型。因此，要想彻底掌握Keystone的作用，就需要重点理清这些对象模型的概念及其子集的对应关系，以及对象模型在整个Keystone服务中的作用。 Policy：安全访问策略，每个OpenStack服务都在相关的策略文件中定义其资源的访问策略（Policy） 。类似于Linux中的权限管理，不同角色的用户或用户组将会拥有不同的操作权限。在OpenStack中也可以针对每种服务（内部或外部）设定不同用户或用户组的访问策略，从而实现不同用户或用户组对访问服务的分权分域管理。各服务的访问策略文件是一个JSON格式的文件，并不是必须的，可以由管理员自行手动创建和配置，配置后无需重启服务即刻生效，一般放在服务配置目录下，即/etc/SERVICE_NAME/policy.json。 Service：服务，在一个或多个端点（Endpoint）上公开的一组内部服务。包括：Identity、Resource、Assignment、Token、Catalog等。除内部服务外，Keystone还负责与OpenStack其他服务（ Service ）进行交互，例如计算，存储或镜像，提供一个或多个端点，用户可以通过这些端点访问资源并执行操作。因此，Keystone提供Service对象模型，可以简单理解为Service=内部服务+外部服务。 Identity：提供身份凭证数据以及用户和用户组的数据。它有两个子集，分别是User和Group。User就是指单个OpenStack服务的使用者，必须属于某个特定域（Domain），用户名不是全局唯一，但是在其归属域（Domain）内是唯一的。Group是借鉴了Linux操作系统中Group的概念，在M版本之后才引入，作用就是把多个用户作为一个整体进行管理。Group也必须属于某个特定域（Domain），与User一样，在全局范围内Group不是唯一的，但是在其归属域（Domain）内必须唯一。通常情况下，用户和用户组数据由Identity服务管理，允许它处理与这些数据关联的所有CRUD操作。如果使用LDAP进行认证，则Keystone的Identity服务就是LDAP认证的前端，User和Group的管理由权威后端LDAP负责，Keystone的Identity此时只是作为认证的中继。 Resource：资源，提供有关项目（Project）和域（Domain）的数据。它也有两个子集，分别是Project和Domain。Project就是以前OpenStack中tenant，也就租户，在N版本以后改为项目Project，是OpenStack中资源拥有者的基本单元，OpenStack中所有的资源都属于特定的项目。Project必须属于某个特定域，如果创建Project时没有指定Domain，则它默认属于Default域。与User和Group一样，Project也并非全局唯一，但是在其归属域（Domain）必须唯一。Domain这个概念也是Keystone v3版本开始引入，可以将其理解为资源的集合，也就是说某一资源必须属于一个特定域。为了将Domain内部的资源进一步细分使用权限和使用者，因此在Domain中可以定义所有归属Project的资源配额，在Project中又可以定义所有归属User和Group的资源配额，在Group中可以集中定义所有归属User的权限、角色和资源配额。Domain只可以通过命令行CLI或API接口创建，无法通过Web UI创建。 Assignment：角色分配，提供有关角色（Role）和角色分配（Assignment Role）的数据。Role规定最终用户可以获得的授权级别，角色Role可以在域Domain或项目Project级别授予，也可以在单个用户User或组Group级别分配角色，角色名称在该角色的归属域中必须是唯一的。角色分配Role Assignment是一个三元组，包括：Role、Resource和Identity等服务，也就是说OpenStack通过Identity、Resource和Role三类对象模型决定某个用户User拥有哪些资源，对拥有的资源能够执行哪些操作。 Catalog：服务目录，用于提供查询端点（Endpoint）的端点注册表，以便外部访问OpenStack的服务，因此它只有Endpoint一个子集。Endpoint本质上就是一个URL，拥有三种类型，分别是public、admin和internal，分别提供给不同的User，用于其访问OpenStack的服务。比如，提供给普通用户使用的就是public类型的Endpoint，提供给管理员使用的就是admin类型的Endpoint，而提供给OpenStack内部其他服务的就是internal类型的Endpoint。 Token：令牌服务，提供用户访问服务的凭证，代表着用户的账户信息。Token一般包含User信息，Scope信息（Project、Domain或者Tenant），Role信息，本质上就是代替用户的一个凭证。用户首次访问OpenStack时，均需要将用户名和密码发给Keystone，Keystone将其转换为有时效性的Token发给用户，用户将其缓存在本地客户端，后续访问其他服务，Token将代替用户进行。因此，Token必须和用户绑定。客户端在调用POST/Tokens后拿到返回结果，如果用户名和密码验证成功，则拿到诸如User、Project、Token、metadata、catalog等数据。从catalog中找到要访问Service的Endpoint，让后在Headers中放入｛X-Auth-Token:token_id｝信息，就向该Service发起访问请求。Service的WSGI APP在收到该Request请求后，首先验证该token_id是否有效，该验证一般都使用一个keystonemiddlerware的中间件来完成，如下图所示： 在OpenStack中Token支持的类型有：UUID、PKI、PKIZ和Fernet集中，OpenStack官方社区目前默认采用Fernet类型Token。几种Token类型对比如下： Token 类型 UUID PKI PKIZ Fernet 大小 32 Byte KB 级别 KB 级别 约 255 Byte 支持本地认证 不支持 支持 支持 不支持 Keystone 负载 大 小 小 大 存储于数据库 是 是 是 否 携带信息 无 user, catalog 等 user, catalog 等 user 等 涉及加密方式 无 非对称加密 非对称加密 对称加密(AES) 是否压缩 否 否 是 否 UUID类型的令牌，token_id只是一个纯粹的32位十六进制字符串；PKI类型的令牌token_id使用完整性和加密算法对token_data进行加密；PKIZ类型的令牌实际上就是对PKI加密后的令牌再进行压缩，压缩率约50%左右；Fernet类型的令牌是一个JSON格式的字符串，约255字节大小，在我们的实验环境中，通过API方式得到的Fernet类型的Token令牌如下： 上图中，令牌信息部分包含了Token当前的时间，有效截止时间，审计id，验证的方法等内容。在账户部分包含了生成该Token的账户id信息、账户名、域信息等内容。Token令牌类型的选择涉及多个因素，包括Keystone server的负载、region数量、安全因素、维护成本以及令牌本身的成熟度。Region的数量影响PKI/PKIZ令牌的大小。从安全的角度上看，UUID无需维护密钥，PKI需要妥善保管Keystone server上的私钥，Fernet需要周期性的更换密钥。因此，从安全、维护成本和成熟度上看，UUID &gt; PKI/PKIZ &gt; Fernet 。采用何种类型Token，目前华为在自有FusionSphere OpenStack解决方案的建议如下： Keystone server 负载低，region少于3个，采用UUID令牌。 Keystone server 负载高，region少于3个，采用PKI/PKIZ令牌。 Keystone server 负载低，region大于或等于3个，采用UUID令牌。 Keystone server 负载高，region大于或等于3个，目前OpenStack新版本默认采用Fernet令牌。 除了上述对象模型外，OpenStack还有Region、AZ、HA（Host Aggregate）的资源模型概念（这些其实属于计算资源池的概念）。Region是物理位置上划分的资源使用范围，可以简单理解一个数据中心DC就是一个Region，而AZ是Region内部从故障隔离角度划分的一个资源使用范围，其本质上是一组主机构成的资源池，资源池与资源池之间故障隔离（电源、网络、布线等），主要的划分依据还是动力故障隔离。HA（Host Aggregate）主机组是从物理资源的规格角度划分的资源使用范围，比如同是Dell 骁龙处理器的主机可以划分为一个HA，同是万兆网卡的主机可以划分一个HA等等。从资源使用者角度来看，其使用资源的范围从大到小的排序为：Region&gt;AZ&gt;HA&gt;Domain&gt;Project&gt;Group&gt;User，整个Keystone的对象模型的分配关系如下： Keystone的认证方式Keystone的认证方式主要包括三种：基于令牌的认证方式、基于外部的认证方式和基于本地的认证方式。生产环境中，最常用的是基于令牌的认证方式，需要重点学习。 基于令牌的认证方式：最常用的认证方式，认证请求发送时在HTTP/HTTPS头部添加一个X-Auth-Token的头域，Keystone检查该头域中token_id值，并与数据库SQL中的令牌值比对验证。 基于外部的认证方式：采用集成第三方的认证系统，客户端在认证请求头中天剑remote_user信息，Keystone的Identity服务作为接收该认证请求的前端，将认证请求转发给后端的外部认证系统进行认证，并将认证结果返回给客户端。在这种认证方式中，Keystone作为认证的中继服务单元。 基于本地的认证方式：这是默认的认证方式，即我们采用的用户名和密码认证。 基于令牌的认证方式—UUID令牌类型 该方案由于UUID令牌长度为固定32位十六进制字符串，不携带其他信息，OpenStack其他服务API收到该令牌后会向Keystone验证，并获得用户的其他信息，因此Keystone必须实现令牌的存储和认证，所有的验证均由Keystone完成，故采用这种认证方式的实际生产环境中部署Keystone服务的节点负荷较大。总体上，采用UUID类型的令牌认证共需要5步完成： Step1：客户端发起认证请求，在请求的HTTP/HTTPS的Body体中携带用户名和密码信息； Step2：Keystone通过认证后，给用户返回一个token信息，用于该用户后续访问其他服务的身份凭证，该凭证并非永久有效，而是有一个有效期限，默认为1小时。 Step3：客户端后续访问其他OpenStack服务时，比如访问nova-api，会在请求的HTTP/HTTPS的Header部分增加一个key-value格式的头域X-Auth-Token，其值就是Keystone返回给用户token_id。 Step4：其他服务接收到用户的访问请求后，会将请求消息头部的token_id发送给Keystone进行验证。 Step5：验证通过后，Keystone会返回该用户的三元组信息（User、Project、Role）给其他服务API，其他服务根据本地Policy的设定决定该用户使用其资源的权限。 基于令牌的认证方式—PKI令牌类型 该方案需要部署一台证书服务器（可以由部署Keystone服务的节点兼任），证书服务器将用户的私钥保存在Keystone本地，将用户的公钥分发给OpenStack的其他服务。这种认证方式客户的认证由OpenStack各个服务利用公钥完成，部署Keystone服务节点的负荷较小。总体上，采用PKI类型令牌认证需要4步完成： Step1：客户端发起认证请求，在请求的HTTP/HTTPS的Body体中携带用户名和密码信息。 Step2：Keystone利用本地的私钥对用户认证请求中的用户名和密码进行认证，通过后返回Token给客户端，在Token中携带签名后安全key值。 Step3：客户端后续访问其他OpenStack服务时，比如访问nova-api，会在请求的HTTP/HTTPS的Header部分增加一个key-value格式的头域X-Auth-Token，其值就是Keystone返回给用户前面后的安全key值。 Step4：其他OpenStack服务利用本地存储的公钥对token进行验证，通过后根据本地Policy的设定决定该用户使用其资源的权限。 基于令牌的认证方式—PKIZ令牌类型 该方案与PKI认证方案的流程类似，主要考虑到PKI认证方式的HTTP/HTTPS header头域过大（达到8K字节），为了减小管理平面带宽压力，故对PKI认证的头部采用压缩机制，而解压缩则在OpenStack的其他服务中进行。即与PKI认证方式相比，在Keystone返回的Token是一个压缩过的携带签名证书的Token-Key，而在OpenStack的其他服务首先需要对该token进行解压缩，然后再通过公钥进行认证。流程请参考PKI认证方式，这里不再赘述。 基于令牌的认证方式—Fernet 该方案基于Fernet（一种对称加密算法）的对用户信息进行加密而生成的令牌，与用户相关的所有的认证信息都保存在令牌中，故令牌本身就包含了用户的三元组信息（User、Project、Role），也因此Fernet令牌无需持久化，但是一旦Fernet令牌被攻破，那么就可以做所有该用户能做到的事情，因此采用该令牌类型的认证方式其令牌一般都有一个有效期，默认为1小时，当有效期到期后，需要重新申请令牌。与UUID、PKI、PKIZ令牌类型认证方式相比，Fernet类型的令牌值无需存储在后端数据库中，只需要放在内存中即可。相比UUID方式，Keystone数据库的眼里不大，无需查表；相比PKI/PKIZ方式，HTTP/HTTPS的header头部较小，占用管理平面带宽较小。采用该令牌类型的认证方式，如果Keystone服务采用高可用集群部署，需要在所有部署Keystone服务的节点共享秘钥用于Token的解密和认证。总体上，该流程也需要5步： Step1：客户端发起认证请求到Keystone服务器，请求HTTP/HTTPS的Body体中携带用户名和密码信息。 Step2：Keystone验证通过后，利用对称加密算法AES对返回给客户端的Token进行加密，该加密后的Token中携带用户的三元组信息（User、Project、Role）。 Step3：客户端在后续访问其他OpenStack服务的API时，会在HTTP/HTTPS的Header头部携带X-Auth-Token头域，其值就是加密后的token_id，也就是上面Token令牌示意图中的adjust_ids值。 Step4：其他OpenStack服务的API将该token_id发送给Keystone进行验证。 Step5：Keystone首先完成token_id的解密，验证通过后，将用户的三元组信息（User、Project、Role）发给其他OpenStack服务，其他服务根据本地Policy的设定决定该用户使用其资源的权限。 基于角色的访问控制—RBAC在上面分析过程中，Keystone服务只用于验证Token是否有效，而验证通过后用户资源的操作权限是通过各服务的Policy来决定。至于Policy通过什么设定来决定？首先需要了解下Policy文件的格式，如下： 上图是Neutron服务的Policy文件的一部分，可以看出该文件采用JSON格式，每一对｛｝内部通过key-value格式的键值对来定义服务的访问规则，每一条规则（Rule）就是一个键值对key-value。比如，上图中创建子网create_subnet这个操作，只有具备admin角色或该子网归属network的属主才能执行，那么可以推理出执行创建的子网的操作只有管理员admin，用户neutron或该子网归属network的租户project才具有权限，不符合上述条件的其他用户执行该命令时都会返回鉴权失败错误码403 Forbidden。而创建带VLAN ID的子网create_subnet:segment_id这个操作，只有具备admin角色的用户（管理员admin或用户neutron）才能执行，其他不符合条件的用户执行该操作都会返回鉴权失败错误码403 Forbidden。因此，通过我们的推理，就可以理解Policy模块在权限检查中所起的作用，以及整个基于角色的访问控制流程RBAC，而policy.json文件就是RBAC流程实现的关键，如下图： 上图中，用户发起操作请求Request到OpenStack服务的api进程，OpenStack服务的api进程首先去Keystone的Token模块进行Token有效性验证。验证通过后，返回的Token上下文中携带用户的三元组信息（User、Project、Role），并且Keystone的Policy模块调用OpenStack各服务本地Policy.json文件，根据用户操作请求Request的action结合用户的三元组信息判断用户该操作是否合法？不通过，则直接向客户端返回403 Forbidden；通过，则将验证结果返回给OpenStack的其他相关服务，OpenStack的其他相关服务按照用户的操作请求执行下一步操作Next。 从上面的分析中，我们可以知道Keystone的Policy模块根据各服务的policy.json文件来检查用户的操作权限，其检测时主要需要三方面的数据： 各服务的policy.json策略配置文件； Auth_token添加到HTTP/HTTPS头部的token数据； 用户的请求资源数据； 最后，放上一张Keystone的流程示意图总结Keystone如何实现认证和权限控制，如下： Keystone服务的实战操作（CLI命令行方式）这里只展示Keystone服务的命令CLI操作方式，至于Web UI的操作太过于简单，这里不再赘述。需要注意一点，Keystone的资源模型Domain只有命令行方式可以操作。 域Domain的实战操作步骤1：执行以下命令，导入admin-openrc.sh的环境变量，进入管理员视角。 1source admin-openrc.sh 步骤 2：执行以下命令，查看OpenStack域相关命令的用法。 1openstack domain -h 各子命令具体用法可通过如下方式进行查看： 步骤 3：执行以下命令，创建域“Demo_Domain” 1openstack domain create --enable --description "This is a temporary test domain" Demo_Domain 步骤 4：执行以下命令，查看OpenStack域列表信息 1openstack domain list 步骤 5：执行以下命令，查看刚刚创建的Demo_Domain域的详细信息 1openstack domain show 3c660f1059b94bc3864074200cc12981 #&lt;===这里我用了域的id信息进行查询，也可以使用域名，因为一个Region中域名唯一 步骤6：执行以下命令，设置刚刚创建的Demo_Domain为非激活态，再次查看刚刚创建的Demo_Domain域的详细信息 1openstack domain set --disable Demo_Domain 步骤7：执行以下命令，删除刚刚创建的Demo_Domain为非激活态，再次查看域列表信息 1openstack domain delete Demo_Domain 角色role、用户user及用户组group的相关操作实战步骤1：执行以下命令，查看OpenStack角色相关命令的用法（后续其他资源的操作指令使用帮助查询与此类似，不再赘述） 1openstack role -h 上图列出了角色role的相关操作包括CRUD，角色的分配，角色的移除等等。。。 步骤2：执行以下命令，查看OpenStack角色列表信息 上图中展示了当前OpenStack中角色有member、user、heat_stack_owner、admin、heat_stack_user、creator和reader。 步骤3：执行以下命令，创建角色”Kk_Role” 1openstack role create --domain kkutysllb Kk_Role &lt;===创建角色时，可以指定角色的归属域，也可以后续通过set指令指定，建议在创建时指定 步骤4：执行以下命令，查看新创建角色”Kk_Role”的详细信息 1openstack role show Kk_Role 我们会发现刚刚创建的角色Kk_Role不存在，但是刚刚明明是创建成功的。这是为什么？原因是我们在刚刚创建Kk_Role时制定了角色归属域为kkutysllb，而当前我们直接使用openstack role show Kk_Role显示角色Kk_Role详细信息时，由于没有显式制定归属域参数，所以默认显示的是Default域的角色信息，而Kk_Role并不归属Default域，自然就没有相关信息展示。因此，执行如下命令显示角色Kk_Role的详细信息和域kkutysllb的所有角色列表。 1openstack role show --domain kkutysllb Kk_Role 1openstack role list --domain kkutysllb 步骤5：执行以下命令，查看域kkutysllb下用户user列表 1openstack user list --domain kkutysllb &lt;===可以发现没有任何用户信息 步骤6：执行以下命令，创建一个域kkutysllb下的租户project 1openstack project create --domain kkutysllb kkutysllb 步骤7：执行以下命令，创建一个域kkutysllb下的两个用户kk_user01和kk_user02，密码均为123456 1openstack user create --domain kkutysllb --project kkutysllb --password-prompt kk_user01 1openstack user create --domain kkutysllb --project kkutysllb --password-prompt kk_user02 步骤8：执行以下命令，给两个用户kk_user01和kk_user02添加角色Kk_Role 123openstack role add --domain kkutysllb --project-domain kkutysllb --role-domain kkutysllb --user kk_user01 Kk_Roleopenstack role add --domain kkutysllb --project-domain kkutysllb --role-domain kkutysllb --user kk_user02 Kk_Role 步骤9：执行以下命令，查看两个用户kk_user01和kk_user02的角色分配情况 1openstack role assignment list --names | grep kk_user 步骤10：执行以下命令，创建一个用户组Kk_Group，将上面创建的两个用户kk_user01和kk_user02加入该组 1openstack group create --domain kkutysllb Kk_Group 1openstack group add user --group-domain kkutysllb --user-domain kkutysllb Kk_Group kk_user01 kk_user02 步骤11：执行以下命令，将kk_user02用户从用户组Kk_Group中移除 1openstack group remove user Kk_Group kk_user02 步骤 12：执行以下命令，禁用用户“kk_user02” 1openstack user set --domain kkutysllb --disable kk_user02 步骤13：执行以下命令，删除用户“kk_user02” 1openstack user delete --domain kkutysllb kk_user02 项目project相关操作实战步骤1：执行以下命令，查看域kkutysllb下的项目列表，可以查看到所有的项目 1openstack project list --domain kkutysllb 步骤2：执行以下命令，将项目kkutysllb去激活必过查看该项目详情 1openstack project set --disable --domain kkutysllb kkutysllb 步骤3：执行以下命令，将项目kkutysllb删除 1openstack project delete --domain kkutysllb kkutysllb 步骤4：执行以下命令，在Default域下的创建libing用户，并添加admin的角色 步骤6：执行以下命令，查看Default域下的admin项目的资源配额 1openstack quota show admin 步骤7：执行以下命令，修改项目“admin”的默认配额，如将实例数量修改为“5”，卷数量修改为“5”，网络修改为“10” 1openstack quota set --instance 5 --volumes 5 --network 10 admin 服务和服务端点的实战步骤 1：执行以下命令，查看OpenStack服务相关命令的用法和子命令的用法 步骤2：执行以下命令，创建swift服务，服务类型位object-store 1openstack service create --name swift --description "OpenStack Object Storage" object-store 步骤3：执行以下命令，创建服务“swift”的服务端点 1openstack endpoint create --region RegionOne object-store publick http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s 1openstack endpoint create --region RegionOne object-store admin http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s 1openstack endpoint create --region RegionOne object-store internal http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s 思考1、为什么不能修改admin帐号域归属信息为kkutysllb后，再来展示域kkutysllb内部的角色信息？ 2、在给用户添加角色时，能否同时指定用户的归属域Domain和项目Project？为什么？通过什么操作实现？ 3、用户通过CLI创建一个VM，Keystone如何验证该用户？如何验证该用户具有创建VM的权限？ 4、用户通过Horizon创建一个VM，Keystone如何验证该用户？如何验证该用户具有创建VM的权限？ 5、上述两种创建VM的方式，Keystone的验证流程有何区别？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-Web UI管理服务Horizon]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-Web-UI%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Horizon%2F</url>
    <content type="text"><![CDATA[概述OpenStack的用户界面由两部分组成：一是Web界面，二是Shell CLI界面。Horizon负责展现Web仪表盘，用户可以通过浏览器直接操作、管理、运维OpenStack的一些功能。由于OpenStack项目队伍不断壮大，Danshboard并不能展现所有的OpenStack功能，因此，最新的功能一般会先开发Shell命令行，也就是将CLI（Command Line Interface）提供给Linux用户操作。 在OpenStack的7个核心服务中，Horizon服务本质上就是一个Web Server，基于Django框架开发的，与Keystone服务一样，需要与其他提供业务的服务（计算nova、存储cinder、网络neutron等）进行全互联。如下图所示，如果将OpenStack的7个核心服务分为三层，从上到下依次是入口层、业务层和认证层。 Horizon服务最早出现OpenStack的Essex版本中，提供基于Web的控制界面，使得云管理员和租户/用户能够可视化的管理OpenStack的服务和资源。Horizon唯一依赖的服务就是Keystone认证服务，同时还可以与其他服务结合使用，例如镜像服务、计算服务、网络服务等。并且，Horizon还可以在独立服务（比如对象存储中）的环境中单独使用。 REST ful与WSGI介绍OpenStack各个项目都提供了API接口服务，通过endpoint提供访问URL。无论我们通过Horizon访问OpenStack的服务和资源，还是通过命令行CLI访问OpenStack的服务和资源，其本质都是通过各服务/资源的API的endpoint进行访问，即均是基于HTTP/HTTPS协议的Web服务访问。而这些API接口都是基于一种叫做“REST ful”的架构。RESTful是目前流行的一种互联网软件架构，REST（Representational State Transfer）就是表述状态转移的意思。 RESTful架构的一个核心概念是“资源”（resource）。从RESTful的角度看，网络里的任何东西都是资源，它可以是一段文本、一张图片、一首歌曲、一种服务等，每个资源都对应一个特定的URL（统一资源定位符）并用它进行标示，访问这个URL就可以获得这个资源。资源可以有多种具体表现形式，也就是资源的“表述”（representation），例如，一张图片可以使用JPEG格式，也可以使用PNG格式。URL只是代表了资源的实体，并不能代表它的表现形式。客户端和服务端之间进行互动传递的就只是资源的表述，我们上网的过程就是调用资源的URL，获取它不同表现形式的过程。这个互动只能使用无状态协议HTTP，也就是说，服务端必须保存所有的状态，客户端可以使用HTTP的几个基本操作，包括GET（获取）、POST（创建）、PUT（更新）、DELETE（删除），使服务端上的资源发生状态转化（State Transfer），也就是所谓的“表述性状态转移”。 OpenStack各个项目都提供了RESTful架构的API作为对外提供的接口，而RESTful架构的核心是资源与资源上的操作，也就是说，OpenStack定义了很多的资源，并实现了针对这些资源的各种操作函数。OpenStack的API服务进程接收到客户端的HTTP请求时，一个所谓的“路由”模块会将请求的URL转换成相应的资源，并路由到合适的操作函数上。这个所谓的路由模块Routes源自于对Rails（Ruby on Rails）路由系统的重新实现，采用MVC（Model-View-Controller）模式，收到浏览器发出的HTTP请求后，路由系统会将这个请求指派到对应的Controller。RESTful只是设计风格而不是标准，Web服务中通常使用基于HTTP的符合RESTful风格的API。而WSGI（Web Server Gateway Interface，Web服务器网关接口）则是Python语言中所定义的Web服务器和Web应用程序或框架之间的通用接口标准。 WSGI，顾名思义就是Web服务器网关接口。所有客户端访问Web服务，都需要经过该网关接口进行转发。而Web应用的本质就是客户端发送一个HTTP的请求，服务器收到请求生成一个HTML文档，服务器将该HTML文档作为HTTP响应的Body发送给客户端，客户端从HTTP响应的Body体中提取内容并显示出来。像这类生成HTML，接受HTTP请求、解析HTTP请求、发送HTTP响应等代码如果由我们自己来写，则需要耗费大量的时间和精力。正确的做法是底层代码由专门的服务器软件实现，需要一个统一的接口，让程序员专心编写业务层面的代码，这个接口就是WSGI。 WSGI有两方：服务器方和应用程序方，如下图所示： 1）服务器方：其调用应用程序，给应用程序提供环境信息和回调函数，这个回调函数用来将应用程序设置的HTTP Header和Status等信息传递给服务器方。 2）应用程序：用来生成返回的Header、Body和Status，以便返回给服务器方。 从名称上看，WSGI是一个网关，作用就是在协议之间进行转换。换句话说，WSGI就是一座桥梁，桥梁的一端称为服务端或者网关端，另一端称为应用端或者框架端。当处理一个WSGI请求时，服务端为应用端提供上下文信息和一个回调函数，应用端处理完请求后，使用服务端所提供的回调函数返回相对应请求的响应。 WSGI的服务方和应用程序之间还可以加入Middleware中间件，从服务端方面看，中间件就是一个WSGI应用；从应用端方面看，中间件则是一个WSGI服务器。这个中间件可以将客户端的HTTP请求，路由给不同的应用对象，然后将应用处理后的结果返回给客户端。如下图示，我们也可以将WSGI中间件理解为服务端和应用端交互的一层包装，经过不同中间件的包装，便具有不同的功能，比如URL路由分发，再比如权限认证。这些不同中间件的组合便形成了WSGI的框架，比如Paste。 OpenStack使用Paste的Deploy组件（http://pythonpaste.org/deploy/）来完成WSGI服务器和应用的构建，每个项目源码的etc目录下都有一个Paste配置文件，比如Nova中的etc/nova/api-paste.ini，部署时，这些配置文件会被复制到系统/etc//目录下。Paste Deploy的工作便是基于这些配置文件。除了Routes与Paste Deploy外，OpenStack中另一个与WSGI密切相关的是WebOb（http://webob.org/）。WebOb通过对WSGI的请求与响应进行封装，来简化WSGI应用的编写。 WebOb中两个最重要的对象，一是webob.Request，对WSGI请求的environ参数进行封装；一是webob.Response，包含了标准WSGI响应的所有要素。此外，还有一个webob.exc对象，针对HTTP错误代码进行封装。除了这3个对象，WebOb提供了一个修饰符（decorator），即webob.dec.wsgify，以便可以不使用原始的WSGI参数和返回格式，而全部使用WebOb替代。 除了Paste提供的WSGI框架外，为了解决Paste组合框架的Restful API代码过于臃肿，导致项目的可维护性变差的弊端。一些OpenStack的新项目选了Pecan框架来实现Restful API。Pecan是一个轻量级的WSGI网络框架，其设计并不想解决Web世界的所有问题，而是主要集中在对象路由和Restful支持上，并不提供对话（session）和数据库支持，用户可以自由选择其他模块与之组合。 Horizon服务的实操体验Step1：使用浏览器登录OpenStack Dashboard：http://rocky-controller/dashboard/，输入**域名，用户名**和**密码**，单击“Sign In”，进入管理员视图Overview主界面。 Step2：登录成功后，默认进入项目概览页面，如下： 后面，将以发放一个简单的cirros虚拟机实例为例，来帮助大家快速熟悉OpenStack Dashboard的基本操作界面。 首先，要创建一个虚拟机VM实例，需要完成虚拟机规格的定义，在左侧导航栏，选择“Admin &gt; Compute &gt; Flavors”，进入规格列表，单击页面右上角的“Create Flavor”。 弹出创建规格对话框，输入如下信息： 然后，需要创建虚拟机VM实例挂载的网络信息，其本质上就是创建虚拟机的虚拟网卡（后面Neutron部分会讲）。在左侧导航栏，选择“Admin &gt; Network &gt; Networks”，进入网络列表，单击页面右上角的“Create Network”。 在Network标签页面，填入如下信息，然后点击Next。 在Subnet标签页，填写如下信息，然后点击Next。 在Subnet Details页面，填写如下信息，然后点击Create按钮。 由于虚拟机VM实例所需要的镜像cirros在安装Glance服务时已经完成上传，因此完成上述虚拟机VM实例所需的最小资源配置后，下面就可以通过页面拉起一个虚拟机VM实例。 Step1：在左侧导航栏，选择“Project &gt; Compute &gt; Instances”，进入虚拟机实例列表，单击页面右上角的“Launch Instance”。 Step2：弹出发放实例对话框，在“Details”页签，输入虚拟机实例的名称“Web_Srv01”，其他保持默认，单击“Next”。 Step3：在“Source”页签，“Create New Volume”下方选择“No”，单击“Available” 下方列表中cirros镜像（环境安装完成后，系统默认创建的测试镜像）的上传箭头，“Allocated”下方列表中将显示选择的镜像，其他保持默认，单击“Next”。 Step4：在“Flavor”页签，单击“Available”下方列表中刚刚创建的规格“Flavor_web”的上传箭头 ，“Allocated”下方列表中将显示选择的规格，单击“Launch Instance”，完成虚拟机实例的创建。 Step5：返回虚拟机实例列表，查看创建的虚拟机实例的状态，等待状态变为“Active”，表示虚拟机VM实例启动成功。 Step6：单击虚拟机实例名称“Web_Srv01”，进入虚拟机实例概览页面，可查看虚拟机实例的详细信息，如下所示： Step7：单击“Console”页签，可进入虚拟机实例的终端页面，如下所示： 以上，就是通过Horizon服务利用Web页面可视化创建一个虚拟机VM实例的全过程。通过上图中Interface页签，log页签还可以查看虚拟机VM实例的网卡信息及操作日志。同时，Horizon页面不仅用来创建虚机，还可以创建Domain、Project、User、UserGroup、Network、Glance、Volume等资源，在后续各服务的实操环节会有展示。 思考1、注销admin用户，通过普通用户登录后，观察Project的Overview视图与admin用户下的Overview视图的区别，并分析之间产生差异的原因？ 2、尝试通过命令行CLI的方式完成上述虚拟机VM实例创建的操作。**（提示：命令行的用法可以通过openstack &lt;资源类型名&gt; &lt;动作&gt; –help方式查看帮助获得，比如：虚机资源类型名为server，网络资源类型名为network，子网资源类型名为subnet，镜像资源类型名为image，规格资源类型名为flavor等等）** 3、如果在创建虚拟机VM实例的过程中，选择镜像时要求创建卷，那么所创建的VM实例的系统盘由哪个服务创建？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-03-中间件分布式缓存Memcache和Redis]]></title>
    <url>%2F2020%2F03%2F03%2F2020-03-03-%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98Memcache%E5%92%8CRedis%2F</url>
    <content type="text"><![CDATA[概述缓存系统在OpenStack集群部署中有着非常重要的应用，在开源OpenStack的解决方案中一共使用了两种分布式缓存技术，一种是用于前端API服务访问的Memcache缓存，另一种就是用于后端计量和告警信息上报的Redis缓存。无论是Memcache缓存还是Redis缓存，均是一种分布式缓存。所谓分布式缓存，就是指缓存服务可以部署多个相互独立的服务器节点上，可以彼此分散独立存取数据，减少单节点的数据存取压力，但是各节点之间的数据并非要求一定保持一致性。大多数的OpenStack服务都会使用Memcache或Redis缓存系统存储如Token等临时数据，缓存技术的应用场景一般都是应用在有大并发访问需求的地方，比如使用Web服务的门户网站，淘宝、京东等购物网站等等。缓存系统可以认为是基于内存的数据库，相对于后端大型生产数据库MySQL等，基于内存的缓存系统能够提供快速的数据访问操作，从而提高客户端的数据请求访问，并降低后端数据库的访问压力，避免了访问重复数据时数据库查询所带来的频繁磁盘IO和大型关系表查询时的时间开销。 Memcache分布式缓存Memcache利用系统内存对客户端经常进行反复读写和访问的数据进行缓存，来减轻后端数据库的访问负载和提高客户端的数据访问效率。Memcache中缓存的数据经过HASH之后被存放到位于内存上的HASH表内，而HASH表中的数据以Key-Value的形式存放。在Memcache集群中，Memcache客户端的API通过32Bit的循环冗余校验码（CRC-32）将存储数据的键值对进行HASH计算后，存储到不同的Memcached节点服务器上，而当Memcached服务器节点的物理内存剩余空间不足时，Memcached将使用最近最少使用算法（LRU，LeastRecentlyUsed）对最近不活跃的数据进行清理，从而腾出多余的内存空间供缓存服务使用。在OpenStack的集群部署中，大部分子项目都会用到内存缓存系统进行客户端访问数据的缓存，从而提高访问速率并减轻数据库的负载压力。 Memcache缓存系统在工作流程的设计上比较简单，其主要思想就是Memcache的客户端根据用户访问请求中的Key，到Memcached服务器的内存HASH表中获取对应此Key的Value，Memcache客户端取到值之后直接返回给用户，而不用再到数据库中进行数据查询。当然，内存HASH表中不可能预知和存储全部客户端需要的Key-Value值对，因此，如果在Memcache服务器中没有找到与请求中的Key对应的Value，则转向后端数据库进行查询，并将查询到的Value与Key进行HASH后存入Memcached服务器中，从而保证曾经存储过的数据一定能被查询到，并将数据库中查询到的数据缓存到Memcache服务器中，从而提高下一次缓存查询的命中率，其工作流程如下图所示。 上述流程中，Memcache的整个工作流程被分为七步实现，但是并非每次查询都需要经历七个步骤，这需要根据是否在缓存中命中请求Key对应的Value来决定。具体流程如下： 1~2：用户终端向Web服务器发起请求（Web服务器中部署有Memcache客户端），Web服务器中的Memcache客户端向Memcached服务器发起查询请求，查询目标为用户请求中包含的Key所对应的Value。 1~2~3~7：如果Memcache客户端在Memcached服务器中查询到与请求Key对应的Value，则直接返回结果，本次数据查询过程结束，即整个过程不会访问数据库。 1~2~4~5~7~6：如果经过步骤1和2没有查询到数据，则表明Memcached服务器没有缓存请求中的Key对应的Value，即本次缓存查询未命中，但是查询到此并不结束，而是转向数据库中继续查询该值，并将查询到的数据返回给客户端，同时将该数据缓存到Memcached服务器的HASH表中，以便客户端再次发起相同请求时，该值在Memcached服务器中能够直接命中，而无需再次查询数据库。 需要注意的是：当数据库中存放的永久数据发生更新时，Memcached服务器中缓存的值也需要同时更新。如果Memcached服务器中分配给Memcached使用的内存空间耗尽，则Memcache缓存系统将使用LRU算法和数据的到期失效策略清理非活跃的冷数据，在这个过程中，已达到策略设置中的保存期限的数据将会首先被清除，然后再清除最近最少使用的数据。 由于Memcache缓存系统中的每个Memcached服务器节点独立存取数据，彼此之间不存在数据的镜像同步机制，因此，如果某一个Memcached服务节点故障或者重启，则该Memcached节点上缓存在内存中的全部数据均会丢失，丢失缓存数据后，访问将无法在缓存中命中任何Key，所有Key的访问都需要到数据库重新查询一遍，同时将查询的数据再缓存到Memcached服务器的内存缓存中，即Memcached服务器每重启一次，其储存的数据就要被重新缓存一次。同时，Memcache缓存系统以Key-Value为单元进行数据存储，能够存储的数据Key尺寸大小为250字节，能够存储的Value尺寸大小为1MB，超过这个值不允许存储。并且Memcache缓存系统的内存存储单元是按照Chunk来分配的，这意味着不可能所有存储的Value数据大小都正好等于一个Chunk的大小，因此必然会造成内存数据碎片而浪费存储空间。 Memcache集群的配置极为简单，服务器端只需运行Memcached服务，客户端只需在应用程序的配置文件中指定要访问的Memcached服务器节点IP地址和端口组成的列表。因为Memcache集群无需在Memcached服务器端进行配置，数据的分布存储完全取决于Memcache客户端的节点选取算法。如下图所示，当应用程序向Memcache集群存储数据时，Memcache客户端会根据请求存储数据的Key-Value值对，利用一定算法从Memcached集群存储节点中选取某一个Memcached服务器节点来存储数据，当算法选定该节点后，对应Key的Value就会被存储到该节点中。 Memcache客户端在选取缓存节点时所采用的算法对数据的最终存储位置有着决定性的作用，而节点选取算法也是Memcache缓存系统中最重要的部分。在通常使用时，Memcache缓存系统中最常使用的两个算法分别是余数HASH算法和一致性HASH算法。 余数HASH算法就是将需要存储的Key-Value值对的Key字符通过HASH算法后得到HASH Code，这里的HASH Code是一个数字，而HASH算法的作用就是将一个字符串通过HASH后得到一个数值。通常，相同的字符串经过相同的HASH算法后一定会得到相同的HASH Code，因此，存储和提取相同Key的Memcached目标节点一定是同一个节点。比如Key经过HASH之后得到的HASH Code为100，Memcache集群缓存系统中有3个Memcached节点，那么余数HASH就是将HASH Code 100除以节点数目3，然后取余得到的结果，这里结果为1，这样选取到的Memcached服务器节点为编号为1，即Memcached1节点。因为不同的Key值得到的HASH Code是不同的，这样取余后得到的结果是比较随机均匀的，数据就会随机存储到不同的Memcached服务器节点上，因此，如果在一个系统架构中，Memcached服务器节点数目比较固定，使用过程中无需扩容节点，则余数HASH算法可以很好地满足Memcache集群中的节点选取过程。然后，当需要扩容节点时，比如当增加3个Memcached节点后，现在Memcached服务器节点数目为6，HASH Code仍然为100，可余数HASH的结果却为2，而不是之前的1，即Memcache客户端在提取这个Key的值Value时，算法将会路由到Memcached2节点去，但是扩容之前该Key的Value是存储在Memcached1节点中的，因此客户端将不能在缓存系统命中此Key。 为了解决这种扩容引起的缓存命中失效问题，引入一致性HASH算法。其实，引入一致性HASH并不能完全解决缓存命中失效的问题，但是可以大大降低缓存命中失效发生的概率，而且随着缓存节点数量逐渐增多，这种缓存命中失效带来的IO瓶颈问题几乎可以忽略不计。一致性HASH通过将Memcached节点和Key字符串HASH后放到一个HASH环上，然后Key字符串的HASH值再以顺时针的方式找到距其最近的节点HASH值，该节点HASH值对应的节点便是Key的Value存储节点（一致性HASH在分布式存储部分会详细讲解，这里简单提一下）。当集群中增加缓存节点，只有HASH Code靠近增加节点HASH值的数据会受影响，其余节点仍保持原来的缓存映射关系，所以一致性HASH相比余数HASH在节点变化的场景下，缓存命中失效的概率要低很多。对于Memcache缓存系统而言，究竟选择余数HASH还是一致性HASH，需要根据Memcache缓存系统的实际使用情况而定。 如前所述，Memcache集群只能解决数据访问的瓶颈问题，并不能解决数据的高可用问题。如果要实现Memcache集群的数据HA，就必须借助第三方工具来实现。Memcache缓存系统最常使用的HA方式是Memcached节点代理，其中最常使用的代理软件为Magent。通过Magent缓存代理，可以防止缓存系统的单点故障，同时将缓存代理服务器配置为主备模式，即可实现代理服务器的高可用。在配置有Magent代理的Memcache缓存系统中，当客户端发起缓存数据请求时，客户端将首先连接到缓存代理服务器，然后缓存代理服务器再连接到Memcached服务器，Magent缓存代理服务器可以连接多台后端Memcached服务器，进而可以将相同的缓存数据同时存储到多个Memcached服务器，最终实现缓存数据的同步存储。如下图所示，实现Memcache服务的数据HA非常简单，只需在两台同样配置的服务器上分别安装memcache和magent软件，稍做简单配置即可实现Magent代理的高可用缓存系统。 Redis分布式缓存Redis，俗称远程字典服务器（Remote Dictionary Server，Redis），是一款由ANSI C语言编写、遵守BSD协议、支持网络访问、基于系统内存的开源软件，同时，Redis也是支持日志型和Key-Value键值对类型数据持久化的数据库。由于Redis中存储的Value值类型可以是字符串（String）、哈希（HASH）、列表（List）、集合（Sets）和有序集合（Sorted Sets）等类型，因此Redis通常也被称为数据结构服务器。在Redis中，这些数据类型都支持Push/Pop、Add/Remove以及针对集合的交集、并集和差集等丰富的数据操作，并且这些操作都具有保持数据一致性的原子性，而且Redis还支持各种不同的数据排序方式。 与Memcache缓存系统类似，为了保证高效的数据存取效率，Redis的数据都被缓存在内存中，但是Redis与Memcache在处理数据存储方式上有很大的差异，即Redis默认会周期性地把缓存中更新的数据写入磁盘进行永久保存，并将对缓存数据的修改操作追加到日志文件中，由于Redis本质上就是一个基于内存的数据库，因此Redis具备很多如日志记录等通用数据库的共性。Redis为了实现数据高可用，其利用已存入磁盘中的数据在不同节点之间进行同步操作，从而实现了Master-Slave模式的主从数据高可用，Redis的数据可以从Master服务器向任意数量的Slave服务器上进行同步。 在Redis缓存系统中，并非所有通过Redis客户端写入的数据都永久性地存储在物理内存中，这是Redis和Memcache缓存系统最为明显的区别，Redis会定期性地将物理内存中的数据写入磁盘，其原理类似内存虚拟化技术中内存置换技术。Redis实现了用虚拟内存（VM，Virtual Memory）机制来扩充逻辑可用的内存空间，当Redis认为物理内存空间使用量达到某个预设值时，Redis将自动进行内存页的交换（Page Swap）操作，从而将缓存中的数据写入磁盘中。 Redis的VM机制使得Redis可以存储超过系统本身物理内存大小的数据，但是，这并不意味着可以无限减小物理内存并不断增加虚拟内存以满足Redis的整体内存需求，主要有以下3点约束： 1）Redis将全部的Key保存在物理内存中，从而可以快速命中Key，并且由于Redis在读取数据时只在物理内存中进行Key的索引，找到Key之后再到物理内存或者虚拟内存中读取对应的Value，因此为Redis预留的物理内存空间至少要满足Key的存储。 2）过度地减小为Redis预留的物理内存空间，会使得Redis频繁发生Swap操作，极大降低Redis的数据稳定性，并对Redis的数据访问造成响应缓慢等影响。 3）在Redis将内存中的数据交换到磁盘的时候，提供访问服务的Redis主线程和进行交换操作的子线程会共享这部分内存空间，所以如果此时有请求要更新这部分内存空间的数据，则Redis将会阻塞这个请求操作，直到子线程完成Swap操作后才可以进行数据的更新修改。 上述的第三点约束是一种默认情况下的机制，即Redis会使用阻塞方式来使I/O请求等待Swap操作将全部所需的Value数据块读回内存，然后才进行下一步操作。这种机制在小量访问对数据访问的影响可以忽略，但是在大并发场景下，其影响直接造成客户体验变差。因此，在大并发场景下，需要配置Redis的IO线程池来解决，使得Redis并发进行磁盘到物理内存的数据读回操作，减少IO阻塞时间。 在Redis节点重新启动之后，丢失的缓存数据会从磁盘重新加载或者从日志文件中重新恢复，与Memcache相比，天生具备数据持久化功能。针对两种不同的缓存数据恢复方式，Redis提供了两种数据持久化的方式，分别是Redis数据库方式（RDB，Redis DataBase）和日志文件方式（AOF，Append Only File）。RDB持久化方式就是将Redis缓存数据进行周期性的快照，并将其写入磁盘等永久性存储介质中，而AOF采用的是与RDB完全不一样的数据持久化方式，在AOF方式下，Redis会将所有对缓存数据库进行数据更新的指令全部记录到日志文件中，待Redis重新启动时，Redis将会把日志文件中的数据更新指令从头到尾重复执行一遍，这是一个重现数据变更的过程，当执行到日志文件末尾时即可实现缓存数据的恢复，AOF恢复方式与DB2数据库的redo日志回滚恢复方式原理类似。在实际的Redis使用过程中，RDB和AOF这两种数据持久化方式可以同时使用，并且这也是官方推荐的数据持久化方式，在两种方式同时开启的情况下，如果Redis数据库重新启动，则会优先采用AOF方式来进行数据恢复，这是因为AOF方式对数据恢复的完整性要高于RDB方式（RDB因为是周期性存储快照数据，所以数据完整性至少丢失一个快照存取周期）。 在Redis3.0版本发行之前，Redis自身并不具备集群高可用功能，尽管Redis3.0中增加了Redis的Cluster功能，但是，使用Redis Cluster功能的用户相对还是很少，其可靠性和稳定性相对来说还需要进一步验证。目前，针对Redis的集群高可用功能，更多的是借助第三方负载均衡和集群管理软件（Pacemaker+HAProxy/Keepalived）来实现或者是通过对Redis进行二次开发来实现。在借助第三方负载均衡和集群管理软件的方案中，通过负载均衡软件的VRRP协议创建虚拟IP资源以对外提供高可用的Redis虚拟IP，并通过集群管理软件创建Redis的Master-Slave多状态资源，当Redis的Master节点故障后，通过集群管理软件的资源管理功能，利用Redis的Agent脚本将Slave节点提升为Master，同时虚拟IP迁移到新的Master节点继续对外提供服务。 Redis主从模式的集群能够实现数据高可用的关键，是Redis提供的数据Replication功能，即数据能够在Redis的主从节点之间进行复制，从而保证了数据的高可用性。其复制是异步复制，在复制的过程中，Master节点是非阻塞的，不会受到Slave节点数据同步的影响，在Slave进行同步的时候，Master继续提供高效的内存数据查询服务。同时，在复制的过程中，Slave节点也是非阻塞的。在数据同步时，通过配置可以允许Slave节点利用之前的数据集提供查询功能活着此时返回一个错误，并提示数据正在同步中。但是不管怎么设置，在Slave同步完成之后，Slave节点的陈旧数据将会被删除，新同步来的数据将会被加载到内存，而在这个删除与加载的过程中，Slave的访问是暂时被禁止的。Redis的Replication功能除了数据冗余之外，还可以将数据的只读查询分散到多个Slave节点，以让Slave节点来负责这类数据的排序操作。 在Redis的Master-Slave集群模式下，如果配置了一个或多个Slave节点并将其连接到Master节点，则不管Slave节点是第一次连接Master还是重新连接到Master，当连接完成之后，Slave都会发送一个同步命令给Master，Master接到命令后开始在后台进行内存数据快照，并将在快照期间对当前缓存数据进行更新的命令全部缓存起来。当Master上的数据以RDB方式保存完成后，Master将该RDB数据文件传送给Slave，Slave将数据文件写入本地磁盘，然后再将这些来自Master的RDB数据文件加载到内存中，之后Master将数据快照期间缓存的命令发送给Slave，Slave依次执行这些数据变更命令，使得每次同步操作后都能确保自身数据与Master节点缓存数据的一致性。当Slave与Master的连接因某些原因断开后，Slave会自动与Master重新建立连接。如果Master同时收到多个Slave发送的数据同步请求，则Master仅会对缓存数据做一次快照保存，然后将保存的RDB数据文件发送给多个Slave节点。当同步中的Slave与Master断开又重新连接后，Slave将会重新发送同步全部数据的请求，但是从Redis2.8开始，重新连接的Slave可以通过断点续传功能仅同步上次未完成同步的数据请求，其机制是Master和所有的Slave将同步操作的日志记录到内存文件中，同时还记录了数据同步的偏移量和Master此时的运行标志（Run ID），如果数据同步过程被断开，Slave将会重新连接并请求Master继续同步。此时，如果Master的Run ID还没有改变，并且设置的同步偏移量在同步日志中可用，那么同步操作将会从中断点继续同步，如果不能满足这两个条件，则数据同步操作只能重新开始。此外，如果Master的Run ID没有保存到磁盘上，则无法进行续传同步。如果Master节点的磁盘I/O性能较差，而不同的Slave节点不在同一时间点发生同步请求，则数据同步过程必然增加Master的工作负载。为了解决这个问题，Redis2.8.18中提出了无盘复制（Diskless Replication）的设计，在这种模式下，Master直接将RDB文件传送到Slave节点，而无须再在本机上进行I/O操作，无盘复制技术的提出极大提升了Master在数据同步复制过程中的响应效率。 Redis Cluster集群技术上面提到Redis Cluster是Redis官方在3.0版本以后推出一种集群技术，作为官方的重点推荐，虽然其可靠性和稳定性还有待验证，但是不影响我们理解其工作原理和机制。Redis Cluster是Redis的分布式实现，RedisCluster在设计之初就考虑到了去中心化的问题，因此集群中不存在任何中心控制节点，每一个节点都是功能对等的集群成员，每个节点除了保存有自己的配置元数据外，还保存着整个集群的状态信息。由于每个集群节点之间彼此互联，并且随时保持活动连接，客户端只需连接到集群中的任一个节点，便可获取到存储在其他节点上的数据。机制示意图如下： Redis Cluster中的各个节点之间保持相互连接，并且彼此之间可以通信，客户端随意连接到任何一个集群节点就能将整个Redis集群作为一个整体来访问，同时客户端也无需知道Redis Cluster将其提交的数据存入哪个Redis节点中，数据存储完全由Redis Cluster根据自己的算法来决定。在Redis Cluster中，数据被分散存储到不同的Redis节点上，Redis Cluster并没有采用一致性HASH来分配数据存放到不同的节点，而是采用了一种称为哈希槽（HASH Slot）的技术来分配数据。Redis Cluster默认将存储空间分配为16384 Slots，每个节点承担其中的一部分Slots。如果Redis需要扩充Redis节点数，只需重新分配每个节点的槽位。当客户端通过SET命令来保存一个Key-Value键值的时候，Redis Cluster会采用CRC16（Key）对16384取模来决定该Key应该被存储到哪个Slots中，具体的计算公式就为：slot_num=CRC（’Key_name’）％16384。假设通过公式计算得到的Slot_num为14201，则数据会被存储在14201槽位对应的节点上。为了保证数据的高可用，Redis Cluster用到了Master-Slave的主从数据复制模式，即为每个Master都设置一个或多个Slave，Master负责数据存取，而Slave负责数据的同步复制，当Master故障时，其中的某个Slave会被提升为Master。假设集群中有A、B、C三个主节点，同时分别为其设置了A1、B1、C1三个Slave节点，此时如果B节点故障，则集群会将B1节点选取为Master节点继续提供服务，当B节点恢复之后，它就变成了B1的Slave节点。当然，如果B和B1同时故障，则集群B就不可用了。 Redis在OpenStack中的应用在OpenStack开源云的计费项目Ceilometer中，当进行规模化集群部署时，通常使用Redis插件来为运行在控制节点集群上的多个Ceilometer Agents提供协调机制，Redis插件使用具有Redis后端的Tooz库来为Agents提供一组轮询使用的资源集。在部署了Redis插件之后，Ceilometer Agents的部署可以分布到每个控制节点上，并且这些分布的Agents将会自动加入协调组（Coordination Group）中，在这种分布式集群部署中，通常使用Pacemaker创建Redis-server资源以监控Redis插件进程的运行状态，同时插件会自动配置Redis-sentinel进程来监控Redis集群状态，而Redis-sentinel的主要作用是当Redis集群的Master节点故障时，通过一定的机制重新选取新的Master节点，同时将Ceilometer的代理重新定向到新的Master节点，并进行Redis集群节点之间的数据同步。 在OpenStack集群高可用部署中，Redis被用作Ceilometer组件的分布式协调组后端缓存，同时集合集群资源管理软件Pacemaker，将Redis配置为Pacemaker的资源以实现Redis-server的高可用Master-Slave模式。此外，还需要为Redis集群配置一个高可用IP地址，高可用IP可以通过Pacemaker的虚拟IP资源实现。在三节点的OpenStack控制节点集群架构中，将其中一个控制节点作为Redis的Master节点，另外两个控制节点作为Redis的Slave节点，Redis和Ceilometer的Central agent在三个控制节点中的部署拓扑如下所示： 在集成Pacemaker的集群中，Redis服务被配置成为Pacemaker的多状态资源，即Master/Salve资源组。通过Pacemaker配置的Redis高可用Master/Slave集群中，controller01为Redis的Master节点，controller02和controller03为Redis的Slave节点，同时Redis对外提供服务的虚拟IP地址运行在Master节点，这是通过Pacemaker的Colocation约束设置的，即Redis的VIP只会运行在Master节点上。如果Redis的Master节点故障，则Pacemaker将会从两个Slave节点中重新选举一个Mastre节点，而VIP也会相应地自动移动。 总的来说，memcache缓存技术主要用来存储见到key-value格式的数据，如OpenStack各个服务的API交互数据。如果需要缓存数据类型多样、数据可靠性高的场景，如OpenStack的ceilometer服务提供的计量和告警等数据，则采用Redis这种支持数据持久化且支持多种数据结构的缓存技术更加合适。 思考：1、Memcache技术的服务端口号是多少？在OpenStack的最小系统配置下（keystone、nova、neutron、cinder、glance、horizon），控制节点最少需要开启多少个memcache服务进程？ 2、nova服务至少需要开启多少个memcache服务进程？]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-02-中间件消息队列服务RabbitMQ]]></title>
    <url>%2F2020%2F03%2F02%2F2020-03-02-%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%9C%8D%E5%8A%A1RabbitMQ%2F</url>
    <content type="text"><![CDATA[概述OpenStack是由Nova、Neutron、Cinder等多个组件构成的开源云计算项目，各组件之间通过REST接口进相互通信和彼此调用，而组件之间的REST接口调用，是建立在基于高级消息队列协议（AdvancedMessageQueueProtocol，AMQP）上的RPC通信。在OpenStack中，AMQP Broker（可以是Qpid Broker，也可以是RabbitMQ Broker）位于OpenStack的任意两个内部组件之间，并使得OpenStack内部组件以松耦合的方式进行通信。纵观OpenStack的组件架构设计，其中的消息队列在OpenStack全局架构中扮演着至关重要的组件通信作用，也正是因为基于消息队列的分布式通信技术，才使得OpenStack的部署具有灵活、模块松耦合、架构扁平化和功能节点弹性扩展等特性，所以消息队列在OpenStack的架构设计和实现中扮演着极为核心的消息传递作用。同时，消息队列系统的消息收发性能和消息队列的高可用性也将直接影响OpenStack的集群性能。 在OpenStack的部署过程中，用户可以选择不同的消息队列系统来为OpenStack提供消息服务，但是在众多的消息队列系统中，RabbitMQ是使用最多也是综合性能最接近生产系统要求的消息队列系统。华为FusionSPhere OpenStack的解决方案中，也是采用的RabbitMQ作为消息队列系统。 AMQP-高级消息队列协议AMQP是应用层协议的一个开放标准，专为面向消息的中间件而设计，同时，AMQP也是面向消息、队列、路由、可靠性和安全性而设计的高级消息队列系统。AMQP提供统一消息服务的应用层标准协议，客户端与消息中间件基于此协议传递消息，并且消息传递不受不同客户端/中间件产品和不同开发语言等条件的限制。在消息的传递过程中，消息中间件主要用于组件之间的解耦，即消息生产者不用关系消息由谁来消费，消息消费者也不用关系消息由谁产生。 AMQP协议本质上是一种二进制协议，可以让客户端应用与消息中间件之间异步、安全、高效地交互。该协议框架也是分层结构，总体上分为三层，分别是：Model层、Seesion层和Transport层，消息在Session层与Transport层之间传递，用于执行Model层的Command，本质上就是二进制的封装和解封装过程。 Model层：决定了基本域模型所产生的行为，这种行为在AMQP中用“Command”表示。 Session层：定义客户端与Broker之间的通信，通信双方都是一个Peer，可互称作Partner，会话层为模型层的Command提供可靠的传输保障。 Transport层：专注于数据传送，并与Session保持交互，接受上层的数据并组装成二进制流，数据传送到Receiver后再进行解析并交付给Session层。Session层需要Transport层完成网络异常情况的汇报，顺序传送Command等工作。 RabbitMQ中的基本概念RabbitMQ用Erlang语言开发，是AMQP的开源实现。在RabbitMQ的部署使用和故障排除过程中，将会接触到很多RabbitMQ的基础概念，了解和掌握这些概念，是使用RabbitMQ为集群系统提供消息服务的基础。 Connection和ChannelConnectionFactory、Connection和Channel都是RabbitMQ对外提供的API中最基本的对象。Connection是RabbitMQ的Socket连接，本质上是一个TCP协议连接，消息的生产者Producer和消费者Consumer都通过Connection建立的TCP链接连接到RabbitMQ Server，因此RabbitMQ服务启动时的初始化过程就是创建一个Connection的连接，而ConnnectionFactory是Connection对象的工厂函数，用来初始化Connection对象。Channel是客户端与RabbitMQ交互消息最重要的一个接口，客户端大部分的业务操作是在Channel这个API接口中完成的，包括定义Queue和Exchange、绑定Queue与Exchange、发布消息等操作。Channel本质上是一个虚拟连接，它建立在TCP连接中，数据流动都是在Channel中进行，通常，程序启动后首先建立TCP连接，接着就是建立Channel对象。RabbitMQ使用Channel而不直接使用TCP，就是因为TCP的建立和拆除系统开销过大，容易引起消息队列系统瓶颈。 QueueQueue是RabbitMQ的内部对象，用于存储消息，Queue可以看成是存储消息的容器。消息的生产者与消费者之间可以是一对多或者多对多的关系，即多个消费者可以订阅同一个Queue，这时，Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。如下图所示： Message AcknowledgmentRabbitMQ会要求消费者在处理完消息后发送一个回执应答给RabbitMQ Broker，RabbitMQ收到消息回执（Message-Acknowledgment）后才将该消息从Queue中移除。如果RabbitMQ没有收到回执并检测到消费者与RabbitMQ Broker的连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）继续处理。RabbitMQ的消息处理过程中并不存在Timeout的概念，即一个消费者处理消息所花的时间再长，只要其连接还存在，RabbitMQ就不会将该消息发送给其他消费者。这种情况会产生另外一个问题，如果开发人员在处理完业务逻辑后，忘记发送回执给RabbitMQ，这将会导致Queue中堆积的消息越来越多，而消费者重新连接到RabbitMQ后会重复消费这些消息并重复执行业务逻辑。因此，在处理完业务逻辑后一定要向RabbitMQ发送应答回执，否则会造成应用系统存在重大BUG。 Message Durability如果希望即使在RabbitMQ服务重启的情况下，消息也不会丢失，则可以将Queue与Message均做持久化设置，即MessageDurability，这样便可保证绝大部分情况下RabbitMQ的消息不会丢失。但是，消息持久化方法依然解决不了小概率丢失消息事件的发生，如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了，如果需要解决这种小概率事件下的消息丢失情况，那么就要用到事务RabbitMQ的高级功能或者将RabbitMQ部署为高可用集群。 Prefetch Count如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平分给多个消费者。这时如果每个消费者处理消息的时间不同，就有可能会导致某些消费者一直处于繁忙状态，而另外一些消费者因为处理能力较强而很快就处理完并一直处于空闲状态。要解决这一情况以提高整个消息系统的消息处理效率，可以设置预获取数目（PrefetchCount）来限制Queue每次发送给某个消费者的消息数，默认PrefetchCount=1，则Queue每次给每个消费者发送一条消息，消费者处理完这条消息后Queue会再给该消费者发送下一条消息。 Exchange消息投递过程实际上是生产者将消息发送到Exchange，由Exchange将符合转发规则的消息路由到一个或多个Queue中，而将不符合规则的消息直接丢弃。从功能实现上来看，Exchange的功能就像一个路由器，符合路由规则的数据则转发到对应的目标地址，其他数据则被拒绝或者丢弃。 Routing KeyRoutingKey定义了消息的路由规则，但是，RoutingKey需要与ExchangeType和BindingKey共同使用才能最终决定消息投递到哪个队列中。在实际使用中，ExchangeType与BindingKey通常为预先设定值，而消息生产者在发送消息给Exchange时，需为消息设定相应的RoutingKey，便可决定消息应该投递到哪个Queue。通常，RabbitMQ为RoutingKey设定的长度限制为255字节。 Binding和BindingKeyBinding是RabbitMQ中将Exchange与Queue关联起来的操作，绑定的过程需要用到消息的RoutingKey和绑定自身的BindingKey。BindingKey代表了Queue与Exchange之间的对应关系。消息生产者将消息发送给Exchange时，通常会给消息指定一个RoutingKey。在Exchange中，如果BindingKey与RoutingKey相匹配，则带有该RoutingKey的消息将会被路由到BindingKey所对应的Queue中，即实现了消息到特定队列的投递过程。但是，消息到队列的投递另一个决定因素就是Exchange Type，投递过程会因Exchange Type的不同而有所不同。根据Exchange Type类型不同，在绑定多个Queue到同一个Exchange的时候，Binding操作允许使用相同的BindingKey，也可以不使用BindingKey。比如：在Fanout类型的Exchange中，绑定操作就不会用到BindingKey，而是将消息路由到所有绑定到该Exchange的Queue中。 Exchange TypeRabbitMQ使用不同的交换器类型来将不同的消息投递到特定的队列中，不同类型的交换器使用不同的方式进行消息投递，常用的ExchangeType有Fanout、Direct、Topic和Headers四种。 Fanout类型：会把所有发送到该Exchange的消息直接投递到所有与它绑定的Queue中，其功能就像路由交换中的广播，主要作用就是将多个Queue绑定到同一个Exchange中，从而实现消息生产者与队列之间一对多的对应关系。在Fanout类型的Exchange中，消息到队列的投递过程并不依赖消息的RoutingKey和Binding的BindingKey，Exchange仅起到消息传递的作用。 Direct类型：Direct仅把RoutingKey与Binding的BindingKey匹配的消息路由到对应的Queue中，其功能就像路由交换中的点到点路由，主要作用就是实现消息的精确匹配投递。 Topic类型：其与Direct类型的Exchage相似，也是将消息路由到BindingKey与RoutingKey相匹配的Queue中，但Topic采用的匹配规并非完全匹配，而是更加灵活的通配符模式匹配，Topic匹配规则具有如下限定： RoutingKey由点号“.”分隔的字符串组成（通常将句点号“.”分隔开的每一段独立字符串称为一个单词），如值为“stock.usd.nyse”的RoutingKey，其有三个单词组成，而值为“nyse.vmw”的RoutingKey，则由两个单词组成。 BindingKey与RoutingKey一样也是由句点号“.”分隔的字符串。 BindingKey中可以包含两种特殊字符“”与“#”，用于做模糊匹配，其中“”用于匹配一个单词，“#”用于匹配零个或多个单词，但是，BindingKey中的匹配最小单位为第一条约束中定义的单词，而不是常见的字母匹配。 如上图所示，RoutingKey为“quick.orange.rabbit”的消息会被Exchange同时路由到Q1与Q2队列，因为根据BindingKey的匹配规则，Q1与Q2的BindingKey均与此RoutingKey匹配。RoutingKey为“lazy.brown.fox”的消息只会被路由到Q2队列，因为BindingKey中的“#”字符匹配零个或多个单词，只有Q2中的BindingKey匹配此消息的RoutingKey。同样，根据匹配规则，RoutingKey为“lazy.orange.fox”的消息只会被路由到Q1队列。RoutingKey为“lazy.pink.rabbit”的消息尽管与Q2的两个BindingKey都匹配，但是此消息只会投递给Q2一次。当消息的RoutingKey为“quick.brown.fox”、“orange”或“quick.orange.male.rabbit”时，由于没有任何匹配的BindingKey规则，这些消息将会被Exchange丢弃。 Topic是一种推送-订阅（Pub-Sub，Publish-Subscribe）模式的模糊路由匹配，由于Topic类型的Exchange具有灵活自动的匹配模式，其在OpenStack的应用场景中是使用最多的Exchange类型。 Headers类型：Headers类型的Exchange不依赖于RoutingKey和BindingKey的匹配规则来路由消息，而是根据消息内容中的Headers属性来进行Queue选择。在Headers类型的Exchange中，在绑定Queue与Exchange时通常会设定一组Key-Value键值对，而当消息发送到Exchange时，RabbitMQ会提取此消息的Headers值（其值也是一个Key-Value键值对），并对比其中的键值对是否完全匹配Queue与Exchange绑定时设定的键值对，如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。该类型交换机并不常见。 Remote Procedure Call在实际的应用场景中，用户很可能需要进行某些同步处理，因此需要同步等待客户端将用户发送的消息处理完成后再进行下一步处理，而这相当于远程过程调用（RPC，Remote Procedure Call），RabbitMQ也支持远程过程调用RPC。RabbitMQ中实现RPC的机制为：消息生产者在发送消息时，在消息的属性（AMQP协议中定义了14种消息属性，这些属性会随着消息一起发送）中设置两个值，分别为ReplyTo和CorrelationId。其中，ReplyTo的值是一个Queue名称，用于告诉消息的消费客户端消息处理完成后将应答消息发送到指定的这个Queue中，CorrelationId表示此次请求的标识号，客户端处理完成后需要将此属性一并返还，消息发送端将根据返回值中的这个id值来判断执行成功或失败的是已经发送出去的那条消息。消息接收客户端收到消息并处理，处理完消息后，将会生成一条应答消息到ReplyTo指定的Queue，同时带上CorrelationId属性，消息发送端之前已订阅了ReplyTo指定的Queue，因此可以从中接收客户端的应答消息，并根据其中的CorrelationId属性分析哪条请求已经被执行，然后根据执行结果进行后续的业务处理。 RabbitMQ的工作原理和集群配置RabbitMQ是AMQP的一个开源实现，其主要作用是在分布式集群中进行功能组件之间的异步消息传递，简单的描述就是消息生产者将带有RoutingKey的消息发送至交换器Exchange，Exchange使用BindingKey与Queue进行绑定，然后Exchange将RoutingKey与BindingKey进行匹配对比，并将匹配的消息投递至对应的Queue中。如下图所示： 1）客户端连接至消息队列服务器Broker，在TCP连接中建立一个虚拟Channel。 2）客户端声明一个Exchange，并设置相关属性。 3）客户端声明一个Queue，并设置相关属性。 4）客户端在Exchange和Queue之间建立好绑定关系，并设置BindingKey。 5）客户端投递带有RoutingKey的消息到Exchange中。 6）Exchange接收到消息后，根据消息的RoutingKey和已经设置的BindingKey，进行消息路由，将消息投递到一个或多个队列里。 上图主要由三个环节构成，分别是发送消息的客户端、解耦消息发送端与接收端的RabbitMQServer以及消息的接收客户端。RabbitMQ Sever也称为Broker Server，其作用主要是负责消息从Producer到Consumer的传递路径，Broker接收从客户端发送过来的消息，然后转发给接收消息的客户端，Broke将发送与接收客户端进行了解耦，从而实现消息发送端与接收端的异步工作。客户端A和B是消息的发送方，即消息的生产者Producer，Producer发出的Messages由Playload和Label两部分数据组成。其中Playload是消息的主体部分，Label是消息的属性部分，属性部分包含了详细的RoutingKey，当消息到达Broker Server后，消息会被RabbitMQ根据消息属性（Label中的RoutingKey）转发到相应的队列中。客户端1、2和3是消息的接收者，即消息消费者Consumers。Broker Server根据消息的Label属性和Consumers对队列（队列已经绑定到Exchange中）的订阅（Subscribe）情况，将消息的Playload主体转发给相应的Consumers。Consumers接收到的消息是没有Label属性的，而仅有消息的主体Playload部分，此外，Consumers也不知道该消息是来自发送消息的客户端A还是B。 正常情况下，系统中成功安装RabbitMQServer程序后，用户只需启动RabbitMQ服务便可使其正常运行，即RabbitMQ使用自带的默认配置便可提供强大的异步消息传递服务。在某些情况下，用户可能希望自定义RabbitMQ服务，因此RabbitMQ提供了三种自定义配置Broker Server的方式，分别是环境变量配置方式、配置文件修改方式和运行时参数修改方式。具体的配置方式，可以根据各厂家产品解决方案的描述和RabbitMQ社区相关文档进行了解，这里不再赘述。但是，需要强调一点的是，无论采用哪种配置方式，以下几项参数的配置是必须有的： RABBITMQ_NODE_IP_ADDRESS：该变量主要用于设定RabbitMQServer服务运行时监听的接口IP地址，默认为/etc/hosts配置文件中主机名对应的接口IP地址。 RABBITMQ_NODE_PORT：该变量主要用于设置RabbitMQServer服务运行时监听的IP端口号，默认为系统的5672端口。 RABBITMQ_NODENAME：该变量表示RabbitMQ集群的节点名称，默认为rabbit@hostname格式，其中“hostname”为当前节点的主机名，对于FQN格式的主机名，如node1.exmple.com，则RabbitMQ节点的名称为默认为rabbit@node1。 RABBITMQ_USE_LONGNAME：该变量的定义与RABBITMQ_NODENAME类似，不过此变量代表的是RabbitMQ的长节点名，而RABBITMQ_NODENAME为短节点名称。在RabbitMQ集群的配置中，如果此变量设置为True，则RabbitMQ的节点名称将使用FQN全名，即rabbit@node1.exmple.com。 RabbitMQ Broker是一个或几个运行RabbitMQ应用的Erlang节点的组合，这些节点之间共享Users、VirtualVosts、Queues、Bindings、Exchanges和运行时参数，通常把这些运行RabbitMQ服务的节点组合称为RabbitMQ的集群。在RabbitMQ集群中，RabbitMQ Broker运行所需的元数据和状态信息会自动在集群节点之间进行复制。但是，在普通的RabbitMQ集群配置中，消息队列Queues不会在多个节点之间复制，即集群Queues通常只位于集群中的某一个节点上，而其他节点虽然可以看到和访问这个节点上的消息队列，但是不会将该节点上的消息队列复制到其本地。对于普通的RabbitMQ集群模式，假设集群由A和B两个RabbitMQ节点构成，则A、B两个节点都有相同的集群元数据，但是只有A（或者B）节点持有集群消息队列，当消息进入A节点的Queues后，如果Consumer从B节点提取消息，则RabbitMQ会临时在A、B间进行消息传输，把A中的消息取出并经过B发送给Consumer。由于消息集中存放在A节点的队列中，无论Consumer从A或B提取消息，消息总要从A发出，这势必会导致A节点出现性能瓶颈。此外，这种普通集群模式当A节点故障后，B节点无法提取到A节点中还未消费的消息实体，最终因A节点的单点故障而导致整个集群消息系统不可用。解决这个问题一种办法就是将A节点的Queues持久化，尽管可以对A中的Queues做消息持久化，但在A故障后，也必须等待A节点恢复才可继续提供消息传递服务。但是，如果没有消息持久化，则即使A节点恢复，也无法恢复A中的队列消息。如果要将集群队列Queues进行镜像复制，则需要用到RabbitMQ的HeightAvailable功能。 在RabbitMQ的集群中，节点通常被划分为两类，即磁盘（Disk/Disc）节点和内存（RAM）节点。而在多数情况下，集群中的节点均默认是Disk节点，内存节点主要用于具有深度队列和大量Exchanges的集群中以提高消息传递的性能。由于内存节点将消息数据保存到内存中，因此重启内存节点会导致消息丢失。所以，如果存在内存节点，则消息队列必须做持久化，在做了持久化的消息队列中，即使在内存节点上，消息也会被保存到磁盘上，因此重启内存节点也可保证不丢失消息队列。 如上所述，在默认的RabbitMQ集群中，消息队列Queue不会在集群节点之间进行复制备份，而仅位于集群中的某个节点上，通常该节点是最初声明Queues的节点。与Queues不同，Exchanges和Bindings信息会复制到集群中的每个节点上，因此在默认的RabbitMQ集群配置中，尽管集群的Exchanges和Bindings信息能够避免单点故障，但是由于Queues及其保存的Messages是中心化单点存放的，所以集群中的消息队列仍然具有单点故障而无法实现彻底的高可用，如果拥有Queues的节点发生故障，则虽然整个RabbitMQ集群可以继续提供消息服务，但是即使在消息做了持久化存储，之前位于Queues中还未被取走的消息将会丢失或者暂时不可用。 为了避免消息队列Queue的单点故障，通常的做法是将队列在节点之间进行镜像复制。在队列镜像模式下，每个镜像的队列由一个Master和一个或多个Slave提供。如果当前的Master故障，则最先成为Slave角色的节点会被选举为新的Master节点。同时，消息生产者投递到Queues中的消息会被复制到所有Slave节点上，并且不论消费者Consumers连接到哪个集群节点，其最终都是到Master节点的Queues中提取消息。又因为Master需要将消息复制到多个Slaves节点，所以队列镜像模式虽然增加了RabbitMQ集群的高可用性，但是并没有将集群的消息服务负载分散到每个集群节点中。 实现RabbitMQ集群队列镜像的最主要方式就是RabbitMQ提供的Policy功能，Policy功能可以在集群运行的任何时候使用，即可以动态地将一个未镜像的集群消息队列改变为镜像队列。因此，创建镜像队列最简单有效的方式就是先创建一个非镜像的队列，然后通过Policy设置成为镜像队列。但是，刚开始设置的镜像队列与非镜像队列是有区别的：非镜像队列因为没有额外的镜像操作，所以其运行效率相对要高很多。因此，在RabbitMQ集群的镜像队列设置中，我们可以选择性地对某些队列进行镜像，而其他队列可以不用镜像。RabbitMQ设置Policy时，最关键的两个参数分别是ha-mode和ha-params，ha-params参数的值根据ha-mode的取值不同而不同，具体的组合如下： ha-mode=all，ha-params=NULL时，将队列镜像到全部集群节点上。 ha-mode=exactly，ha-params=count时，将队列镜像到count个节点上。如果实际节点数小于count，则镜像到全部节点；如果实际节点数据大于count，则节点孤战过后，重新镜像到count个节点上。 ha-mode=nodes，ha-params=node name时，将队列镜像到node name指定的节点上。 示例如下： RabbitMQctl set_policy ha-all “^ha.“ ‘{“ha-mode”:”all”}’ #&lt;===将名称全部以“ha.”开头的队列镜像到所有集群节点中 RabbitMQctl set_policy ha-two “^ha.“ ‘{“ha-mode”:”exactly”,”ha-params”:2,”ha-sync-mode”:”automatic”}’ #&lt;===将名称全部以“ha.”开头的队列镜像到集群中任意两个节点 RabbitMQctl set_policy ha-nodes “^nodes.“‘{“ha-mode”:”nodes”,”ha-params”:[“rabbit@nodeA”, “rabbit@nodeB”]}’ #&lt;===将名称全部以“nodes.”开头的队列镜像到集群中特定节点上 队列镜像中的一个特殊情况是独占队列，独占队列在声明队列的连接终止后会被RabbitMQ自动清除，就如程序段中的一个临时变量。因此，针对独占队列没有必要持久化存储。而且，即使队列名称匹配镜像策略的模式，独占队列也不会被RabbitMQ的镜像策略进行镜像。同时，即使对独占队列进行持久化声明，其也不会被持久化。 在RabbitMQ集群中，设置了镜像策略节点上的队列并非任何时候都是彼此同步的。如果一个节点新加入集群，并且集群Policy将其设为队列镜像的节点，则队列会将此节点当做一个新的Slave节点。但是，此时的新Slave节点上并没有任何的队列，或者说此时新Slave节点上的队列是空的。正常情况下，新加入的Slave节点只会接收晚于其加入镜像队列时间点新增的消息，并最终随着时间推移，新加入的Slave节点中的队列消息会逐步与原集群队列尾部的消息同步，并且随着原有队列头部消息被不断消耗，新Slave节点队列中不同步的消息数目会不断减少并最终达到完全同步。因此，基于RabbitMQ的这种队列同步模式，默认设置下，新加入的Slave节点对其加入前已经存在的集群消息并不能形成任何的冗余和高可用，除非对原有集群队列执行显式的同步操作。 从RabbitMQ3.6开始，RabbitMQ新增了ha-sync-batch-size参数，使得显式的队列同步在速度上有了极大提升，显示队列同步可以有两种实现方式，即手工同步和自动同步。如果队列被配置为自动同步，则不论新的Slave节点何时加入集群，集群中的队列都会被自动同步到新增的Slave节点。但是，同步过程中的消息队列相应地会变迟钝，这种响应变慢的过程会一直持续到队列同步完成为止。队列自动同步的设置很简单，只需在Policy的镜像设置中指定ha-sync-mode参数，使其值为automatic即可。ha-sync-mode参数允许的值是manual和automatic，如果不显式指定automatic，则其值默认为manual。同时，默认情况下，队列在镜像时对消息进行逐条同步，而在RabbitMQ3.6之后，新增了批量同步参数ha-sync-batch-size，用户通过设置该参数的值，即可实现批量同步消息，默认值为1。 RabbitMQ可以工作在Active/Passive或者Active/1active模式的集群中。当RabbitMQ集群运行在Active/Passive模式时，如果Active节点故障，则正常运行时由Active节点持久化写入磁盘中的消息队列可以被Passive节点恢复。当然，在Active/Passive模式下，如果Active节点的消息队列没有进行持久化操作，则Active节点故障后位于其上的消息就会丢失，尽管Passive节点可以重新提供消息服务，但是之前未被取走的消息却已无法恢复，因此Active/Passive模式下的消息队列必须进行持久化操作。Active/Passive模式的另一可能的问题是，当Active节点故障，并需要提升Passive节点为Active节点以恢复和接管消息时，RabbitMQ集群的消息服务可能会出现短暂中断。Active/1active高可用模式的本质是将RabbitMQ集群中的队列在集群节点上实现彼此镜像，在Active/1active模式下，集群中的某一RabbitMQ节点故障后，RabbitMQ服务会自动切换到其他节点，并使用该节点上的镜像队列继续提供消息服务，从而实现消息系统的服务高可用性。在部署OpenStack的高可用集群中，推荐部署的RabbitMQ高可用模式便是Active/1active高可用集群模式。 RabbitMQ在OpenStack中的应用分析在实际应用中，RabbitMQ Broker位于OpenStack的任意两个组件之间，OpenStack的任意两个组件通过RabbitMQ Broker进行松耦合通信。更准确地说，OpenStack的各个组件之间通过远程过程调用RPC来实现彼此的通信，并且组件间的RPC消息传递是基于RabbitMQ的推送-订阅（Publish/Subscribe）模式实现的。OpenStack在消息传递过程中使用到的交换器Exchange的类型主要是Direct、Fanout和Topic类型。OpenStack组件之间关系示意如下图所示： 在OpenStack中，默认使用kombu（一种实现了AMQP协议的Python函数库）连接到RabbitMQ服务器，因为消息的收发者都需要一个连接到RabbitMQ服务器的Connetion对象。以nova为例，在Nova的各个连接到RabbitMQ Broker的组件中，其中某个组件可能是消息的发送者Publisher，如Nova-api和Nova-scheduler，也可能是消息的接收者Consumer，如Nova-compute和Nova-network（现已被Neutron代替，但是不影响我们理解AMQP交互）。此外，某个组件在不同的时刻既可以是Publisher，也可以是Consumer。组件发送消息有两种方式，即同步调用（RPC.CALL）和异步调用（RPC.CAST），当Nova发起RPC.CALL调用的时候，Nova-api充当的就是消息的Consumer，否则是消息的Publisher。 在启动Nova的服务时，初始化函数会创建两个队列，其中的一个队列用于接收RoutingKey格式为“NODE-TYPE.NODE-ID”的消息，如compute.node1，其中的compute表示该节点为计算节点；另一个队列用于接收RoutingKey格式为”NODE-TYPE”格式的消息，如compute。比如当Nova的客户端发送“nova stop instance_name”命令到Nova-api时，Nova-api就会将此命令以消息形式投递到第一种队列中（具体消息的路由投递由Exchange操作），从而通过RoutingKey中的NODE-ID（节点主机名）找到目标计算节点，并将命令转发到此Hypervisor主机上以进行实例的停止操作。 在实际应用中，Nova的各个功能模块根据其逻辑功能可以划分为两类，即Invoker组件和Worker组件。其中Invoker组件的主要功能是向消息队列中发送系统请求消息，如Nova-api和Nova-Scheduler通常属于Invoker；而Worker模块则负责从消息队列中提取Invoker模块发送的消息并向其返回应答消息，如Nova-Compute和Nova-Network通常属于Worker。Invoker通过RPC.CALL和RPC.CAST两个进程发送系统请求消息，然后Worker从消息队列中接收消息，并对RPC.CALL做出应答响应，如下图所示： Topic Publisher：该对象在Nova中的组件发起RPC.CALL调用时创建，消息发送出去之后便结束，生命周期短暂，主要用于将消息发送到队列系统中，每个Topic Publisher都会连接到Topic类型的Exchange中。 Direct Consumer：该对象创建于RPC.CALL调用之后，专门用来接收RPC.CALL调用返回的应答消息，接收到应答消息后便结束。每个Direct Consumer连接到一个以msg_id为BindingKey的专有队列中，同时该队列绑定到一个只接收消息的RoutingKey为msg_id的交换器Exchange中，而这个消息的msg_id被封装在Topic Publisher发出的消息中，并且是由一个专门的UUID生成器所生成。 Topic Consumer：该对象在Nova组件服务启动时创建，在服务结束时候销毁，主要用于接收消息队列中的消息。每个Worker都有两个Topic Consumer，一个连接BindingKey为Topic的队列，一个连接BindingKey为Topic.host的队列。每个Topic Consumer通过一个独占或者共享的队列与Topic Publisher连接到同一个Topic类型的Exchange。 Direct Publisher：该对象创建于Nova中的RPC.CALL调用返回应答消息时，当Response消息发送完成后便结束，与BindingKey为特定msg_id的Direct类型Exchange连接。 以Nova客户端发出创建实例请求为例，实例的创建过程伴随着Nova组件之间的消息传递，Topic Publisher（Nova-api）会将消息发送至NODE-TYPE（这里节点类型为compute）的共享队列中，从而全部Topic Consumer（全部运行Nova-compute的节点）都可以提取到共享队列的消息。如果客户端发出的是针对已有实例的Update、Reboot、Stop和Start等操作，由于消息必须转发到拥有该实例的某个特定计算节点上，因此Topic Publisher（Nova-api）会将消息转发至NOTE-TYPE.HOST（compute.hostname）的专有队列中。 在Openstack的Nova项目中，主要通过两种RPC调用来实现消息传递，即RPC.CALL和RPC.CAST。 1）RPC.CALL属于请求/响应类型的调用，当请求发送出去以后，需要等待执行结果的响应，这类调用需要指定执行请求的目标对象节点，并等待目标返回的执行结果。 Step1：初始化一个TopicPublisher，用以将发送消息到RabbitMQ的消息队列。此外，在发送消息之前，需要初始化一个DirectConsumer并以msg_id作为Direct类型Exchange的名称，用于等待消息执行后的应答响应。 Step2：请求消息被Exchange路由到NODE-TYPE.HOST消息队列，然后，订阅了此队列的相应服务节点（Nova中为compute节点）上的Topic-Consumer会从该队列中获取消息。 Step3：TopicConsumer从队列中提取消息后，服务结点根据消息内容（调用函数及参数）调用相应函数完成消息请求处理，处理完成之后，DirectPublisher被初始化，并根据请求消息msg_id，将应答消息发送到相应的Exchange和消息队列中。 Step4：应答消息被位于发起RPC.CALL调用方的DirectConsumer获取到，RPC.CALL调用过程完成。 2）RPC.CAST属于单向RPC请求，即只将请求发送出去，无须等待返回执行结果。因此，RPC.CAST调用不关心请求由哪个服务节点完成，只需将请求发送到消息队列即可，而接收消息的队列通常为共享队列，该队列中的消息可以被某一类型的多个节点（如Nova中的全部计算节点）接收。 Step1：Invoker初始化TopicPublisher，并将消息发送到RabbitMQ消息服务器中。 Step2：RabbitMQ的交换器将消息转发到NODE-TYPE类型的共享消息队列中，并被相应服务节点（Nova中为Compute节点）的TopicConsumer获取。 Step3：TopicConsumer提取到订阅队列的消息后，Woker便开始处理消息请求，至此，RPC.CAST的过程已经完成，Invoker不会再等待Woker返回请求消息执行后的结果。 RabbitMQ集群管理实战集群配置现假设三个节点系统中都安装了RabbitMQ-server软件包，RabbitMQ服务已经正常启动，并且RabbitMQ的命令行工具RabbitMQctl已经可以正常使用。由于RabbitMQ节点之间使用Erlang Cookie来建立连接，因此要想让各个RabbitMQ节点之间彼此可以通信，则各个节点需要共享同一个Erlang Cookie。Erlang Cookie是在RabbitMQ-server启动过程中创建的一个随机字符串，在Linux系统中，Cookie保存在/var/lib/RabbitMQ/.erlang.cookie文件中。要让RabbitMQ的各个节点共享同一个Erlang Cookie，最简单的方式就是在某个节点（kvm-server01）中启动RabbitMQ-server，然后将kvm-server01中的/var/lib/RabbitMQ/.erlang.cookie拷贝到kvm-server02和kvm-server03中。另外一种方式就是在启动RabbitMQ-sever过程中或者在RabbitMQctl的命令行中通过参数“-setcookie cookie_str”的方式将特定的Erlang Cookie传入给当前节点的RabbitMQ服务。如果节点之间的Erlang Cookie未能正确匹配，则RabbitMQ的log中会有“Connection attempt from disallowed node”和“Could not auto-Cluster”的记录。我们这里采用第一种方式，如下： Step1： kvm-server01、kvm-server02、kvm-server03三个节点首先停止服务： 1systemctl stop rabbitmq-server.service Step2：将kvm-server01的erlang.cookie拷贝到kvm-server02和kvm-server03节点上： Step3：重启三个节点的rabbitmq服务 1systemctl restart rabbitmq-server.service Step4：待各个节点的RabbitMQ-server服务启动完成后，便创建了三个独立的RabbitMQ Broker，即每个节点是一个独立的RabbitMQ Broker，通过RabbitMQctl命令行工具可以验证各个节点的Broker的运行状态： # 为了实现一个三节点的RabbitMQ集群，通常的做法是首先创建一个两节点的集群，然后再通过新增节点的形式扩展至三节点集群 Step5：停止kvm-server02上的服务，完成如下操作，并观察节点的服务状态 12345rabbitmqctl stop_apprabbitmqctl join_cluster rabbit@kvm-server01rabbitmqctl start_app Step6：停止kvm-server03上的服务，完成如下操作，并观察节点的服务状态 12345rabbitmqctl stop_apprabbitmqctl join_cluster rabbit@kvm-server02rabbitmqctl start_app 至此，三节点的RabbitMQ集群创建完毕，如上图通过RabbitMQctl命令行工具在任何一个节点上均可验证RabbitMQ集群的运行状态，并且在正常情况下，在任一节点上所看到的集群状态应该是一致的。从节点的集群状态输出中可以看到，每个RabbitMQBroker节点都加入到了集群中，并且集群由三个节点组成，每个节点上都正在运行RabbitMQ服务，并且节点默认都是磁盘类型的节点。 集群节点的启停在一个正在运行的RabbitMQ集群中，可以将任何一个或多个节点停止，并且集群中剩下的节点将会正常运行而不会受到某个节点停止的影响。而当停止的节点重新启动后，该节点又会自动加入集群，集群状态自动恢复正常。 Step1：在kvm-server01节点上，执行如下操作，停止服务 1rabbitmqctl stop Step2：在kvm-server02和kvm-server03上观察集群的状态 Step3：重新启动kvm-server01上的服务，并再次观察kvm-server02和kvm-server03节点上集群运行状态 1rabbitmq-server -detached 可以看到，当重新启动kvm-server01节点后，kvm-server01节点会自动加入集群运行，三节点集群状态自动恢复如初，即集群中三个集群成员都在运行，并可同时对外提供消息服务。在RabbitMQ全部集群节点均被停止并需重新重启时，有两点需要特别指出： 如果整个RabbitMQ集群中的节点都停止，则重启时应该根据节点停止顺序的逆序重新启动节点。如果首先启动的不是最后停止的节点，则启动过程中会给出30s的等待时间，以等待最后停止的节点变为运行状态，如果在30s内最后停止节点仍然未能激活，则节点启动过程就以失败告终。而如果最后停止的节点无法启动，则可以使用forget_Cluster_node命令将该节点从集群中移除，forget_Cluster_node命令的具体使用方式可以参考RabbitMQctl命令行工具的帮助页面。 如果整个集群节点被同时停止或者发生了掉电等意外情况导致全部RabbitMQ节点同时关闭，则集群中每个节点都会认为自己不是集群中最后停止的节点，而会认为其他节点将在自己后面停止。如果出现这种情况，则可使用RabbitMQctl的force_boot命令来重新启动节点。 集群节点的移除在正常运行的RabbitMQ集群中，当一个节点再也无须加入RabbitMQ集群并作为其成员节点运行的时候，就需要将此节点从集群中移除。要移除集群节点，首先需要停止该节点的RabbitMQ应用，然后重置节点，最后重启该节点的RabbitMQ应用。 Step1：执行如下操作，将kvm-server03节点从集群中移除，移除后的节点将作为一个独立节点提供服务 12345rabbitmqctl stop_apprabbitmqctl resetrabbitmqctl start_app Step2：观察三个节点的运行状态 可见，在移除kvm-server03后，集群中只有kvm-server01和kvm-server02两个节点，同时kvm-server03节点中只有一个RabbitMQ Broker在运行。如果集群中某个节点已经失去了响应，且不能通过正常方式将其移除，则可以在本地节点通过forget_Cluster_node命令以远程移除的方式来将此节点从集群中移除。 Step3：在kvm-server01节点上停止服务，并在kvm-server02上将kvm-server01节点强制移除 1234567# server01rabbitmqctl stop_app# server02rabbitmqctl forget_cluster_node rabbit@kvm-server01 Step4：kvm-server01在本地节点以远程方式从集群中移除后，其仍然会认为自己还属于集群节点，因此在启动本地kvm-server01节点的RabbitMQ应用时会报错，在重启之前将其重置即可解决 123# server01rabbitmqctl reset 如果此时将kvm-server03以同样办法从集群移除，则三个节点将恢复独立运行状态。但是kvm-server02仍然保存有集群的状态信息，即kvm-server02仍然还是集群成员，只不过集群有且只有kvm-server02一个成员。而kvm-server01和kvm-server03已经重新初始化为独立的RabbitMQ Broker，因此这两个节点不会保存任何集群信息，而如果要将kvm-server02也重新初始化为独立的RabbitMQ Broker，需要完成如下操作： 1234567# server02rabbitmqctl stop_apprabbitmqctl resetrabbitmqctl start_app 思考：1、在OpenStack中哪些服务之间交互使用了消息队列机制？哪些场景在哪些服务之间或子服务之间的RPC.CALL的调用关系？哪些场景在哪些服务之间或子服务之间是RPC.CAST的调用关系？（从虚拟机全生命周期管理的场景举例即可） 2、在请详细描述一下虚拟机实例销毁流程中，Nova各个组件之间的RPC调用流程？ 3、在OpenStack中哪些服务或子服务之间没有使用消息队列的交互机制？（从keystone、glance、nova、neutron、cinder、ceilometer、heat等服务中举例即可）]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-OpenStack概述]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-OpenStack%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[OpenStack是什么？2010年7月，RackSpace和美国国家航空航天局合作，分别贡献出RackSpace云文件平台代码和NASA Nebula平台代码，并以Apache许可证开源发布了OpenStack。OpenStack由此诞生，至此已经走过10个年头，最新发布的版本编号已到达T版本。OpenStack是目前最成熟并且符合生产部署的一套开源云操作系统，全球各大IT，CT厂商，甚至运营商都为此投入了大量的人力、物力和精力。由于其开源的特性，全世界的云计算爱好者也为此贡献了很多。OpenStack是开源云操作系统，可控制整个数据中心的大型计算，存储和网络资源池。用户能够通过Web界面、命令行或API接口配置资源。 OpenStack不是虚拟化在1959年国际信息处理大会上，Christopher Strachey亲手为虚拟化埋下了种子，他在名为《大型高速计算机中的时间共享》的报告中，提出了“虚拟化”的概念，从此拉开了虚拟化发展的帷幕。到21世纪，VMware的亮相，开启了虚拟化的X86时代，虚拟化的发展进入了一个爆发期。至此，目前业界主流的虚拟化解决方案有开源的KVM、Xen，商业的VMware、Hyper-V、Xen-Server和华为的FusionSphere。虚拟化技术的出现，主要出于提升资源利用率，降低资源投入成本的目的。从这一点上说，显然OpenStack不是虚拟化，因为OpenStack只是系统的控制面，并不包括系统的数据面组件。比如：Hypervisor、存储和网络设等。OpenStack与虚拟化有着本质上的区别，它本身并不提供虚拟化技术，而是对多种虚拟化技术架构进行管理，并能实现异构统一纳管。就像我们的win10操作系统，能够利用驱动程序对底层多种硬件进行统一管理，协调各类硬件的I/O占用。所以，这就是OpenStack被成为Cloud OS的原因，它本身只是个管理层面的系统。反而，虚拟化只是OpenStack的底层具体实现技术之一，但不是核心关注点。 OpenStack不是云计算1983年，Sun提出“网络即是电脑”（“The Network is the Computer”），这被认为是云计算的雏形，而随后计算机技术的迅猛发展以及互联网行业的兴起，似乎都在向这个概念不断靠拢。在这个不断靠拢的过程中，首先写上浓重一笔的是亚马逊。2006年3月，亚马逊推出弹性计算云（Elastic Computing Cloud，EC2），按用户使用的资源进行收费，开启了云计算商业化的元年。云计算概念诞生之初，市场上对其概念有很多种理解，经过一段时间的争论，现在大家一般来说都认可的就是美国标准与技术研究院的它给出的一个最标准的定义。它把云计算定义为一种模式，而不是一种技术。这种模式既可以是商业模式，也可以是服务模式。显然，OpenStack并不是一种模式，它只是实现云计算这种模式的技术之一，但却是构建云计算模式的关键技术，整个云计算模式的内核、骨干、框架、总线均由OpenStack来构建。随着容器云概念的提出，另一种构建云计算的关键技术也随之出现，也就是我们常说的K8S（Kubernetes）。需要明确一点，OpenStack和K8S这两种构建云计算的关键技术既可以独立部署，也可以混合部署，并且在混合部署下丝毫没有违和感。 OpenStack的体系架构在介绍OpenStack的体系架构之前，不得不提一下AWS Cloud。因为，OpenStack最早就是作为AWS Cloud的追随者出现的。AWS由一系列服务组成，去实现IaaS系统所需要的功能。AWS架构由5层组成，自下而上分别是AWS全球基础架构、基础服务、应用平台服务、管理和用户应用程序（华为的FusionSphere解决方案也很类似，不过华为是4层架构）而就服务类型本身而言，AWS主要提供6类服务：计算和网络、存储和内容分发、数据库、分析、部署管理和应用服务。AWS的功能十分强大，而且还在不断发展之中，OpenStack从诞生之初就一直向AWS模仿和学习，同时，OpenStack也提供开放接口去兼容各种AWS服务。做为一个IaaS范畴的云平台，完整的OpenStack系统通过网络将用户和网络背后丰富的硬件资源分离开。OpenStack一方面负责与运行在物理节点上的Hypervisor进行交互，实现对各种硬件资源的管理与控制；另一方面为用户提供一个满足要求的虚拟机。至于OpenStack内部，作为AWS的一个跟随者，它的体系结构里不可避免地体现着前面所介绍的AWS各个组件的痕迹。比如：AWS中最为核心的EC2模块，负责计算资源的管理，以及虚拟机的调度和管理，在OpenStack中对应的就是Nova项目；AWS中的简单存储服务S3，在OpenStack中有Swift项目与其功能相近；AWS中的块存储模块EBS，对应OpenStack的Cinder项目；AWS的验证访问管理服务IAM，对应OpenStack的KeyStone项目；AWS的监控服务CloudWatch，对应OpenStack的Ceilometer项目；AWS有CloudInformation，OpenStack则有Heat项目；AWS支持关系数据库RDS和NoSQL数据库DynamoDB，OpenStack也支持mySQL、postgre和NoSQL数据库MongoDB等等。 OpenStack共有7个核心组件，分别是计算（Nova）、对象存储（Swift）、认证（Keystone）、用户界面（Horizon）、块存储（Cinder）、网络（Neutron）、镜像服务（Glance）。每个组件都是多个服务的集合，一个服务意味着运行着的一个进程，根据部署Openstack的规模，决定了你是选择将所有服务运行在同一个机器上还是多个机器上。随着OpenStack发展至今，除了上述7个组件，还出现了其他社区高度关注并重点发展的项目，所有组件按照运营层、编排层、服务层和共享层的全景视图如下： OpenStack的生产环境部署架构如下： OpenStack的关键组件介绍 认证服务Keystone：首次出现在OpenStack的“Essex”版本中。Keystone提供身份验证，服务发现和分布式多租户授权，支持LDAP，OAuth，OpenID Connect，SAML和SQL。Keystone服务不依赖其他OpenStack服务，为其他OpenStack服务提供认证支持。 操作界面Horizon：首次出现在OpenStack的“Essex”版本中。Horizon提供基于Web的控制界面，使云管理员和用户能够管理各种OpenStack资源和服务，它依赖于Keystone服务。 镜像服务Glance：首次出现在OpenStack的“Bexar”版本中。Glance提供发现、注册和检索虚拟机镜像功能，提供的虚拟机实例镜像可以存放在不同地方，例如本地文件系统、对象存储、块存储等。它同样依赖于Keystone服务。 计算服务Nova：首次出现在OpenStack的“Austin”版本中。Nova提供大规模、可扩展、按需自助服务的计算资源，支持管理裸机，虚拟机和容器。它依赖于Keystone、Neutron和Glance三个服务。 块存储服务Cinder：首次出现在OpenStack的“Folsom”版本中。Cinder提供块存储服务，为虚拟机实例提供持久化存储，调用不同存储接口驱动，将存储设备转化成块存储池，用户无需了解存储实际部署的位置或设备类型。它依赖于服务Keystone。 对象存储服务Swift：首次出现在OpenStack的“Austin”版本中。Swift提供高度可用、分布式、最终一致的对象存储服务，可以高效、安全且廉价地存储大量数据，非常适合存储需要弹性扩展的非结构化数据。它同样不依赖于其他服务，可以为其他服务提供对象存储服务。 网络服务Neutron：首次出现在OpenStack的“Folsom”版本中。Neutron负责管理虚拟网络组件，专注于为OpenStack提供网络即服务。它依赖于服务Keystone。 编排服务Heat：首次出现在OpenStack的“Havana”版本中。Heat为云应用程序编排OpenStack基础架构资源，可以提供OpenStack原生Rest API和CloudFormation兼容的查询API。它依赖于服务Keystone。 OpenStack服务间的交互示例 以创建虚拟机为例，各组件的配合工作基本流程为： 1）用户首先接触到的是界面，即Horizon。通过Horizon上的简单界面操作，一个创建虚拟机的请求被发送到OpenStack系统后端。 2）既然要启动一个虚拟机，就必须指定虚拟机操作系统是什么类型，同时下载启动镜像以供虚拟机启动使用。这件事情就是由Glance来完成的，而此时Glance所管理的镜像有可能存储在Swift上，所以需要与Swift交互得到需要的镜像文件。 3）在创建虚拟机的时候，自然而然地需要Cinder提供块服务和Neutron提供网络服务，以便该虚拟机有volume可以使用，能被分配到IP地址与外界网络连接，而且之后该虚拟机资源的访问要经过Keystone的认证之后才可以继续。 4）至此，OpenStack的所有核心组件都参与了这个创建虚拟机的操作。 建议多思考总结，从生活中熟悉的例子去理解OpenStack各服务之间的交互关系。 如何学习OpenStack？OpenStack需要很强的动手能力，最好能够准备好带虚拟化设备的物理机或者服务器供学习研究使用，动手是最重要的！此外，还须查阅各种关于OpenStack的资料。首先，OpenStack官网是不容错过的。在OpenStack官方网址上，发布了关于OpenStack各种最新的动态。此外，还提供了极为详细的文档，可以说OpenStack的官方文档是所有开源社区写得最好的，没有之一。在官方网址的博客中，提供了各种关于OpenStack的有趣的活动及技术沙龙。 学习好OpenStack，首先需要顺利地安装OpenStack的各个组件。在安装成功的基础上，学会使用OpenStack系统创建和管理虚拟机、虚拟网络及存储资源。如果需要再深入地研究，那么就需要阅读OpenStack的源代码了。代码的获得主要有两个来源，较为稳定的发行版位于 https:// launchpad. net/。而OpenStack最新的代码，则位于GitHub（https:// github. com/ openstack）。在学习的时候，建议使用launchpad网站上的稳定版本的代码。对OpenStack有了相当了解之后，在学习的过程中，发现一些Bug需要提交Patch的时候，就需要用到GitHub上面最新的代码了。 学习好OpenStack之后，也可以基于OpenStack做一些有意思的二次开发，无论是开发公有云或者私有云，都变得比较有意思。了解OpenStack的内部机制之后，添加一些自定义的模块或者驱动都是相当容易。也就真正地能够将OpenStack握在手中，为我所用了。]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-CentOS-7%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2Openstack-Rocky%E7%89%88%E6%9C%AC%E7%AC%94%E8%AE%B0%EF%BC%88%E5%90%AB%E5%BC%80%E6%BA%90NFVO-VNFM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[title: 2020-03-01-CentOS 7手动部署Openstack Rocky版本笔记（含开源NFVO/VNFM）date: 2020-03-01 20:03:21tags: 云计算category: OpenStack 本篇博文记录了在VMware Workstations上手动部署双节点OpenStack Rocky发行版本的部署笔记，包括OpenStack的9大基础核心组件：Keystone、Glance、Nova、Neutron、Cinder、Swift、Heat和Telemetry等服务，以及开源NFVO/VNFM项目tacker，同时将本地OpenStack的环境注册为VIM，构造一个开源的MANO环境。其中，9大核心组件的Swift在对象存储管理服务—Swift文中介绍部署方法，通过单节点双硬盘的方式模拟vNode，同时模拟实际网络云环境中对象存储主要用于数据备份和镜像存储的功能。 需要说明一点：服务器操作系统安装和初始化配置请参考本站Linux相关文章或网上其他文章，本篇博文不会再赘述。 部署前准备节点规划：共两个节点，一个控制节点，一个计算节点。其中，控制节点兼做计算节点和块存储节点，计算节点主要做业务计算。各节点主机名、地址规划如下： 网络规划： 初始化配置要求：以下为要求为基础配置，请自行完成。 所有节点关闭防火墙和SELinux； 所有节点开启三层转发功能； 所有节点修改YUM源和PIP源为阿里云的源； 所有节点如果存在双网关，需要配置明细路由策略； 安装时钟同步服务控制节点：1）安装软件包： 1yum install chrony -y 2）配置时钟同步服务文件 12# 在/etc/chrony.conf文件中添加修改如下选项第26行修改：allow 10.28.101.0/24 #&lt;===修改为管理平面网段 3）配置时钟同步服务开机启动 1systemctl enable chronyd.service &amp;&amp; systemctl start chronyd.service 计算节点：1）安装软件包： 1yum install chrony -y 2）配置时钟同步服务文件 12345cp /etc/chrony.conf&#123;,.bak&#125;# 在/etc/chrony.conf文件中添加修改如下选项# 注释掉3，4，5，6行# 第7行添加：server rocky-cotroller iburst #&lt;===添加控制节点主机名为NTP Server 3）配置时钟同步服务开机启动 1systemctl enable chronyd.service &amp;&amp; systemctl start chronyd.service 在所有节点执行验证操作：chronyc sources，验证结果如下： 控制节点： 计算节点： 安装OpenStack软件包控制节点&amp;&amp;计算节点： 1）添加rocky版本软件仓库： 1yum install centos-release-openstack-rocky -y 2）更新系统： 1yum upgrade -y 3）安装OpenStack客户端 1yum install python-openstackclient -y 4）安装OpenStack安全策略组件： 1yum install openstack-selinux -y 安装SQL数据库控制节点： 1）安装软件包： 1yum install mariadb mariadb-server python2-PyMySQL -y 2）创建和编辑/etc/my.cnf.d/openstack.cnf文件，创建一个[mysqld]的Session，并将bind-address 密钥设置为控制器节点的管理IP地址，以允许其他节点通过管理网络进行访问。设置其他键以启用有用的选项和UTF-8字符集，如下： 3）配置数据库服务开机启动：vim 1systemctl enable mariadb.service &amp;&amp; systemctl start mariadb.service 4）通过执行如下脚本，设置mysql数据root帐号和密码： 1mysql_secure_installation 安装消息队列服务控制节点： 1）安装软件包： 1yum install rabbitmq-server -y 2）配置开机启动消息队列服务： 1systemctl enable rabbitmq-server.service &amp;&amp; systemctl start rabbitmq-server.service 3）添加OpenStack用户并添加密码: 1rabbitmqctl add_user openstack openstack 4）设置openstack用户的访问权限： 1rabbitmqctl set_permissions openstack ".*" ".*" ".*" 安装分布式缓存memcache控制节点： 1）安装软件包： 1yum install memcached python-memcached -y 2）配置/etc/sysconfig/memcached文件，在OPTIONS中添加控制节点主机名： 1cp /etc/sysconfig/memcached&#123;,.bak&#125; 3）配置开机启动memcache服务 1systemctl enable memcached.service &amp;&amp; systemctl start memcached.service 安装分布式键值数据库etcd服务控制节点： 1）安装软件包： 1yum install etcd -y 2）编辑/etc/etcd/etcd.conf文件，并设置ETCD_INITIAL_CLUSTER， ETCD_INITIAL_ADVERTISE_PEER_URLS，ETCD_ADVERTISE_CLIENT_URLS， ETCD_LISTEN_CLIENT_URLS控制器节点，以使经由管理网络通过其他节点的访问的管理IP地址： 1cp /etc/etcd/etcd.conf&#123;,.bak&#125; 3）配置开机启动ETCD服务： 1systemctl enable etcd &amp;&amp; systemctl start etcd 安装Horizon仪表盘服务控制节点： 1）安装软件包： 1yum install openstack-dashboard -y 2）编辑 /etc/openstack-dashboard/local_settings 文件并完成以下修改操作： 123456789cp /etc/openstack-dashboard/local_settings&#123;,.bak&#125;# 184行修改为：OPENSTACK_HOST = "rocky-controller"# 38行修改为：ALLOWED_HOSTS = ['*'] #&lt;===生产环境建议根据访问需求修改，不建议修改为*# 160行添加：SESSION_ENGINE = 'django.contrib.sessions.backends.cache'# 166行添加：'LOCATION': 'rocky-controller:11211', #&lt;===注意后面的逗号不能少# 75行修改为：OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True# 64、66、67、68、70、97行去掉注释# 189行修改为：OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"# 467行修改为：TIME_ZONE = "Asia/Shanghai" 3）在/etc/httpd/conf.d/openstack-dashboard.conf中添加以下行 ： 4）重启web服务和分布式缓存服务： 1systemctl restart httpd.service memcached.service 安装keystone服务控制节点： 1）安装keystone数据库 12345mysql -u root -pCREATE DATABASE keystone;GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';exit； 2）安装软件包： 1yum install openstack-keystone httpd mod_wsgi -y 3）配置/etc/keystone/keystone.conf文件，完成[database]和[token]部分配置 123456cp /etc/keystone/keystone.conf&#123;,.bak&#125;[database] #部分：connection = mysql+pymysql://keystone:keystone@rocky-controller/keystone[token] #部分：provider = fernet 4）初始化keystone数据库 1su -s /bin/sh -c "keystone-manage db_sync" keystone 5）初始化fernet秘钥 12keystone-manage fernet_setup --keystone-user keystone --keystone-group keystonekeystone-manage credential_setup --keystone-user keystone --keystone-group keystone 6）建立引导服务：（注意修改密码，更改主机名） 1keystone-manage bootstrap --bootstrap-password &lt;你自己的admin用户密码&gt; --bootstrap-admin-url http://rocky-controller:5000/v3/ --bootstrap-internal-url http://rocky-controller:5000/v3/ --bootstrap-public-url http://rocky-controller:5000/v3/ --bootstrap-region-id RegionOne 7）编辑/etc/httpd/conf/httpd.conf文件并配置 ServerName选项以引用控制器节点： 1cp /etc/httpd/conf/httpd.conf&#123;,.bak&#125; 8）创建到/usr/share/keystone/wsgi-keystone.conf文件的链接： 1ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ 9）启动Apache HTTP服务，并将其配置为在系统启动时启动： 1systemctl enable httpd.service &amp;&amp; systemctl start httpd.service 10）创建和编辑admin-openrc文件 11）创建domain、project、user和role 123456openstack domain create --description "KkutysLLB-Domain" kkutysllbopenstack project create --domain default --description "Service Project" serviceopenstack project create --domain default --description "Demo Project" kkprojectopenstack user create --domain default --password-prompt kkutysllbopenstack role create kkroleopenstack role add --project kkproject --user kkutysllb kkrole 12）创建和编辑kkutysllb-openrc文件 13）验证admin用户和kkutysllb用户的token 1234source admin-openrc.shopenstack token issuesource kkutysllb-openrc.shopenstack token issue 安装Glance服务控制节点： 1）安装glance数据库 1234CREATE DATABASE glance;GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance';GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glance';exit; 2）获取管理员权限： 1source admin-openrc.sh 3）创建glance用户、角色和服务endpoint 123openstack user create --domain default --password-prompt glanceopenstack role add --project service --user glance adminopenstack service create --name glance --description "OpenStack Image" image 4）创建glance服务的API端点： 123openstack endpoint create --region RegionOne image public http://rocky-controller:9292openstack endpoint create --region RegionOne image internal http://rocky-controller:9292openstack endpoint create --region RegionOne image admin http://rocky-controller:9292 5）安装软件包： 1yum install openstack-glance -y 6）编辑/etc/glance/glance-api.conf文件并完成以下配置： 12345678910111213141516171819202122cp /etc/glance/glance-api.conf&#123;,.bak&#125;[database] #部分添加：connection = mysql+pymysql://glance:glance@rocky-controller/glance[keystone_authtoken] #部分添加：www_authenticate_uri = http://rokcy-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = glancepassword = glance[paste_deploy]#部分添加：flavor = keystone[glance_store] #部分添加：stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/ 7）编辑/etc/glance/glance-registry.conf文件并完成以下配置： 1234567891011121314151617cp /etc/glance/glance-registry.conf&#123;,.bak&#125;[database] #部分添加：connection = mysql+pymysql://glance:glance@rocky-controller/glance[keystone_authtoken] #部分添加：www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = glancepassword = glance[paste_deploy] #部分添加：flavor = keystone 8）初始化glance数据库： 1su -s /bin/sh -c "glance-manage db_sync" glance 9）设置开机启动服务 12systemctl enable openstack-glance-api.service openstack-glance-registry.servicesystemctl start openstack-glance-api.service openstack-glance-registry.service 10）验证服务，上传镜像 1234wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.imgopenstack image create "cirros" --file cirros-0.4.0-x86_64-disk.img --disk-format qcow2 --container-format bare --publicopenstack image list 安装Nova服务控制节点：1）创建nova数据库： 1234567891011121314mysql -u root -pCREATE DATABASE nova_api;CREATE DATABASE nova;CREATE DATABASE nova_cell0;CREATE DATABASE placement;GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'nova';GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'nova';GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY 'nova';GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' IDENTIFIED BY 'placement';GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' IDENTIFIED BY 'placement';exit; 2）获取管理员权限： 1source admin-openrc.sh 3）创建nova用户、添加角色和服务实体： 123openstack user create --domain default --password-prompt novaopenstack role add --project service --user nova adminopenstack service create --name nova --description "OpenStack Compute" compute 4）创建API服务端点endpoint 123openstack endpoint create --region RegionOne compute public http://rocky-controller:8774/v2.1openstack endpoint create --region RegionOne compute internal http://rocky-controller:8774/v2.1openstack endpoint create --region RegionOne compute admin http://rocky-controller:8774/v2.1 5）创建placement用户、添加角色和服务实体，placement用于资源的追踪展示 123openstack user create --domain default --password-prompt placementopenstack role add --project service --user placement adminopenstack service create --name placement --description "Placement API" placement 6）创建placement API服务端点： 123openstack endpoint create --region RegionOne placement public http://rocky-controller:8778openstack endpoint create --region RegionOne placement internal http://rocky-controller:8778openstack endpoint create --region RegionOne placement admin http://rocky-controller:8778 7）安装软件包： 1yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api -y 8）编辑/etc/nova/nova.conf文件并完成以下配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950cp /etc/nova/nova.conf&#123;,.bak&#125;[DEFAULT] #部分添加：enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:openstack@rocky-controllermy_ip = 10.28.101.81use_neutron = truefirewall_driver = nova.virt.firewall.NoopFirewallDriver[api_database] #部分添加：connection = mysql+pymysql://nova:nova@rocky-controller/nova_api[database] #部分添加：connection = mysql+pymysql://nova:nova@rocky-controller/nova[placement_database] #部分添加：connection = mysql+pymysql://placement:placement@rocky-controller/placement[api] #部分添加：auth_strategy = keystone[keystone_authtoken] #部分添加：auth_url = http://rocky-controller:5000/v3memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = novapassword = nova[vnc] #部分添加：enabled = trueserver_listen = $my_ipserver_proxyclient_address = $my_ip[glance] #部分添加：api_servers = http://rocky-controller:9292[oslo_concurrency] #部分添加：lock_path = /var/lib/nova/tmp[placement] #部分添加：region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://rocky-controller:5000/v3username = placementpassword = placement 9）修复一个bug，修改 /etc/httpd/conf.d/00-nova-placement-api.conf配置如下： 12345678910&lt;Directory /usr/bin&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt;&lt;/Directory&gt; 10）重启web服务： 1systemctl restart httpd 11）初始化nova-api和placement数据库： 1su -s /bin/sh -c "nova-manage api_db sync" nova 12）注册cell0数据库： 1su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova 13）创建cell1网格： 1su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova 14）初始化nova数据库： 1su -s /bin/sh -c "nova-manage db sync" nova 15）验证nova cell0和cell1是否正确注册： 1su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova 16）配置开机启动服务：（备注：nova-consoleauth服务自18.0版本开始不再使用，建议以后每个单元都部署控制台代理） 12systemctl enable openstack-nova-api.service openstack-nova-consoleauth openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.servicesystemctl start openstack-nova-api.service openstack-nova-consoleauth openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service 计算节点：1）安装软件包： 1yum install openstack-nova-compute -y 2）拷贝控制节点的nova.conf文件至计算节点，并修改以下配置： 12345678910cp /etc/nova/nova.conf&#123;,.bak&#125;scp /etc/nova/nova.conf rocky-compute:/etc/nova/nova.conf[DEFAULT] #部分修改计算节点管理网口IP：my_ip = 10.28.101.82[vnc] #部分添加以下配置：novncproxy_base_url = http://rocky-controller:6080/vnc_auto.html[libvirt] #部分添加以下配置：virt_type = qemu #&lt;===因为我们环境是虚拟机安装，所以选择qemu，生产环境根据实际情况可以选择KVM、Xen、VMware等 3）配置compute服务开机启动： 12systemctl enable libvirtd.service openstack-nova-compute.servicesystemctl start libvirtd.service openstack-nova-compute.service 控制节点：1）获取管理员权限： 1source admin-openrc.sh 2）列出计算主机： 1openstack compute service list --service nova-compute 3）在数据库中同步发现的计算主机： 1su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova 4）（可选）在控制节点的nova.conf添加scheduler选项，用于自动发现主机，扫描周期可配置 12[scheduler]discover_hosts_in_cells_interval = 300 5）重启nova-api和nova-scheduler服务： 1systemctl restart openstack-nova-api.service openstack-nova-scheduler.service 安装Neutron服务控制节点&amp;&amp;网络节点：1）安装neutron数据库： 12345mysql -u root -pCREATE DATABASE neutron;GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutron';GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutron';exit； 2）获取管理员权限： 1source admin-openrc.sh 3）创建neutron用户、服务实体、添加角色： 123openstack user create --domain default --password-prompt neutronopenstack role add --project service --user neutron adminopenstack service create --name neutron --description "OpenStack Networking" network 4）创建API的服务端点： 123openstack endpoint create --region RegionOne network public http://rocky-controller:9696openstack endpoint create --region RegionOne network internal http://rocky-controller:9696openstack endpoint create --region RegionOne network admin http://rocky-controller:9696 选择网络选项二，私有网络 5）安装软件包： 1yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables -y 6）配置neutron.conf文件，添加以下选项： 123456789101112131415161718192021222324252627282930313233343536cp /etc/neutron/neutron.conf&#123;,.bak&#125;[DEFAULT] #部分添加：core_plugin = ml2service_plugins = routerallow_overlapping_ips = truetransport_url = rabbit://openstack:openstack@rocky-controllerauth_strategy = keystonenotify_nova_on_port_status_changes = truenotify_nova_on_port_data_changes = true[database] #部分添加：connection = mysql+pymysql://neutron:neutron@rocky-controller/neutron[keystone_authtoken] #部分添加：www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = neutron[nova] #部分添加：auth_url = http://rocky-controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = nova[oslo_concurrency] #部分添加：lock_path = /var/lib/neutron/tmp 7）配置ml2_conf.ini文件，添加以下选项： 123456789101112cp /etc/neutron/plugins/ml2/ml2_conf.ini&#123;,.bak&#125;[ml2] #部分添加以下选项：type_drivers = flat,vlan,vxlantenant_network_types = vxlanextension_drivers = port_securitymechanism_drivers = openvswitch,l2population[ml2_type_vxlan]#部分添加如下选项：vni_ranges = 1:1000[securitygroup]#部分添加如下选项：enable_ipset = true 8）配置openvswitch_agent.ini文件，添加如下选项： 1234567891011cp /etc/neutron/plugins/ml2/openvswitch_agent.ini&#123;,.bak&#125;[agent] #部分添加如下选项：tunnel_types = vxlanl2_population = True[ovs] #部分添加如下选项：bridge_mappings = provider:br-extnetlocal_ip = 99.64.101.81[securitygroup] #部分添加如下选项：firewall_driver = iptables_hybrid 9）配置l3_agent.ini文件，添加如下选项： 1234cp /etc/neutron/l3_agent.ini&#123;,.bak&#125;[DEFAULT] #部分添加如下选项：interface_driver = openvswitchexternal_network_bridge = 10）配置dhcp_agent.ini文件，添加如下选项： 12345cp /etc/neutron/dhcp_agent.ini&#123;,.bak&#125;[DEFAULT] #部分添加如下选项：interface_driver = openvswitchdhcp_driver = neutron.agent.linux.dhcp.Dnsmasqenable_ioslated_metadata = true 11）配置metadata_agent.ini文件，添加如下选项： 1234cp /etc/neutron/metadata_agent.ini&#123;,.bak&#125;[DEFAULT] #部分添加如下选项：nova_metadata_host = rocky-controllermetadata_proxy_shared_secret = neutron 12）修改nova.conf配置文件，添加neutron模块： 123456789101112[neutron]url = http://rocky-controller:9696auth_url = http://rocky-controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = neutronservice_metadata_proxy = truemetadata_proxy_shared_secret = neutron 13）创建ml2_conf.ini文件的软连接，用户网络服务的初始化： 1ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini 14）初始化neutron数据库： 1su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron 15）重启nova-api服务： 1systemctl restart openstack-nova-api.service 16）配置OVS服务开机启动： 1systemctl enable openvswitch &amp;&amp; systemctl start openvswitch &amp;&amp; systemctl status openvswitch 17）添加外部网桥br-extnet和端口port配置文件 123cd /etc/sysconfig/network-scripts/touch ifcfg-br-extnetcp ifcfg-eth2&#123;,.bak&#125; 18）添加OVS外部网桥，并绑定端口eth2 12ovs-vsctl add-br br-extnetovs-vsctl add-port br-extnet eth2 19）配置neutron服务开机启动： 12systemctl enable neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.servicesystemctl start neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service 20）检查OVS网桥状态： 1ovs-vsctl show 计算节点：1）安装软件包： 1yum install -y openstack-neutron-openvswitch ebtables 2）修改neutron.conf配置文件，添加如下选项： 123456789101112131415cp /etc/neutron/neutron.conf&#123;,.bak&#125;[DEFAULT] #部分添加如下选项：transport_url = rabbit://openstack:openstack@rocky-controllerauth_strategy = keystone[keystone_authtoken] #部分添加如下选项：www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = neutron 3）修改openvswitch_agent.ini 配置文件，添加如下选项： 1234567cp /etc/neutron/plugins/ml2/openvswitch_agent.ini&#123;,.bak&#125;[ovs] #部分添加如下选项：local_ip = 99.64.101.82[agent] #部分添加如下选项：tunnel_types = vxlanl2_population = True 4）修改nova.conf配置文件，添加neutron模块： 123456789101112[neutron]url = http://rocky-controller:9696auth_url = http://rocky-controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = neutronservice_metadata_proxy = truemetadata_proxy_shared_secret = neutron 5）配置OVS服务开机启动： 1systemctl enable openvswitch &amp;&amp; systemctl start openvswitch &amp;&amp; systemctl status openvswitch 6）重启nova-compute服务： 1systemctl restart openstack-nova-compute.service 7）配合开机启动OVS AGENT服务： 1systemctl enable neutron-openvswitch-agent.service &amp;&amp; systemctl start neutron-openvswitch-agent.service &amp;&amp; systemctl status neutron-openvswitch-agent.service 控制节点：12#验证服务：openstack network agent list 安装heat服务控制节点： 1）安装heat数据库： 12345mysql -u root -pCREATE DATABASE heat;GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'localhost' IDENTIFIED BY 'heat';GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'%' IDENTIFIED BY 'heat';exit; 2）获取管理员权限: 1source admin-openrc.sh 3）创建heat用户、添加角色，创建heat和heat-cfn服务实体 1234openstack user create --domain default --password-prompt heatopenstack role add --project service --user heat adminopenstack service create --name heat --description "Orchestration" orchestrationopenstack service create --name heat-cfn --description "Orchestration" cloudformation 4）创建编排服务的API端点： 123456openstack endpoint create --region RegionOne orchestration public [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)openstack endpoint create --region RegionOne orchestration internal [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)openstack endpoint create --region RegionOne orchestration admin [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)openstack endpoint create --region RegionOne cloudformation public http://rocky-controller:8000/v1openstack endpoint create --region RegionOne cloudformation internal http://rocky-controller:8000/v1openstack endpoint create --region RegionOne cloudformation admin http://rocky-controller:8000/v1 5）创建编排需要的身份服务中的其他信息来管理堆栈 123456openstack domain create --description "Stack projects and users" heatopenstack user create --domain heat --password-prompt heat_domain_adminopenstack role add --domain heat --user-domain heat --user heat_domain_admin adminopenstack role create heat_stack_owneropenstack role add --project kkproject --user kkutysllb heat_stack_owneropenstack role create heat_stack_user 6）安装软件包： 1yum install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine -y 7）编辑/etc/heat/heat.conf文件并完成以下配置： 1234567891011121314151617181920212223242526272829303132cp /etc/heat/heat.conf&#123;,.bak&#125;[database] #部分添加：connection = mysql+pymysql://heat:heat@rocky-controller/heat[DEFAULT] #部分添加：transport_url = rabbit://openstack:openstack@rocky-controllerheat_metadata_server_url = http://rocky-controller:8000heat_waitcondition_server_url = http://rocky-controller:8000/v1/waitconditionstack_domain_admin = heat_domain_adminstack_domain_admin_password = heatstack_user_domain_name = heat[keystone_authtoken] #部分添加：www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = heatpassword = heat[trustee] #部分添加：auth_type = passwordauth_url = http://rocky-controller:5000username = heatpassword = heatuser_domain_name = default[clients_keystone] #部分添加：auth_uri = http://rocky-controller:5000 8）初始化heat数据库： 1su -s /bin/sh -c "heat-manage db_sync" heat 9）配置开机启动服务： 12systemctl enable openstack-heat-api.service openstack-heat-api-cfn.service openstack-heat-engine.servicesystemctl start openstack-heat-api.service openstack-heat-api-cfn.service openstack-heat-engine.service 10）安装heat-dashboard 1234pip install heat-dashboardcp /usr/lib/python2.7/site-packages/heat_dashboard/enabled/_[1-9]*.py /usr/share/openstack-dashboard/openstack_dashboard/enabled/systemctl restart httpd 11）验证安装： 12source admin-openrc.shopenstack orchestration service list 安装cinder服务控制节点：1）安装cinder数据库： 12345mysql -u root -pCREATE DATABASE cinder;GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinder';GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinder';exit； 2）获取管理员权限： 1source admin-openrc.sh 3）创建cinder用户、v2/v3服务实体，添加角色： 1234openstack user create --domain default --password-prompt cinderopenstack role add --project service --user cinder adminopenstack service create --name cinderv2 --description "OpenStack Block Storage" volumev2openstack service create --name cinderv3 --description "OpenStack Block Storage" volumev3 4）创建块存储服务API端点： 123456openstack endpoint create --region RegionOne volumev2 public http://rocky-controller:8776/v2/%\(project_id\)sopenstack endpoint create --region RegionOne volumev2 internal http://rocky-controller:8776/v2/%\(project_id\)sopenstack endpoint create --region RegionOne volumev2 admin http://rocky-controller:8776/v2/%\(project_id\)sopenstack endpoint create --region RegionOne volumev3 public http://rocky-controller:8776/v3/%\(project_id\)sopenstack endpoint create --region RegionOne volumev3 internal http://rocky-controller:8776/v3/%\(project_id\)sopenstack endpoint create --region RegionOne volumev3 admin http://rocky-controller:8776/v3/%\(project_id\)s 5）安装软件包： 1yum install openstack-cinder -y 6）编辑/etc/cinder/cinder.conf文件并完成以下配置： 12345678910111213141516171819202122cp /etc/cinder/cinder.conf&#123;,.bak&#125;[DEFAULT] #部分添加如下选项：transport_url = rabbit://openstack:openstack@rocky-controllerauth_strategy = keystonemy_ip = 10.28.101.81[database]#部分添加如下选项：connection = mysql+pymysql://cinder:cinder@rocky-controller/cinder[keystone_authtoken] #部分添加如下选项：www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = cinderpassword = cinder[oslo_concurrency] #部分添加如下选项：lock_path = /var/lib/cinder/tmp 7）初始化cinder数据库： 1su -s /bin/sh -c "cinder-manage db sync" cinder 8）编辑/etc/nova/nova.conf文件并向其中添加cinder模块： 12[cinder]os_region_name = RegionOne 9）重启nova-api服务： 1systemctl restart openstack-nova-api.service 10）配置块存储服务开机启动： 12systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.servicesystemctl start openstack-cinder-api.service openstack-cinder-scheduler.service 存储节点（实验环境控制节点兼任）：1）挂载第二块硬盘，大小200G： 2）针对第二块硬盘新建2个100G的分区： 12echo -e 'n\np\n1\n\n+100G\nw' | fdisk /dev/sdbecho -e 'n\np\n2\n\n\nw' | fdisk /dev/sdb 3）格式化两个分区为ext4文件系统格式： 12mkfs.ext4 /dev/sdb1mkfs.ext4 /dev/sdb 4）创建nfs_volume目录，并将第二块/dev/sdb2挂载在其下，同时设置开机自动挂载： 123mkdir /nfs_volumemount -t ext4 /dev/sdb2 /nfs_volumedf -h | grep /dev/sdb2 123echo "mount -t ext4 /dev/sdb2 /nfs_volume" &gt;&gt;/etc/rc.d/rc.localtail -1 /etc/rc.d/rc.localchmod +x /etc/rc.d/rc.local 5）安装lvm2软件包： 1yum install lvm2 device-mapper-persistent-data -y 6）设置LVM2逻辑卷服务开机启动： 1systemctl enable lvm2-lvmetad.service &amp;&amp; systemctl start lvm2-lvmetad.service 7）创建物理卷PV、卷组cinder_kklvm01 123pvcreate /dev/sdb1vgcreate cinder_kklvm01 /dev/sdb1vgdisplay 123#在devices &#123; &#125;部分添加 filter = [ "a/sdb1/", "r/.*/"]sed -i '141a filter = [ "a/sdb1/", "r/.*/"]' /etc/lvm/lvm.conf #在141行后添加systemctl restart lvm2-lvmetad.service 8）安装NFS服务，作为第二个后端存储： 1234567yum install nfs-utils rpcbind -ymkdir -p /nfs_volume/&#123;cinder_nfs1,cinder_nfs2&#125;chown root:cinder /nfs_volume/cinder_nfs1chmod a+w /nfs_volume/cinder_nfs1echo '10.28.101.81:/nfs_volume/cinder_nfs1'&gt;/etc/cinder/nfs_shareschmod a+w /etc/cinder/nfs_shareschown root:cinder /etc/cinder/nfs_shares 9）配置NFS服务，并启动： 1234echo "/nfs_volume/cinder_nfs1 *(rw,root_squash,sync,anonuid=165,anongid=165)"&gt;/etc/exportsexportfs -rsystemctl enable rpcbind nfs-server &amp;&amp; systemctl restart rpcbind nfs-servershowmount -e localhost 10）安装cinder-volume服务软件包： 1yum install openstack-cinder targetcli python-keystone -y 11）编辑/etc/cinder/cinder.conf文件，添加如下配置：（由于实验环境存储节点与控制节点合一，所以只需要添加个别选项即可。如果生产环境需要按照要求严格配置） 1234567891011121314151617181920[DEFAULT] #部分增加如下选项：log_dir = /var/log/cinderstate_path = /var/lib/cinderenabled_backends = lvm,nfsglance_api_servers = http://rocky-controller:9292[lvm] #部分增加如下选项：volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriveriscsi_helper = lioadmiscsi_protocol = iscsivolume_group = cinder_kklvm01iscsi_ip_address = 10.28.101.81volumes_dir = $state_path/volumesvolume_backend_name = kklvm01[nfs] #部分增加如下选项：volume_driver = cinder.volume.drivers.nfs.NfsDrivernfs_shares_config = /etc/cinder/nfs_sharesnfs_mount_point_base = $state_path/mntvolume_backend_name = kknfs01 12）配置开机启动cinder-volume服务： 12systemctl enable openstack-cinder-volume.service target.servicesystemctl start openstack-cinder-volume.service target.service 安装barbican服务控制节点： 1）创建barbican数据库 12345mysql -u root -pCREATE DATABASE barbican;GRANT ALL PRIVILEGES ON barbican.* TO 'barbican'@'localhost' IDENTIFIED BY 'barbican';GRANT ALL PRIVILEGES ON barbican.* TO 'barbican'@'%' IDENTIFIED BY 'barbican';exit； 2）获取管理员权限： 1source admin-openrc.sh 3）创建barbican用户、添加角色和服务实体： 12345openstack user create --domain default --password-prompt barbicanopenstack role add --project service --user barbican adminopenstack role create creatoropenstack role add --project service --user barbican creatoropenstack service create --name barbican --description "Key Manager" key-manager 4）创建barbican服务API端点： 123openstack endpoint create --region RegionOne key-manager public http://rocky-controller:9311openstack endpoint create --region RegionOne key-manager internal http://rocky-controller:9311openstack endpoint create --region RegionOne key-manager admin http://rocky-controller:9311 5）安装软件包： 1yum install openstack-barbican-api -y 6）编辑/etc/barbican/barbican.conf文件并完成以下配置： 12345678910111213141516cp /etc/barbican/barbican.conf&#123;,.bak&#125;[DEFAULT] #部分添加如下选项：sql_connection = mysql+pymysql://barbican:barbican@rocky-controller/barbicantransport_url = rabbit://openstack:openstack@rocky-controllerdb_auto_create = False[keystone_authtoken] #部分添加如下选项：www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = barbicanpassword = barbican 7）初始化数据库： 1su -s /bin/sh -c "barbican-manage db upgrade" barbican 8）创建/etc/httpd/conf.d/wsgi-barbican.conf文件，添加如下内容： 1234567891011121314151617181920212223242526Listen 9311&lt;VirtualHost *:9311&gt; #ServerName rocky-controller ## Logging ErrorLog "/var/log/httpd/barbican_wsgi_main_error_ssl.log" LogLevel debug ServerSignature Off CustomLog "/var/log/httpd/barbican_wsgi_main_access_ssl.log" combined WSGIApplicationGroup %&#123;GLOBAL&#125; WSGIDaemonProcess barbican-api display-name=barbican-api group=barbican processes=2 threads=8 user=barbican WSGIProcessGroup barbican-api WSGIScriptAlias / "/usr/lib/python2.7/site-packages/barbican/api/app.wsgi" WSGIPassAuthorization On &lt;Directory /usr/lib&gt; &lt;IfVersion &gt;= 2.4&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.4&gt; Order allow,deny Allow from all &lt;/IfVersion&gt; &lt;/Directory&gt;&lt;/VirtualHost&gt; 9）重启web服务： 1systemctl restart httpd 10）配置开机启动： 1systemctl enable openstack-barbican-api.service &amp;&amp; systemctl restart openstack-barbican-api.service &amp;&amp; systemctl status openstack-barbican-api.service 11）安装barbican客户端： 1yum install python-barbicanclient -y 12）使用OpenStack CLI存储密钥： 1openstack secret store --name kksecret --payload j4=]d21 13）通过检索来确认机密已存储： 1openstack secret get http://localhost:9311/v1/secrets/ba3380a4-bbb6-4666-bba2-6c1390478eb3 14）检索秘钥的有效载荷： 1openstack secret get http://localhost:9311/v1/secrets/ba3380a4-bbb6-4666-bba2-6c1390478eb3 --payload 安装Mistral服务控制节点： 1）安装依赖软件包： 1yum install -y python-devel python-setuptools python-pip libffi-devel libxslt-devel libxml2-devel libyaml-devel openssl-devel 2）安装Mistra软件包： 1yum -y install openstack-mistral-api.noarch openstack-mistral-engine.noarch openstack-mistral-executor.noarch openstack-mistral-ui.noarch 3）创建数据库： 12345mysql -u root -pCREATE DATABASE mistral;GRANT ALL PRIVILEGES ON mistral.* TO 'mistral'@'localhost' IDENTIFIED BY 'mistral';GRANT ALL PRIVILEGES ON mistral.* TO 'mistral'@'%' IDENTIFIED BY 'mistral';exit； 4）创建用户、添加角色和服务实体： 123openstack user create --domain default --password=mistral mistralopenstack role add --project service --user mistral adminopenstack service create --name mistral --description 'OpenStack Workflow service' workflowv2 5）创建Mistral服务的API端点： 123openstack endpoint create --region RegionOne workflowv2 public http://rocky-controller:8989/v2openstack endpoint create --region RegionOne workflowv2 internal http://rocky-controlle:8989/v2openstack endpoint create --region RegionOne workflowv2 admin http://rocky-controlle:8989/v2 6）编辑配置/etc/mistral/mistral.conf文件，添加如下选项： 123456789101112131415161718192021222324252627cp /etc/mistral/mistral.conf&#123;,.bak&#125;[DEFAULT]debug = truetransport_url = rabbit://openstack:openstack@rocky-controllerauth_type = keystonerpc_backend = rabbit[database]connection = mysql+pymysql://mistral:mistral@rocky-controller/mistral[keystone_authtoken]www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = mistralpassword = mistral[oslo_messaging_rabbit]rabbit_host = rocky-controllerrabbit_port = 5672rabbit_hosts = $rabbit_host:$rabbit_portrabbit_userid = openstackrabbit_password = openstack 7）初始化数据库，添加缺省项 12mistral-db-manage --config-file /etc/mistral/mistral.conf upgrade headmistral-db-manage --config-file /etc/mistral/mistral.conf populate（报错请忽略） 8）配置开机启动： 1234systemctl enable openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.servicesystemctl restart openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.servicesystemctl status openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.servicesystemctl restart httpd 9）启动服务： 1mistral-server --server api,engine,executor,notifier --config-file /etc/mistral/mistral.conf 安装tacker服务控制节点： 1）修改admin-openrc.sh文件如下，增加图中画红框的部分 2）创建tacker数据库： 12345mysql -uroot -pCREATE DATABASE tacker;GRANT ALL PRIVILEGES ON tacker.* TO 'tacker'@'localhost' IDENTIFIED BY 'tacker';GRANT ALL PRIVILEGES ON tacker.* TO 'tacker'@'%' IDENTIFIED BY 'tacker';exit； 3）获取管理员权限： 1source admin-openrc.sh 4）创建tacker用户、角色，服务实体和端点： 123openstack user create --domain default --password tacker tackeropenstack role add --project service --user tacker adminopenstack service create --name tacker --description "Tacker Project" nfv-orchestration 5）创建tacker服务API的端点： 123openstack endpoint create --region RegionOne nfv-orchestration public http://rocky-controller:9890/openstack endpoint create --region RegionOne nfv-orchestration internal http://rocky-controller:9890/openstack endpoint create --region RegionOne nfv-orchestration admin http://rocky-controller:9890/ 6）从github上下载源码，编译安装： 123456mkdir -p /appcd /appgit clone https://github.com/openstack/tacker -b stable/rockycd tacker/pip install -r requirements.txtpython setup.py install 7）创建日志目录和配置目录，生成服务配置文件并进行修改 123456789101112131415161718192021222324252627282930313233343536373839mkdir -p /var/log/tackermkdir -p /etc/tacker. tools/generate_config_file_sample.shcp -rf /app/tacker/etc/tacker/* /etc/tacker/cp /app/tacker/etc/tacker/tacker.conf.sample /etc/tacker/tacker.conf# 修改/etc/tacker/tacker.conf文件如下：[DEFAULT]auth_strategy = keystonepolicy_file = /etc/tacker/policy.jsondebug = Trueuse_syslog = Falsebind_host = 10.28.101.81bind_port = 9890service_plugins = nfvo,vnfmstate_path = /var/lib/tacker[nfvo_vim]vim_drivers = openstack[keystone_authtoken]memcached_servers = 11211region_name = RegionOneauth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultusername = tackerproject_name = servicepassword = tackerauth_url = http://rocky-controller:5000www_authenticate_uri = http:/rocky-controller:5000[agent]root_helper = sudo /usr/bin/tacker-rootwrap /etc/tacker/rootwrap.conf[database]connection = mysql+pymysql://tacker:tacker@rocky-controller:3306/tacker?charset=utf8[tacker]monitor_driver = ping,http_ping 8）初始化tacker数据库： 1/usr/bin/tacker-db-manage --config-file /etc/tacker/tacker.conf upgrade head 9）将tacker.service and tacker-conductor.service复制到/etc/systemd/system/下，并重启systemctl守护进程 123cp /app/tacker/etc/systemd/system/tacker.service /etc/systemd/system/cp /app/tacker/etc/systemd/system/tacker-conductor.service /etc/systemd/system/systemctl daemon-reload 10）源码方式安装tacker客户端： 1234cd /appgit clone https://github.com/openstack/python-tackerclient -b stable/rockycd python-tackerclientpython setup.py install 11）安装tacker的dashboard 123456cd /appgit clone https://github.com/openstack/tacker-horizon -b stable/rockycd tacker-horizonpython setup.py installcp tacker_horizon/enabled/* /usr/share/openstack-dashboard/openstack_dashboard/enabled/systemctl restart httpd 12）新开两个终端（或者让进程在后台运行也可以，我的选择），分别运行tacker的服务和执行体： 12tacker-server --config-file /etc/tacker/tacker.conf --log-file /var/log/tacker/takcer.log &amp;tacker-conductor --config-file /etc/tacker/tacker.conf --log-file /var/log/tacker/tacker-conductor.log &amp; 13）创建vim_config.yaml文件，注册OPS为默认的VIM 1vim vim_config.yaml 1openstack vim register --config-file vim_config.yaml --description 'kk first vim' --is-default kkvim 14）验证VIM是否注册成功 15）创建简单实例VNFD模版文件和VNFD文件 12vim sample_vnfd.yamlopenstack vnf descriptor create --vnfd-file sample_vnfd.yaml kk-first-vnfd 图太长，未截取全 16）创建VNF 1openstack vnf create --vnfd-name kk-first-vnfd sample_vnf01 安装aodh服务控制节点： 1）安装aodh数据库： 12345mysql -u root -pCREATE DATABASE aodh;GRANT ALL PRIVILEGES ON aodh.* TO 'aodh'@'localhost' IDENTIFIED BY 'aodh';GRANT ALL PRIVILEGES ON aodh.* TO 'aodh'@'%' IDENTIFIED BY 'aodh';exit； 2）获取管理员权限： 1source admin-openrc.sh 3）创建aodh用户、角色和服务实体： 123openstack user create --domain default --password aodh aodhopenstack role add --project service --user aodh adminopenstack service create --name aodh --description "Telemetry" alarming 4）创建aodh服务的API端点： 123openstack endpoint create --region RegionOne alarming public http://rocky-controller:8042openstack endpoint create --region RegionOne alarming internal http://rocky-controller:8042openstack endpoint create --region RegionOne alarming admin http://rocky-controller:8042 5）安装软件包： 1yum install openstack-aodh-api openstack-aodh-evaluator openstack-aodh-notifier openstack-aodh-listener openstack-aodh-expirer python-aodhclient -y 6）编辑/etc/aodh/aodh.conf文件并完成以下配置： 1234567891011121314151617181920212223242526272829cp /etc/aodh/aodh.conf&#123;,.bak&#125;[DEFAULT]transport_url = rabbit://openstack:openstack@rocky-controllerauth_strategy = keystone[database]connection = mysql+pymysql://aodh:aodh@rocky-controller/aodh[keystone_authtoken]www_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000memcached_servers = rocky-controller:11211auth_type = passwordproject_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = aodhpassword = aodh[service_credentials]auth_type = passwordauth_url = http://rocky-controller:5000/v3project_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = aodhpassword = aodhinterface = internalURLregion_name = RegionOne 7）初始化aodh数据库： 1aodh-dbsync 8）配置开机启动服务： 12systemctl enable openstack-aodh-api.service openstack-aodh-evaluator.service openstack-aodh-notifier.service openstack-aodh-listener.servicesystemctl start openstack-aodh-api.service openstack-aodh-evaluator.service openstack-aodh-notifier.service openstack-aodh-listener.service 安装ceilometer服务控制节点1）获取管理员权限： 1source admin-openrc.sh 2）创建ceilometer用户、角色： 12openstack user create --domain default --password ceilometer ceilometeropenstack role add --project service --user ceilometer admin 3）创建gnocchi用户、角色和服务实体： 123openstack user create --domain default --password gnocchi gnocchiopenstack service create --name gnocchi --description "Metric Service" metricopenstack role add --project service --user gnocchi admin 4）创建Metric服务API端点： 123openstack endpoint create --region RegionOne metric public http://rocky-controller:8041openstack endpoint create --region RegionOne metric internal http://rocky-controller:8041openstack endpoint create --region RegionOne metric admin http://rocky-controller:8041 5）安装gnocchi软件包： 1yum install openstack-gnocchi-api openstack-gnocchi-metricd python-gnocchiclient -y 6）创建gnocchi数据库： 12345mysql -u root -pCREATE DATABASE gnocchi;GRANT ALL PRIVILEGES ON gnocchi.* TO 'gnocchi'@'localhost' IDENTIFIED BY 'gnocchi';GRANT ALL PRIVILEGES ON gnocchi.* TO 'gnocchi'@'%' IDENTIFIED BY 'gnocchi';exit； 7）编辑/etc/gnocchi/gnocchi.conf文件变添加如下选项： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 配置gnocchi功能参数，log地址以及对接redis url端口[DEFAULT]debug = trueverbose = truelog_dir = /var/log/gnocchiparallel_operations = 4coordination_url = redis://rocky-controller:6379# 配置gnocchi工作端口信息，host为控制节点管理IP[api]auth_mode = keystonehost = 10.28.101.81port = 8041uwsgi_mode = http-socketmax_limit = 1000# 配置ceilometer默认收集测试指标策略[archive_policy]default_aggregation_methods = mean,min,max,sum,std,count# 配置允许的访问来源，这里是grafana的地址，允许前端直接访问gnocchi获取计量数据，需要配置允许跨域访问（Keystone中完成）[cors]allowed_origin = http://rocky-controller:3000# 配置keystone认证信息，该模块需要另外添加[keystone_authtoken]auth_type = passwordwww_authenticate_uri = http://rocky-controller:5000auth_url = http://rocky-controller:5000/v3memcached_servers = rocky-controller:11211project_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = gnocchipassword = gnocchiinterface = internalURLregion_name = RegionOneservice_token_roles_required = true# 配置元数据默认存储方式。[indexer]url = mysql+pymysql://gnocchi:gnocchi@rocky-controller/gnocchi# 配置gnocchi存储方式以及位置，在这种配置下将其存储到本地文件系统。[storage]coordination_url = redis://rocky-controller:6379file_basepath = /var/lib/gnocchidriver = file# 配置数据库检索的策略[metricd]workers = 4metric_processing_delay = 60greedy = truemetric_reporting_delay = 120metric_cleanup_delay = 300 8）安装redis软件包： 1yum install redis -y 9）编辑/etc/redis.conf ，修改以下配置： 123# 配置redis可以在后台启动：daemonize yes# 配置redis关闭安全模式：protected-mode no# 配置redis绑定控制节点主机：bind 10.28.101.81 10）启动redis-server，设置开机启动 123redis-server /etc/redis.confecho "redis-server /etc/redis.conf" &gt;&gt; /etc/rc.localchmod a+x /etc/rc.local 11）安装uwsgi插件 1yum install uwsgi-plugin-common uwsgi-plugin-python uwsgi -y 12）赋予/var/lib/gnocchi文件可读写权限： 1chmod -R 777 /var/lib/gnocchi 13）初始化gnocchi数据库： 1gnocchi-upgrade 14）配置gnocchi服务开机启动： 123systemctl enable openstack-gnocchi-api.service openstack-gnocchi-metricd.servicesystemctl start openstack-gnocchi-api.service openstack-gnocchi-metricd.servicesystemctl status openstack-gnocchi-api.service openstack-gnocchi-metricd.service 15）安装ceilometer软件包： 1yum install openstack-ceilometer-notification openstack-ceilometer-central -y 16）编辑/etc/ceilometer/pipeline.yaml文件并完成以下gnocchi配置： 17）编辑/etc/ceilometer/ceilometer.conf文件并完成以下配置： 123456789101112131415161718192021222324cp /etc/ceilometer/ceilometer.conf&#123;,.bak&#125;[DEFAULT]debug = falseauth_strategy = keystonetransport_url = rabbit://openstack:openstack@rocky-controllerpipeline_cfg_file = pipeline.yaml[service_credentials]auth_type = passwordauth_url = http://rocky-controller:5000/v3project_domain_id = defaultuser_domain_id = defaultproject_name = serviceusername = ceilometerpassword = ceilometerinterface = internalURLregion_name = RegionOne [notification]store_events = falsemessaging_urls = rabbit://openstack:openstack@rocky-controller [polling]cfg_file = polling.yaml 13）初始化数据库，在Gnocchi上创建资源，要求Gnocchi已运行并在Keystone配置了endpoint。 1ceilometer-upgrade 14）配置开机启动： 12systemctl enable openstack-ceilometer-notification.service openstack-ceilometer-central.servicesystemctl start openstack-ceilometer-notification.service openstack-ceilometer-central.service 计算节点1）安装软件包 1yum install openstack-ceilometer-compute openstack-ceilometer-ipmi -y 2）编辑/etc/ceilometer/ceilometer.conf文件并完成以下操作： 1# 用scp指令将控制节点的文件拷贝过来即可 3）编辑/etc/nova/nova.conf文件并在以下[DEFAULT]部分配置通知： 12345678910[DEFAULT]instance_usage_audit = Trueinstance_usage_audit_period = hour[notifications]notify_on_state_change = vm_and_task_statenotification_format = unversioned[oslo_messaging_notifications]driver = messagingv2 4）（可选）配置轮询ipmi（虚拟机方式下用不到） 12345678# 编辑/etc/sudoers文件并包含：ceilometer ALL = (root) NOPASSWD: /usr/bin/ceilometer-rootwrap /etc/ceilometer/rootwrap.conf *# 编辑/etc/ceilometer/polling.yaml，添加以下度量项目（注意格式对齐）：- name: ipmi interval: 300 meters: - hardware.ipmi.temperature 5）完成计算节点安装 123systemctl enable openstack-ceilometer-compute.service openstack-ceilometer-ipmi.servicesystemctl start openstack-ceilometer-compute.service openstack-ceilometer-ipmi.servicesystemctl restart openstack-nova-compute.service —————————————————————————————结束————————————————————————————————]]></content>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-KVM虚拟机网络管理实战]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[上一篇文章我们介绍完KVM的存储管理实战后，接下来我们本篇文章就开始主要介绍KVM的网络管理实战。KVM的网络管理模型与OpenStack Neutron中的网络管理非常类似，而在学些OpenStack时，网络服务Neutron往往是很多人初学OpenStack的一个难点。因此，掌握了本篇内容的知识点，对于后续学习OpenStack Neutron部分的内容有事半功倍的效果。 virsh中的网络管理基础在介绍KVM中常见的网络模型之前，我们先来看看virsh有哪些网络管理的命令。与存储管理实战和全生命周期管理实战类似，virsh主要提供对节点上的物理网络接口和分配给虚拟机的虚拟网络进行管理的命令。包括：创建节点上的物理接口、编辑节点上物理接口的XML配置文件，查询节点上物理接口、创建虚拟机的虚拟网络、编辑虚拟机的虚拟网络和删除虚拟机的虚拟网络等。常用的命令和作用如下： virsh网络管理常用命令 命令 功能描述 iface-list 显示出物理主机的网络接口列表 iface-list 根据网络接口名称查询其对应的MAC地址 iface-name 根据MAC地址查询其对应的网络接口名称 iface-edit 编辑一个物理主机的网络接口XML配置文件 iface-dumpxml 以XML配置文件转存出一个网络接口的状态信息 iface-destroy 关闭宿主机上的一个物理网络接口 net-list 列出libvirt管理的虚拟网络 net-info 根据名称查询一个虚拟网络的基本信息 net-uuid 根据名称查询一个虚拟网络的uuid net-name 根据uuid查询一个虚拟网络的名称 net-create&lt;net.xml&gt; 根据一个网络的xml配置文件创建一个虚拟网络 net-edit 编译一个虚拟网络的XML配置文件 net-dumpxml 转存出一个虚拟网络的XML配置文件 net-destroy 销毁一个虚拟网络 有了上面的命令基础，我们就可以几个简单验证。比如，我们现在想看下当前节点有哪几个物理接口，可以使用如下命令： 现在，我们想把br0这个网桥的XML文件导出转存，可以使用下面的命令的实现。 以后就可以按照上面的xml配置文件，写一个网桥，然后通过iface-define定义，然后再通过iface-start命令启动即可（需要注意MAC不能重复，可以使用random工具随机生成MAC）。 我们完成物理接口的简单验证后，接下来看看虚拟网络的信息。比如，我们现在需要看下当前节点有哪些虚拟网络，可以通过net-list命令实现。如下： 上图表示当前节点只有一个虚拟网络default，我们通过net-info命令可以看下default网络的详细信息。如下： 同理，我们也可以将default网络的xml配置转存，便于后续自己编写网络xml文件。如下： KVM中常见的网络模型有了上面基础认知，下面我们就来看下KVM中常见的4种简单网络模型，分别如下： 隔离模型：虚拟机之间组建网络，该模式无法与宿主机通信，无法与其他网络通信，相当于虚拟机只是连接到一台交换机上。其对应OpenStack Neutron中local网络模型。 路由模型：相当于虚拟机连接到一台路由器上，由路由器(物理网卡)，统一转发，但是不会改变源地址。 NAT模型：在路由模式中，会出现虚拟机可以访问其他主机，但是其他主机的报文无法到达虚拟机，而NAT模式则将源地址转换为路由器(物理网卡)地址，这样其他主机也知道报文来自那个主机，在docker环境中经常被使用。 桥接模型：在宿主机中创建一张虚拟网卡作为宿主机的网卡，而物理网卡则作为交换机。 为了加深大家的理解，我们后面就对上述4种网络模型逐一进行验证。 隔离网络模型 如上图所示，VM0和VM1都是在宿主机上创建的虚拟机，虚拟机的网卡分为前半段和后半段，前半段位于虚拟机上，后半段在宿主机上，按照图中所示，前半段就是eth0，它是在虚拟机内部看到的网卡名字，而后半段就是vnet0和vnet1，它们是在宿主机上看到的网卡名字。实际上，在VM1上所有发往eth0的数据就是直接发往vnet0，是由vnet0进行数据的传送处理。 在隔离模式下，宿主机创建一个虚拟交换机vSwitch，然后把vnet0和vnet1接入到该虚拟交换机，交换机也可以叫做bridge，因为vnet0和vnet1在一个网桥内，所以可以互相通信，而虚拟机的eth0是通过后半段进行数据传输，所以只要虚拟机的前半段ip在一个网段内，就可以互相通信，这就是隔离模式。下面，我们就通过实例进行验证。 Step1：创建一个网桥br1，且不连接宿主机的任何物理网卡。 Step2：我们用centos7.5模板虚拟机镜像直接迅速拉起两个测试虚拟机VM0和VM1。 由于我们没有给网桥配置IP和DHCP分配范围，所以虚拟机没有IP地址。我们可以手动给虚拟机添加IP，VM0的虚拟机地址设置为10.10.101.251。如下 同理，给虚拟机VM1设置IP为192.168.101.252。如下： 此时，我们对VM0与VM1进行ping测试，是可以ping通的。但是，即使两个虚拟机与宿主机（10.10.101.11）在同一个网段，仍然无法ping通。所以，这是一个隔离网络模型。 Step3：此时，我们在宿主机上查询网桥br1的挂接信息和虚拟机的虚拟网卡vnet信息。如下： 路由网络模型 在隔离模型的基础上，将宿主机的一块虚拟网卡virnet0加入到虚拟网桥中，这样virnet0就可以和虚拟机通信，通过将虚拟机的默认网关设置为virnet0的IP地址，然后在宿主机中打开IP地址转发，使得虚拟机可以访问宿主机。不过此时虚拟机仅仅可以将报文发送到外部网络，因为外部网络没有路由到虚拟机中，所以外部网络无法将报文回传给虚拟机。 step1：在宿主机上用tunctl创建一个虚拟网卡，也就是创建一个tap设备virnet0。忘了tunctl是什么的，请回顾本站Linux原生网络虚拟化内容。 step2：将virnet0加入到网桥br1中，并设置网桥br1为网关，地址为10.10.101.100 step3：进入虚拟机设置网关为10.10.101.100，也就是配置一条默认路由即可。 step4：在宿主机中打开ip包转发功能，也就是将宿主机变成一个路由器（详见本站Linux原生网络虚拟化文章内容）。 step5：此时，我们在虚拟机中尝试ping宿主机，发现可以ping通。如下： 但是，我们无法ping通宿主机的网关，如下： 这是因为报文发到网关后，网关找不到回包的路由，所以报文无法回复，这时候，我们通过在宿主机上添加一条iptables规则，使得网关可以回包。至于，iptables如何配置，请参见本站Linux常用运维工具分类中的iptable文章。 1iptables -t nat -A POSTROUTING -s 10.10.101.0/24 -j MASQUERADE 添加包转发规则后，就可以在虚拟机中ping通宿主机网关了。如下： step6：但是，我们在宿主机外的机器上仍无法ping通虚拟机。比如，我们在真实的物理机（win10）上还是无法ping通虚拟机10.10.101.251。如下： 为了解决外部主机ping通虚拟机的问题，我们需要win10上也添加一条路由，将访问虚拟机的数据包发往VMware的虚拟网卡地址192.168.101.11上。完成后，再次进行尝试，发现网络已通，且可以通过win10直接ssh虚拟机vm0。如下：（这里注意必须指定VMware NAT网卡的地址段，因为在VMware内部只有NAT虚拟网卡有三层转发功能） 通过上述实践，也可以发现路由模型的缺陷，虽然虚拟机能同宿主机通信，也能将数据包发到外部网络，但是外部网络无法回传数据包，要想外部网络能与虚拟机通信，就要添加对应的路由规则。这对于大规模的虚拟环境，这显然是不科学的。 NAT网络模型 NAT模型其实就是SNAT的实现，路由中虚拟机能将报文发送给外部主机，但是外部主机因找不到通往虚拟机的路由因而无法回应请求。外部主机能同宿主机通信，所以在宿主机上添加一个NAT转发，从而在外部主机响应能够到达虚拟机，但是不能直接访问虚拟机。这种方式是将虚拟机的IP地址转换为宿主机上的某个地址，从而实现虚拟机与外部网络通信，其实际上只是通过iptables的nat表的POSTROUTING链实现地址转换罢了。如果要实现外部网络直接访问虚拟机，还需要在宿主机上配置DNAT转发，一般不采用这种方式。 在我们创建KVM虚拟机时，如果使用default网络类型，它就是一个NAT网络。我们可以看下它的XML配置文件内容： 有了上面的了解，那我们创建一个NAT网络就很轻松了。怎么办？重新写一份？no…no…这不符合互联网时代的要求！拷贝一份，直接修改。。。。同时，我们可以在自定义的nat网络配置文件中，指定两个虚拟机的IP地址。如下： 完成上面的配置后，现在定义一个网络natnet，并设置开启自动启动。如下： 完成上面配置后，libvirt会自动帮我们创建一个XML文件描述的网桥natbr0，并且将网关IP和MAC按照XML文件的配置自动生成。如下： 同时，libvirt会在宿主机的iptables的nat转发表中，自动帮我增加nat网络的数据包转发策略。如下： 完成上面的配置，此时我们需要修改两个虚拟机XML文件的网络配置信息，然后重新定义启动。虚拟机XML配置文件网络信息修改，只需要修改接口类型为network类型，network的上行接口修改为我们自定义的网络名称即可，其它不变。如下： 然后，我们重新定义虚拟机并启动。如下： 现在，我们进入虚拟机查询的虚拟机的IP确实为我们制定的IP地址188.64.100.2，且自动增加了默认路由指向natbr0网桥上联接口地址188.64.100.1，而且虚拟机和宿主机可以互相ping通。如下 而且，我们在虚拟机下可以ping通外部真实物理机win10，但是从真实物理机win10下ping测虚拟机vm0却发现还是ping不通，这就是典型的SNAT原理，也就是虚拟机可以访问外部网络，但是外部网络看不见内网的虚拟机，这样对内网的虚拟机中应用起到保护作用。如下： 如果，需要虚拟机访问互联网internet，需要给虚拟机中增加DNS解析，nameserver就配置成虚拟网关188.64.100.1即可。如下： 至此，nat网络模型验证完毕。四个基本模型中还有一个桥接网络模型，这也是网上常见的配置模型，这种模型就是将虚拟机与宿主机放在一个局域网内，可以互访，且同一局域网的其他外部主机也能访问虚拟机。而且，如果宿主机能上互联网，那么互联网的服务器也能访问虚拟机。除非有特殊应用场景，否则一般在实际运维不采用这种方式，因为不安全。这种方式我就懒得举例了，网上的教程很多。最后，祝大家在KVM的世界里愉快的玩耍。。。。。。嘿嘿嘿嘿。。。。。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-KVM虚拟机存储管理实战（下篇）]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[在上一篇中我们介绍qemu-img这个对磁盘镜像操作的命令，包括检查镜像磁盘，创建镜像磁盘、查看镜像磁盘信息、转换镜像磁盘格式、调整镜像磁盘大小以及镜像磁盘的快照链操作。但是，在镜像磁盘快照链操作中，我们也提到了通过qemu-img命令创建快照链只能在虚拟机关机状态下运行。如果虚拟机为运行态，只能通过virsh save vm来保存当前状态。那么，本文就专门讲述在KVM虚拟机中如何通过virsh命令来创建快照链，以及链中快照的相互关系，如何缩短链，如何利用这条链回滚我们的虚拟机到某个状态等等。 什么是虚拟机快照链虚拟机快照保存了虚拟机在某个指定时间点的状态（包括操作系统和所有的程序），利用快照，我们可以恢复虚拟机到某个以前的状态。比如：测试软件的时候经常需要回滚系统，以及新手安装OpenStack时为了防止重头再来，每安装成功一个服务就做一次快照等等。 快照链就是多个快照组成的关系链，这些快照按照创建时间排列成链，像下面这样。 1base-image&lt;--guest1&lt;--snap1&lt;--snap2&lt;--snap3&lt;--snap4&lt;--当前(active) 如上，base-image是制作好的一个qcow2格式的磁盘镜像文件，它包含有完整的OS以及引导程序。现在，以这个base-image为模板创建多个虚拟机，简单点方法就是每创建一个虚拟机我们就把这个镜像完整复制一份，但这种做法效率底下，满足不了生产需要。这时，就用到了qcow2镜像的copy-on-write（写时复制）特性。 qcow2(qemu copy-on-write)格式镜像支持快照，具有创建一个base-image，以及在base-image(backing file)基础上创建多个copy-on-write overlays镜像的能力。这里需要解释下backing file和overlay的概念。在上面那条链中，我们为base-image创建一个guest1，那么此时base-image就是guest1的backing file，guest1就是base-image的overlay。同理，为guest1虚拟机创建了一个快照snap1，此时guest1就是snap1的backing file，snap1是guest1的overlay。 backing files和overlays十分有用，可以快速的创建瘦装备实例，特别是在开发测试过程中可以快速回滚到之前某个状态。以CentOS系统来说，我们制作了一个qcow2格式的虚拟机镜像，想要以它作为模板来创建多个虚拟机实例，有两种方法实现： 1）每新建一个实例，把centosbase模板复制一份，创建速度慢。说白了就是复制原始虚拟机。 2）使用copy-on-write技术(qcow2格式的特性)，创建基于模板的实例，创建速度很快，可以通过查看磁盘文件信息，进行大小比较。也就是我们将存储虚拟化特性中提到的链接克隆。 如下，我们有一个centosbase的原始镜像(包含完整OS和引导程序)，现在用它作为模板创建多个虚拟机，每个虚拟机都可以创建多个快照组成快照链，但是不能直接为centosbase创建快照。 上图中centos1，centos2，centos3等都是基于centosbase模板创建的虚拟机(guest)，接下来做的测试需要用到centos1_sn1、centos1_sn2、centos1_sn3等centos1的快照链实现。也就是说，我们可以只用一个backing files创建多个虚拟机实例(overlays)，然后可以对每个虚拟机实例做多个快照。这里需要注意：backing files总是只读的文件。换言之，一旦新快照被创建，他的后端文件就不能更改(快照依赖于后端这种状态)。 virsh命令实现KVM虚拟机的内置快照qemu/kvm有三种快照，分别是内部（保存在硬盘镜像中）/外部（保存为另外的镜像名）/虚拟机状态 ，很多网站上提供的资料和教程也大多是内部快照功能。内部快照不支持raw格式的镜像文件，所以如果想要使作内部快照，需要先将镜像文件转换成qcow2格式。 内置快照在虚拟机运行状态和关闭状态都可以创建。在关机状态下，它通过单个qcow2镜像磁盘存储快照时刻的磁盘状态，并没有新磁盘文件产生。在虚机开机状态下，可以同时保存内存状态，设备状态和磁盘状态到一个指定文件中。当需要还原虚拟机状态时，将虚机关机后通过virsh restore命令还原回去。以上这些就是虚拟机的内置快照链操作，一般用于测试场景中的不断的将vm还原到某个起点，然后重新开始部署和测试。下面，我们就来玩玩KVM虚拟机的内置快照链。 Stpe1：首先，我们找一台运行的虚拟机，然后通过控制台console登录，建一个空目录flags并写一个测试文件test01作为标记。在后面快照回滚时，可以通过对比该文件查看具体的效果 。 Step2：由于内部快照不支持raw格式的磁盘镜像文件，所以我们首先需要查看下当前虚拟机的磁盘镜像格式，如果不符合要求，需要利用qemu-img命令进行磁盘格式转换。 Step3：使用snapshot-create-as命令创建第一个快照snap01。其实，还有个类似命令的snapshot-create，它创建的快照名称是系统随机生成的，一般我们使用snap-create-as命令指定快照名创建。 step4：使用snapshot-current命令查看当前快照的详细信息。这里，其实查看的是/var/libvirt/qemu/snapshot/centos7.5/下的虚拟机的快照xml配置文件信息。 step5：此时，我们再次在虚拟机创建测试内容，在test01文件追加内容456，然后再次创建快照snap02。 同时，我们利用qemu-img命令查看虚拟机的磁盘信息，发现创建的快照后磁盘的容量有所增加，这也符合常理。 step6：此时，我们利用快照回滚虚拟机。在回滚之前最好先关闭虚拟机 virsh shutdown 或virsh destroy ，在不关闭的情况下也可以做回滚。但是，此时如有新数据写入时不过会出现问题。所以，在实际运维中，还是建议先停机再做回滚。如果真的要求不停机回滚，最好加上–force选项表示强制回滚，此时即使有数据写入也会被丢弃。不加–force选项时，老版本的libvirt会报错，但是新版本不会报错，不过不建议这样使用。 此时，我们在虚拟机运行态下，再次利用快照snap02进行回滚。而且，我们没有加–force选项。 上面完成快照的创建，回滚验证后，还有个操作时快照的删除的，也就是利用snapshot-delete命令删除，这个没有什么好说的。但是需要提示一点，快照删除后，虚拟机的磁盘大小并不会变小。 以上，就是KVM虚拟机的内置快照操作内容，我觉得我是讲明白了。。。关于kvm虚拟机的状态备份，也就是save命令，时间比较长，一般要5－10分钟左右，造成该问题的原因是：save vm保存的是当前客户机系统的运行状态（包括：内存、寄存器、CPU执行等的状态），保存为一个文件，而且要在load vm时可以完全恢复，这个过程比较复杂，如果客户机里面的内存很大、运行的程序很多，save vm比较耗时，也是可以理解的。暂时很难有什么改进方法。 而往往我们并不需要去备份一个虚拟机当前状态完整的快照，实际运维中中可能只需要对disk做一个快照就OK了。所以，这就要提到外部快照（External snapshot）。 virsh命令实现KVM虚拟机的外置快照KVM的外部快照功能比较实用，可以支持仅对disk进行快照，也支持live snapshot，很多虚拟化云方案中一般也会使用外部快照功能创建快照链。不过，KVM虚拟机要支持外部快照功能，需要qemu版本在1.5.3及以上，否则只能通过下载最新的qemu源码包进行编译安装。可以通过rpm -aq qemu查看当前宿主机的qemu软件包的版本，也可以通过qemu-kvm –version命令来查看。 这里需要注意一点，centos系统默认将qemu-kvm命令放在/usr/libexec/目录下，所以在当前命令行执行qemu-kvm会提示找不到命令，需要带上命令的全路径执行，也就是/usr/libexec/qemu-kvm –version，或者通过创建软链接的方法，在/usr/bin目录下创建一个qemu-kmv命令的软链接，就可以执行qemu-kvm命令了。（我采用的就是这种办法） step1：在虚拟机关机状态下，我们创建一个外部快照ext_snap01。 从上面的查询中不难看出centos75.ext_snap01的backing file来自于/kvm_host/vmdisk/centos75.img 。同样，也可以利用下面的命令进行查看： 创建完外部快照后，原磁盘镜像文件会变成只读，新的变更都会写入到新的快照文件中。也就是我们常说链接克隆中创建的差分磁盘。忘了的，请回顾本站存储虚拟化相关文章。。。。。。 step2：我们做完快照后，写一个200M的测试文件到虚拟机centos7.5的系统/opt/目录下，并做如下验证。 如上图，在未写入测试文件data01前，虚拟机外部快照盘centos75.ext_snap01的大小为5.5M（上图红框部分），在写入data01测试文件后变为207M（上图黄框部分），且虚拟机原始磁盘大小仍为4.3G（上图蓝框部分）。至于，显示大小不准确的问题，是因为我们通过-h选项通过ls命令查看，在转换成可读模式大小时的转换误差导致，并非真实文件大小不准确。其实，用原始字节数表示就没有误差，但是那样反人类啊。。。 至于外部快照的删除、回滚等操作与内部快照一直，我就懒得演示验证了。。。。不过，外部快照多了一个快照合并的功能，也就是将差分磁盘中的数据变化写入到原始磁盘镜像中。主要有两种方式：一种是通过blockcommit命令完成，从top文件合并数据到base，也就是从快照文件向backing file合并；另一种是blockpull命令完成，从base文件合并数据到top，也就是从backing file向快照文件合并。截止目前只能将backing file合并至当前的active的快照镜像中，也就是说还不支持指定top的合并。 但是，在centos系统执行virsh blockcommit或者virsh blockpull等命令时，都会报QEMU不支持的二进制操作错误或者是版本不支持等错误，这是CentOS内部集成的qemu-kvm版本问题导致。因此，此时有两种解决办法，一种是升级QEMU-KVM，通过源码编译内核完成升级。另一种就是使用qemu-img命令，commit对应上面blockcommit，rebase对应blockpull操作。 通过qemu-img commit命令将快照文件的数据的合并到原始镜像文件中（图中红色框部分），此时不需要指定backing file文件。而且，我们此时查看原始镜像文件大小，发现变大为4.5G（图中黄色框部分）。此时，我们可以删除磁盘快照文件，进入虚拟机查看我们当初的测试文件是否还存在。 以上，commit的操作，如果采用rebase命令，反向合并时，就必须要指定backing file文件。其他操作与commit操作一致，大家自己尝试。。。。。 在openstack等平台中，使用的快照功能大都是基于外置快照的形式。这里需要特别注意一点，KVM的快照不要和Vmware的快照混为一谈，Vmware的所有快照默认情况下都是基于baseimage，创建的overlays，也就是所有快照的backing file都是baseimage，删除任一个快照对其他快照的使用和还原都不会有影响，而KVM不同，其快照之间存在链式关系，snap01是基于baseimage，snap02是基于snpa01，以此类推。。。基中任一环节出现问题，都会出现无法还原到之间状态。所以，在做快照的合并，删除等操作前，一定要提前通过qemu-img info –backing-chain查看虚拟机的快照链关系，避免发生不可挽回的数据丢失错误。 libguestfs-tools工具使用总结libguestfs是一组Linux下的C语言的API ，用来访问虚拟机的磁盘映像文件。其项目官网是http://libguestfs.org/ 。该工具包含的软件有virt-cat、virt-df、virt-ls、virt-copy-in、virt-copy-out、virt-edit、guestfs、guestmount、virt-filesystems、virt-partitions等工具，具体用法也可以参看官网。该工具可以在不启动KVM guest主机的情况下，直接查看guest主机内的文内容，也可以直接向img镜像中写入文件和复制文件到外面的物理机，甚至也可以像mount一样，支持挂载操作。 现在，我们将虚拟机centos7.5关机，然后查看其镜像磁盘使用情况，可以通过virt-df命令实现。如下： 同样，我们想查看虚拟机关机态下根分区下详细文件/目录信息，可以使用virt-ls命令实现。如下： 此时，我们需要从关机态下的虚拟中拷贝一个文件到宿主机中，可以使用copy-out命令实现。但是，需要带上-d选项指定哪个虚拟机。如下： 既然能从虚拟机中拷贝文件到本地宿主机，自然也能从本地宿主机拷贝文件到虚拟机，通过copy-in命令就能实现。其实，这些命令工具的用法和Linux的原始命令非常类似，所以熟悉Linux常用命令的使用是必须要掌握的技能。 接下来，我们查看下虚拟机的文件系统以及分区信息，通过filesystems命令实现。如下： 完成虚拟机文件系统和分区信息的查询后，我们可以通过guestmount命令，将虚拟机centos7.4的系统盘离线挂载到宿主机的/mnt分区，与挂载光驱操作类似，但是我们可以设置挂载镜像的操作方式。比如：只读、只写、读写等。如下： 以上是挂载linux系统的镜像磁盘，如果需要挂载windows虚拟机的磁盘，需要额外安装ntfs -3g的工具来识别windows系统的NTFS文件系统格式。 实际应用中的KVM主机也会遇到像物理机一样的情况，如系统崩溃、无法引导等情况。物理机出现该情况时，我们可以通过光盘引导、单用户模式、PE引导、修复或升级安装等方式获取系统内的文件和数据，KVM中同样也可以使用上述方法，既可以上面的利用libguestfs-tools的工具进行挂载修改，也可以通过linux系统原生的mount -o loop方式进行挂载修改。下面，我们就通过mount命令对raw格式和qcow2格式磁盘进行挂载做个演示。 raw磁盘镜像的挂载由于raw格式简单原始，其通常做为多种格式互相转换的中转格式，所以对raw格式的磁盘挂载操作时需重点掌握。raw格式的分区挂载也有两种方法：一种是通过计算偏移量offset方式挂载。另一种是通过kpartx分区映射方式实现挂载。 计算偏移量offset方式的思路为找出分区的开始位置，使用mount命令的offset参数偏移掉前面不需要的，即可得到真正的分区。如下： 上图中，我们可以通过fdisk -lu命令查看磁盘镜像的分区信息，可以发现centos74.img的磁盘一共有3个分区，每个分区都有对应的起始扇区和结束扇区编号，且每个扇区的大小为512字节。通过这些信息，我们就可以计算分区的offset值，从而实现找到真正的分区内容存放位置。如下： 然后，通过mount -o loop，offset=xxxx命令用其实扇区的偏移字节数实现分区挂载，如下： 除了上述计算偏移量的方法外，还可以通过kpartx分区映射方式实现挂载。首先通过kpartx工具对磁盘镜像做个分区映射。如下： 上图中，通过-av选项添加磁盘镜像并将映射结果显示出来（图中红色框部分），然后就可以知道磁盘镜像有3个分区，分别映射为loop0p1、loop0p2和loop0p3（图中黄色框部分）。完成映射后，我们就可以像挂载普通磁盘或光驱那样对映射分区进行挂载。如下： 注意映射的设备的文件位置在/dev/mapper目录下。我们可以看下该目录下的具体信息，如下： 上图中可以很清楚的发现上面的映射分区是通过软链接的方式建立的与磁盘内部分区的映射关系。通过磁盘映射方式下实现挂载后，记得使用完成不仅要卸载挂载点，还需要删除映关系。如下： 上图中可以很清楚的发现上面的映射分区是通过软链接的方式建立的与磁盘内部分区的映射关系。通过磁盘映射方式下实现挂载后，记得使用完成不仅要卸载挂载点，还需要删除映关系。如下： 以上就是raw格式镜像挂载，需要注意的如果虚拟机使用了LVM逻辑卷，那么针对逻辑卷的挂载操作需要使用losetup工具完成，具体可以查询使用方法，非常简单这里就不在赘述了。而qcow2格式的镜像的挂载不能通过kaprtx直接映射，可以先转换成raw的格式进行处理，也可以通过libguestfs-tools工具处理，还可以使用qemu-nbd直接挂载。就速度上而言qemu-nbd的速度肯定是最快的。不过由于centos/redhat原生内核和rpm源里并不含有对nbd模块的支持及qemu-nbd（在fedora中包含在qemu-common包里）工具，所以想要支持需要编译重新编译内核并安装qemu-nbd包 。 至此，KVM虚拟机的存储管理实战全部介绍完毕，后面我们进入KVM虚拟机的网络管理实战。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-KVM虚拟机存储管理实战（上篇）]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[我们上一篇介绍了KVM虚拟机的全生命周期管理实战，如果与OpenStack的实操对应，正好与nova服务的实操相符。在云计算体系中，运维管理主要涉及计算nova、存储cinder和网络neutron三部分。因此，本篇文章我们主要介绍KVM的存储管理实战。KVM的存储选项有多种，包括虚拟磁盘文件、基于文件系统的存储和基于设备的存储。同样，也有对应的virsh管理命令。 KVM的存储选项和常用管理命令为实现KVM存储管理，可以使用LVM逻辑卷和创建存储池。储存池Storage Pool 是宿主机上可以看到的一片存储空间，可以是多种型。而逻辑卷Volume 是在 存储池Storage Pool 中划分出的一块空间，宿主机将 Volume 分配给虚拟机，Volume 在虚拟机中看到的就是一块硬盘。 虚拟磁盘文件当系统创建KVM虚拟机的时候，默认使用虚拟磁盘文件作为后端存储。安装后，虚拟机认为在使用真实的磁盘，但实际上看到的是用于模拟硬盘的虚拟磁盘文件。就是因为加了一层额外的文件系统层，所以系统的I/O读写性能会降低，但是基于文件系统的虚拟磁盘可以用于其他虚拟机，且具备快照、链接克隆、弹性扩缩容等特性，方便虚拟机的迁移。因此，可以说是各有利弊，根据实际使用场景来选择。 基于文件系统的KVM存储在安装KVM宿主机时，可选文件系统为dir（directory）或fs（formatted block storage）作为初始KVM存储格式。默认为dir，通过指定本地文件系统中的一个目录用于创建磁盘镜像文件。而fs选项可以允许指定某个格式化文件系统的分区，把它作为专用的磁盘镜像文件存储。两种KVM存储选项之间最主要的区别在于：fs文件系统不需要挂载到某个特定的目录。两种选项所指定的文件系统，都可以是本地文件系统或位于SAN上某个物理宿主机上的网络文件系统。后者具备数据共享的优势，可以很轻易地实现多个主机同时访问。 还有一种基于文件的磁盘存储方式是netfs，可以指定一个网络文件系统的名称，如NFS，用这种方式作为KVM存储与SAN类似，可以访问到位于其它服务器上的虚拟磁盘，同时也可以多台宿主机共享访问磁盘文件。 但是，所有的这些基于文件系统的KVM存储方式都有一个缺点：文件系统固有缺陷。因为虚拟机的磁盘文件不能直接读取或写入KVM存储设备，而是写入宿主机OS之上的文件系统。这也就意味着在访问和写入文件时中间增加了额外一层，因此会降低性能。所以，如果只是单纯要求性能好，就需要考虑考虑基于设备的存储。 基于设备的KVM存储另外一种KVM存储的方式就是使用基于设备的方式。共支持四种不同的物理存储：磁盘、iSCSI、SCSI和lvm逻辑盘。磁盘方式指直接读写硬盘设备。iSCSI和SCSI方式可选，取决于通过SCSI或iSCSI地址与磁盘设备连接。这种KVM存储方式的优势在于磁盘的名称固定，不依赖宿主机OS搜索到磁盘设备的顺序。但是，这种连接磁盘的方式也有缺点：灵活性不足。虚拟磁盘的大小很难改变，而且这种连接方式的KVM存储不支持快照。 如果要提升基于设备的KVM存储灵活性，可以使用LVM。LVM的优势在于可以使用快照，但是快照并不是KVM虚拟化的特性，而是LVM自带的特性。 LVM可以把所有存储放到一个卷组里，该卷组是物理磁盘设备的一个抽象，所以如果超出可用磁盘空间最大值，还可以向卷组中添加新的设备，增加的空间在逻辑卷中直接可以使用。使用LVM使得磁盘空间分配更加灵活，而且增加和删除存储也更为容易。LVM除了在单机场景下使用外，还可以在多机场景下使用。在多宿主机环境中，可以在SAN上创建逻辑卷，且如果使用Cluster LVM（集群LVM），可以很容易的配置成多个主机同时访问某个逻辑卷。详见本站Linux常用运维工具分类中LVM逻辑卷一文。 virsh也可以对节点上的存储池和存储卷进行管理。virsh常用于存储池和存储卷的管理命令如下： 命令 功能描述 pool-list 显示libvirt管理的存储池 pool-define &lt;pool.xml&gt; 根据xml文件定义一个存储池 pool-define-as &lt;–type\ target&gt; 定义一个存储池，指定pool名，并指定pool类型和存储路径 pool-build 构建一个存储池 pool-start 激活一个存储池 pool-autostart 设置存储池开机自动运行 pool-info 根据一个存储池名称查询其基本信息 pool-uuid 根据存储池名称查询其uuid信息 pool-create&lt;pool.xml&gt; 根据xml配置文件信息创建一个存储池 pool-edit 编辑一个存储池的xml配置文件 pool-destroy 关闭一个存储池 pool-delete 删除一个存储池，不可恢复 vol-list 查询一个存储池中的存储卷的列表 vol-name 查询一个存储卷的名称 vol-path –pool 查询一个存储卷的路径 vol-create&lt;vol.xml&gt; 根据xml配置创建一个存储卷 vol-create-as &lt;–pool\ name\ capacity\ allocation\ format&gt; 创建一个卷，指定归属pool，卷名、预分配大小、占用大小、磁盘格式 vol-clone 克隆一个存储卷 vol-delete 删除一个存储卷 vol-pool 根据存储卷名或路径查询归属存储池信息 与全生命周期管理一样，可以通过virsh help|egrep ‘(pool*|vol*)’查看全量存储管理命令。 KVM虚拟机的存储管理实战存储池和存储卷的信息查询有了上面的命令知识储备，我们通过pool-list命令首先看下当前宿主机（192.168.101.251）上存储池信息。如下： 上图中表示当前宿主机上有两个KVM虚拟机存储池，一个是网络文件系统NFS的存储池，名称为nfsdir，另一个是本地文件系统存储池，名称为root。其实，这两个存储池是在我们前面创建虚拟机通过disk选项制定虚拟机系统盘时，默认创建的。我们可以通过pool-info命令查看对应存储池的详细信息。如下： 1）nfsdir存储池信息 2）root存储池信息 这里需要明确一个概念，上图中的存储池的总空间大小并不是创建的存储池空间大小，而是存储池所在的磁盘分区的大小，由于我们前面在创建虚拟机时指定的虚拟机系统盘文件都在系统根分区下创建（/root and /mydata/nfsdir/），所以这里的存储池总大小为根分区的大小。这一点可以通过查看根分区大小来确认。如下： 掌握了存储池信息查看后，我们可以通过vol-list命令查看该存储池下有哪些存储卷。如下： 上图中表示nfsdir存储池下有一个存储卷cto67s.img，它位于/mydata/nfsdir目录下。接下来，我们还可以通过vol-info命令查看该存储卷的详细信息。如下： 如上图，在查看存储卷的详细信息时，需要通过–pool选项制定该存储卷对应的存储池名称。上图中表示cto67s.img存储卷的类型为file，总大小为2.28GB。 存储池和存储卷的增、删、改实战上面查看存储池和存储卷的命令只能浏览大致的一些信息，其实，我们还可以通过命令pool-deumpxml查看具体的存储卷xml配置详细信息。如下： 上图中显示的存储池nfsdir的xml配置信息，从配置信息中我们可以知道存储池nfsdir的类型type、name、uuid、空间的大小、路径path、权限、归属的用户和用户组等信息。同理，我们如果想查看某个存储卷的xml配置信息，可以使用vol-dumpxml命令实现。如下： 在上图中，我们不仅知道存储卷的类型type、name、key、空间大小、路径、磁盘格式、文件权限，归属用户&amp;用户组，还知道该存储卷的访问时间，修改时间和状态改变时间。 有了上面的知识储备后，我们就可以通过xml文件创建一个存储池和一个存储卷。我们首先创建一个存储池POOLB，xml配置内容如下： 如上图，我们只需要定义要创建的存储池类型、名称、路径、权限和归属用户及用户组等信息，其他如uuid、空间大小等信息会在存储池创建后，系统自动生成。由于我们定义的存储池类型为dir目录类型，因此实际上空间大小等信息就是存储池目录所在的磁盘分区大小信息。 有了上面存储池的xml配置文件后，就可以通过pool-create命令来创建一个存储池poolB了。需要注意，由于我们定义的存储池类型为dir目录类型，所以需要提前在xml文件指定的路径下创建目标目录，否则会提示创建存储池失败。如下： 在/mydata目录下创建poolB子目录后，结果如下： 上图中，我们完成存储池创建后，在该存储池中创建一个卷data01.img。同理，我们还是通过xml文件来创建，首先创建卷的xml配置文件data01.xml。如下： 然后，我们通过命令vol-cretae命令来创建卷data01.img。如下： 完成存储池和卷的创建验证后，我们现在来验证删除存储池和卷。既然我们创建的时候，先创建存储池，再创建卷。那么，删除的时候自然是反着来，是不是就是先删除卷再删除存储池。如果不删除存储池中的卷就直接删除存储池行不行？答案是可以的。。。所以，我们既可以先通过vol-delete命令删除卷，然后通过pool-destroy命令删除池。也可以直接通过pool-destroy命令删除池。如下： 在删除池时还有一个pool-delete命令，这个命令请慎用。因为一旦通过它删除池就无法恢复。所以，我们一般使用pool-destroy命令来销毁一个池，一旦需要时还可以通过pool-start命令恢复。即使真的不需要这个池，也可以通过find+rm命令来删除。 大家可能发现了，通过上面的方式创建的存储池是不能自动开始的。其实，KVM的virsh命令还有另一种方式来创建存储池和存储卷。就是通过pool-define-as命令先定义一个存储池，然后通过pool-build命令来构建一个存储池，再通过pool-start命令激活存储池，最后通过pool-autostart命令设置该存储池开机自动运行。如下： 其实，通过xml配置文件也能通过这种方式创建。如下： 完成上面pool的创建后，我们也可以通过vol-create-as命令来创建一个卷。如下： 如上图，通过制定归属pool，卷名、预分配大小、占用大小和磁盘格式，通过vol-create-as命令完成一个卷的创建。 删除卷和池的方式与上面类似，这里就不再举例了。存储池和卷的其他命令大家可以参考virsh help的信息自行研究。接下来，我们来看一个经常使用qemu-img命令，也就是KVM存储磁盘管理命令。 qemu-img命令实战镜像算不算存储？在我初学OpenStack时，因为镜像管理服务是glance，存储管理服务是cinder和swift，所以当初单纯以为的镜像不算存储。后来通过深入了解和研究，发现其实镜像也是一种存储，可以把它简单理解为“虚拟机的元数据”，既然是元数据，当然是存储。 我们都知道，在创建KVM虚拟机时，首先需要通过qemu-img命令去创建一个虚拟系统盘。这个qemu-img命令就是是QEMU的磁盘镜像管理工具，在完成qemu-kvm源码编译或rpm包安装后就会默认编译好qemu-img这个二进制文件。qemu-img也是QEMU/KVM使用过程中一个比较重要的工具，我们下来就对其常用用法总结验证下。 qemu-img支持非常多种的文件格式，可以通过“qemu-img-h”查看其命令帮助得到，它支持20多种格式：file，quorum，blkverify，luks，dmg，sheepdog，parallels，nbd，vpc，bochs，blkdebug，qcow2，vvfat，qed，host_cdrom，cloop，vmdk，host_device，qcow，vdi，null-aio，blkreplay，null-co，raw等。 qemu-img工具的命令行基本用法为：qemu-img command[command options]，也就是说qemu-img工具通过后面命令和命令选项实现各种磁盘镜像管理功能。它支持的命令分为如下几种： 检查镜像磁盘的数据一致性check[-f fmt]filename该命令用于对磁盘镜像文件进行一致性检查，查找镜像文件中的错误，目前，仅支持对“qcow2”、”qed”、”vdi”格式文件的检查。如下： 如上图，提示该磁盘镜像没有错误，并且其中显示了该镜像磁盘在物理盘的I/O偏移地址等存储信息。在上面的命令中，我们使用-f参数指定镜像磁盘的格式为qcow2，它和qed都是QEMU支持的磁盘镜像格式，qed是qcow2的增强磁盘文件格式，避免了qcow2格式的一些缺点，也提高了性能，不过目前还不够成熟。而另一种磁盘镜像格式vdi（Virtual Disk Image）是Oracle的VirtualBox虚拟机中的存储格式。最后的filename参数是磁盘镜像文件的名称（包括路径）。 创建磁盘镜像文件create[-f fmt][-o options]filename[size]该命令用于创建一个格式为fmt，大小为size，文件名为filename（包含路径）的镜像文件。如下： 上图中，我们创建了一个raw格式的，大小为20G的镜像磁盘metdata001.raw，位于/mydata/poolB目录下。其实，可以根据文件格式fmt的不同，添加一个或多个选项（options）来附加对该文件的各种功能设置，可以使用”-o?”来查询某种格式文件支持哪些选项，在”-o”选项中各个选项用逗号来分隔。如下： 如上图，如果“-o”选项中使用了backing_file这个选项来指定其后端镜像文件，那么这个创建的镜像文件仅记录与后端镜像文件的差异部分。后端镜像文件不会被修改，除非在QEMU monitor中使用“commit”命令或者使用“qemu-img commit”命令去手动提交这些改动。这种情况下，size参数不是必须需的，其值默认为后端镜像文件的大小。另外，镜像文件的大小（size）也并非必须写在命令的最后，它也可以被写在“-o”选项中作为其中一个选项。而且，直接使用“-b backfile”参数也与“-o backing_file=backfile”效果等价。如下： 修改磁盘镜像文件格式convert[-c][-f fmt][-O output_fmt][-o options]filename[filename2[…]]output_filename该命令用于将fmt格式的filename镜像文件根据options选项转换为格式为output_fmt的名为output_filename的镜像文件。比如我们先创建test01的qcwo2格式磁盘文件，然后将其转换为raw格式文件。如下： 如上图，将创建的qcow2格式的test01文件转换为raw格式的文件（蓝色框部分）。这个命令一般在做虚拟机模板镜像文件时经常用到，将非raw格式的磁盘文件转换为raw格式的后缀为.img的镜像模板文件，用于后续的批量虚拟机发放。同时，它支持不同格式的镜像文件之间的转换，比如可以用VMware用的vmdk格式文件转换为qcow2文件，这对从其他虚拟化方案转移到KVM上的用户非常有用。一般来说，输入文件格式fmt由qemu-img工具自动检测到，而输出文件格式output_fmt根据自己需要来指定，默认会被转换为与raw文件格式（且默认使用稀疏文件的方式存储以节省存储空间）。 命令中，“-c”参数是对输出的镜像文件进行压缩，不过只有qcow2和qcow格式的镜像文件才支持压缩，而且这种压缩是只读的，如果压缩的扇区被重写，则会被重写为未压缩的数据。同样可以使用“-o options”来指定各种选项，如：后端镜像、文件大小、是否加密等等。使用backing_file选项来指定后端镜像，让生成的文件是copy-on-write的增量文件，这时必须让转换命令中指定的后端镜像与输入文件的后端镜像的内容是相同的，尽管它们各自后端镜像的目录、格式可能不同。 如果使用qcow2、qcow、cow等作为输出文件格式来转换raw格式的镜像文件（非稀疏文件格式），镜像转换还可以起到将镜像文件转化为更小的镜像，因为它可以将空的扇区删除使之在生成的输出文件中并不存在。 查看镜像磁盘信息info [-f fmt] filename该命令用于显示镜像磁盘的详细信息。如果文件是使用稀疏文件的存储方式，也会显示出它的本来分配的大小以及实际已占用的磁盘空间大小。如果文件中存放有客户机快照，快照的信息也会被显示出来。比如，我们想查看cto67s.img这个镜像磁盘的信息。如下： 这个命令比较简单，聪明如你，一学就会。。。。。。 镜像磁盘的快照命令集snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename这是一组命令，“-l” 选项是查询并列出镜像文件中的所有快照，“-a snapshot”是让镜像文件使用某个快照，“-c snapshot”是创建一个快照，“-d”是删除一个快照。比如，我们现在看当下宿主机中有没有虚拟机磁盘cto74d.img的快照，可以使用-l选项查看，结果发现没有。如下： 此时，我们使用-c选项对该磁盘创建一个快照，此时需要注意raw格式的磁盘文件不支持快照，因此对raw格式的磁盘创建快照时，需要将其转换为qcow2格式。同时，无论是使用快照还是创建快照都需要在关闭虚拟机的情况下进行，如果虚拟机时运行状态需要使用另一个命令virsh save vm。如下： 上述快照创建成功后，我们在当前目录下查看是否新的镜像文件产生。如下： 发现并没有新的镜像文件产生，说明通过qemu-img该步并不会创建一个新的镜像，但是磁盘镜像的快照确实存在，因为通过-l选项可以查看。这样，我们在需要的时候，就可以使用-a选项利用快照恢复磁盘。同样，如果我们不需要快照，可以通过-d选项将其删除。如下： 修改磁盘镜像文件的大小resize filename[+|-]size该命令用于改变镜像文件的大小。“+”和“-”分别表示增加和减少镜像文件的大小，size也支持K、M、G、T等单位的使用。比如，我们现在将cto67s.img磁盘镜像大小增大5G。如下： 在上图中，源磁盘大小为40G（红色框部分），通过resize命令增大5G后（黄色框部分），变成了45G（蓝色框部分）。需要注意的是：使用resize命令时需要小心（做好备份），如果失败，可能会导致镜像文件无法正常使用，而造成数据丢失。同时，缩小镜像的大小之前，需要在虚拟机中保证其中的文件系统有空余空间，否则数据会丢失。另外，qcow2格式文件不支持缩小镜像的操作。 至此，KVM虚拟机的存储管理实战上篇介绍完了。我们主要讲述了如何利用virsh命令行工具来管理KVM虚拟机的存储池以及存储卷等操作，以及介绍了qemu-img这个常用的命令几种用法等内容。这些内容是下一篇通过virsh命令完成快照链操作的基础，因此需要重点掌握。光说不练假把式，IT的东西就需要大量实践才能掌握，这也是IT领域没有科技创建只有最佳实践的原因。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-KVM虚拟机全生命周期管理实战]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%85%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[在上一篇我们介绍了KVM最重要的管理工具libvirt，它是KVM其他管理工具的基础，处于KVM管理架构的中间适配层。本篇我们主要要介绍libvirt的命令行管理工具的virsh，它也是将libvirt作为基础，通过封包调用实现的。所以，在看本篇内容之前，最好将上一篇的内容做个预习。 libvirt命令行工具virshvirsh通过调用libvirt API实现虚拟化管理，与virt-manager工具类似，都是管理虚拟化环境中的虚拟机和Hypervisor的工具，只不过virsh是命令行文本方式的，也是更常用的方式。 在使用virsh命令行进行虚拟化管理操作时，可以使用两种工作模式：交互模式和非交互模式。交互模式是连接到Hypervisor上，然后输入一个命令得到一个返回结果，直到输入quit命令退出。非交互模式是直接在命令上通过建立URI连接，然后执行一个或多个命令，执行完后将命令的输出结果返回到终端上，然后自动断开连接。两种操作模式的截图如下： 我们经常在本地使用virsh命令，这是一种特殊的交互模式，也是最常用的模式，它本质上就是默认连接到本节点的Hypervisor上。 libvirt中实现的功能和最新的QEMU/KVM中的功能相比有一定的滞后性，因此virsh只是实现了对QEMU/KVM中的大多数而不是全部功能的调用。同时，由于virsh还实现了对Xen、VMware等其他Hypervisor的支持，因此有部分功能对QEMU/KVM无效。下面，我们还是按照“边验证边理解”的套路，对virsh常用的命令进行分类整理说明。 域管理（虚拟机管理）命令virsh的最重要的功能之一就是实现对域（虚拟机）的管理，但是与虚拟机相关的命令是很多的，包括后面的网络管理、存储管理也都有很多是对域（虚拟机）的管理。为了简单起见，本文使用“”来表示一个域的唯一标识（而不专门指定为“”这样冗长的形式）。常用的virsh域管理命令如下： virsh中的虚拟机管理命令 命令 功能描述 list 获取当前节点上多有虚拟机的列表 domstate 获取一个虚拟机的运行状态 dominfo 获取一个虚拟机的基本信息 domid 根据虚拟机的名称或UUID返回虚拟机的ID domname 根据虚拟机的ID或UUID返回虚拟机的名称 dommemstat 获取一个虚拟机的内存使用情况的统计信息 setmem 设置一个虚拟机的大小（默认单位的KB） vcpuinfo 获取一个虚拟机的vCPU的基本信息 vcpupin 将一个虚拟机的vCPU绑定到某个物理CPU上运行 setvcpus 设置一个虚拟机的vCPU的个数 vncdisplay 获取一个虚拟机的VNC连接IP地址和端口 create&lt;dom.xml&gt; 根据虚拟机的XML配置文件创建一个虚拟机 define&lt;dom.xml&gt; 定义一个虚拟机但不启动，此时虚拟机处于预定义状态 start 启动一个预定义的虚拟机 suspend 暂停一个虚拟机 resume 恢复一个虚拟机 shutdown 对一个虚拟机下电关机 reboot 让一个虚拟机重启 reset 与reboot的区别是，它是强制一个虚拟机重启，相当于在物理机上长按reset按钮，可能会循环虚拟机系统盘的文件系统 destroy 立即销毁一个虚拟机，相当于物理机上直接拔电源 save&lt;file.img&gt; 保存一个运行中的虚拟机状态到一个文件中 restore&lt;file.img&gt; 从一个被保存的文件中恢复一个虚拟机的运行 migrate&lt;dest_uri&gt; 将一个虚拟机迁移到另一个目标地址 dump&lt;core.file&gt; coredump一个虚拟机保存到一个文件 dumpxml 以XML格式转存出一个虚拟机的信息到标准输出中 attach-device&lt;device.xml&gt; 向一个虚拟机添加xml文件中的设备，也就是热插拔 detach-device&lt;device.xml&gt; 将一个XML文件中的设备从虚拟机中移除 console 连接到一个虚拟机的控制台 上表中，只是列出常用的几个KVM虚拟机全生命周期管理命令，如果想查找全量KVM虚拟机全量生命周期管理命令，可以使用命令帮助。如下： 如上图，由于输出太长，我们只截取了一部分。上图中每个命令后面都有详细的文字说明来描述命令的用途。 虚拟机全生命周期管理实战虚拟机状态信息查询有了上面的命令储备后，我们下来在环境中进行实操验证。首先，我们通过list命令，查看下当前节点的虚拟机个数，如下： 如上图，本节点有2个虚拟机，当前状态均为shut off，也就是没有启动。那么，我们现在通过start命令，将上述两个虚拟机同时启动，如下： 启动后，我们可以通过domstate命令查看虚拟机当前的状态，并且通过dominfo命令查看虚拟机的详细信息，如下： 同时，我们可以通过dommemstate和vcpuinfo命令，查看虚拟机的虚拟内存信息及vCPU信息。如下： 虚拟机vCPU与vMEM管理操作如上图，通过上面的vCPU的详细信息，我们发现cto74d的两个vCPU都与pCPU0默认绑定。那么，我们现在可以通过vcpupin命令将vCPU1与pCPU1进行绑定。如下： 同时，我们发现cto74d虚拟机只有2个vCPU。现在，我们需要给它增加到4个vCPU怎么办？在KVM中，可以通过setvcpus命令完成cto74d虚拟机的vCPU在线热添加。但是，在热添加之前，我们需要明确一个概念，那就是虚拟机的vCPU最大可分配数与当前vCPU数的区别。为了讲明白这个概念，我们还是先来看虚拟机cto74d的XML配置文件，可以通过dumpxml命令将虚拟机XML配置文件输出到屏幕上。如下： 上图中表示cto74d虚拟机的vCPU最大可分配数为2。而且，我们还可以通过指令emulatorpin查看虚拟机当前的vCPU使用情况。如下： 上图中表示虚拟机cto74d当前使用的vCPU个数也为2。所以，在这种配置要求下，自然无法通过指令将cto74d虚拟机的vCPU数量调整为4个。为了实现我们的需求，首先需要通过shutdown命令将虚拟机下电，然后修改配置文件如下： 上图中，红色框部分表示我们将虚拟机下电，黄色框部分我们将虚拟机的vCPU配置修改为：最大支持4个vCPU，当前使用2个vCPU。我们可以查看XML文件验证下： 完成上述修改后，我们需要通过define命令重新定义下cto74d这个虚拟机，然后通过start命令启动虚拟机。如下： 现在，我们具备条件后，就可以使用命令setvcpus命令在线对cto74d虚拟机的vCPU进行热添加，添加到4个。如下： 在完成vCPU的热插后，那么vCPU是否可以热拔呢？答案是不可以，因为当vCPU被分配给虚拟机使用时，虚拟机中的程序进程就会占用刚分配的vCPU，此时如果我们进行热拔操作，前提是需要将上面的进程迁移到别的vCPU，但是我们是无法确切知道有哪些进程的。所以，vCPU自然不支持热拔操作。那么，既然KVM支持CPU和内存的虚拟化，内存是否也支持在线调整呢？答案是必须支持。我们首先通过dominfo命令看下当前虚拟机的内存情况。如下： 上图表示，虚拟机当前最大内存和可用内存都是4194304KB，也就是4GB大小。我们可以通过通过setmem指令设置虚拟机的可用内存。与vCPU概念类似，这种方式支持的在线调整范围只能小于当前虚拟机的最大内存配额，也就是4GB以内。否则，需要关机，先调整虚拟机支持的最大内存，然后再调整虚拟机支持的可用内存。如下： 从上图，我们发现，内存在线调整内存是可以减小的，即在线将虚拟机原内存从4G调整为3G。另外，setmaxmem指令用于调整虚拟机的最大可支持内存，只能在虚拟机下电后执行。否则会报如下错误： 虚拟机磁盘和网卡的热插拔完成了vCPU和内存的在线调整实战后，我们接下来玩玩热插拔。先来玩热插拔磁盘，在KVM中通过virsh管理工具向虚拟机热插拔磁盘，可以使用attach-disk命令。为了实现上述需求，首先我们需要创建一个虚拟磁盘文件，然后通过attach-disk命令将其挂载到虚拟机中，然后通过控制台进入虚拟机对新挂载的磁盘进行格式化，并创建对应目录对其挂载。最后，我们向挂载目录中写入测试文件测试。同时，需注意向虚拟机挂载raw格式qcow2格式的磁盘是不一样的。 我们先来热插raw格式的磁盘，操作过程如下： Step1：创建一个虚拟磁盘cto74d_data.raw，磁盘格式为raw，大小为5G Step2：为虚拟机热插磁盘，需要注意指定磁盘的位置要使用绝对路径，并设置虚拟盘符为vdb Step3：通过控制台进入虚拟机cto74d，查看磁盘信息 Step4：对新挂载的磁盘格式化，并创建一个/data目录进行挂载 Step5：写入一个大文件进行测试 Step6：我们在宿主机上通过domblklist命令查看虚拟机的磁盘信息 如上图，可以看见当前虚拟机有两块磁盘，一块系统盘，一块数据盘。我们查看虚拟机XML文件信息进行验证。如下： 以上就是raw格式磁盘的热插操作过程，如果想热插qcow2格式的磁盘（后缀为.qcow2）或想热插qcow2格式的镜像文件（后缀为.img），在Step1创建磁盘时，需要通过-o size选项指定预分配大小，否则系统会默认挂载一个几百K大小虚拟盘，这样会导致空间不够。同事，还需要通过preallocation选项指定磁盘格式为metadata，也就是元数据格式。除此之外，这两种类型磁盘的热插操作与raw格式磁盘一致。 上面完成了磁盘的热插，那么要实现磁盘的热拔呢？可以通过detach-disk这个命令完成。但是，这时需要注意一点：使用virsh指令删除磁盘会直接强制将虚拟机中磁盘删除，如果磁盘已经挂载使用，要停止该磁盘的写操作，否则会造成数据丢失，拔掉的磁盘并没有删除，仍然存储在当时创建的位置，需要使用可以再挂载使用。因此，在热拔磁盘之前，我们需要先进入虚拟机将对该磁盘的写入进程全部暂停，然后将磁盘从挂载点卸载，最后回到宿主机通过detach-disk命令完成磁盘的热拔。这里，大家自己玩吧，我就不陪着演示了。。。。。 磁盘的热插拔我们验证了，接下来我们玩网卡的热插拔。在KVM虚拟机中，要实现网卡的热插，可以使用attach-interface命令完成。同理，热拔的命令就是detach-interface。首先，我们通过domiflist命令查看当前虚拟机的网络和网卡信息，如下： 如上图，黄色框部分就表示当前虚拟机cto74d的网络信息，类型（type）为network，源设备（source）为default。如果在创建KVM虚拟机时，网络配置是自定义网桥，那么这里的类型（type）为bridge，源设备（source）为自定义网桥名称，如br0， 有了上面的知识储备后，下面我们通过attach-interface命令来热插一个网卡，如下： 同时，我们可以通过domifaddr命令查看新增加的网卡动态获得的IP地址，如下： 接下来，我们尝试下在宿主机通过SSH方式远程连接vnet2的地址是否成功。如下： 上图表示，可以通过vnet2的地址192.168.122.230远程连接虚拟机cto74d，表明我们新增加的网卡有效且地址分配正常（这个地址是通过DHCP方式动态分配的），通过输入密码就可以远程登录。 搞定网卡的热插后，我们接下来自然要实现网卡的热拔。与磁盘类似，在对网卡热拔时，首先需要停止网卡的工作，这样后续数据包就不会转发到该网卡上，不到导致网络丢包。而且，在对网卡热拔时，我们要根据网卡的MAC地址来实现，而不能通过网卡名称，否则libvirt内部会将该网卡名称归属的同一个二层设备上所有端口销毁。对网卡热拔的命令就是detach-ifinterface。如下： 上图中，通过–mac参数指定mac地址删除，就不会发生误删除事件。 虚拟机的热迁移接下来，我们再来玩一个更高级的操作，就是KVM虚拟机的热迁移。在KVM中虚拟机的迁移分为热迁移和冷迁移。每一种迁移方式又细分为基于本地存储和基于共享存储两种。 两种方式下的冷迁移均比较简单，就是将原虚拟机的磁盘文件和配置文件拷贝至目标服务器，然后通过配置文件定义一个新的虚拟机启动即可。而基于本地存储的热迁移与此类似，也需要在目标主机创建同一个存储文件，通过暴露TCP端口，基于socket完成迁移，同样比较简单，但是过程很慢。有兴趣的可参考https://www.cnblogs.com/lyhabc/p/6104555.html这篇文章。 我们这里演示的是基于共享存储的热迁移（电信云采用分布式存储，每一个计算组规划区都是一个共享存储池），它实现的前提就是需要源目服务器之间有共享存储。因此，为了演示这个操作，我们需要在源主机（192.168.101.251）和目标主机（192.168.101.252）之间通过NFS方式建立共享文件系统，也可以使用GFS2集群文件系统来实现。至于，NFS网络共享文件系统如何配置，请参见本站的Linux常用运维工具分类中的NFS网络文件共享系统一文，这里不再赘述。 整个实验的拓扑如下，源主机和目标主机都作为NFS的客户端与NFS服务器（192.168.101.11）之间互通，实现数据的共享存储。 Step1：首先确保两个NFS客户端服务器与NFS服务器端共享目录正常，且两个客户端服务器上存放虚拟机磁盘的目录一致。如下： Step2：在251服务器首先将虚拟机磁盘创建在/mydata/nfsdir下，格式为qcow2格式，然后去252服务器的对应目录下检查确保文件共享正常。如下： Step3：在251服务器创建虚拟机cto67s，并启动。如下： 在安装过程中，我们手动通过图形化方式安装，因此需要查询vnc图形化界面的连接地址和服务端口。可以通过vncdisplay命令查看虚拟机的VNC客户端连接地址及服务端口，如下： 然后，还是通过vnc界面安装虚拟机，如下： 完成，虚拟机OS安装后，需要重启生效。然后，源主机（192.168.101.251）中通过start命令，启动虚拟机cto67s。如下： Step4：开始虚拟机cto67s的热迁移。如下： 如上图，当前虚拟机cto67s在源主机（192.168.101.251）处于开机运行状态，为了清晰说明热迁移的过程，我们需要知道虚拟机cto67s的ip地址，这样在虚拟机热迁移时，由于内存热数据的迭代拷贝，会有一个暂停-恢复-暂停-恢复的过程，反映在网络测就会出抖动或丢包。如下： 完成上述的准备工作后，我们通过migrate命令开始进行虚拟机热迁移。如下： 如上图，我们期望的热迁移失败啦。。。。这是为啥？我们看上图红色框部分的报错内容，提示：域名解析失败。其实，在迁移期间，目标主机上运行的libvirtd会从其希望接收迁移数据的地址和端口创建URI，并将其发送回在源主机上运行的libvirtd。在我们的例子中，目标主机（192.168.122.252）的名称设置为“c7-test02”。出于某种原因，在该主机上运行的libvirtd无法将该名称解析为可以发回的IP地址，因此它发送了主机名，希望源libvirtd在解析名称时会更成功。但是，源主机的libvirt因为也没有配置这个主机名的解析，所以提示：Name or service not know。 有了上面的分析，那就简单了，无非就是在源主机和目的主机上配置主机名和IP的解析就可解决上述问题。这里，有两种配置方法：一种是在/etc/resolve中配置DNS解析，但是不建议这样配置，因为这里一般是配置公网域名解析的地方。另一种就是在/etc/hosts中配置主机名与IP的映射关系。我们采用第二种方法解决。如下： 完成上面的配置后，我们再次尝试热迁移。如下： 同时，我们在源主机和目标主机分别查看虚拟机cto67s的状态。如下： 此时，虽然虚拟机cto67s已经在目标主机上启动，但是目标主机上还没有虚拟机cto67s的配置文件。所以需要根据当前虚拟机的状态，创建配置文件并定义虚拟机。如下： 最后，我们从目标主机上进入虚拟机cto67s进行验证，如下： 至此，KVM虚拟机热迁移演示完成。这里需要提示一下，由于我们创建的虚拟机都是nat网络模式的，这样在迁移后，在源主机ping虚拟机测试会提示目标不可达，但是在虚拟机内部ping源主机可以ping通，只是迁移过程中会有1-2个ICMP包丢失或抖动。如果，非要在宿主机上ping测试虚拟机来观察迁移过程中的丢包或抖动过程，可以将虚拟机的网络模式改为桥接bridge。而且，源主机和目标主机必须归属同一个网桥bridge。 以上，就是我们KVM虚拟机全生命周期管理实战的全部内容，至于虚拟机的挂起、恢复等操作都比较简单，我也就懒得举例演示，各位可以自行玩玩，挺有意思的，真的。。。。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-KVM虚拟机的各种安装方法]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%90%84%E7%A7%8D%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[virt-instal工具简介virt-install是一个命令行工具，它能够为KVM、Xen或其它支持libvrit API的hypervisor创建虚拟机并完成GuestOS安装；此外，它能够基于串行控制台、VNC或SDL支持文本或图形安装界面。安装过程可以使用本地的安装介质如CDROM，也可以通过网络方式如NFS、HTTP或FTP服务实现。对于通过网络安装的方式，virt-install可以自动加载必要的文件以启动安装过程而无须额外提供引导工具。当然，virt-install也支持PXE方式的安装过程，也能够直接使用现有的磁盘映像直接启动安装过程。 virt-install命令有许多选项，这些选项大体可分为下面几大类，同时对每类中的常用选项也做出简单说明。 普通选项 选项 子选项 说明 -n或–name 虚拟机名称，全局唯一 -r或–ram 虚拟机内存大小，单位为MB –vcpus maxvcpu，sockets，cores，threads vCPU的个数及相关配置 –cpu CPU模式及特性，可以使用qemu-kvm -cpu ? 来获取支持的类型 安装方法选项 选项 子选项 说明 -c或–cdrom 光盘安装介质 -l或–location 安装源URL，支持FPT，HTTP及NFS，如:ftp://172.0.6.1/pub –pxe 基于PXE完成安装 –livecd 把光盘当做启动引导CD –os-type 操作系统类型，如linux、windows或unix等 –os-variant 某类型操作系统的发行版，如rhel7、Ubuntud等 -x或–extra-args 根据–location指定的方式安装GuestOS时，用于传递给内核的参数选项，例如指定kickstart文件的位置。 –boot 指定安装过程完成后的配置选项，如指定引导设备的次序、使用指定的而非安装kernel/initrd来引导系统。比如：–boot cdrom、hd、network：指定引导次序分别为光驱、硬盘和网络；–boot kernel=KERNEL,initrd=INITRD,kernel_args=”console=/dev/ttyS0”：指定启动系统的内核及initrd文件，并创建一个模拟终端 存储配置 选项 子选项 说明 –disk 指定存储设备的路径及其属性 device 设备类型，如cdrom、disk或floppy等，默认为disk bus 磁盘总线类型，可以为ide、scsi、usb、virtio或xen perms 访问权限，如rw、ro或sh（共享可读写），默认为rw size 新建磁盘镜像大小，单位为GB cache 缓存类型，可以为none、writethrouth及writeback format 磁盘镜像格式，如raw、qcow2、vmdk等 sparse 磁盘镜像的存储数据格式为稀疏格式，即不立即分配指定大小的空间 –nodisks 不适用本地磁盘，在LiveCD模式中常用 网络配置 选项 子选项 说明 -w或–network 将虚拟机连入虚拟网络中 bridge=BRIDGE 虚拟网络为名称为BRIDGE的网桥设备 network=NAME 虚拟网络为名称NAME的网络 model GuestOS中看见的虚拟网络设备类型 mac 配置固定的MAC地址，省略此选项时，MAC地址随机分配，但是无论何种方式，KVM的网络设备的MAC地址前三段必须为52:54:00 –nonetworks 虚拟机不使用网络 图形配置 选项 子选项 说明 –graphics 指定图形显示相关的配置，此选项不会配置任何硬件，而是指定虚拟机启动后对其访问的图形界面接口 TYPE 指定显示类型，可以为vnc，spice、sdl或none port TYPE为vnc或spice时其监听的端口 listen TYPE为vnc或spice时其监听的IP地址，默认为127.0.0.1，可以通过修改/etc/libvirt/qemu.conf定义新的默认值 password TYPE为vnc或spice时为远程访问监听的服务指定认证密码 –noautoconsole 禁止自动连接到虚拟机的控制台 设备选项 选项 子选项 说明 –serial 附加一个串行设备到当前虚拟机，根据设备类型不同，可以使用不同的选项，格式为–serial type,opt1=val1,opt2=val2 pty 创建伪终端 dev,path=HOSTPATH 附加主机设备至此虚拟机 –video 指定显卡设备类型，如cirrus、vga、qxl或vmvga 虚拟化平台 选项 子选项 说明 -v或–hvm 当宿主机同时支持全虚和半虚时，使用此选项指定硬件辅助的全虚 -p或–paravirt 指定使用半虚 –virt-type 指定使用的hypervisor，如kvm、xen、qemu等，其值可以通过virsh capabilities获得 其它 选项 子选项 说明 –autostart 指定虚拟机是否在物理机启动后自动启动 –print-xml 如果虚拟机不需要安装过程(–import、–boot)，则显示生成的XML而不是创建虚拟机。默认情况下，此选项仍会创建虚拟磁盘 –force 禁止进入命令交互模式，如需回答yes或no，默认自动回答yes –dry-run 执行创建虚拟机的整个过程，但不创建虚拟机，改变主机上的设备配置信息及将其创建的需求通知给libvirt； -d或–debug 显示debug信息； 尽管virt-install命令有着类似上述的众多选项，但实际使用中，其必须提供的选项仅包括–name、–ram、–disk（也可是–nodisks）及安装过程相关的选项。此外，有时还需要使用括–connect=CONNCT选项来指定连接至一个非默认的hypervisor。 图形化界面安装这里的使用图形化界面不是通过virt-manager和virt-view工具在图形化服务器或宿主机来安装，而是通过–graphics选项指定vnc或spice方式安装，安装过程中需要宿主机通过vnc client连接图形化接口实现。在我们的KVM实践初步一文中介绍安装ubuntu1204桌面版虚拟机就是采用这种方式。忘了的，可以回顾本站那篇文章。 现在，我们这里演示的是另一种图形化安装方法，还是通过vnc实现，安装介质使用ubuntu18.10桌面版ISO。如下： step1：我们先查看下当前激活的虚拟网络有哪些，方便后续安装虚拟机时配置网络（图中红色框部分）。同时，我们创建一个qcow2格式的虚拟机磁盘镜像，大小为120G（图中黄色框部分）。 step2：通过virt-install工具安装第一个模板虚拟机ubt1810d。如下： 上图中，我们指定了虚拟机当前vcpu为2个，最大可分配4个（黄色框部分），同时指定了虚拟机的镜像磁盘格式为qcow2格式（采用qcow2格式，此项必须明确指定，并且通过size选项指定大小，否则生成的磁盘大小异常，或者在创建镜像磁盘时通过-o选项明确指定也可以），总线类型为virtio类型（图中蓝色框部分） step3：通过vncdisplay命令查询当前虚拟机图形界面接口服务vnc监听的端口号，并通过物理机的vnc client连接开始安装。如下： step4：完成虚拟机系统安装后，我们通过vnc client登录虚拟机，打开虚拟机的console控制接口，便于后续通过宿主机console连接虚拟机控制台。如下： 以上，就是通过图形界面接口实现kvm虚拟机的安装过程。一般通过图形界面接口安装，主要用于安装桌面版的操作系统，比如上面的ubuntu desktop版本，windows的各种版本等。由于ubuntu系统默认不开启root用户的ssh登录，为了方便后续管理，我们可以在ubuntu系统打开ssh的root登录权限。但是，记得首先要设置root的登录密码。这些操作就可以通过宿主机console控制台来完成。如下： 文本字符界面安装上面的图形化安装方式主要借助vnc或spice客户端来实现图形界面安装接口，主要用于桌面操作系统的安装。在实际运维时，一般用于app的虚拟机大部分采用非桌面系统，而且打开vnc或spice监听端口不便于批量安装。这种情况下，就需要通过文本界面实现自动安装。主要通过-x选项打开一个串行接口终端tty和ttyS0实现。但是，如果使用-x选项安装，安装介质必须使用–location选项，而不能使用–cdrom来完成。此时，有两个选择，一是将iso镜像拷贝到宿主机的某个目录，通过-l（–location）选项指定即可，类似–cdrom方式。另一种是将iso挂载到某个目录，通过http、nfs或ftp方式发布，然后在-l选项指定URL即可。我们这里采用第二种方式的http方式进行安装，如下： 上述安装方式，通过-l选项指定安装介质的挂载目录，通过-x选项安装使用kiskstart文件，以及打开串行终端console，通过–nographics选项指定不采用图形界面方式安装。命令执行后，安装界面如下，且通过kickstart文件自动化安装。 以上这种安装方式，也是实际运维中常用的安装方式，同时可以在kickstart文件中指定初始化脚本，这样在系统安装完成后自动完成初始化配置。后续，就可以将此虚拟机作为模板镜像，批量分发。 但是，如果要将当前虚拟机作为模板镜像，需要删除当前虚拟机中的MAC和UUID，同时清空本地规则和系统规则中网卡配置文件，这样后续通过该虚拟机的模板镜像生成的新的虚拟机就会自动生成新的MAC和网卡名称，而不用通过在XML配置文件中手动指定MAC，实现自动化运维。上面的要求，需要在虚拟机中完成以下配置，然后关机作为后续分发虚拟机的模板。 虚机模板方式安装 模板方式安装一般有两种方式：一种是通过–import选项导入镜像虚拟机的磁盘来生成一个新的虚拟机，这种方式只能生成一个新虚拟机，当要生成第二个虚拟机时会提示磁盘被占用错误。另一种方式就是利用模板镜像虚拟机的XML文件，生成新的虚拟机的XML配置文件，需要删除源模板镜像虚拟机XML文件中的uuid、mac address，并修改磁盘路径为新虚拟机的磁盘（差分盘或新磁盘都可），而且需要修改XML文件中的虚拟机name为新建虚拟机name。完成后，再通过virsh define预定义一个新虚拟机，然后通过virsh start启动即可。下面，我们就对这两种方式逐一进行验证。 1）通过–import选项导入现有虚拟机磁盘生成一个新的虚拟机 完成上面的操作后，我们就能立即生成一个新的虚拟机cirros01。然后，我们可以通过virsh console命令登录并执行相关操作。如下： 此时，我们如果需要再通过上面镜像盘生成一个虚拟机时，会提示镜像盘被占用的错误。如下： 因此，这种方式只能生成一个新虚拟机，一般用于很小的初始化配置测试。所以，在实际运维中，主要通过第二种方式利用模板镜像虚拟机来批量创建虚拟。 2）通过模板镜像虚机利用virsh-define和virsh-start实现新虚机创建 首先，我们利用模板镜像虚机磁盘创建一个差分磁盘，用于新虚机的系统盘（也可以创建非差分盘，看实际业务需求而定）。如下，通过-b选项指定backing file为模板镜像虚机的磁盘。 然后，利用模板镜像虚机的xml文件生成新虚机的xml配置文件。需要删除uuid，mac address配置，修改磁盘存储位置为新创建的虚机差分盘，修改name为新虚机的name。如下： 然后，通过virsh-define定义新虚机，再通过virsh-start启动新虚机，然后通过console控制台进入虚机。如下： 如下图，可以发现系统为我们自动生成mac和ip，而且mac必然是52:54:00开头。 而且，新生成的虚拟机具备访问外网和宿主机的权利。如下： 批量创建KVM虚拟机的脚本掌握上面各种创建KVM虚拟机的方法后，我们可以写一个shell脚本来完成批量创建虚拟机的任务。 首先，我们需要准备创建虚拟机的镜像磁盘，主要有两种方法：一种是拷贝模板虚拟机的镜像磁盘，这种主要用于创建独立虚拟机，优点是不依赖于模板虚拟机磁盘镜像，可以完整保留数据。缺点就是占用存储空间较大。另一种方式就是我们上面的通过建立差分磁盘实现，优点是存储空间占用较小，缺点就是模板镜像磁盘一旦损坏将造成所有依赖虚拟机无法启动。 完成虚拟磁盘创建后，我们就可以按照上面办法通过模板镜像虚机的XML配置文件批量创建新的虚拟机XML配置文件，并修改里面的NAME、UUID、MAC ADDRESS和DISK PATH等配置项。最终，通过define和start命令完成新虚拟机的定义和启动。 通过上面的分析，要实现KVM虚拟机批量创建shell脚本，只需要通过两个for循环就能完成。同时，在程序开始要检查程序脚本的执行权限，从管理角度来说非root用户不能执行。还要给脚本进行传参，用来设定需要批量拉起的KVM虚拟机的上限值和下限值，也就是确定批量拉起的虚机个数。那么，传递的参数就必须是2个，不能多也不能少，且两个参数都必须是数字，参数1的值要小于参数2。 以上，就是我们程序要实现的逻辑，可以采用函数式编程的概念封装成三个函数，实现简单模块化设计。为了讲述清楚，我这里就不封装了。接下来，我们就来一步步写这个脚本。 Step1：通过第一个for循环批量创建虚拟机的虚拟磁盘，这里我们采用差分磁盘的方式。如下 脚本执行后效果如下： Step2：通过第二个for循环批量创建虚拟机的XML配置文件，并定义和启动虚拟机。如下： 完成上面的替换，在脚本执行virsh defiine和virsh start命令定义新的虚拟机并完成启动。脚本执行后效果如下： 至此，KVM虚拟机的各种创建方法介绍完毕，网上还有一种通过qemu-kvm命令创建虚拟机的方法，与通过virt-install方法创建大同小异，且红帽官方并不建议这种方法。因此，只要掌握virt-install方法创建虚拟机即可。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-KVM管理工具libvirt]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-KVM%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7libvirt%2F</url>
    <content type="text"><![CDATA[通过前面两篇文章的介绍，相信大家对KVM虚拟机的原理、软件架构、运行机制和部署方式等有所了解。但是，那些东西只能算是非常基础入门的东西。尤其我们介绍的部署KVM虚拟机时用到的virt-install命令，不仅参数很多，很难记忆，而且要想用好virt-系列工具首先需要对libvirt工具有更深刻的了解。因为，virt-系列工具其实是对libvirt工具的封装调用，而libvirt工具又是对底层qemu工具的封装调用，其目的都是为了使命令更加友好与用户交互。本篇文章就详细介绍这几种与libvirt相关的管理工具，为后续各种配置打下良好基础。 libvirt管理工具提到KVM的管理工具，就不得不提大名鼎鼎的libvirt，因为libvirt是目前使用最为广泛的对KVM虚拟机进行管理的工具和应用程序接口，而且一些常用的虚拟机管理工具（如virsh、virt-install、virt-manager等）和云计算框架平台（如OpenStack、ZStack、OpenNebula、Eucalyptus等）都在底层使用libvirt的应用程序接口，也就是说libvirt实际上是一个连接底层Hypervisor与上层应用的中间适配层。 如上图，libvirt作为一个中间适配层，可以让底层Hypervisor对上层用户空间的管理工具是完全透明的，也就是说上层管理应用无需关注底层的差别，只需要调用libvirt去完成，因此只需要对接libvirt的对外开放api接口即可。同时，libvirt对底层多种不同的Hypervisor的支持是通过一种基于驱动的架构来实现的，类似neutron中的ml2层。libvirt对不同的Hypervisor提供了不同的驱动。比如：对Xen有xen_dirver.c驱动文件，对vmware有vmware_dirver.c驱动文件，对kvm有qemu_driver.c驱动文件等等。。。 在libvirt中涉及节点Node、Hypervisor、域Domain等几个概念，其逻辑关系如下： 节点（Node）是一个物理机器，上面可能运行着多个虚拟客户机。Hypervisor和Domain都运行在节点上。 Hypervisor也称虚拟机监控器（VMM），如KVM、Xen、VMware、Hyper-V等，是虚拟化中的一个底层软件层，它可以虚拟化一个节点让其运行多个虚拟机（不同虚拟机可能有不同的配置和操作系统）。 域（Domain）是在Hypervisor上运行的一个虚拟机操作系统实例。域也被称为实例（instance，如在亚马逊的AWS云计算服务中客户机就被称为实例）、客户机操作系统（guest OS）、虚拟机，它们都是指同一个概念。 libvirt相关的配置文件都在/etc/libvirt/目录之中，如下： 主要涉及四个文件： 1）/etc/libvirt/libvirt.conf：用于配置一些常用libvirt连接（通常是远程连接）的别名。比如：uri_aliases = [ “remote1=qemu+ssh://root@192.168.101.252/system” ]，有这个别名后，就可以在用virsh等工具或自己写代码调用libvirt API时使用这个别名，比如在Python可以这样调用： 1conn = libvirt.openReadOnly('remote1') 2）/etc/libvirt/libvirtd.conf：是libvirt的守护进程libvirtd的配置文件，被修改后需要让libvirtd重新加载配置文件（或重启libvirtd）才会生效。在文件的每一行中使用“配置项=值”（如tcp_port=”16509”）这样key-value格式来设置。我们上篇文章介绍的说监听tcp链接和端口，就是在这里配置。 3）/etc/libvirt/qemu.conf：是libvirt对QEMU的驱动的配置文件，包括VNC、SPICE等，以及连接它们时采用的权限认证方式的配置，也包括内存大页、SELinux、Cgroups等相关配置。 4）/etc/libvirt/qemu/目录：存放的是使用QEMU驱动的域的配置文件，也就是kvm虚拟机的配置文件以及networks目录存放默认网络配置文件。如下： 要让某个节点能够利用libvirt进行管理（无论是本地还是远程管理），都需要在这个节点上运行libvirtd这个守护进程，以便让其他上层管理工具可以连接到该节点，libvirtd负责执行其他管理工具发送给它的虚拟化管理操作指令，比如：virsh、virt-manager、nova-compute等等。在CentOS 7.x系统中，libvirtd作为一个系统服务存在，可以利用systemctl工具进行启动、停止、重启、重载等操作。另外，libvirtd守护进程的启动或停止，并不会直接影响正在运行中的客户机。libvirtd在启动或重启完成时，只要客户机的XML配置文件是存在的，libvirtd会自动加载这些客户的配置，获取它们的信息。 libvirtd除了是系统服务外，本身还是一个可执行程序，可以通过一些选项完成系统进程的启动。比如：-d选项可以让libvirtd作为守护进程（daemon）在后台运行。-f选项可以指定libvirtd的配置文件启动服务，而不使用默认的配置文件。-l选项开启配置文件中指定的TCP/IP连接，监听TCP socket和服务端口。-p指定进程PID，不使用系统默认分配的PID号等等。 解读虚拟机的XML配置文件在使用libvirt对虚拟化系统进行管理时，很多地方都是以XML文件作为配置文件的，包括虚拟机的配置、宿主机网络接口配置、网络过滤、各个客户机的磁盘存储配置、磁盘加密、宿主机和虚拟机的CPU特性等等。所以，对XML文件进行解析，对后续使用libvirtd管理KVM虚拟机理解的更为深刻。 我们首先来看下，我们前一篇文章创建的虚拟机ubt1204d的XML配置文件如下： 123456789101112131415161718192021222324252627282930313233343536[root@c7-test01 ~]# cat /etc/libvirt/qemu/ubt1204d.xml &lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BEOVERWRITTEN AND LOST. Changes to this xml configuration should be made using: virsh edit ubt1204dor other application using the libvirt API.--&gt;&lt;domain type='kvm'&gt; &lt;name&gt;ubt1204d&lt;/name&gt; &lt;uuid&gt;4c4797b9-776c-44e4-8ef5-3ad445092d0f&lt;/uuid&gt; &lt;memory unit='KiB'&gt;2097152&lt;/memory&gt; &lt;currentMemory unit='KiB'&gt;2097152&lt;/currentMemory&gt; &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt; &lt;os&gt; &lt;type arch='x86_64' machine='pc-i440fx-rhel7.0.0'&gt;hvm&lt;/type&gt; &lt;boot dev='hd'/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;/features&gt; &lt;cpu mode='custom' match='exact' check='partial'&gt; &lt;model fallback='allow'&gt;Broadwell-IBRS&lt;/model&gt; &lt;/cpu&gt; &lt;clock offset='utc'&gt; &lt;timer name='rtc' tickpolicy='catchup'/&gt; &lt;timer name='pit' tickpolicy='delay'/&gt; &lt;timer name='hpet' present='no'/&gt; &lt;/clock&gt; &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt; &lt;on_reboot&gt;restart&lt;/on_reboot&gt; &lt;on_crash&gt;destroy&lt;/on_crash&gt; &lt;pm&gt; 。。。。省略若干行。。。&lt;/domain&gt; 如上，可以看到在该虚拟机的XML文件中，所有有效配置都在和标签之间，这表明该配置文件是一个域的配置。而XML文档中的注释是在两个特殊的标签之间，如&lt;！–注释–&gt;。 通过libvirt启动客户机，经过文件解析和命令参数的转换，最终也会调用qemu命令行工具来实际完成客户机的创建。用这个XML配置文件启动的客户机，它的qemu命令行参数是非常详细、非常冗长的一行，如下： 下面，我们就逐个模块来解析虚拟机的XML配置文件。 CPU的配置 上面的XML文件中关于CPU的配置项如上图所示，vcpu标签，表示客户机中vCPU的个数，这里为2。features标签，表示Hypervisor为客户机打开或关闭CPU或其他硬件的特性，这里打开了ACPI、APIC等特性。而cpu标签中定义了CPU的基础特性，在创建虚拟机时，libvirt自动检测硬件平台，默认使用Broadwell类型的CPU分配给虚拟机。在CPU模型中看见的Broadwell-IBRS，可以在/usr/share/libvirt/cpu_map.xml文件查看其具体信息，如下： 对于CPU模型的配置，有以下3种模式。 1）custom模式：就是这里示例中表示的，基于某个基础的CPU模型，再做个性化的设置。 2）host-model模式：根据物理CPU的特性，选择一个与之最接近的标准CPU型号，如果没有指定CPU模式，默认也是使用这种模式。xml配置文件为：。 3）host-passthrough模式：直接将物理CPU特性暴露给虚拟机使用，在虚拟机上看到的完全就是物理CPU的型号。xml配置文件为：。 对vCPU的分配，可以有更细粒度的配置，如下： 12345&lt;domain&gt; ... &lt;vcpu placement='static' cpuset="1-4,^3,6" current="1"&gt;2&lt;/vcpu&gt; ...&lt;/domain&gt; cpuset表示允许到哪些物理CPU上执行，这里表示客户机的两个vCPU被允许调度到1、2、4、6号物理CPU上执行（^3表示排除3号）；而current表示启动客户机时只给1个vCPU，最多可以增加到使用2个vCPU。除了这种方式外，libvirt还提供cputune标签来对CPU的分配进行更多调节，如下： 12345678910111213141516&lt;domain&gt; ... &lt;cputune&gt; &lt;vcpupin vcpu="0" cpuset="1"/&gt; &lt;vcpupin vcpu="1" cpuset="2,3"/&gt; &lt;vcpupin vcpu="2" cpuset="4"/&gt; &lt;vcpupin vcpu="3" cpuset="5"/&gt; &lt;emulatorpin cpuset="1-3"/&gt; &lt;shares&gt;2048&lt;/shares&gt; &lt;period&gt;1000000&lt;/period&gt; &lt;quota&gt;-1&lt;/quota&gt; &lt;emulator_period&gt;1000000&lt;/emulator_period&gt; &lt;emulator_quota&gt;-1&lt;/emulator_quota&gt; &lt;/cputune&gt; ...&lt;/domain&gt; 还记得我们在介绍DPDK的亲和性技术不？这里就是设置亲和性特性的调优配置，其中vcpupin标签表示将虚拟CPU绑定到某一个或多个物理CPU上，如“&lt;vcpupin vcpu=”2”cpuset=”4”/&gt;”表示客户机2号虚拟CPU被绑定到4号物理CPU上；“”表示将QEMU emulator绑定到1~3号物理CPU上。在不设置任何vcpupin和cpuset的情况下，虚拟机的vCPU可能会被调度到任何一个物理CPU上去运行。而“2048”表示虚拟机占用CPU时间的加权配置，一个配置为2048的域获得的CPU执行时间是配置为1024的域的两倍。如果不设置shares值，就会使用宿主机系统提供的默认值。 除了亲和性绑定外，还有NUMA架构的调优设置，可以配置虚拟机的NUMA拓扑，以及让虚拟机针对宿主机NUMA特性做相应的策略设置等，主要在标签和标签中完成配置。NUMA特性配置需要在真实物理服务器上完成，手头暂时没有对应环境，也就不列举配置项了。但是，能够对NUMA进行配置的前提是了解NUMA的原理，所以理解前面DPDK技术和计算虚拟化中NUMA技术原理是重点。 内存的配置 在KVM虚拟机的XML配置文件中，内存的大小配置如上，大小为2,097,152KB（即2GB），memory标签中的内存表示虚拟机最大可使用的内存，currentMemory标签中的内存表示启动时分配给虚拟机使用的内存。在使用QEMU/KVM时，一般将二者设置为相同的值。 另外，还记得我们讲内存虚拟化时，提到的内存气球技术不？在KVM虚拟机创建时，默认采用内存气球技术给虚拟机提供虚拟内存，它的配置项在标签对中的memballoon子标签中，如下： 如上图，该配置将为虚拟机分配一个使用virtio-balloon驱动的内存气球设备，以便实现虚拟机内存的ballooning调节。该设备在客户机中的PCI设备编号为0000:00:06.0，也就是设备的I/O空间地址。 虚拟机启动项配置 如上图，这样的配置表示客户机类型是hvm类型，HVM（hardware virtual machine，硬件虚拟机）原本是Xen虚拟化中的概念，它表示在硬件辅助虚拟化技术（Intel VT或AMD-V等）的支持下不需要修改客户机操作系统就可以启动客户机。因为KVM一定要依赖于硬件虚拟化技术的支持，所以在KVM中，客户机类型应该总是hvm，操作系统的架构是x86_64，机器类型是pc-i440fx-rhel7.0.0，这个机器类型是libvirt中针对RHEL 7系统的默认类型，也可以根据需要修改为其他类型。 boot选项用于设置虚拟机启动时的设备，这里只有hd（即硬盘）一种，表示从虚拟机硬盘启动，如果有两种及以上，就与物理机中BIOS设置一样，按照从上到下的顺序先后启动。 网络的配置 如上图，上面虚拟机XML文件的网络时配置是一种NAT方式的网络配置，这里type=’network’和就是表示使用NAT的方式，并使用默认的网络配置，虚拟机将会分到192.168.122.0/24网段中的一个IP地址，如下： 使用NAT网络配置的前提是宿主机中要开启DHCP和DNS服务，一般libvirtd进程默认使用dnsmasq同时提供DHCP和DNS服务，在宿主机中通过以下命令可以查看： 这里的NAT网络的配置在/etc/libvirt/qemu/networks/defaule.xml中配置，如下： 如上，这里配置就是提供分布式虚拟交换机virbr0，用于KVM虚拟机的虚拟接入。详细配置在前一篇文章有介绍，这里不再赘述。由于使用的Linux bridge作为虚拟交换机，因此可以在宿主机中查看该网桥的实际端口，指令如下： 上图中的vnet0就是KVM虚拟机ubt1204d的虚拟网卡，这样就实现了虚拟机ubt1204d与宿主机之间的网络互通。 除了NAT网络模式外，还可以通过自建一个Linux bridge的网桥，在创建虚拟机时，–network选项中指定自建的网桥，这样虚拟机安装完毕后，就会自动分配自建的网桥IP地址段中的一个地址，这种方式称为桥接模式。同理，这种方式也需要开启DHCP和DNS服务。通过自建网桥方式，在虚拟机XML文件的配置字段如下： 123456&lt;interface type='bridge'&gt; &lt;mac address='52:54:00:e9:e0:3b'/&gt; &lt;source bridge='br0'/&gt; &lt;model type='virtio'/&gt; &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt; &lt;/interface&gt; 上述配置表示，是用bridge为虚拟机提供网络服务，mac address指的是虚拟机的MAC地址，表示使用宿主机中的br0网络接口来建立网桥，这个网络接口需要在宿主机中创建配置文件，类似普通网卡的配置，并制定一个物理网卡与该网桥进行绑定，作为上联接口。表示在虚拟机中使用virtio-net驱动的网卡设备，也配置了该网卡在虚拟机中的PCI设备编号为0000:00:03.0。 除了上述两种常见的配置外，还有一种用户模式网络的配置，类似Virtual Box中的内部网络模式。如下： 其在虚拟机的XML字段描述如下： 123&lt;interface type='user'&gt; &lt;mac address="00:11:22:33:44:55"/&gt; &lt;/interface&gt; 如上，type=’user’表示该客户机的网络接口是用户模式网络，是完全由QEMU软件模拟的一个网络协议栈，也就是一个纯用户态内部虚拟交换机。因为没有虚拟接口与宿主机互通，所以宿主机无法与这样的虚拟机通信，这种网络模式下只有同一个内部虚拟交换机上不同虚拟机才能互通。 还记得我们在介绍I/O虚拟化时，讲过网卡的硬件直通技术VMDq和SR-IOV吗？在KVM虚拟机中也可以绑定这类硬件直通网卡，在其XML文件中目前有两种方式：新方式通过标签来绑定，在其中指定驱动的名称为vfio，并指定硬件的I/O地址即可，但是这种新方式目前只支持SR-IOV方式。由于我们条件限制，特意找了个网上的配置给大家曹侃，示例如下： 1234567&lt;interface type='hostdev'&gt; &lt;driver name='vfio'/&gt; &lt;source&gt; &lt;address type='pci' domain='0x0000' bus='0x08' slot='0x10' function= '0x0'/&gt; &lt;/source&gt; &lt;mac address='52:54:00:6d:90:02'&gt; &lt;/interface&gt; 如上，用指定使用哪一种分配方式（默认是VFIO，如果使用较旧的传统的device assignment方式，这个值可配为’kvm’），用标签来指示将宿主机中的哪个VF分配给宿主机使用，还可使用来指定在客户机中看到的该网卡设备的MAC地址。 由于新方式支持的直通网卡类型较少，为了支持更多的硬件直通设备，一般还是采用老方式通过标签来绑定。这种方式不仅支持有SR-IOV功能的高级网卡的VF的直接分配，也支持无SR-IOV功能的普通PCI或PCI-e网卡的直接分配。但是，这种方式并不支持对直接分配的网卡在客户机中的MAC地址的设置，在客户机中网卡的MAC地址与宿主机中看到的完全相同。同样，我们在网上找了个配置示例给大家参考： 12345&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt; &lt;source&gt; &lt;address domain='0x0000' bus='0x08' slot='0x00' function='0x0'/&gt; &lt;/source&gt;&lt;/hostdev&gt; 如上，表示将宿主机中的PCI 0000:08:00.0设备直接分配给客户机使用。 存储的配置 如上图，每个存储设备都由一对标签来描述，上述配置我们当前的虚拟机有两个存储设备，一个虚拟机硬盘，另一个是虚拟机光驱。 在虚拟机硬盘和光驱设备描述中，类型均是file，表示虚拟机硬盘使用文件方式。除此之外，还有block、dir或network取值，分别表示块设备、目录或网络文件系统作为虚拟机磁盘的来源。后面device属性表示让虚拟机如何来使用该磁盘设备，其取值为floppy、disk、cdrom或lun中的一个，分别表示软盘、硬盘、光盘和LUN（逻辑存储单元），默认值为disk（硬盘）。 子标签用于定义Hypervisor如何为该磁盘提供驱动，它的name属性用于指定宿主机中使用的后端驱动名称，QEMU/KVM仅支持name=’qemu’，但是它支持的类型type可以是多种，包括raw、qcow2、qed、bochs等。除了这两个属性外，还有cache属性（我们没有设置），它表示在宿主机中打开该磁盘时使用的缓存方式，可以配置为default、none、writethrough、writeback、directsync和unsafe，其具体含义详见本站DPDK系列文章。 子标签表示磁盘的位置，当标签的type属性为file时，应该配置为这样的模式，而当type属性为block时，应该配置为这样的模式。 子标签表示将磁盘暴露给虚拟机时的总线类型和设备名称。dev属性表示在客户机中该磁盘设备的逻辑设备名称，而bus属性表示该磁盘设备被模拟挂载的总线类型，bus属性的值可以为ide、scsi、virtio、xen、usb、sata等。如果省略了bus属性，libvirt则会根据dev属性中的名称来“推测”bus属性的值，比如，sda会被推测是scsi，而vda被推测是virtio。 子标签表示该磁盘设备在虚拟机中的I/O总线地址，这个标签在前面网络配置中也是多次出现的，如果该标签不存在，libvirt会自动分配一个地址。 域的配置我们前面介绍过，在libvirtd进程中，域、实例和Gust OS是一个概念，就是我们常说的虚拟机。所以，在KVM虚拟机的XML配置文件中，标签是范围最大、最基本的标签，是其他所有标签的根标签。如下： 在标签中可以配置两个属性：一个是type，用于表示Hypervisor的类型，可选的值为xen、kvm、qemu、lxc、kqemu、VMware中的一个；另一个是id，其值是一个数字，用于在该宿主机的libvirt中唯一标识一个运行着的客户机，如果不设置id属性，libvirt会按顺序分配一个最小的可用ID。这个系统自动的分配的ID可以通过如下指令查看： 如上图，我们知道系统给我们当前ubt1204d虚拟机分配的最小可用ID是2。 域的元数据配置 在域的XML文件中，有一部分是用于配置域的元数据，即meta data。元数据的概念我们不陌生，在讲解存储虚拟化时介绍过，它主要用来描述数据的属性。在KVM虚拟机中域的元数据就表示域的属性，主要用于区别其他的域。其中，name用于表示该虚拟机的名称，uuid是唯一标识该虚拟机的UUID。在同一个宿主机上，各个虚拟机的名称和UUID都必须是唯一的。除此之外，还有其他的配置，在后续讲解操作实例时遇到了我们还会介绍，这里不再列举。 QEMU模拟器配置 在KVM虚拟机的XML配置文件中，需要制定使用的设备模型的模拟器。如上图，在emulator标签中配置模拟器的绝对路径为/usr/libexec/qemu-kvm。如果我们自己下载最新的QEMU源码编译了一个QEMU模拟器，要使用的话，需要将这里修改为/usr/local/bin/qemu-system-x86_64。不过，创建虚拟机时，可能会遇到error: internal error Process exited while reading console log output错误，这是因为自己编译的QEMU模拟器不支持配置文件中的pc-i440fx-rhel7.0.0机器类型，因此，需要同步修改如下选项： 1&lt;type arch='x86_64' machine='pc'&gt;hvm&lt;/type&gt; 图形显示方式配置 如上图，表示通过VNC的方式连接到客户机，其VNC端口为libvirt自动分配，也就是用VNC客户端模拟虚拟机的显示器。除此之外，也支持其他多种类型的图形显示方式，以下示例代码就表示就配置了SDL、VNC、RDP、SPICE等多种客户机显示方式。 123456789&lt;graphics type='sdl' display=':0.0'/&gt; &lt;graphics type='vnc' port='5904'&gt; &lt;listen type='address' address='1.2.3.4'/&gt; &lt;/graphics&gt; &lt;graphics type='rdp' autoport='yes' multiUser='yes' /&gt; &lt;graphics type='desktop' fullscreen='yes'/&gt; &lt;graphics type='spice'&gt; &lt;listen type='network' network='rednet'/&gt; &lt;/graphics&gt; 虚拟机的声卡和显卡配置 如上图，标签表示的是显卡配置，其中子标签表示为虚拟机模拟的显卡的类型，它的类型（type）属性可以为vga、cirrus、vmvga、xen、vbox、qxl中的一个，vram属性表示虚拟显卡的显存容量（单位为KB），heads属性表示显示屏幕的序号。在我们的虚拟机中，显卡的配置为cirrus类型、显存为16384（即16MB）、使用在第1号屏幕上。 由于我们的虚拟机中没有配置声卡，也就没有标签，即使有也非常简单，只需要了解他的model属性即可，也就是声卡的类型，常用的选项有es1370、sb16、ac97和ich6。 串口和控制台配置 如上图，设置了虚拟机的编号为0的串口（即/dev/ttyS0），使用宿主机中的伪终端（pty），由于这里没有指定使用宿主机中的哪个伪终端，因此libvirt会自己选择一个空闲的伪终端（可能为/dev/pts/下的任意一个），如下伪终端均为字符型设备： 除了系统默认指定外，也可以加上配置来明确指定使用宿主机中的哪一个虚拟终端。在部署安装虚拟机virt-install命令增加–extra-args ‘console=ttyS0,115200n8 serial’来指定。 通常情况下，控制台（console）配置在客户机中的类型为’serial’，此时，如果没有配置串口（serial），则会将控制台的配置复制到串口配置中，如果已经配置了串口（我们前面安装的虚拟机ubt1204d就是如此），则libvirt会忽略控制台的配置项。同时，为了让控制台有输出信息并且能够与虚拟机交互，也需在虚拟机中配置将信息输出到串口。比如，在Linux虚拟机内核的启动行中添加“console=ttyS0”这样的配置。 输入设备配置 如上图，这里的配置会让QEMU模拟PS2接口的鼠标和键盘，还提供了tablet这种类型的设备，即模拟USB总线类型的键盘和鼠标，并能让光标可以在客户机获取绝对位置定位。 PCI控制器配置 如上图，libvirt会根据虚拟机的不同架构，默认会为虚拟机模拟一些必要的PCI控制器，这类PCI控制器不需要在XML文件中指定，而上图中的需要显式指定都是特殊的PCI控制器。这里显式指定了4个USB控制器、1个pci-root和1个idel控制器。libvirt默认还会为虚拟机分配一些必要的PCI设备，如PCI主桥（Host bridge）、ISA桥等。使用上面的XML配置文件启动虚拟机，在客户机中查看到的PCI信息如下： 使用libvirt API进行虚拟化管理要使用libvirt API进行虚拟化管理，就必须先建立到Hypervisor的连接，有了连接才能管理节点、Hypervisor、域、网络等虚拟化要素。对于libvirt连接，可以简单理解为C/S架构模式，服务器端运行Hypervisor，客户端通过各种协议去连接服务器端的Hypervisor，然后进行相应的虚拟化管理。前面提到过，要实现这种连接，前提条件是libvirtd这个守护进行必须处于运行状态。但是，这里面有个例外，那就是VMware ESX/ESXi就不需要在服务器端运行libvirtd，依然可以通过libvirt客户端以另外的方式连接到VMware。 为了区分不同的连接，libvirt使用了在互联网应用中广泛使用的URI（Uniform Resource Identifier，统一资源标识符）来标识到某个Hypervisor的连接。libvirt中连接的标识符URI，其本地URI和远程URI是有一些区别的，具体如下： 1）在libvirt的客户端使用本地的URI连接本系统范围内的Hypervisor，本地URI的一般格式如下： 1driver[+transport]:///[path][?extral-param] 其中，driver是连接Hypervisor的驱动名称（如qemu、xen、xbox、lxc等），transport是选择该连接所使用的传输方式，可以为空；path是连接到服务器端上的某个路径，？extral-param是可以额外添加的一些参数（如Unix domain sockect的路径）。 2）libvirt可以使用远程URI来建立到网络上的Hypervisor的连接。远程URI和本地URI是类似的，只是会增加用户名、主机名（或IP地址）和连接端口来连接到远程的节点。远程URI的一般格式如下： 1driver[+transport]://[user@][host][:port]/[path][?extral-param] 其中，transport表示传输方式，其取值可以是ssh、tcp、libssh2等；user表示连接远程主机使用的用户名，host表示远程主机的主机名或IP地址，port表示连接远程主机的端口。其余参数的意义与本地URI中介绍的完全一样。 无论本地URI还是远程URI，在libvirt中KVM使用QEMU驱动，而QEMU驱动是一个多实例的驱动，它提供了一个root用户的实例system和一个普通用户的实例session，来设定客户端连接到服务器端后，操作权限的范围，类似Linux系统中的root用户与普通用户的区别。使用root用户实例连接的客户端，拥有最大权限，可以查询和控制整个节点范围虚拟机以及相关设备等系统资源；使用普通用户实例连接的客户端，只拥有服务器端对应用户的操作权限。那么，我们通过这一点也就知道libvirtd也是具备分权分域虚拟化管理能力的。 有了上面的知识储备，我们就可以使用URI建立到Hypervisor的连接，比如，我们在252机器上，通过SSH方式连接掉251机器上，查看251机器上的KVM虚拟机信息，如下： 也可以在251机器上创建本地URI连接，进行管理，大家参照自行练习，我就懒得截图了，谁让我佛系嘛。。。 我们前面提到过，virsh等工具都是对libvirt的封装和调用，所以我们上面的指令看似简单，但是对应libvirt底层就不是那么回事了。在libvirt的底层是由virConnectOpen函数来建立到Hypervisor的连接的，这个函数需要一个URI作为参数，当传递给virConnectOpen的URI为空值（NULL）时，libvirt会依次根据如下3条规则去决定使用哪一个URI。 1）试图使用LIBVIRT_DEFAULT_URI这个环境变量。 2）试用使用客户端的libvirt配置文件中的uri_default参数的值。 3）依次尝试用每个Hypervisor的驱动去建立连接，直到能正常建立连接后即停止尝试。 如果这3条规则都不能够让客户端libvirt建立到Hypervisor的连接，就会报出建立连接失败的错误信息（“failed to connect to the hypervisor”）。 除了针对QEMU、Xen、LXC等真实Hypervisor的驱动之外，libvirt自身还提供了一个名叫“test”的傀儡Hypervisor及其驱动程序。test Hypervisor是在libvirt中仅仅用于测试和命令学习的目的，因为在本地的和远程的Hypervisor都连接不上（或无权限连接）时，test这个Hypervisor却一直都会处于可用状态。使用virsh连接到test Hypervisor的示例操作如下： 如上图，输入help后，就会出现各种命令行指令，新手可以那这个test虚拟机来进行指令练习和学习，不会影响正常的虚拟机，类似Linux中namespace对所有资源拷贝一份然后与真实资源隔离。 下面，我们通过Python来调用libvirt API查询虚拟机信息，在使用Python调用libvirt API之前，需要确定系统是否安装了libvirt-Python，如下： 上图结果表示系统已经安装libvirt-Python。否则，需要手动安装或自行编译安装相应基础包。 完成上面的确认后，我们写一个Python小程序脚本GetinfoDm.py，通过调用libvirt的Python API来查询虚拟机的一些信息。代码如下： 上面的脚本只是简单地调用libvirt Python API获取一些信息，需要注意的是“import libvirt”语句引入了libvirt.py这个API文件，然后才能够使用libvirt.openReadOnly、conn.lookupByName等libvirt中的方法。在本示例中，引入的libvirt.py这个API文件的绝对路径是/usr/lib64/python2.7/site-packages/libvirt.py，它实际调用的是/usr/lib64/python2.7/site-packages/libvirtmod.so这个共享库文件。运行上面的脚本后，结果如下： 通过上面的示例，我们知道无论是virsh等命令还是Python等高级语言，在对KVM虚拟机进行操作时都是调用libvirt的API库函数来完成，这就说明libvirt API是libvirt管理虚拟机的核心。在libvirt中，外部应用接口API函数大致分为8类，是libvirt实现虚拟化管理的基石。在本文的最后，我们通过一张表对其中最常用的6类API进行总结，方便我们后续编写自动化脚本时查询。 库函数类别 功能说明 常用函数 连接Hypervisor相关API，以virConnect开头的系列函数 只有在与Hypervisor建立连接之后，才能进行虚拟机管理操作，所以连接Hypervisor的API是其他所有API使用的前提条件。与Hypervisor建立的连接为其他API的执行提供了路径，是其他虚拟化管理功能的基础。 virConnetcOpen()：建立一个连接，返回一个virConnectPtr对象。virConnetcReadOnly()：建立一个只读连接，只能使用查询功能。virConnectOpenAuth()：根据认证建立安全连接。virConnectGetCapabilities()：返回对Hypervisor和驱动的功能描述XML字符串。virConnectListDomains()：返回一系列域标识符，只返回活动域信息。 域管理API，以virDomain开头的系列函数 要管理域，首先要获取virDomainPtr这个域对象，然后才能对域进行操作。 virDomainLookupByID(virConnectPtr conn,int id)：根据域的id值到conn连接上去查找相应的域。virDomainLookupByName(virConnectPtr conn,string name)：根据域的名称去conn连接上查找相应的域。virDomainLookupByUUID(virConnectPtr conn,string uuid)：根据域的UUID去conn连接上查找相应的域。virDomainGetHostname()：获取相应域的主机名。virDomainGetinfo()：获取相应域的信息。virDomainGetVcpus()：获取相应域vcpu信息。virDomainCreate()：创建域。virDomainSuspend()：挂起域。virDomainResume()：恢复域等等。。。 节点管理API，以virNode开头的系列函数 节点管理的多数函数都需要使用一个连接Hypervisor的对象作为其中的一个传入参数，以便可以查询或修改该连接上的节点信息。 virNodeGetInfo()：获取节点的物理硬件信息。virNodeGetCPUStats()：获取节点上各个CPU的使用统计信息virNodeGetMemoryStats()：获取节点上内存使用统计信息virNodeGetFreeMemory()：获取接上空闲内存信息virNodeSetMemoryParameters()：设置节点上内存调度参数virNodeSuspendForDurarion()：控制节点主机运行 网络管理API，以virNetwork开头的系列函数和部分virInterface开头系列函数 libvirt首先需要创建virNetworkPtr对象，然后才能查询或控制虚拟网络。 virNetworkGetName()：获取网络名称。virNetworkGetBridgeName()：获取网桥名称。virNetworkGetUUID()：获取网络UUID标识。virNetWorkGetXMLDesc()：获取网络以XML格式描述信息。virNetworkIsActive()：查询网络是否在用。virNetworkCreateXML()：根据XML格式创建一个网络。virNetworkDestroy()：注销一个网络。virNetworkUpdate()：更新一个网络。virInterfaceCreate()：创建一个网络端口。。。。等等 存储卷管理API，以virStorageVol开头的系列函数 libvirt对存储卷的管理，首先需要创建virStorageVolPtr这个存储卷对象，然后才能对其进行查询或控制操作。 virStorageVolLookupByKey()：根据全局唯一键值获取一个存储卷对象。virStorageVolLookupByName()：根据名称在一个存储资源池获取一个存储卷对象。virStorageVolLookupByPath()：根据节点上路径获取一个存储卷对象。virStorageVolGetInfo()：查询某个存储卷的使用信息。virStorageVolGetName()：获取存储卷的名称。。。。等等 存储池管理API，以virStoragePool开头的系列函数 libvirt对存储池（pool）的管理包括对本地的基本文件系统、普通网络共享文件系统、iSCSI共享文件系统、LVM分区等的管理。libvirt需要基于virStoragePoolPtr这个存储池对象才能进行查询和控制操作。 virStoragePoolLookupByName()：可以根据存储池的名称来获取一个存储池对象。virStoragePoolLookupByVolume()：可以根据一个存储卷返回其对应的存储池对象。virStoragePoolCreateXML()：可以根据XML描述来创建一个存储池（默认已激活）virStoragePoolDefineXML()：可以根据XML描述信息静态地定义一个存储池（尚未激活）virStoragePoolCreate()：可以激活一个存储池。virStoragePoolIsActive()：可以查询存储池状态是否处于使用中。。。。等等]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020-03-01-KVM实践初步]]></title>
    <url>%2F2020%2F03%2F01%2F2020-03-01-KVM%E5%AE%9E%E8%B7%B5%E5%88%9D%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[在x86-64架构的处理器中，KVM需要的硬件辅助虚拟化分别为Intel的虚拟化技术（Intel VT）和AMD的AMD-V技术。在前面我们介绍过，CPU不仅要在硬件上支持VT技术，还要在BIOS中将其功能打开，KVM才能使用到。目前，大多数服务器和PC的BIOS都默认已经打开VT。至于如何在VMware Workstations虚拟机和物理服务器上打开，详见本站《KVM到底是个啥？》一文。 部署和安装KVMKVM的部署和安装主要有两种方式：一种源码编译安装，另一种就是通过CentOS的YUM工具安装。作为学习实验途径，我们这里主要介绍YUM工具安装方式。至于源码编译安装一般属于生产研发人员的操作，这里我们只给一些关键提示，有兴趣的同学可自行研究。 1）源码编译安装方式。KVM作为Linux内核的一个module，是从Linux内核版本2.6.20开始的，所以要编译KVM，你的Linux操作系统内核必须在2.6.20版本以上，也就是CentOS 6.x和CentOS 7.x都天生支持，但是CentOS 5.x以下版本需要先升级内核。 下载KVM源代码，主要通过以下三种方式： 下载KVM项目开发中的代码仓库kvm.git。 下载Linux内核的代码仓库linux.git。 使用git clone命令从github托管网站上下载KVM的源代码。 根据上面三种途径下载源代码后就可以通过make install的方式编译安装了，编译安装完成后还需要根据KVM官方指导手册进行相关的配置。。。我们这里不展开，大家可自行搜索源码方式安装。需要注意一点儿的是，除了下载KVM的源码外，还需要同时下载QEMU的源码进行编译安装，因为它俩是一辈子的好基友嘛。。。 2）YUM工具安装。首先，需要查看 CPU是否支持VT技术，就可以判断是否支持KVM。然后，再进行相关工具包的安装。最后，加载模块，启动libvirtd守护进程即可。具体步骤如下： Step1：确认服务器是否支持硬件虚拟化技术，如果有返回结果，也就是结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的，否则就是不支持。如下图所示，表示服务器支持Intel的VT-x技术，且有4个CPU。 Step2：确认系统关闭SELinux。如下图所示，表示系统已经关闭Linux。 如果没有关闭，可以使用如下命令永久关闭： 1234[root@C7-Server01 ~]# setenforce 0 # 临时关闭# 永久关闭[root@C7-Server01 ~]# sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config[root@C7-Server01 ~]# sed -i 's/^SELINUXTYPE=.*/SELINUXTYPE=disabled/g' /etc/selinux/config Step3：安装rpm软件包。 由于KVM是嵌入到Linux内核中的一个模块，我们这里首先安装用户安装用户空间QEMU与内核空间KMV交互的软件协议栈QEMU-KVM，代码如下： 1[root@C7-Server01 ~]# yum -y install qemu-kvm 如上图，不仅完成qemu-kvm（红色框部分）安装，还同步安装qemu-img、qemu-kvm-comm两个依赖包的安装（黄色框部分）。注意：由于我已经安装过，上面提示是updated，没有安装的话，应该是installed。 完成QEMU-KVM软件协议栈的安装后，我们安装libvirt、virt-等管理工具，代码如下： 1[root@c7-test01 ~]# yum install -y libvirt* virt-* 如上图，将KVM的管理工具libvirt系列软件包和virt-系列软件包（红色框部分）及相关依赖软件包（黄色框部分）进行了安装。 最后，需要安装一个二层分布式虚拟交换机，用于KVM虚拟机的虚拟接入，可以选择OVS，也可以选择Linux bridge或是其他商用的分布式交换机如VMware、华为FC等。我们这里只出于学习的目的，选择安装Linux bridge即可。代码如下： 1[root@c7-test01 ~]# yum install -y bridge-utils 如上图，提示我们系统已经安装该软件包且是最新版本（红色框部分），如果你没有安装过，系统会直接安装。 3）加载KVM模块，使得Linux Kernel变成一个Hypervisor。首先，加载KVM内核模块，然后查看KVM内核模块的加载情况即可。具体步骤如下： 首先，通过modprobe指令加载kvm内核模块，如下： 1[root@c7-test01 ~]# modprobe kvm 然后，通过lsmod指令查看kvm模块的加载情况，如下： 1[root@c7-test01 ~]# lsmod | grep kvm 4）启动libvirtd守护进程，并设置开机启动。因为libvirt管理工具，是用于KVM虚机全生命周期的统一管理工具，它由外部统一API接口、守护进程libvirtd和CLI工具virsh三部分组成，其中守护进程libvirtd用于所有虚机的全面管理。同时，其他管理工具virt-*、openstack等都是调用libvirt的外部统一API接口完成KVM的虚机的管理。所以，我们需要启动libvirtd守护进程，并设置开机启动。代码如下： 1[root@c7-test01 ~]# systemctl enable libvirtd &amp;&amp; systemctl start libvirtd &amp;&amp; systemctl status libvirtd 这里注意一下，一般情况我们单机玩KVM虚拟机时，libvirtd守护进程默认监听的是UNIX domain socket套接字，并没有监听TCP socket套接字。为了同时让libvirtd监听TCP socket套接字，需要修改/etc/libvirt/libvirtd.conf文件中，将tls和tcp，以及tcp监听端口前面的注释取消。然后重新通过libvirtd的原生命令加载配置文件，使其生效。而系统默认的systemctl命令由于不支持–listen选项，所以不能使用。参考代码如下： 123[root@c7-test01 ~]# systemctl stop libvirtd [root@c7-test01 ~]# libvirtd -d --listen 通过netstat命令查看libvirtd监听的tcp端口为16509，如下： 最后，进行tcp链接验证，如果连接成功，表示服务启动正常且tcp监听正常。如下： 安装第一个KVM虚拟机安装虚拟机之前，我们需要创建一个镜像文件或者磁盘分区等，来存储虚拟机中的系统和文件。首先，我们利用qemu-img工具创建一个镜像文件。这个工具不仅能创建虚拟磁盘，还能用于后续虚拟机镜像管理。比如，我们要创建一个raw格式的磁盘，具体代码如下： 123[root@c7-test01 ~]# qemu-img create -f raw ubuntu1204.img 20G Formatting 'ubuntu1204.img', fmt=raw size=21474836480 如上，表示在当前目录下(/root）创建了一个20G大小的，raw格式的虚拟磁盘，名称为：ubuntu1204.img。虽然，我们看它的大小时20G，实际上并不占用任何存储空间，如下： 这是因为qemu-img聪明地为你按实际需求分配文件的实际大小，它将随着image实际的使用而增大。如果想一开始就分配实际大小为20G的空间，不仅要使用raw格式磁盘，还需加上-o preallocation=full参数选项，这样创建速度会很慢，但是创建的磁盘实实在在占用20G空间。我们这里用创建一个5G磁盘来演示，如下： 除raw格式以外，qemu-img还支持创建其他格式的image文件，比如qcow2，甚至是其他虚拟机用到的文件格式，比如VMware的vmdk、vdi、Hyper-v的vhd等，不同的文件格式会有不同的“-o”选项。为了演示我们的第一个虚拟机，我们现在创建一个qcow2格式的虚拟磁盘用作虚拟机的系统磁盘，大小规划40G，如下： 上面创建的虚拟磁盘实际就是KVM虚拟机后续的系统盘，在创建完虚拟机磁盘后，我们将要安装的系统镜像盘上传到当前目录下或者通过光驱挂载也可。我们这里为了安装速度问题，采用上传到宿主机本地的方式，如下： 注意：这里的宿主机指的是我们的VMware虚拟机，这里的虚拟机指的是我们在VMware虚拟机中创建的KVM虚拟机，千万别搞混了。 然后，就是我们的关键一步，通过virt-install命令安装虚拟机，代码如下： 1234567891011[root@c7-test01 ~]# virt-install --name ubt1204d \--virt-type kvm \--ram 2048 --vcpus=2 \--disk path=/root/ubt1204d.img,format=qcow2,size=40 \--network network=default,model=virtio \--graphics vnc,listen=192.168.101.251 --noautoconsole \--cdrom /root/ubuntu-12.04.5-desktop-amd64.iso Starting install...Domain installation still in progress. You can reconnect to the console to complete the installation process. 如上，上面的指令表达的意思为：–name指定虚拟机的名称，–virt-type指定虚拟机的类型为kvm，–ram指定给虚拟机分配的虚拟内存大小为2GB，–vcpus指定给虚拟机分配的虚拟cpu为2个，–disk指定虚拟机的系统盘，就是我们刚才创建的虚拟磁盘，–network指定虚拟机使用的虚拟交换机，这里使用系统的默认配置，默认配置文件为/etc/libvirt/qemu/networks/default.xml，其详细信息如下： 上图中网络模式采用nat方式就表示虚拟机可以通过网桥访问internet或者外部网络。–graphics指定虚拟机的模拟显示器界面，这里采用vnc方式，并监听宿主机地址192.168.101.251。–cdrom选项指定虚拟机的安装镜像配置，也就是系统安装盘iso的位置。 上面虚拟机正式启动后，可以通过本地机器的vnc客户端连接到KVM虚拟机，作为虚拟机的模拟显示器。为此，首先需要查看VNC连接的端口，代码如下： 如上图黄色框部分，通过本地机器VNC客户端连接目标机器192.168.101.251的0端口就能模拟虚拟机的显示器，进一步完成图形化安装配置，如下： 连接成功后，与真实物理机装系统的操作一致，可以使用键盘、鼠标完成各类安装配置操作。虚拟机模拟显示器的界面如下： 由于我们安装的是ubuntu12.04的桌面版系统，所以完成虚拟机的安装后，后续每次使用虚拟机都需要使用VNC客户端进行连接操作。如下： 同时，我们可以通过virsh命令工具查看本机上所有虚拟机的状态，如下： 最后，提醒一下：我们在自动化运维中介绍过通过kicstart或cobbler批量安装宿主机操作系统，这种方式也是可以在KVM虚拟机安装系统中使用，通过在virt-install命令增加–extra-args选项就可实现。但是，一般我们不这么玩，在云计算和虚拟化场景下，均是通过手动安装一台模板虚拟机，将其系统盘转换为模板镜像格式文件，然后通过批量分发虚拟机的方式（就是我们在存储虚拟化中讲的链接克隆和快照方式）完成虚拟机批量部署操作，电信云中通过VNFD描述的多台虚拟机资源部署也是同样的方式完成，只不过里面借助了OpenStack的编排引擎。这种方式我们在后续介绍KVM虚拟镜像格式实操一文中会详细介绍。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-27-打造文件实时同步架构之sersync篇]]></title>
    <url>%2F2019%2F06%2F27%2F2019-06-27-%E6%89%93%E9%80%A0%E6%96%87%E4%BB%B6%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E4%B9%8Bsersync%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前文讲述的rsync主要用于较大量数据同步，一般简单的服务器数据传输会使用ftp/sftp等方式，但是这样的方式效率不高，不支持差异化增量同步也不支持实时传输。针对数据实时同步需求大多数人会选择rsync+inotify-tools的解决方案，但是这样的方案也存在一些缺陷，（下文会具体指出），sersync是国人基于前两者开发的工具，不仅保留了优点同时还强化了实时监控，文件过滤，简化配置等功能，帮助用户提高运行效率，节省时间和网络资源。 inotify+rsync模式如果要实现定时同步数据，可以在客户端将rsync加入定时任务，但是定时任务的同步时间粒度并不能达到实时同步的要求。在Linux kernel 2.6.13后提供了inotify文件系统监控机制。通过rsync+inotify组合可以实现实时同步。 inotify实现工具有几款：inotify本身、sersync、lsyncd。其中sersync是金山的周洋开发的工具，克服了inotify的缺陷，且提供了几个插件作为可选工具。此处先介绍inotify的用法以及它的缺陷，通过其缺陷引出sersync，并介绍其用法。 安装inotify-toolsinotify由inotify-tools包提供。在安装inotify-tools之前，请确保内核版本高于2.6.13，且在/proc/sys/fs/inotify目录下有以下三项，这表示系统支持inotify监控，关于这3项的意义，下文会简单解释。 12345[root@c7-test01 ~]# ls -l /proc/sys/fs/inotify/total 0-rw-r--r-- 1 root root 0 Jun 3 18:30 max_queued_events-rw-r--r-- 1 root root 0 Jun 3 18:30 max_user_instances-rw-r--r-- 1 root root 0 Jun 3 18:30 max_user_watches epel源上提供了inotify-tools工具，可以先安装epel-release源，然后通过yum方式安装，也可以通过下载源码编译方式安装。由于我们部署了本地yum源，所以选择第一种方式。 12[root@c7-test01 ~]# yum install -y epel-release[root@c7-test01 ~]# yum install -y inotify-tools inotify-tools工具只提供了两个命令：inotifywait和inotifywatch。如下： 123[root@c7-test01 ~]# rpm -ql inotify-tools/usr/bin/inotifywait/usr/bin/inotifywatch 其中，inotifywait命令用于等待文件发生变化，所以可以实现监控(watch)的功能，该命令是inotify的核心命令。inotifywatch用于收集文件系统的统计数据，例如发生了多少次inotify事件，某文件被访问了多少次等等，一般用不上。inotify相关的内核参数如下： 1）/proc/sys/fs/inotify/max_queued_events：调用inotify_init时分配到inotify instance中可排队的event数的最大值，超出的事件被丢弃，但会触发队列溢出Q_OVERFLOW事件。 2）/proc/sys/fs/inotify/max_user_instances：每一个real user可创建的inotify instances数量的上限。 3）/proc/sys/fs/inotify/max_user_watches：每个inotify实例相关联的watches的上限，即每个inotify实例可监控的最大目录、文件数量。如果监控的文件数目巨大，需要根据情况适当增加此值。 inotify的不足inotify是监控工具，监控目录或文件的变化，然后触发一系列的操作。假如有一台站点发布服务器A，还有3台web服务器B/C/D，目的是让服务器A上存放站点的目录中有文件变化时，自动触发同步将它们推到web服务器上，这样能够让web服务器最快的获取到最新的文件。需要搞清楚的是，监控的是A上的目录，推送到的是B/C/D服务器，所以在站点发布服务器A上装好inotify工具。除此之外，一般还在web服务器BCD上将rsync配置为daemon运行模式，让其在873端口上处于监听状态(并非必须，即使是sersync也非必须如此)。也就是说，对于rsync来说，监控端是rsync的客户端，其他的是rsync的服务端。 以上描述的只是最可能的使用情况，并非一定需要如此。况且，inotify是独立的工具，它和rsync无关，它只是为rsync提供一种比较好的实时同步方式而已。 虽然inotify已经整合到了内核中，在应用层面上也常拿来辅助rsync实现实时同步功能，但是inotify因其设计太过细致从而使得它配合rsync并不完美，所以需要尽可能地改进inotify+rsync脚本或者使用sersync工具。 另外，inotify存在bug—当向监控目录下拷贝复杂层次目录(多层次目录中包含文件)，或者向其中拷贝大量文件时，inotify经常会随机性地遗漏某些文件。这些遗漏掉的文件由于未被监控到，所有监控的后续操作都不会执行，例如不会被rsync同步。实际上，上面描述的问题不是inotify的缺陷，而是inotify-tools包中inotifywait工具的缺陷。 Sersync模式sersync主要用于服务器同步，web镜像等功能。目前使用的比较多的同步解决方案是inotify-tools+rsync ，另外一个是google开源项目Openduckbill（依赖于inotify- tools），这两个都是基于脚本语言编写的。相比较上面两个项目，sersync优点是： 1）sersync是使用c++编写，而且对linux系统文件系统产生的临时文件和重复的文件操作进行过滤，所以在结合rsync同步的时候，节省了运行时耗和网络资源。 2）sersync配置起来很简单，其中bin目录下已经有基本上静态编译的二进制文件，配合bin目录下的xml配置文件直接使用即可。 3）相比较其他脚本开源项目，使用多线程进行同步，尤其在同步较大文件时，能够保证多个服务器实时保持同步状态。 4）有出错处理机制，通过失败队列对出错的文件重新同步，如果仍旧失败，则按设定时长对同步失败的文件重新同步。 5）自带crontab功能，只需在xml配置文件中开启，即可按您的要求，隔一段时间整体同步一次。无需再额外配置crontab功能。 6）具备socket与http插件扩展，满足您二次开发的需要。 sersync架构图如下所示，其架构说明如下： 1 ) 线程组线程是等待线程队列的守护线程，当事件队列中有事件产生的时候，线程组守护线程就会逐个唤醒同步线程。当队列中 Inotify 事件较多的时候，同步线程就会被全部唤醒一起工作。这样设计的目的是为了能够同时处理多个 Inotify 事件，从而提升服务器的并发同步能力。同步线程的最佳数量=核数 x 2 + 2。 2 ) 之所以称之为线程组线程，是因为每个线程在工作的时候，会根据服务器上新写入文件的数量去建立子线程，子线程可以保证所有的文件与各个服务器同时同步。当要同步的文件较大的时候，这样的设计可以保证每个远程服务器都可以同时获得需要同步的文件。 3 ) 服务线程的作用有三个： 处理同步失败的文件，将这些文件再次同步，对于再次同步失败的文件会生成 rsync_fail_log.sh 脚本，记录失败的事件。 每隔10个小时执行 rsync_fail_log.sh 脚本一次，同时清空脚本。 crontab功能，可以每隔一定时间，将所有路径整体同步一次。 4 ) 过滤队列的建立是为了过滤短时间内产生的重复的inotify信息，例如：在删除文件夹的时候，inotify就会同时产生删除文件夹里的文件与删除文件夹的事件，通过过滤队列，当删除文件夹事件产生的时候，会将之前加入队列的删除文件的事件全部过滤掉，这样只产生一条删除文件夹的事件，从而减轻了同步的负担。同时对于修改文件的操作的时候，会产生临时文件的重复操作。 Sersync安装安装规划如下： 服务器A（主服务器）：192.168.101.11 服务器B（从服务器/备份服务器）:192.168.101.251 rsync默认TCP端口为873 Step1：服务器B上安装rsync 1[root@c7-test01 ~]# yum install -y rsync Step2：配置/etc/rsyncd.conf 123456789101112131415161718192021222324252627[root@c7-test01 ~]# cat /etc/rsyncd.conf #服务器B上的rsyncd.conf文件内容uid=rootgid=root##最大连接数max connections=36000##默认为true，修改为no，增加对目录文件软连接的备份 use chroot=no##定义日志存放位置log file=/var/log/rsyncd.log##忽略无关错误ignore errors = yes##设置rsync服务端文件为读写权限read only = no ##认证的用户名与系统帐户无关在认证文件做配置，如果没有这行则表明是匿名auth users = rsync##密码认证文件，格式(虚拟用户名:密码）secrets file = /etc/rsync.pass##这里是认证的模块名，在client端需要指定，可以设置多个模块和路径[rsync]##自定义注释comment = rsync##同步到B服务器的文件存放的路径path=/app/data/site/[img]comment = imgpath=/app/data/site/img Step3：创建rsync认证文件，可以设置多个，每行一个用户名:密码，注意中间以“:”分割 1[root@c7-test01 ~]# echo "rsync:rsync" &gt; /etc/rsync.pass Step4：设置文件所有者和读写权限 12[root@c7-test01 ~]# chmod 600 /etc/rsyncd.conf [root@c7-test01 ~]# chmod 600 /etc/rsync.pass Step5：启动服务器B上rsync服务，并查询监听端口 123456789[root@c7-test01 ~]# systemctl enable rsyncd &amp;&amp; systemctl start rsyncdCreated symlink from /etc/systemd/system/multi-user.target.wants/rsyncd.service to /usr/lib/systemd/system/rsyncd.service.[root@c7-test01 ~]# netstat -an | grep 873tcp 0 0 0.0.0.0:873 0.0.0.0:* LISTEN tcp6 0 0 :::873 :::* LISTEN [root@c7-test01 ~]# lsof -i tcp:873COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMErsync 1960 root 4u IPv4 25279 0t0 TCP *:rsync (LISTEN)rsync 1960 root 5u IPv6 25280 0t0 TCP *:rsync (LISTEN) 以下安装步骤是在服务器A上执行！！！ Step6：在服务器A上安装rsync服务 1[root@c7-server01 ~]# yum install -y rsync Step7：安装inotify-tools工具 1[root@c7-server01 ~]# yum install -y inotify-tools Step8：安装sersync软件包 1234567wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/sersync/sersync2.5.4_64bit_binary_stable_final.tar.gz[root@c7-server01 local]# tar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz GNU-Linux-x86/GNU-Linux-x86/sersync2GNU-Linux-x86/confxml.xml[root@c7-server01 local]# mv GNU-Linux-x86 sersync[root@c7-server01 local]# cd sersync/ Step9：配置密码文件并修改权限为600。这个密码用于访问服务器B，需要的密码和上面服务器B的密码必须一致： 12[root@c7-server01 sersync]# echo "rsync" &gt; /app/local/sersync/user.pass[root@c7-server01 sersync]# chmod 600 /app/local/sersync/user.pass Step10：配置confxml.xml文件 Step11：在服务器A上运行sersync服务 123456[root@c7-server01 sersync]# cd /app/local/sersync/[root@c7-server01 sersync]# ./sersync2 -r -d-d:启用守护进程模式-r:在监控前，将监控目录与远程主机用rsync命令推送一遍-n: 指定开启守护线程的数量，默认为10个-o:指定配置文件，默认使用confxml.xml文件 效果验证1234#在服务器A（192.168.101.11）的/tmp目录创建10个文件，比如：tets01..test10[root@c7-server01 sersync]# touch /tmp/test&#123;01..10&#125;# 在服务器B（192.168.101.251）的/app/data/site/目录下查看效果 最后，在服务器A上设置sersync开机启动，整体文件实时同步的架构部署完毕。 1echo "/app/local/sersync/sersync2 -r -d -o /opt/sersync/confxml.xml &gt;/app/local/sersync/rsync.log 2&gt;&amp;1 &amp;" &gt;&gt; /etc/rc.local]]></content>
      <categories>
        <category>Linux常用运维工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-27-基础正则的五朵金花]]></title>
    <url>%2F2019%2F06%2F27%2F2019-06-27-%E5%9F%BA%E7%A1%80%E6%AD%A3%E5%88%99%E7%9A%84%E4%BA%94%E6%9C%B5%E9%87%91%E8%8A%B1%2F</url>
    <content type="text"><![CDATA[在前一篇《正则表达式基础入门》中，我们总结了跟“位置匹配”的相关的正则，它其实也是基础正则五朵金花之一，我们只是在入门介绍中单独拿出来进行科普。这篇文章我们就来掰扯掰扯基础正则的其它四朵金花。 基础正则的连续次数匹配连续次数匹配是什么意思？空口白话不容易说清楚，我们还是举个例子来说明。为了说明连续次数匹配的，我们首先得重新创建一个测试文件，如下： 现在，我们想查找测试文件中哪些行包含的连续的两个字母a。聪明如你，肯定想到了如下的查找办法： 如上图，我们通过匹配“aa”就能将包含连续两个a的行搜索出来（注意：上面的条件是指至少包含两个连续的a）。那么，我们现在需要找出连续10个a的行呢？简单。。。查找“aaaaaaaaaa”不就行了。但是，如果要找包含连续100个a的行呢？上面的笨办法显然不科学，这时候就要用了基础正则中连续次数匹配条件—”{n}“了，里面n代表具体的数字，也就是连续多少个的意思。上面的示例，我们使用基础正则中的连续匹配条件可以改写成下面的格式： 如上图，结果符合我们的预期。“{2}”表示连续出现2次的意思，a\{2\}就表示连续出现两次a的意思。现在，你肯定能举一反三。比如：如果我们要匹配连续100个a，可以写成如下匹配条件“a\{100\}”，也就是我们只需要修改其中的数字n即可。 上面的用法刚才也提到了，它实际的意思的是：“包含至少2个连续a的行”。如果，我们只想找到“只包含连续两个a的行”时怎么办？这时候，就要用到我们上一篇提到的“位置规则”匹配条件。我们可以配合使用单词界定符”\&lt;,\&gt;”或者”\b”，具体命令如下： 如上图，我们单词界定符“\b”（前后的\b表示单词界定符，中间的字母b表示要查找的内容，后面花括号中的数字2表示要匹配的次数），将测试文件test01中的“只包含连续两个b的行找了出来”。那么，我们现在再继续延伸一下，首先来验证下d\{2,3\}的匹配条件表示什么意思，如下： 上图中的匹配条件将“包含至少2个连续的d，至多3个连续的d的行”查找了出来。也就是说“\{x,y\}”这类匹配条件，表示的是“至少连续出现x次，至多连续出现y次”的意思。举一反三，“\{x ,\}”表示“至少连续出现x次，至多不限次数”的意思，”\{,y\}”表示“至少出现0次，至多连续出现y次”的意思。 我们现在用中学数学的方法小结一下：\{x,y\}表示“至少连续出现x次，至多连续出现y次”。如果y=0，表示“至少连续出现x次，至多次数不限”，且“\{x\}”和“\{x,\}”的意思是一样的。如果x=0，表示“至多出现y次，至少出现0次”的意思。 现在，我们再来认识另一个匹配次数的正则符号“*”。其实，在通配符中也有符号“*”，表示匹配任意长度的任意字符。但是，在正则表达式中，符号“*”表示之前的字符连续出现任意次的意思。注意：千万不要和通配符中的“*”混淆了。具体使用方法如下： 如上图，表示“搜索字母a前面出现任意个d的行”，任意个d当然也包含0个d。那么，如何在正则表达式中查找任意长度的任意字符呢？我们可以使用“.*”来实现，如下： 上图中，表示匹配字母a后面接任意多个任意字符的行。其实，在正则表达式中“.”符号表示匹配任意单个字符。而在通配符中“?”表示单个字符，这一定同样要注意不要混淆。如下： 上图中，匹配条件“aa.”表示aa后面跟任意单个字符的行都会被匹配到，”aa..”表示aa后面跟任意两个字符的行都会被匹配到（空格也算字符）。理解了上述规则，回过头来我们再看“.”就非常容易理解了，**”.\“就表示任意单个字符连续出现多次的意思。** 接下来，我们再认识两个新的匹配条件：“\?”和“\+”。“\?”表示匹配其前面的字符出现0次或1次，也就说要么没有，要么最多出现1次。“\+”表示匹配前面的字符出现至少1次，也就是说前面的字符必须至少出现1次。我们再通过示例验证下： 如上图，“a\?”表示a这个字符出现0次或1次的行，不仅匹配到了空行，也将包含连续多个a的也匹配到了。我们再看下面的例子： 上图中，将a字符至少出现1次的行都匹配输出，那么空行也就自然不输出了。 到此，基础正则连续次数匹配规则就全部介绍完了，国际惯例，我们还是总结一下，便于后续用到了查询： *符号：表示前面的字符连续出现任意次，包括0次。 .符号：表示单个任意字符。 .*组合：表示任意长度的任意字符。 \?组合：表示匹配前面的的字符0次或1次。 \+组合：表示匹配其前面的字符至少1次，或连续多次，连续次数上不封顶。 \{n\}符号：表示前面的字符连续出现n次，将会被匹配到 \{x,y\}组合：表示前面的字符至少连续出现x次，最多连续出现y次，都能被匹配到，换句话说就是之前的字符连续出现的次数在x和y之间，即可被匹配。 \{,n\}组合：表示之前的字符连续出现至多n次，最少0次，即可被匹配。 \{n,\}组合：表示之前的字符连续出现至少n次，才会被匹配到。 基础正则中的常用符号之前介绍的使用正则表达式去“匹配连续次数”中，我们使用过特殊符号“.”和”*”，分别用于匹配单个任意字符和连续出现任意次数。如果我们需要更精确的匹配呢？比如我们想从测试文本中找出a字母后接任意3个的字母的字串所在行，我们可以使用如下规则： 如上图，“[[:alpha:]]”就是我们现在需要认识的新符号。在正则表达式中，“[[:alpha:]]”表示“任意字母（不区分大小写）”。“[[:alpha:]]”这个符号看上去有些复杂，吐啊。。。吐啊。。。习惯就好了。其实，这个符号可以拆分成两部分去理解，所以也没那么可怕。“[[:alpha:]]”的第一部分是最外层的[ ]，表示指定范围内的任意单个字符，第二部分是最内层的[:alpha:]，表示不区分大小写的任意字母。所以，当两部分合起来就是表示任意单个字母（不区分大小写）。 上面的[[:alpha:]]表示不区分大小写的任意字母，如果我们现在需要区分大小写呢？比如，上面的示例我们再细化一下，需要匹配a后面跟随的3个字母必须是小写字母，可以使用如下规则： 上图中，[[:lower:]]就是我们现在认识的第二个符号，它在正则表达式中表示任意单个小写字母。同样的，[[:upper:]]就表示任意单个大写字母。 聪明如你，一定发现了一些规律，那就是替换“[[: :]]”中的单词，就可表示不同的含义。在基础正则中，主要包含如下特殊符号： [[:alpha:]]：表示任意1个不区分大小写的字母。 [[:lower:]]：表示任意1个小写字母。 [[:upper:]]：表示任意1个大写字母。 [[:digit:]]：表示0-9之间任意1个数字，包括0和9。 [[:alnum:]]：表示任意1个数字或字母。 [[:space:]]：表示任意1个空白字符，包括“空格”、“tab键”等。 [[:punct:]]：表示任意一个标点符号。 除了上面的特殊符号表示法外，还有其他办法实现 上述的匹配效果。还记得我们在正则表达式基础入门一文中举得一个[a-z]*的例子不？示例中，[a-z]其实就表示任意一个小写字母，效果与[[:lower:]]一样。同理，[A-Z]就与[[:upper:]]是等价的，表示任意一个大写字母；[0-9]与[[:digit:]]是等价的，表示任意一个0-9的数字。如下所示： 那么，有了上面的基础，“[a-z][A-Z][0-9]”就表示任意一个不区分大小写的字母或0-9的数字，等价于[[:alnum:]]。 上述两种表示的特殊符号使用没有强制性，看个人喜好而定。这里还需要强调下[ ]的作用，它表示匹配指定范围内的任意单个字符。还是先看下面的示例： 如上图，字母a后面接一个方括号[]，里面有bcd三个字母。既然方括号[ ]表示匹配任意范围内的单字符，那么上面的匹配条件就是“匹配字母a后面紧跟b或c或d三个字母一次的行”。我们活学活用，[Bd#3]表示什么意思呢？它就表示字符是大写B、或者是小写d、或者是特殊符号#、再或者是数字3各一次的场景，都可以被匹配到。如下： 上图中，表示匹配包含om或oe或oo或ob字符串的行。 现在，我们理解了方括号[ ]的含义，那么我们再来认识一下它的性格迥异的亲兄弟[^ ]，它表示匹配指定范围外的任意一个字符，与方括号[ ]的含义正好相反。还是看下面示例： 如上图，与前面示例的效果正好相反，将测试文件中不包含om，或不包含oe，或不包含oo，或不包含ob字符的行匹配输出。同理，[^0-9]就表示匹配单个非数字字符，[^a-z]就表示匹配单个非小写字母等等，这里就不再举例了，很好理解。这里有个疑问，比如前面说了[0-9]与[[:digit:]]是等价的，那么[^0-9]与[^[:digit:]]是否等价？试试就知道了，如下： 针对这些常用符号，我们总结出一张表如下： 符号 等价于 含义 [a-z] [[:lower:]] 任意一个小写字母 [^a-z] [^[:lower:]] 任意一个非小写字母 [A-Z] [[:upper:]] 任意一个大写字母 [^A-Z] [^[:upper:]] 任意一个非大写字母 [0-9] [[:digit:]] 任意一个0-9的数字 [^0-9] [^[:digit:]] 任意一个非0-9的数字 [a-zA-Z] [[:alpha:]] 任意一个不区分大小写的字母 [^a-zA-Z] [^[:alpha:]] 任意一个非字母的字符 [a-zA-Z0-9] [[:alnum:]] 任意一个数字或字母 [^a-zA-Z0-9] [^[:alnum:]] 任意一个非数字或字母的字符，比如符号 其实，不仅[0-9]、[[:digit:]]可以匹配数字，还有一种简写的格式符号也能表示数字，比如”\d”就是表示十进制数字0-9。但是，并不是所有的正则表达式解释器都能够识别这些简写格式。因此，建议大家还是采用上述非简写格式来编写规则。 基础正则中的分组与后向引用在正则表达式中除了使用“位置匹配”、“连续次数匹配”和“常用符号”外，还可以使用分组和后向引用。那么，什么是分组？我们还是通过示例来说明，如下先创建一个测试文件test02： 我们先使用如下规则进行匹配： 如上图，后面的匹配次数2只影响前面单字母b，所以上例中kkutysllbb、kkutysllbbb都会被匹配到。如果，我们想将上面示例中匹配要求改成连续匹配两次kkutysllb怎么办？这个时候，我们就需要使用分组的匹配的规则，将kkutysllb当做一个整体group来匹配。命令规则如下： 上图中，\( \)就表示分组，它将其中的内容看做一个整体。分组还可以嵌套，什么意思呢？接着往下看： 如上图，为了说明分组嵌套，我们将上例中存在重复字母的地方做了单字母分组，实际环境时没必要这样分组。在上图中，我们将字母k和字母l做了两个子分组，且指定每个子分组各连续出现两次。同时，将kkutysllb做了一个大分组，包括上述的两个子分组，且指定整个大分组也连续出现两次。 我想我把分组的概念应该是说清楚了，接下来我们再看看什么是后向引用？之所以先讲分组，后介绍后向引用，是因为后向引用是以分组为前提的。还是以示例切入介绍，我们再创建一个测试文件test03，如下： 现在，我们通过正则表达式将上述文件中的各行进行匹配，如下： H.\{4\}表示大写字母H的后面跟随了4个任意字符，其中”.”表示任意单个字符，前面已经说过了。通过上面的匹配规则，Hello和Hilll两个单词都会被匹配到，也就是上述测试文件三行都符合规则。我们现在有了新需求，需要找出kkutysllb单词前后两个词一致的行，通过上面的规则明显无法满足需求，这时候就需要用到后向引用了。如下： 上图中，后面那个\1就表示后向引用，它的意思是kkutysllb后面的单词必须与前面\(H.\{4\}\)完全一致才符合规则。这究竟是什么道理呢？其实，\1的意思是等价整个正则中第1个分组的正则匹配到的结果。大白话就是，上例中整个正则只有一个分组\(H.\{4\}\)，当它与测试文件的第一行文本匹配时，会匹配到Hello，这时\1也为Hello。当它与测试文件第二行文本匹配时，会匹配到Hilll，这时\1也为HIlll。换句话说，\1引用整个正则中第1个分组的正则，同理，\2就引用了整个正则中第2个分组的正则，以此类推。。。如果你还是不明白，那就看下图理解吧，如果仍然不明白，那我也没招了。。。 以上就是后向引用，再次强调，使用后向引用的前提是将需要引用的部分进行分组。 在前述内容的例子中，我们使用了分组嵌套，那么在对分组嵌套后向引用时，到底哪个是分组1，哪个是分组n呢？我们来看下图： 在上图中，红色部分是一对分组，蓝色部分是另一对分组，当我们需要后向引用时，外层分组也就是红色分组是第1个分组，内层分组也就是蓝色分组是第2个分组。这是因为分组的顺序取决于分组符号的左侧部分的顺序，由于红色分组的左侧部分排在最前面，所以红色分组是第1个。 至此，分组和后向引用我们掰扯完了，我们还是保持国际惯例，对分组和后向引用做个总结，便于我们后续查询，如下： \( \)：表示分组，我们可以将其中的内容当做一个整体，分组是可以嵌套的。 \(ab\)：表示将ab当做一个整体去处理。 \1：表示引用整个正则表达式中第1个分组中的正则匹配到的结果。 \2：表示引用整个正则表达式中第2个分组中的正则匹配到的结果。 基础正则中的转义符现在，我们来认识下正则中的常用符合，就是反斜杠”\“。反斜杠有什么用？我们还是先不解释，通过示例来描述。如下，我们还是创建的一个测试文件test04，内容如下： 1234567[root@c7-test01 ~]# cat test04basea1#$ddda-!@cccca.. 如上，此时我们想匹配a..这行，利用我们前面了解的知识，规则可能会这样写。匹配条件如下： 上图中，虽然匹配结果包含了我们的需求，但是也多了其他三行，这让我们很不爽。主要是因为点符号”.”在基础正则中表示任意1个字符。因此，我们如果想要在匹配条件中匹配点点，就需要使用转移符号反斜杠“\”。转义符号“\”与正则中的符号结合起来就表示这个符号本身。因此，上面的需求我们可以使用如下的匹配条件： 如上图，\.表示单个点符号，同理\*就表示单个星符号。在前面提到过，在基本正则中，\?表示其前面的字符出现0次或1次，\+表示前面的字符出现至少1次。那么，如果我们相匹配问号?和加号+呢？难道我们还要再在前面加一个反斜杠\\？或\\+，答案是否定的。在Linux中，如果想要在正则表达式中匹配问号?或加号+，只需要在匹配条件中直接输入?或+即可，大家可以自己尝试。但是，在某些时候，我们需要匹配反斜杠自身，这时需要在反斜杠前面再加上反斜杠就行。 下面我们写个经常会使用的正则表达式—-匹配ifconfig输出中的各个网卡的ip地址。如下： 此时，我们需要首先对ipv4的地址做个分析，首先它是由4组三位数组成，且每组数字不大于255，前3组三位数后面都带一个点号”.”，那么我们的正则规则可以如下来写： 上图中，([0-2]{,1})([0-9]{1,2}).表示至少为1位数字，最多为3位数字，且首位数字不大于2，后面以点号“.”结尾的字段，然后将这部分作为一个分组连续匹配3次，最后这一段([0-2]{,1})([0-9]{1,2})也表示至少为1位数字，最多为3位数字，且首位数字不大于2。注意：上面的ip地址匹配并不精确，我们其实并没有限制每组数字范围在1-254。不过，没关系，当我们了解了扩展正则就能写一个更精确的ip地址匹配规则。 至此，基础的正则的五朵金花也就介绍完了。在正则入门时我们说过，Linux中正则表达式分为基础正则表达式与扩展正则表达式，在下一篇文章中我们会介绍扩展正则，其实它们的用法都是相似的，而且写法也差不多，基础正则理解并掌握了，扩展正则几乎不费力。]]></content>
      <categories>
        <category>shell编程</category>
      </categories>
      <tags>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-24-Linux原生网络虚拟化实践]]></title>
    <url>%2F2019%2F06%2F24%2F2019-06-24-Linux%E5%8E%9F%E7%94%9F%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[本篇是虚拟化技术系列最后一片文章（容器技术作为专题单独介绍），介于后续OpenStack的Neutron在利用libvirt构建虚拟化网络服务时，利用了很多Linux虚拟网络功能（Linux内核中的虚拟网络设备以及其他网络功能）。因此，在网络虚拟化的收尾部分，特意安排一篇Linux虚拟网络的基础实践，一方面巩固大家对网络虚拟化的认知，另一方面也为后续学习OpenStack打好基础。下面，就来带大家了解一下Linux系统原生的与Neutron密切相关的虚拟网络设备。 TapTap这个概念大家应该不陌生，Tap交换机总听说过吧，信令平台采集探针数据收敛必备设备，是个纯二层设备。Linux中的tap属于虚拟网络设备，也是个二层虚拟设备。而在linux中所指的“设备”并不是我们实际生产或生活中常见路由器或交换机这类设备，它其实本质上往往是一个数据结构、内核模块或设备驱动这样含义。 在linux中，tap和tun往往是会被并列讨论，tap位于二层，tun位于三层。为了说明这一点我们先看看下面linux用于描述tap和tun的数据结构内容： 123456789Struct tun_strruct &#123;Char name [8]; //设备名称Unsigned long flags； //区分tun和tap的标志位；Struct fasync_struct *fasync; //文件异步调用通知接口Wait_queue_head_t read_wait; //消息队列读等待标志Struct net_device dev; //定义一个抽象网络设备Struct sk_buff_head txq; //定义一个缓存区Struct net_device_stats stats; //定义一个网卡状态信息结构&#125;； 从上面的数据结构可以看出，tap和tun的数据结构其实一样的，用来区别两者的其实就是flags。Tap从功能上说，位于二层也就是数据链路层，而二层的主要网络协议包括： 点对点协议（Point-to-Point Protocol） 以太网协议（Ethernet） 高级数据帧链路协议（High-Level Data Link Protocol） 帧中继（Frame Relay） 异步传输模式（Asynchronous Transfer Mode） 而tap只是与以太网协议（Ethernet）对应，所以tap常被称为“虚拟以太网设备” 。要想使用Linux命令行操作一个tap，首先Linux得有tun模块（Linux使用tun模块实现了tun/tap），检查方法如下所示，输入modinfo tun命令如果有如下输出，就表示Linux内核具备tun模块。 接下来，还要看看Linux内核是否加载tun模块。检查办法就是输入指令lsmod|grep tun，查看是否有如下图所示信息输出。如果有，就表示已加载，否则输入modprobe tun指令进行加载，然后再次输入lsmod|grep tun指令进行确认（这种加载-确认的操作方式是一种好的操作习惯，要养成！！！） 当我们确认了Linux加载了tun模块后，我们还需要确认Linux是否有操作tun/tap的命令行工具tunctl。在Linux命令行输入tunctl help进行确认。 如上图所示，如果提示command not found，在CentOS 6.x系统下直接输入yum install -y tunctl安装即可。但是，在CentOS 7.x系统下，需要先指定一个特定仓库，然后按照指定的仓库进行安装，否则会提示找不到tunctl rpm包。代码如下： 123456789101112# 首先创建一个指定仓库cat &lt;&lt; EOF &gt; /etc/yum.repos.d/nux-misc.repo&gt; [nux-misc]&gt; name=Nux Misc&gt; baseurl=http://li.nux.ro/download/nux/misc/el7/x86_64/&gt; enabled=0&gt; gpgcheck=1&gt; gpgkey=http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro&gt; EOF# 按照指定仓库进行安装[root@C7-Server01 ~]# yum -y --enablerepo=nux-misc install tunctl 安装完后，再次执行tunctl help命令后，有如下信息输出，表示安装成功。 具备了tun和tunctl后，我们就亦可以创建一个tap设备了，创建命令使用-t选项指定创建的tap设备名称。 此时，我们输入ip link list或ifconfig命令可以查看到刚才创建的tap设备llb_tap1。 通过上面的命令输出，我们发现这个tap设备还没有绑定ip，可以通过执行ip addr或ifconfig命令为该tap设备绑定一个ip地址，我们给它规划一个192.168.1.1/24的地址。 12345# 通过ip addr命令绑定ip地址[root@C7-Server01 ~]# ip addr local 192.168.1.1/24 dev llb_tap1# 或者使用ifconfig命令绑定ip地址[root@C7-Server01 ~]# ifconfig llb_tap1 192.168.1.1/24 注意：上述操作方式，个人推荐第一种，毕竟我们要与时俱进。。。 到此为止，一个tap设备就创建完毕了，在OpenStack的Neutron中创建的虚拟网络，虚拟机的vNIC与虚拟交换机之间就是通过一个tap桥接，而这个tap设备就是俗称的端口组，所以也就有了同一个端口组内的port归属同一个network的说法。后续通过测试用例，我们会再次讲述tap的用法。 Namespacenamespace是Linux虚拟网络的一个重要概念。传统的Linux的许多资源是全局的，比如进程ID资源。而namespace的目的首先就是将这些资源进行隔离。容器技术的两大实现基石之一就是namespace，它主要负责容器虚拟化中资源隔离，另一个就是cgroups，主要负责容器虚拟化中资源的管理。 在Linux中，可以在一个Host内创建许多namespace，于是那些原本是Linux全局的资源，就变成了namespace范围内的“全局”资源，而且不同namespace的资源互相不可见、彼此透明。Linux中会被namespace隔离的全局资源包括： uts_ns：UTS是Unix Timesharing System的简称，包含内核名称、版本、底层体系结构等信息。 ipc_ns：所有与进程通信（IPC）有关的信息。 mnt_ns：当前装载的文件系统； pid_ns：有关进程的id信息。 user_ns：资源配额信息。 net_ns：网络信息。 至于为什么这些全局资源会被隔离，是由Linux全局资源的数据结构定义决定的，如下所示，Linux的全局资源数据结构定在在文件nsproxy.h中。 123456789Struct nsproxy &#123;Atomic_t count;Struct uts_namespace *uts_ns;Struct ipc_namespace *ipc_ns;Struct mnt_namespace *mnt_ns;Struct pid_namespace *pid_ns;Struct user_namespace *user_ns;Struct net *net_ns;&#125;; 从资源的隔离的角度来说，Linux namespace的示意图如下所示。 上图表明，每个namespace里面将原本是全局资源的进行了隔离，彼此互相不可见。同时在Linux的Host或者每一个VM中，都各自有一套相关的全局资源。借助虚拟化的概念，我们可以将Linux Host的namespace称为root namespace，其它虚拟机namespace称为guest namespace。 单纯从网络的视角来看，一个namespace提供了一份独立的网络协议栈。包括：网络设备接口、IPv4、IPv6、IP路由、防火墙规则、sockets等。一个Linux Device，无论是虚拟设备还是物理设备，只能位于一个namespace中，不同namespace的设备之间通过veth pair设备进行连接，veth pair设备也是一个虚拟网络设备，可以暂时将其理解为虚拟网线，后面会详细介绍。 Linux中namespace的操作命令是ip netns，这个命令的帮助如下所示： 首先，我们通过ip netns list命令查看一下当前系统的namespace列表信息，由于当前系统没有创建namespace，所以没有任何信息返回。然后，我们通过ip netns add命令添加一个namespace（llb_ns1）。最后，再通过ip netns list命令进行查看，结果如下图所示。 接下来，我们可以通过ip link set名ing把刚才创建的虚拟tap设备迁移到llb_ns1中去。这时候，我们在root namespace下执行ifconfig命令就找不到llb_tap1设备了。 我们可以通过ip netns exec [NAME] CMD的方式操作，namespace中的设备，其中NAME参数为namespace名称，CMD是要执行的指令。 在namespace中给llb_tap1重新绑定ip地址。因为，llb_tap1在root namesapce中绑定的ip归属root namesapce的资源，当llb_tap1更改归属namespace时，原来的资源自然无法使用，必须重新配置。 到此为止，namespace的创建就讲述完了，后续还会根据实例继续讲解其它用法。 Veth pairveth pair不是一个设备，而是一对设备，以连接两个虚拟以太端口，类似网线。操作veth pair，需要跟namespace一起配合，不然就没有意义。其实，veth pair设备本质上有三个接口，一端连接linux内核，另两端连接两个tap设备，是一个Y型结构，实现上更像一个HUB。如下图所示： 两个namespace llb_ns1/llb_ns2中各有一个tap设备，组成veth pair，两者的ip如上图所示，测试两个ip进行互ping。配置代码如下： 123456789101112131415161718# 创建veth pair设备[root@C7-Server01 ~]# ip link add llb_tap1 type veth peer name llb_tap2# 创建两个namespace：llb_ns1，llb_ns2[root@C7-Server01 ~]# ip netns add llb_ns1[root@C7-Server01 ~]# ip netns add llb_ns2# 将veth pair设备两端的两个tap设备移动到对应的namespace中[root@C7-Server01 ~]# ip link set llb_tap1 netns llb_ns1[root@C7-Server01 ~]# ip link set llb_tap2 netns llb_ns2# 设置两个tap设备的ip地址，并启动[root@C7-Server01 ~]# ip netns exec llb_ns1 ifconfig llb_tap1 192.168.1.1/24 up[root@C7-Server01 ~]# ip netns exec llb_ns2 ifconfig llb_tap2 192.168.1.2/24 up# 测试两个tap设备互ping[root@C7-Server01 ~]# ip netns exec llb_ns1 ping -c 5 192.168.1.2[root@C7-Server01 ~]# ip netns exec llb_ns2 ping -c 5 192.168.1.1 通过veth pair我们可以连接两个namespace中的两个tap设备，但是veth pair一端只能连接两个tap，如果是三个或多个namespace内的tap要实现互联怎么办？这时候，就只能使用Linux Bridge来完成。 Linux Bridge/vSwitch在Linux的网络部分，Bridge和Switch是一个概念。所以，大家把Linux Bridge看做一个交换机来理解就行，也就是2层的一个汇聚设备。Linux实现Bridge功能的是brctl模块。在命令行里敲一下brctl，如果能显示相关内容，则表示有此模块，否则还需要安装。安装命令是： 1[root@C7-Server01 ~]# yum install -y bridge-utils Bridge本身的概念，我们通过一个综合测试用例来讲述Bridge的基本用法，同时也涵盖前面所述的几个概念：tap、namesapce、veth pair。实验拓扑图如下： 上图中，有4个namespace，每个namespace都有一个tap与交换机上一个tap口组成veth pair。这样4个namespace就通过veth pair及Bridge互联起来。配置代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 创建veth pair设备[root@C7-Server01 ~]# ip link add llb_tap1 type veth peer name tap1_peer[root@C7-Server01 ~]# ip link add llb_tap2 type veth peer name tap2_peer[root@C7-Server01 ~]# ip link add llb_tap3 type veth peer name tap3_peer[root@C7-Server01 ~]# ip link add llb_tap4 type veth peer name tap4_peer# 创建namespace[root@C7-Server01 ~]# ip netns add llb_ns1[root@C7-Server01 ~]# ip netns add llb_ns2[root@C7-Server01 ~]# ip netns add llb_ns3[root@C7-Server01 ~]# ip netns add llb_ns4# 把tap设备移动到对应namespace中[root@C7-Server01 ~]# ip link set llb_tap1 netns llb_ns1[root@C7-Server01 ~]# ip link set llb_tap2 netns llb_ns2[root@C7-Server01 ~]# ip link set llb_tap3 netns llb_ns3[root@C7-Server01 ~]# ip link set llb_tap4 netns llb_ns4# 创建Bridge[root@C7-Server01 ~]# brctl addbr br1# 把相应的tap设备添加到br1的端口上[root@C7-Server01 ~]# brctl addif br1 tap1_peer[root@C7-Server01 ~]# brctl addif br1 tap2_peer[root@C7-Server01 ~]# brctl addif br1 tap3_peer[root@C7-Server01 ~]# brctl addif br1 tap4_peer# 配置相应tap设备的ip，并启动[root@C7-Server01 ~]# ip netns exec llb_ns1 ifconfig llb_tap1 192.168.1.1/24 up[root@C7-Server01 ~]# ip netns exec llb_ns2 ifconfig llb_tap2 192.168.1.2/24 up[root@C7-Server01 ~]# ip netns exec llb_ns3 ifconfig llb_tap3 192.168.1.3/24 up[root@C7-Server01 ~]# ip netns exec llb_ns4 ifconfig llb_tap4 192.168.1.4/24 up# 将Bridge及所有tap设备状态设置为up[root@C7-Server01 ~]# ip link set br1 up[root@C7-Server01 ~]# ip link set tap1_peer up[root@C7-Server01 ~]# ip link set tap2_peer up[root@C7-Server01 ~]# ip link set tap3_peer up[root@C7-Server01 ~]# ip link set tap4_peer up# 进行互ping测试[root@C7-Server01 ~]# ip netns exec llb_ns1 ping -c 3 192.168.1.4[root@C7-Server01 ~]# ip netns exec llb_ns3 ping -c 3 192.168.1.2[root@C7-Server01 ~]# ip netns exec llb_ns2 ping -c 3 192.168.1.1[root@C7-Server01 ~]# ip netns exec llb_ns4 ping -c 3 192.168.1.2 RouterLinux创建Router不像Bridge一样有一个直接命令，甚至连间接命令都没有。因为它自身就是一个路由器（Router）。不过Linux默认没有打开路由转发功能。可以用less /proc/sys/net/ipv4/ip_forward这个命令验证。这个命令就是查看一下这个文件（/proc/sys/net/ipv4/ip_forward）的内容。该内容是一个数字。如果是“0”，则表示没有打开路由功能。把“0”修改为“1”，就是打开了Linux的路由转发功能，修改命令为echo “1” &gt; /proc/sys/net/ipv4/ip_forward。这种打开方法，在机器重启以后就会失效了。一劳永逸的方法是修改配置文件“/etc/sysctl.conf”，将net.ipv4.ip_forward=0修改为1，保存后退出即可。 下面，我们还是通过一个示例来直观感受一下Route的功能。实验拓扑图如下： 在上图中，llb_ns5/llb_tap5与llb_ns6/llb_tap6不在同一个网段中，中间需要经一个路由器进行转发才能互通。图中的Router是一个示意，其实就是Linux开通了路由转发功能。配置代码如下： 1234567891011121314151617# 创建veth pair设备[root@C7-Server01 ~]# ip link add llb_tap5 type veth peer name tap5_peer[root@C7-Server01 ~]# ip link add llb_tap6 type veth peer name tap6_peer# 创建namespace：llb_ns5、llb_ns6[root@C7-Server01 ~]# ip netns add llb_ns5[root@C7-Server01 ~]# ip netns add llb_ns6# 将tap设备迁移到对应namespace中[root@C7-Server01 ~]# ip link set llb_tap5 netns llb_ns5[root@C7-Server01 ~]# ip link set llb_tap6 netns llb_ns6# 配置tap设备ip地址，并启动[root@C7-Server01 ~]# ip netns exec llb_ns5 ifconfig llb_tap5 192.168.100.5/24 up[root@C7-Server01 ~]# ip netns exec llb_ns6 ifconfig llb_tap6 192.168.200.5/24 up[root@C7-Server01 ~]# ifconfig tap5_peer 192.168.100.1/24 up[root@C7-Server01 ~]# ifconfig tap6_peer 192.168.200.1/24 up 现在，我们先来做个ping测试，提示网络不可达，如下图所示。 我们查看下llb_ns5的路由表信息，如下图所示，llb_ns5并没有到达192.168.200.0/24网段的路由表项，因此需要手工进行添加。 在llb_ns5中添加到192.168.200.0/24网段静态路由信息，同时在llb_ns6中添加到192.168.100.0/24回程路由信息。配置代码如下： 123456789# 在llb_ns5中添加到192.168.200.0/24的静态路由[root@C7-Server01 ~]# ip netns exec llb_ns5 route add -net 192.168.200.0 netmask 255.255.255.0 gw 192.168.100.1# 同理，在llb_ns6中添加到192.168.100.0/24的回程路由[root@C7-Server01 ~]# ip netns exec llb_ns6 route add -net 192.168.100.0 netmask 255.255.255.0 gw 192.168.200.1# 查看llb_ns5、llb_ns6的路由信息[root@C7-Server01 ~]# ip netns exec llb_ns5 route -ne[root@C7-Server01 ~]# ip netns exec llb_ns6 route -ne 再次进行ping尝试，可以ping通，结果如下图所示。 Tun在前面tap的时候就介绍过tun和tap其实同一数据结构，只是通过flags标志位来区分。Tap是二层虚拟以太网设备，那么tun就是三层的点对点的虚拟隧道设备。也就是说Linux原生支持三层隧道技术。至于什么是隧道技术？在上一篇数据中心的网络虚拟化技术有详细介绍，这里不再赘述。 Linux一共原生支持5种三层隧道技术，分别是： ipip：IP in IP，在IPv4报文的基础上再封装一个IPv4报文头，属于IPv4 in IPv4。 GRE：通用路由封装（Generic Routing Encapsulation），定义在任意一种网络层协议上封装任意一个其他网络层协议的协议，属于IPv4/IPv6 over IPv4。 sit：与ipip类似，只不过用IPv4报文头封装一个IPv6报文，属于IPv6 over IPv4。 isatap：站内自动隧道寻址协议，一般用于IPv4网络中的IPv6/IPv4节点间的通信。 vti：全称是Virtual Tunnel Interface，为IPSec隧道提供一个可路由的接口类型。 国际惯例，我们还是通过一个具体的测试实例来理解tun。实验拓扑图如下： 上图的tun1、tun2，如果我们先忽略的话，剩下的就是我们在前面讲述过的内容。测试用例的第一步，就是使图中的tap7与tap8配置能通，借助上面route的配置，这里我们不再重复。当tap7和tap8配通以后，如果我们不把图中的tun1和tun2暂时当做tun设备，而是当做两个“死”设备（比如当做是两个不做任何配置的网卡），那么这个时候tun1和tun2就像两个孤岛，不仅互相不通，而且跟tap7、tap8也没有关系。因此，我们就需要对tun1、tun2做相关配置，以使这两个两个孤岛能够互相通信。我们以ipip tunnel为例进行配置。 首先我们要加载ipip模块，Linux默认是没有加载这个模块的。通过命令行lsmod|grep ip进行查看，如果没有加载，可以通过命令modprobe ipip来加载ipip模块。具体过程参见tap部分，这里不再赘述。加载了ipip模块以后，我们就可以创建tun，并且给tun绑定一个ipip隧道。配置代码如下： 123456789101112131415161718192021# 创建veth pair设备[root@C7-Server01 ~]# ip link add llb_tap7 type veth peer name tap7_peer[root@C7-Server01 ~]# ip link add llb_tap8 type veth peer name tap8_peer# 创建namespace：llb_ns7，llb_ns8[root@C7-Server01 ~]# ip netns add llb_ns7[root@C7-Server01 ~]# ip netns add llb_ns8# 将tap设备移动到对应的namespace[root@C7-Server01 ~]# ip link set llb_tap7 netns llb_ns7[root@C7-Server01 ~]# ip link set llb_tap8 netns llb_ns8# 配置ip地址，并启动[root@C7-Server01 ~]# ip netns exec llb_ns7 ifconfig llb_tap7 192.168.100.6/24 up[root@C7-Server01 ~]# ip netns exec llb_ns8 ifconfig llb_tap8 192.168.200.6/24 up[root@C7-Server01 ~]# ifconfig tap7_peer 192.168.100.1/24 up[root@C7-Server01 ~]# ifconfig tap8_peer 192.168.200.1/24 up# 配置路由和回程路由[root@C7-Server01 ~]# ip netns exec llb_ns7 route add -net 192.168.200.0 netmask 255.255.255.0 gw 192.168.100.1[root@C7-Server01 ~]# ip netns exec llb_ns8 route add -net 192.168.100.0 netmask 255.255.255.0 gw 192.168.200.1 测试互通，结果如下，表明底层underlay网络可达。 下来，我们创建隧道设备tun，构建overlay网络，代码如下 123456789# 在llb_ns7和llb_ns8中创建tun1、tun2和ipip tunnel[root@C7-Server01 ~]# ip netns exec llb_ns7 ip tunnel add tun1 mode ipip remote 192.168.200.6 local 192.168.100.6[root@C7-Server01 ~]# ip netns exec llb_ns8 ip tunnel add tun2 mode ipip remote 192.168.100.6 local 192.168.200.6# 激活tun设备，并配置ip地址[root@C7-Server01 ~]# ip netns exec llb_ns7 ip link set tun1 up[root@C7-Server01 ~]# ip netns exec llb_ns8 ip link set tun2 up[root@C7-Server01 ~]# ip netns exec llb_ns7 ip addr add 10.10.10.50 peer 10.10.20.50 dev tun1[root@C7-Server01 ~]# ip netns exec llb_ns8 ip addr add 10.10.20.50 peer 10.10.10.50 dev tun2 互通测试，结果如下： 把上面的命令行脚本中的ipip换成gre，其余不变，就创建了一个gre隧道的tun设备对。因为我们说tun是一个设备，那么我们可以通过ifconfig这个命令，来看看这个设备的信息，代码如下： 可以看到，tun1是一个ipip tunnel的一个端点，IP是10.10.10.50，其对端IP是10.10.20.50。 再看路由表信息，代码如下： 图中的内容告诉我们，到达目的地10.10.10.50的路由是一个直连路由直接从tun1出去即可，其实tun1和tun2就是overlay网络的VTEP。 至此，Linux原生网络虚拟化介绍完毕。其实，还有个iptables，一般我们把它理解为Linux的包过滤防火墙，其实当其所属服务器处于网络报转发的中间节点时，它也是一个网络防火墙。它的“防火”机制就是通过一个个链结合策略表完成，而那一个个链其本质就是一个三层的虚拟网络设备。由于篇幅原因，iptables会放在Linux常用运维工具分类中介绍。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-24-打造文件实时同步架构之rsync篇]]></title>
    <url>%2F2019%2F06%2F24%2F2019-06-24-%E6%89%93%E9%80%A0%E6%96%87%E4%BB%B6%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E4%B9%8Brsync%E7%AF%87%2F</url>
    <content type="text"><![CDATA[rsync是可以实现增量备份的工具。配合任务计划，rsync能实现定时或间隔同步，配合inotify或sersync，可以实现触发式的实时同步。事实上，rsync有一套自己的算法，其算法原理以及rsync对算法实现的机制可能比想象中要复杂一些。平时使用rsync实现简单的备份、同步等功能足以，没有多大必要去深究这些原理性的内容。如果你对这些原理感兴趣，可以通过rsync命令的man文档、并且借助”-vvvv”分析rsync执行过程，结合rsync的算法原理去深入理解。本篇文章由于篇幅有限，只是介绍rsync的使用方法和它常用的功能，以及rsync和sersync如何配合打造文件实时同步架构。 rsync文件同步机制简介rsync是一款开源的、快速的、多功能的、可实现全量及增量的本地或远程数据镜像同步备份的优秀工具。rsync适用于Unix/Linux/Windows等多种操作系统平台。 rsync可以实现scp的远程拷贝(rsync不支持远程到远程的拷贝，但scp支持)、cp的本地拷贝、rm删除和”ls -l”显示文件列表等功能。但需要注意的是，rsync的最终目的或者说其原始目的是实现两端主机的文件同步，因此实现的scp/cp/rm等功能仅仅只是同步的辅助手段，且rsync实现这些功能的方式和这些命令是不一样的。 rsync的目的是实现本地主机和远程主机上的文件同步(包括本地推到远程，远程拉到本地两种同步方式)，也可以实现本地不同路径下文件的同步，但不能实现远程路径1到远程路径2之间的同步，这个功能scp是可以实现的。 不考虑rsync的实现细节，就文件同步而言，涉及了源文件和目标文件的概念，还涉及了以哪边文件为同步基准。例如，想让目标主机上的文件和本地文件保持同步，则是以本地文件为同步基准，将本地文件作为源文件推送到目标主机上。反之，如果想让本地主机上的文件和目标主机上的文件保持同步，则目标主机上的文件为同步基准，实现方式是将目标主机上的文件作为源文件拉取到本地。当然，要保持本地的两个文件相互同步，rsync也一样能实现，这就像Linux中cp命令一样，以本地某文件作为源，另一文件作为目标文件，但是这两者实现的本质是完全不同的。 既然是文件同步，在同步过程中必然会涉及到源和目标两文件之间版本控制的问题，例如是否要删除源主机上没有但目标上多出来的文件，目标文件比源文件更新(newer than source)时是否仍要保持同步，遇到软链接时是拷贝软链接本身还是拷贝软链接所指向的文件，目标文件已存在时是否要先对其做个备份等等。 rsync同步过程中由两部分模式组成：决定哪些文件需要同步的检查模式以及文件同步时的同步模式。 1）检查模式是指按照指定规则来检查哪些文件需要被同步，例如哪些文件是明确被排除不传输的。默认情况下，rsync使用“quick check”算法快速检查源文件和目标文件的大小、mtime(修改时间)是否一致，如果不一致则需要传输。当然，也可以通过在rsync命令行中指定某些选项来改变quick check的检查模式，比如”–size-only”选项表示”quick check”将仅检查文件大小不同的文件作为待传输文件。rsync支持非常多的选项，其中检查模式的自定义性是非常有弹性的。 2）同步模式是指在文件确定要被同步后，在同步过程发生之前要做哪些额外工作。例如：上文所说的是否要先删除源主机上没有但目标主机上有的文件，是否要先备份已存在的目标文件，是否要追踪链接文件等额外操作。rsync也提供非常多的选项使得同步模式变得更具弹性。 相对来说，为rsync手动指定同步模式的选项更常见一些，只有在有特殊需求时才指定检查模式，因为大多数检查模式选项都可能会影响rsync的性能。 rsync的三种工作模式rsync命令有三种常见模式，具体如下： 1）本地模式：本地文件系统上实现同步，命令行语法格式如下： rsync [option] [SRC] [DEST] rsync [选项] [源文件] [目标文件] 2）通过远程Shell访问模式：本地主机使用远程shell和远程主机通信，命令行语法格式如下： 拉取（Pull）: rsync [option] [USER@]HOST:SRC [DEST] rsync [选项] 用户@主机:源文件 [目标文件] 推送（Push）： rsync [option] [SRC] [USER@]HOST:DEST rsync [选项] [源文件] 用户@主机:目标文件 3）rsync守护进程模式：本地主机通过网络套接字连接远程主机上的rsync daemon，命令行语法格式如下： 拉取（Pull）： rsync [option] [USER@]HOST::SRC [DEST] rsync [选项] 用户@主机::源文件 [目标文件] rsync [option] rsync://[USER@]HOST[:PORT]/SRC [DEST] rsync [选项] rsync://用户@主机:端口/源文件 [目标文件] 推送（Push）： rsync [option] SRC [USER@]HOST::DEST rsync [选项] [源文件] 用户@主机::目标文件 rsync [option] SRC rsync://[USER@]HOST[:PORT]/DEST rsync [选项] [源文件] rsync://用户@主机:端口/目标文件 前两者的本质是通过管道通信，而第三种则是让远程主机上运行rsync服务，使其监听在一个端口上，等待客户端的连接。 其实，还有第四种工作方式：通过远程shell也能临时启动一个rsync daemon，它不同于方式3，它不要求远程主机上事先启动rsync服务，而是临时派生出rsync daemon，属于单用途的一次性daemon，仅用于临时读取daemon的配置文件，当此次rsync同步完成，远程shell启动的rsync daemon进程也会自动消逝。此通信方式的命令行语法格式同方式3，但要求options部分必须明确指定”–rsh”选项或其短选项”-e”。 上述语法格式中，如果仅有一个SRC或DEST参数，则将以类似于”ls -l”的方式列出源文件列表(只有一个路径参数，总会认为是源文件)，而不是复制文件。 另外，使用rsync一定要注意的一点是，源路径如果是一个目录的话，带上尾随斜线和不带尾随斜线是不一样的，不带尾随斜线表示的是整个目录包括目录本身，带上尾随斜线表示的是目录中的文件，不包括目录本身。 由于rsync支持一百多个选项，所以此处只介绍几个常用选项。完整的选项说明以及rsync的使用方法参见系统的man rsync帮助文档。其中，重要参数选项如下： -v：显示rsync过程中详细信息。可以使用”-vvvv”获取更详细信息。 -P：显示文件传输的进度信息。(实际上”-P”=”–partial –progress”，其中的”–progress”才是显示进度信息的)。 -n –dry-run：仅测试传输，而不实际传输。常和”-vvvv”配合使用来查看rsync是如何工作的。 -a –archive：归档模式，表示递归传输并保持文件属性。等同于”-rtopgDl”。 -r –recursive：递归到目录中去。 -t –times：保持mtime属性。强烈建议任何时候都加上”-t”，否则目标文件mtime会设置为系统时间，导致下次更新检查出mtime不同从而导致增量传输无效。 -o –owner：保持owner属性(属主)。 -g –group：保持group属性(属组)。 -p –perms：保持perms属性(权限，不包括特殊权限)。 -D：是”–device –specials”选项的组合，即也拷贝设备文件和特殊文件。 -l –links：如果文件是软链接文件，则拷贝软链接本身而非软链接所指向的对象。 -z：传输时进行压缩提高效率。 -R –relative：使用相对路径。意味着将命令行中指定的全路径而非路径最尾部的文件名发送给服务端，包括它们的属性。 –size-only：默认算法是检查文件大小和mtime不同的文件，使用此选项将只检查文件大小。 -u –update：仅在源mtime比目标已存在文件的mtime新时才拷贝。注意，该选项是接收端判断的，不会影响删除行为。 -d –dirs：以不递归的方式拷贝目录本身。默认递归时，如果源为”dir1/file1”，则不会拷贝dir1目录，使用该选项将拷贝dir1但不拷贝file1。 –max-size：限制rsync传输的最大文件大小。可以使用单位后缀，还可以是一个小数值(例如：”–max-size=1.5m”) –min-size：限制rsync传输的最小文件大小。这可以用于禁止传输小文件或那些垃圾文件。 –exclude：指定排除规则来排除不需要传输的文件。 –delete：以SRC为主，对DEST进行同步。多则删之，少则补之。注意”–delete”是在接收端执行的，所以它是在exclude/include规则生效之后才执行的。 -b –backup ：对目标上已存在的文件做一个备份，备份的文件名后默认使用”~”做后缀。 –backup-dir：指定备份文件的保存路径。不指定时默认和待备份文件保存在同一目录下。 -e ：指定所要使用的远程shell程序，默认为ssh。 –port：连接daemon时使用的端口号，默认为873端口。 –password-file：daemon模式时的密码文件，可以从中读取密码实现非交互式。注意，这不是远程shell认证的密码，而是rsync模块认证的密码。 -W –whole-file：rsync将不再使用增量传输，而是全量传输。在网络带宽高于磁盘带宽时，该选项比增量传输更高效。 –existing：要求只更新目标端已存在的文件，目标端还不存在的文件不传输。注意，使用相对路径时如果上层目录不存在也不会传输。 –ignore-existing：要求只更新目标端不存在的文件。和”–existing”结合使用有特殊功能，见下文示例。 –remove-source-files：要求删除源端已经成功传输的文件。 虽然选项非常多，但最常用的选项组合是”avz”，即压缩和显示部分信息，并以归档模式传输。 rsync本地和远程shell使用示例以下示例既可以通过shell远程链接的工作模式实现，也可以在本地模式实现，具体的实现方式不同，应用的实际场景就不同，这里只是作为示例讲解，在实际使用时要灵活变通。 1）将/etc/hosts文件拷贝到本地/tmp目录下 123[root@c7-test01 ~]# rsync /etc/hosts /tmp[root@c7-test01 ~]# ls -l /tmp/hosts-rw-r--r-- 1 root root 158 Jun 3 14:13 /tmp/hosts 2）将/etc/cron.d目录拷贝到/tmp目录下 123[root@c7-test01 ~]# rsync -r /etc/cron.d /tmp[root@c7-test01 ~]# ls -ld /tmp/cr*drwxr-xr-x 2 root root 4096 Jun 3 14:14 /tmp/cron.d 3）将/etc/cron.d目录拷贝到/tmp下，但要求在/tmp下也生成etc子目录 12345678910# 使用-R选项，利用相对路径拷贝机制[root@c7-test01 ~]# rsync -R -r /etc/cron.d /tmp[root@c7-test01 ~]# tree -l /tmp/etc//tmp/etc/└── cron.d├── 0hourly├── raid-check└── sysstat1 directory, 3 files 其中”-R”选项表示使用相对路径，此相对路径是以目标目录为根的。对于上面的示例，表示在目标上的/tmp下创建etc/cron.d目录，即/tmp/etc/cron.d，etc/cron.d的根”/“代表的就是目标/tmp。 4）如果要拷贝的源路径较长，但只想在目标主机上保留一部分目录结构，例如，要拷贝/var/log/anaconda/*到/tmp下，但只想在/tmp下保留从log开始的目录，这时可以使用一个点代表相对路径的起始位置即可，也就是将长目录进行划分。 1234567891011121314[root@c7-test01 ~]# rsync -R -r /var/./log/anaconda /tmp[root@c7-test01 ~]# tree -l /tmp/log//tmp/log/└── anaconda├── anaconda.log├── ifcfg.log├── journal.log├── ks-script-n7L3eP.log├── ks-script-v6NMcO.log├── packaging.log├── program.log├── storage.log└── syslog1 directory, 9 files 这种方式，从点开始的目录都是相对路径，其相对根目录为目标路径。所以对于上面的示例，将在目标上创建/tmp/log/anaconda/*。 5）对远程目录下已存在的文件做一个备份 123456789101112131415161718192021[root@c7-test01 ~]# rsync -R -r --backup /var/./log/anaconda /tmp[root@c7-test01 ~]# ls -l /tmp/log/anaconda/total 4880-rw------- 1 root root 10945 Jun 3 14:22 anaconda.log-rw------- 1 root root 10945 Jun 3 14:18 anaconda.log~-rw------- 1 root root 18216 Jun 3 14:22 ifcfg.log-rw------- 1 root root 18216 Jun 3 14:18 ifcfg.log~-rw------- 1 root root 1711915 Jun 3 14:22 journal.log-rw------- 1 root root 1711915 Jun 3 14:18 journal.log~-rw------- 1 root root 0 Jun 3 14:22 ks-script-n7L3eP.log-rw------- 1 root root 0 Jun 3 14:18 ks-script-n7L3eP.log~-rw------- 1 root root 0 Jun 3 14:22 ks-script-v6NMcO.log-rw------- 1 root root 0 Jun 3 14:18 ks-script-v6NMcO.log~-rw------- 1 root root 312693 Jun 3 14:22 packaging.log-rw------- 1 root root 312693 Jun 3 14:18 packaging.log~-rw------- 1 root root 29448 Jun 3 14:22 program.log-rw------- 1 root root 29448 Jun 3 14:18 program.log~-rw------- 1 root root 78942 Jun 3 14:22 storage.log-rw------- 1 root root 78942 Jun 3 14:18 storage.log~-rw------- 1 root root 320770 Jun 3 14:22 syslog-rw------- 1 root root 320770 Jun 3 14:18 syslog~ 通过–backup参数可以在目标目录下，对已存在的文件就被做一个备份，备份文件默认使用”~”做后缀，可以使用”–suffix”指定备份后缀。同时，可以使用“–backup-dir”指定备份文件保存路径，但要求保存路径必须存在。指定备份路径后，默认将不会加备份后缀，除非使用”–suffix”显式指定后缀，如”–suffix=~”。代码如下： 123456789101112[root@c7-test01 ~]# rsync -R -r --backup-dir=/tmp/backup --backup /var/./log/anaconda /tmp[root@c7-test01 ~]# ls -l /tmp/backup/log/anaconda/total 2440-rw------- 1 root root 10945 Jun 3 14:22 anaconda.log-rw------- 1 root root 18216 Jun 3 14:22 ifcfg.log-rw------- 1 root root 1711915 Jun 3 14:22 journal.log-rw------- 1 root root 0 Jun 3 14:22 ks-script-n7L3eP.log-rw------- 1 root root 0 Jun 3 14:22 ks-script-v6NMcO.log-rw------- 1 root root 312693 Jun 3 14:22 packaging.log-rw------- 1 root root 29448 Jun 3 14:22 program.log-rw------- 1 root root 78942 Jun 3 14:22 storage.log-rw------- 1 root root 320770 Jun 3 14:22 syslog 6）源地址带与不带斜线（/）的区别的例子 1234567891011121314151617181920212223242526272829303132333435363738# 创建实验环境[root@C7-Server01 ~]# mkdir -p /data1/&#123;test1,test2&#125;/data2# 如果源目录的末尾有斜线，就会复制目录内的内容，而不是复制目录本身[root@C7-Server01 ~]# rsync -av /data1/ /data2/ # 本地同步sending incremental file listcreated directory /data2./test1/test1/data2/test2/test2/data2/sent 149 bytes received 64 bytes 426.00 bytes/sectotal size is 0 speedup is 0.00# 如果源目录没有斜线，则会复制目录本身及目录下的内容[root@C7-Server01 ~]# rsync -av /data1 /data2 # 本地同步sending incremental file listdata1/data1/test1/data1/test1/data2/data1/test2/data1/test2/data2/sent 155 bytes received 36 bytes 382.00 bytes/sectotal size is 0 speedup is 0.00# 查看/data2目录下文件信息[root@C7-Server01 ~]# ls -l /data2total 0drwxr-xr-x 4 root root 32 Apr 28 01:27 data1drwxr-xr-x 3 root root 19 Apr 28 01:27 test1drwxr-xr-x 3 root root 19 Apr 28 01:27 test2 源路径如果是一个目录的话，带上尾随斜线和不带尾随斜线是不一样的，不带尾随斜线表示的是整个目录包括目录本身，带上尾随斜线表示的是目录中的文件，不包括目录本身。 7）删除文件的特殊例子 当一个目录下有几十万或十几万个文件，使用rsync的–deleye选项可以很快进行删除。这里注意一点，是删除目录下的所有文件，而不是目录本身。 选项–delete使tmp目录内容和空目录保持一致，不同的文件及目录将会被删除，即null里有什么内容，tmp里就有什么内容，null里没有的，而tmp里有的就必须要删除，因为null目录为空，因此此命令会删除/tmp目录中的所有内容。使用”–delete”选项后，接收端的rsync会先删除目标目录下已经存在，但源端目录不存在的文件。也就是”多则删之，少则补之”。 1234567891011121314151617181920212223242526272829303132333435# 查看tmp目录下文件信息[root@C7-Server01 ~]# ls -l /tmptotal 4drwxr-xr-x 3 root root 23 Apr 28 00:22 home-rw-r--r-- 1 root root 187 Apr 22 22:56 hostsdrwx------ 3 root root 17 Apr 27 21:43 systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806。。。drwx------ 2 root root 6 Apr 23 18:24 vmware-root_9660-3101179142# 创建一个空目录null[root@C7-Server01 ~]# mkdir /null# 删除tmp目录下所有文件和子目录[root@C7-Server01 ~]# rsync -av --delete /null/ /tmp/sending incremental file listdeleting vmware-root_9660-3101179142/deleting vmware-root_9609-4121731445/deleting vmware-root_9604-3101310240/deleting vmware-root_9573-4146439782/deleting vmware-root_9458-2857896559/deleting vmware-root_9456-2866351085/。。。deleting hosts./sent 47 bytes received 3,911 bytes 7,916.00 bytes/sectotal size is 0 speedup is 0.00# 再次查看/tmp目录下文件和子目录信息[root@C7-Server01 ~]# ls -l /tmptotal 0 8）”–existing”和”–ignore-existing”使用示例 12345678910111213141516# 创建测试环境，测试环境的结构如下：[root@c7-test01 ~]# tree /tmp/&#123;a,b&#125;/tmp/a├── bashrc├── c│ └── find├── fstab├── profile└── rc.local/tmp/b├── crontab├── fstab├── profile└── rc.local1 directory, 9 files “–existing”是只更新目标端已存在的文件。目前/tmp/{a,b}目录中内容如上，bashrc在a目录中，crontab在b目录中，且a目录中多了一个c子目录。如下结果只有3个目标上已存在的文件被更新了，由于目标上没有c目录，所以c目录中的文件也没有进行传输。 1234567[root@c7-test01 ~]# rsync -r -v --existing /tmp/a/ /tmp/b sending incremental file listfstabprofilerc.localsent 2972 bytes received 70 bytes 6084.00 bytes/sectotal size is 204755 speedup is 67.31 而“–ignore-existing”是更新目标端不存在的文件。如下： 1234567[root@c7-test01 ~]# rsync -r -v --ignore-existing /tmp/a/ /tmp/bsending incremental file listbashrcc/c/findsent 231 bytes received 62 bytes 586.00 bytes/sectotal size is 0 speedup is 0.00 “–existing”和”–ignore-existing”结合使用时，有个特殊功效，当它们结合”–delete”使用的时候，文件不会传输，但会删除receiver端额外多出的文件。 123456789101112131415161718192021222324252627282930313233343536373839# 创建实验环境，如下[root@c7-test01 ~]# tree &#123;a,b&#125;a├── stu01├── stu02├── stu03└── stu04b└── a.log0 directories, 5 files# 使用-n选项测试传输，并没有实际效果[root@c7-test01 ~]# rsync -nrv --delete a/ b/sending incremental file listdeleting a.logstu01stu02stu03stu04sent 104 bytes received 33 bytes 274.00 bytes/sectotal size is 0 speedup is 0.00 (DRY RUN)# --existing &amp;&amp; --ignore-existing &amp;&amp; --delete结合使用，是删除接收端（目的端）多余的文件，且不会传输文件[root@c7-test01 ~]# rsync -nrv --existing --ignore-existing --delete a/ b/sending incremental file listdeleting a.logsent 92 bytes received 21 bytes 226.00 bytes/sectotal size is 0 speedup is 0.00 (DRY RUN)[root@c7-test01 ~]# rsync -nrv --existing --ignore-existing --delete b/ a/sending incremental file listdeleting stu04deleting stu03deleting stu02deleting stu01sent 58 bytes received 48 bytes 212.00 bytes/sectotal size is 0 speedup is 0.00 (DRY RUN) 实际上，“–existing”和”–ingore-existing”是传输规则，只会影响receiver要求让sender传输的文件列表，属于传输模式。而receiver决定哪些文件在传输之前如何处理属于同步模式，所以各种同步规则，比如：”–delete”等操作都不会被这两个选项影响。 9）”–remove-source-files”删除源端文件 使用该选项后，源端已经更新成功的文件都会被删除，源端所有未传输或未传输成功的文件都不会被移除。未传输成功的原因有多种，如exclude排除了，”quick check”未选则该文件，传输中断等等。总之，显示在”rsync -v”被传输列表中的文件都会被移除。如下： 1234567891011121314[root@c7-test01 ~]# rsync -r -v --remove-source-files a/ b/ .sending incremental file lista.logstu01stu02stu03stu04sent 331 bytes received 151 bytes 964.00 bytes/sectotal size is 0 speedup is 0.00[root@c7-test01 ~]# ls -l &#123;a/,b/&#125;a/:total 0b/:total 0 上述显示出来的文件在源端全部被删除。 10）”–exclude”排除规则 使用”–exclude”选项指定排除规则，排除那些不需要传输的文件。如下： 123456789[root@c7-test01 ~]# rsync -r -v --exclude=/tmp/a/* /tmp/a/ /tmp/b/* a/sending incremental file listc/sent 81 bytes received 16 bytes 194.00 bytes/sectotal size is 0 speedup is 0.00[root@c7-test01 ~]# ls /tmp/bc[root@c7-test01 ~]# ls /tmp/ac 上面的代码意思是不传送/tmp/a下的所有文件，但是传送/tmp/b下的所有文件，因此在传送列表中只有一个c/目录准备传送。 需要注意的是：一个”–exclude”只能指定一条规则，要指定多条排除规则，需要使用多个”–exclude”选项，或者将排除规则写入到文件中，然后使用”–exclude-from”选项读取该规则文件。 另外，除了”–exclude”排除规则，还有”–include”包含规则，顾名思义，它就是筛选出要进行传输的文件，所以include规则也称为传输规则。它的使用方法和”–exclude”一样。如果一个文件即能匹配排除规则，又能匹配包含规则，则先匹配到的立即生效，生效后就不再进行任何匹配。 关于规则，最重要的一点是它的作用时间。当发送端敲出rsync命令后，rsync将立即扫描命令行中给定的文件和目录(扫描过程中还会按照目录进行排序，将同一个目录的文件放在相邻的位置)，这称为拷贝树(copy tree)，扫描完成后将待传输的文件或目录记录到文件列表中，然后将文件列表传输给接收端。而筛选规则的作用时刻是在扫描拷贝树时，会根据规则来匹配并决定文件是否记录到文件列表中(严格地说是所有文件都会记录到文件列表中的，只不过排除的文件会被标记为hide隐藏起来)，只有记录到了文件列表中的文件或目录才是真正需要传输的内容。换句话说，筛选规则的生效时间在rsync整个同步过程中是非常靠前的，它会影响很多选项的操作对象，最典型的如”–delete”。 实际上，排除规则和包含规则都只是”–filter”筛选规则的两种特殊规则。“–filter”比较复杂，它有自己的规则语法和匹配模式，由于篇幅有限，以及考虑到本文实际应用定位，”–filter”规则不便在此多做解释，仅简单说明下规则类。 以下是rsync中的规则种类，不解之处请结合下文的”–delete”分析： 1）exclude规则：即排除规则，只作用于发送端，被排除的文件不会进入文件列表(实际上是加上隐藏规则进行隐藏)。 2）include规则：即包含规则，也称为传输规则，只作用于发送端，被包含的文件将明确记录到文件列表中。 3）hide规则：即隐藏规则，只作用于发送端，隐藏后的文件对于接收端来说是看不见的，也就是说接收端会认为它不存在于源端。 4）show规则：即显示规则，只作用于发送端，是隐藏规则的反向规则。 5）protect规则：即保护规则，该规则只作用于接收端，被保护的文件不会被删除掉。 6）risk规则：即取消保护规则。是protect的反向规则。 除此之外，还有一种规则是”clear规则”，作用是删除include/exclude规则列表。 rsync在发送端将文件列表发送给接收端后，接收端的generato进程会扫描每个文件列表中的信息，然后对列表中的每个信息条目都计算数据块校验码，最后将数据库校验码发给发送端，发送端通过校验码来匹配哪些数据块是需要传输的，这样就实现了增量传输的功能：只传输改变的部分，不会传输整个文件。而delete删除的时间点是generator进程处理每个文件列表时、生成校验码之前进行的，先将目标上存在但源上不存在的多余文件删除，这样就无需为多余的文件生成校验码。 但是，如果exclude和delete规则同时使用时，却不会删除被exclude排除的文件。这是因为，delete动作是比”–exclude”规则更晚执行的，被”–exclude”规则排除的文件不会进入文件列表中，在执行了delete时会认为该文件不存在于源端，从而会导致目标端将这些文件删除。但这是想当然的，尽管理论上确实是这样的，但是rsync为了防止众多误删除情况，提供了两种规则：保护规则(protect)和取消保护规则(risk)。默认情况下，“–delete”和”–exclude”一起使用时，虽然发送端的exclude规则将文件标记为隐藏，使得接收端认为这些被排除文件在源端不存在，但rsync会将这些隐藏文件标记为保护文件，使得它们不受delete行为的影响，这样delete就删除不了这些被排除的文件。如果还是想要强行删除被exclude排除的文件，可以使用”–delete-excluded”选项强制取消保护，这样即使被排除的文件也会被删除。 11）指定ssh连接参数，如端口、连接的用户、ssh选项等 12345[root@c7-test01 ~]# rsync -e "ssh -p 22 -o StrictHostKeyChecking=no" /etc/fstab 192.168.101.252:/tmpWarning: Permanently added '192.168.101.252' (ECDSA) to the list of known hosts.root@192.168.101.252's password: Permission denied, please try again.root@192.168.101.252's password: 在要求保障数据安全的场景下，可以使用-e选项借助SSH隧道进行加密传输数据，-p是SSH命令的选项，指定SSH传输的端口号为22，-o是SSH的认证方式，上述示例表示不通过秘钥认证，可见直接指定ssh参数是生效的。 12）拉取或推送文件及目录（类似scp命令） 1234567891011121314151617181920212223242526272829303132333435363738# 从Server02上拉取/home/目录到本地/tmp目录下[root@C7-Server01 ~]# rsync -av 192.168.101.82:/home/ /tmp/root@192.168.101.82's password: receiving incremental file list./kkutysllb/kkutysllb/.bash_historykkutysllb/.bash_logoutkkutysllb/.bash_profilekkutysllb/.bashrckkutysllb/202012312234.55kkutysllb/data001kkutysllb/data002kkutysllb/data003kkutysllb/data004。。。sent 767 bytes received 4,507 bytes 958.91 bytes/sectotal size is 1,440 speedup is 0.27# 查看本地/tmp目录下内容[root@C7-Server01 ~]# ls -l /tmptotal 4drwx------ 7 root root 4096 Apr 28 00:00 kkutysllb# 推送本地/etc/udev/目录下所有内容到Server02的/home/kkutysllb/目录下[root@C7-Server01 ~]# rsync -av /etc/udev 192.168.101.82:/home/kkutysllb/root@192.168.101.82's password: sending incremental file listudev/udev/hwdb.binudev/udev.confudev/rules.d/sent 7,944,769 bytes received 66 bytes 1,765,518.89 bytes/sectotal size is 7,942,619 speedup is 1.00 与scp命令复制的结果进行对比可以发现，使用rsync复制时，重复执行复制直至目录下文件相同就不再进行复制了。 rsync使用技巧1）实际运维场景常用选项-avz，相当于-vzrtopg（这是网上文档常见的选项），但是此处建议大家使用-avz选项，更简单明了。如果在脚本中使用也可以省略-v选项。 2）关于z压缩选项的使用建议，如果为内网环境，且没有其他业务占用带宽，可以不使用z选项。不压缩传输，几乎可以满带宽传输（千M网络），压缩传输则网络发送速度就会骤降，压缩的速率赶不上传输的速度。 3）选项n是一个提高安全性的选项，它可以结合-v选项输出模拟的传输过程，如果没有错误，则可以去除n选项真正的传输文件。 rsync daemon模式既然rsync通过远程shell就能实现两端主机上的文件同步，还要使用rsync的服务干啥？试想下，你有的机器上有一堆文件需要时不时地同步到众多机器上去，比如目录a、b、c是专门传输到web服务器上的，d/e、f、g/h是专门传输到ftp服务器上的，还要对这些目录中的某些文件进行排除，如果通过远程shell连接方式，无论是使用排除规则还是包含规则，甚至一条一条rsync命令地传输，这都没问题，但太过繁琐且每次都要输入同样的命令显得太死板。使用rsync daemon就可以解决这种死板问题。而且，rsync daemon是向外提供服务的，这样只要告诉了别人rsync的url路径，外人就能向ftp服务器一样获取文件列表并进行选择性地下载，所以，你所制定的列表，所有人都可以获取到并使用。 在Linux内核官网www.kernel.org上，就提供rsync的下载方式，官方给出的地址是rsync://rsync.kernel.org/pub，可以根据这个地址找出你想下载的内核版本。例如：要找出linux-3.0.15版本的内核相关文件，可以在客户端执行如下命令： 123456789101112[root@c7-test01 ~]# rsync --no-motd -r -v -f "+ */" -f "+ linux-3.0.15*" -f "- *" -m rsync://rsync.kernel.org/pub/receiving file list ... donedrwxr-xr-x 4,096 2019/05/04 03:15:23 .drwxr-xr-x 4,096 2014/11/12 05:50:10 linuxdrwxr-xr-x 4,096 2019/03/12 23:06:47 linux/kerneldrwxr-xr-x 258,048 2019/05/23 13:52:08 linux/kernel/v3.x-rw-r--r-- 76,803,806 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.bz2-rw-r--r-- 96,726,195 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.gz-rw-r--r-- 836 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.sign-rw-r--r-- 63,812,604 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.xzsent 58 bytes received 82,915 bytes 2,720.43 bytes/sectotal size is 237,343,441 speedup is 2,860.49 我们无需关注上面的规则代表什么意思，需要关注的重点是通过rsync可以向外提供文件列表并提供相应的下载。同样，我们还可以根据路径，将rsync daemon上的文件拉取到本地实现下载的功能。 rsync daemon是”rsync –daemon”或再加上其他一些选项启动的，它会读取配置文件，默认是/etc/rsyncd.conf，并默认监听在873端口上，当外界有客户端对此端口发起连接请求，通过这个网络套接字就可以完成连接，以后与该客户端通信的所有数据都通过该网络套接字传输。 rsync daemon的通信方式和传输通道与远程shell不同。远程shell连接的两端是通过管道完成通信和数据传输的，当连接到目标端时，将在目标端上根据远程shell进程fork出rsync进程使其成为rsync server。而rsync daemon是事先在server端上运行好的rsync后台进程(根据启动选项，也可以设置为非后台进程)，它监听套接字等待client端的连接，连接建立后所有通信方式都是通过套接字完成的。 rsync中的server的概念从来就不代表是rsync daemon，server在rsync中只是一种通用称呼，只要不是发起rsync请求的client端，就是server端，可以认为rsync daemon是一种特殊的server，其实daemon更准确的称呼应该是service。 以下是rsync client连接rsync daemon时的命令语法： 1234Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST]rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]Push: rsync [OPTION...] SRC... [USER@]HOST::DESTrsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 连接命令有两种类型，一种是rsync风格使用双冒号的”rsync user@host::src dest”，一种是url风格的”rsync://user@host:port/src dest”。对于rsync风格的连接命令，如果想要指定端口号，则需要使用选项”–port”。 上述语法中，其中daemon端的路径，如user@host::src，它的src代表的是模块名，而不是真的文件系统中的路径。关于rsync中的模块就是其配置文件中定义的各个功能。 daemon配置文件rsyncd.conf默认”rsync –daemon”读取的配置文件为/etc/rsyncd.conf，有些版本的系统上可能该文件默认不存在，需要手动创建。rsyncd.conf默认配置内容如下： 在上述示例配置文件中，先定义了一些全局选项，然后定义了[ftp1]，这个用中括号包围的”[ftp1]”就是rsync中所谓的模块，ftp1为模块ID，必须保证唯一，每个模块中必须定义一项”path”，path定义的是该模块代表的路径，此示例文件中，如果想请求ftp1模块，则在客户端使用”rsync user@host::ftp1”，这表示访问user@host上的/home/ftp目录，如果要访问/home/ftp目录下的子目录www，则”rsync user@host::ftp1/www”。 以下是我自用的配置项，也算是一个配置示例： [ 12345678910111213141516171819202122232425262728293031323334root@c7-server01 ~]# cat /etc/rsyncd.conf # 全局配置参数 port=888 # 指定rsync端口。默认873 uid = 0 # rsync服务的运行用户，默认是nobody，文件传输成功后属主将是这个uid gid = 0 # rsync服务的运行组，默认是nobody，文件传输成功后属组将是这个gid use chroot = no # rsync daemon在传输前是否切换到指定的path目录下，并将其监禁在内 max connections = 200 # 指定最大连接数量，0表示没有限制 timeout = 300 # 确保rsync服务器不会永远等待一个崩溃的客户端，0表示永远等待 motd file = /var/rsyncd/rsync.motd # 客户端连接过来显示的消息 pid file = /var/run/rsyncd.pid # 指定rsync daemon的pid文件 lock file = /var/run/rsync.lock # 指定锁文件 log file = /var/log/rsyncd.log # 指定rsync的日志文件，而不把日志发送给syslog dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 # 指定哪些文件不用进行压缩传输# 指定模块，并设定模块配置参数，可以创建多个模块[kksql] # 模块IDpath = /kksql/ # 指定该模块的路径，该参数必须指定。启动rsync服务前该目录必须存在。rsync请求访问模块本质就是访问该路径。ignore errors # 忽略某些IO错误信息read only = true # 指定该模块是否可读写，即能否上传文件，false表示可读写，true表示可读不可写。所有模块默认不可上传write only = false # 指定该模式是否支持下载，设置为true表示客户端不能下载。所有模块默认可下载list = false # 客户端请求显示模块列表时，该模块是否显示出来，设置为false则该模块为隐藏模块。默认truehosts allow = 10.0.5.0/24 # 指定允许连接到该模块的机器，多个ip用空格隔开或者设置区间hosts deny = 0.0.0.0/32 # 指定不允许连接到该模块的机器auth users = rsync_backup # 指定连接到该模块的用户列表，只有列表里的用户才能连接到模块，用户名和对应密码保存在secrets file中，这里使用的不是系统用户，而是虚拟用户。不设置时，默认所有用户都能连接，但使用的是匿名连接secrets file = /etc/rsyncd.passwd # 保存auth users用户列表的用户名和密码，每行包含一个username:passwd。由于"strict modes"默认为true，所以此文件要求非rsync daemon用户不可读写。只有启用了auth users该选项才有效。#[kkweb] # 以下定义的是第二个模块path=/kkweb/read only = falseignore errorscomment = anyone can access 1）从客户端推到服务端时，文件的属主和属组是配置文件中指定的uid和gid。但是客户端从服务端拉的时候，文件的属主和属组是客户端正在操作rsync的用户身份，因为执行rsync程序的用户为当前用户。 2）auth users和secrets file这两行不是一定需要的，省略它们时将默认使用匿名连接。但是如果使用了它们，则secrets file的权限必须是600。客户端的密码文件也必须是600。 3）关于secrets file的权限，实际上并非一定是600，只要满足除了运行rsync daemon的用户可读即可。是否检查权限的设定是通过选项strict mode设置的，如果设置为false，则无需关注文件的权限。但默认是yes，即需要设置权限。 配置完后，再就是提供模块相关目录、身份验证文件等。 123[root@c7-server01 ~]# useradd -r -s /sbin/nologin rsync[root@c7-server01 ~]# mkdir /&#123;kksql,kkweb&#125;[root@c7-server01 ~]# chown -R rsync.rsync /&#123;kksql,kkweb&#125; 提供模块kksql的身份验证文件，由于rsync daemon是以root身份运行的，所以要求身份验证文件对非root用户不可读写，所以设置为600权限。 12[root@c7-server01 ~]# echo "rsync_backup:Oms_2600" &gt;&gt; /etc/rsyncd.passwd[root@c7-server01 ~]# chmod 600 /etc/rsyncd.passwd 然后启动rsync daemon，启动方式很简单。如果是CentOS 7，则自带启动脚本：systemctl stard rsyncd。 123[root@c7-server01 ~]# rsync --daemon# 或[root@c7-server01 ~]# systemctl enable rsyncd &amp;&amp; systemctl start rsyncd 启动好rysnc daemon后，它就监听在指定的端口上，等待客户端的连接。由于上述示例中的模块kksql配置了身份验证功能，所以客户端连接时会询问密码。如果不想手动输入密码，则可以使用”–password-file”选项提供密码文件，密码文件中只有第一行才是传递的密码，其余所有的行都会被自动忽略。 在客户端上访问daemon上各模块的文件示例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# rsync模式[root@c7-test01 ~]# rsync --list-only --port 888 root@192.168.101.11::kkweb/drwxr-xr-x 4,096 2019/06/03 17:51:49 .-rw-r--r-- 0 2019/06/03 17:51:49 stu01.img-rw-r--r-- 0 2019/06/03 17:51:49 stu02.img-rw-r--r-- 0 2019/06/03 17:51:49 stu03.img-rw-r--r-- 0 2019/06/03 17:51:49 stu04.img-rw-r--r-- 0 2019/06/03 17:51:49 stu05.img-rw-r--r-- 0 2019/06/03 17:51:49 stu06.img-rw-r--r-- 0 2019/06/03 17:51:49 stu07.img-rw-r--r-- 0 2019/06/03 17:51:49 stu08.img-rw-r--r-- 0 2019/06/03 17:51:49 stu09.img-rw-r--r-- 0 2019/06/03 17:51:49 stu10.img[root@c7-test01 ~]# rsync --list-only --port 888 rsync_bakup@192.168.101.11::kkweb/drwxr-xr-x 4,096 2019/06/03 17:51:49 .-rw-r--r-- 0 2019/06/03 17:51:49 stu01.img-rw-r--r-- 0 2019/06/03 17:51:49 stu02.img-rw-r--r-- 0 2019/06/03 17:51:49 stu03.img-rw-r--r-- 0 2019/06/03 17:51:49 stu04.img-rw-r--r-- 0 2019/06/03 17:51:49 stu05.img-rw-r--r-- 0 2019/06/03 17:51:49 stu06.img-rw-r--r-- 0 2019/06/03 17:51:49 stu07.img-rw-r--r-- 0 2019/06/03 17:51:49 stu08.img-rw-r--r-- 0 2019/06/03 17:51:49 stu09.img-rw-r--r-- 0 2019/06/03 17:51:49 stu10.img# url模式[root@c7-test01 ~]# rsync --list-only rsync://root@192.168.101.11:888/kkweb/ drwxr-xr-x 4,096 2019/06/03 17:51:49 .-rw-r--r-- 0 2019/06/03 17:51:49 stu01.img-rw-r--r-- 0 2019/06/03 17:51:49 stu02.img-rw-r--r-- 0 2019/06/03 17:51:49 stu03.img-rw-r--r-- 0 2019/06/03 17:51:49 stu04.img-rw-r--r-- 0 2019/06/03 17:51:49 stu05.img-rw-r--r-- 0 2019/06/03 17:51:49 stu06.img-rw-r--r-- 0 2019/06/03 17:51:49 stu07.img-rw-r--r-- 0 2019/06/03 17:51:49 stu08.img-rw-r--r-- 0 2019/06/03 17:51:49 stu09.img-rw-r--r-- 0 2019/06/03 17:51:49 stu10.img[root@c7-test01 ~]# rsync --list-only rsync://rsync@192.168.101.11:888/kkweb/ drwxr-xr-x 4,096 2019/06/03 17:51:49 .-rw-r--r-- 0 2019/06/03 17:51:49 stu01.img-rw-r--r-- 0 2019/06/03 17:51:49 stu02.img-rw-r--r-- 0 2019/06/03 17:51:49 stu03.img-rw-r--r-- 0 2019/06/03 17:51:49 stu04.img-rw-r--r-- 0 2019/06/03 17:51:49 stu05.img-rw-r--r-- 0 2019/06/03 17:51:49 stu06.img-rw-r--r-- 0 2019/06/03 17:51:49 stu07.img-rw-r--r-- 0 2019/06/03 17:51:49 stu08.img-rw-r--r-- 0 2019/06/03 17:51:49 stu09.img-rw-r--r-- 0 2019/06/03 17:51:49 stu10.img 晕吧？下一篇我们介绍sersync，继续晕。。。嘿嘿]]></content>
      <categories>
        <category>Linux常用运维工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-21-服务器外部交换网络虚拟化]]></title>
    <url>%2F2019%2F06%2F22%2F2019-06-21-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A4%96%E9%83%A8%E4%BA%A4%E6%8D%A2%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[数据中心服务器外部交换网络的虚拟化主要包括：数据中心内部二、三层交换设备虚拟化、大二层组网架构和软件定义网络SDN技术。其中，软件定义网络SDN技术在目前中国移动NovoNet战略中还处于现网试点阶段，预计2020年初会引入电信云数据中心。而数据中心内部二、三层交换设备虚拟化及大二层组网架构已完全成熟，并已在当前电信云数据中心中实际部署，本文也主要讲述这两种技术，其中大二层组网基石VxLAN需要大家重点掌握。 数据中心内部二、三层交换设备虚拟化现有数据中心内部的三层网络架构起源于园区网，传统数据中心。如下图所示，三层架构将网络分为接入(access)、汇聚(aggregation)和核心(core)三层。 接入层通常使用二层交换机，主要负责接入服务器、标记VLAN以及转发二层的流量。汇聚层通常使用三层交换机，主要负责同一POD（所谓POD就是划分范围就是汇聚交换机管理的范围，对应到上图就是底层的三个iStack堆叠组范围）内的路由，并实现ACL等安全策略。核心层通常使用业务路由器，主要负责实现跨POD的路由，并提供Internet接入以及VPN等服务。 在三层的网络规划中，接入层和汇聚层间通常是二层网络，接入交换机双上联到汇聚交换机，并运行STP来消除环路，汇聚交换机作为网关终结掉二层，和核心路由器起IGP来学习路由，少数情况下会选择BGP。 众所周知，MAC自学习是以太网的根基，它以泛洪帧为探针同步转发表，实现简单，无需复杂控制。但是，如果网络中存在环路，泛洪会在极短的时间内使网络瘫痪。为此，引入STP用来破除转发环路，不过STP的引入却带来了更多问题，如收敛慢、链路利用率低、规模受限、难以定位故障等。虽然，业界为STP的优化费劲了心思，但打补丁的方式只能治标而不能治本。随着数据中心I/O密集型业务的大发展，STP成了网络最为明显的一块短板，处理掉STP自然也就成了数据中心网络架构演进打响的第一枪。 物理设备的“多虚一”：VRRP、iStack和虚拟机框物理设备的“多虚一”功能，能够将多台设备中的控制平面进行整合，形成一台统一的逻辑设备，这台逻辑设备不但具有统一的管理IP，而且在各种L2和L3协议中也将表现为一个整体。因此在完成整合后，STP所看到的拓扑自然就是无环的了，这就间接地规避了STP的种种问题。 目前，普遍采用VRRP、堆叠iStack和虚拟机框技术完成物理设备的“多虚一”功能。其中，虚拟机框技术和传统的堆叠技术一样，不同的物理设备分享同一个控制平面，实际上就相当于为物理网络设备做了个集群，也有选主和倒换的过程。相比之下，虚拟机框在组网上的限制较少，而且在可用性方面的设计普遍要好于堆叠，因此可以看作对于堆叠技术的升级，我们只要了解堆叠技术即可。而且，虚拟机框技术以Cisco的VSS、Juniper的Virtual Chassis以及H3C的IRF 2为代表，在中国移动NFV电信云数据中心内华为交换机堆叠技术与此等价。 VRRP技术定义：虚拟路由冗余协议VRRP，通过把几台设备联合组成一台虚拟的路由设备，将虚拟路由设备的IP地址作为用户的默认网关实现与外部网络通信。 如上图所示，VRRP将局域网内的一组路由器划分在一起，形成一个VRRP备份组，它在功能上相当于一台虚拟路由器，使用虚拟路由器号进行标识。VRRP设备有自己的虚拟IP地址和虚拟MAC地址，它的外在表现形式和实际的物理路由器完全一样。局域网内的主机将VRRP设备的IP地址设置为默认网关，通过VRRP设备与外部网络进行通信。 VRRP设备是工作在实际的物理三层设备之上的。它由多个实际的三层设备组成，包括一个Master设备和多个Backup设备。Master设备正常工作时，局域网内的主机通过Master设备与外界通信。当Master设备出现故障时，Backup设备中的一台设备将成为新的Master设备，接替转发报文的工作。 VRRP的工作过程为： 1）VRRP设备中的物理设备根据优先级选举出Master。Master设备通过发送免费ARP报文，将自己的虚拟MAC地址通知给与它连接的设备或者主机，从而承担报文转发任务； 2）Master设备周期性发送VRRP报文，以公布其配置信息（优先级等）和工作状况； 3）如果Master设备出现故障，VRRP设备中的Backup设备将根据优先级重新选举新的Master； 4）VRRP设备状态切换时，Master设备由一台设备切换为另外一台设备，新的Master设备只是简单地发送一个携带VRRP设备的MAC地址和虚拟IP地址信息的免费ARP报文，这样就可以更新与它连接的主机或设备中的ARP相关信息。网络中的主机感知不到Maste设备已经切换为另外一台设备。 5）Backup设备的优先级高于Master设备时，由Backup设备的工作方式（抢占方式和非抢占方式）决定是否重新选举Master。 VRRP根据优先级来确定VRRP设备中每台物理设备的角色（Master设备或Backup设备）。优先级越高，则越有可能成为Master设备。 1）初始创建的设备工作在Backup状态，通过VRRP报文的交互获知VRRP设备中其他成员的优先级： 如果VRRP报文中Master设备的优先级高于自己的优先级，则路由器保持在Backup状态； 如果VRRP报文中Master设备的优先级低于自己的优先级，采用抢占工作方式的路由器将抢占成为Master状态，周期性地发送VRRP报文，采用非抢占工作方式的路由器仍保持Backup状态； 如果在一定时间内没有收到VRRP报文，则本设备切换为Master状态。 2）VRRP优先级的取值范围为0到255（数值越大表明优先级越高），可配置的范围是1到254，优先级0为系统保留给路由器放弃Master位置时候使用，255则是系统保留给IP地址拥有者使用。当设备为IP地址拥有者时，其优先级始终为255。因此，当VRRP设备内存在IP地址拥有者时，只要其工作正常，则为Master设备。 VRRP的的工作方式 主备备份方式表示业务仅由Master设备承担。当Master设备出现故障时，才会由选举出来的Backup设备接替它工作。 如上图左边所示，初始情况下，Device A是Master路由器并承担转发任务，Device B和Device C是Backup路由器且都处于就绪监听状态。如果Device A发生故障，则虚拟路由器内处于Backup状态的Device B和Device C路由器将根据优先级选出一个新的Master路由器，这个新Master路由器继续为网络内的主机转发数据。 负载分担方式是指多台设备同时承担业务，因此负载分担方式需要两个或者两个以上的VRRP设备，每个VRRP设备都包括一个Master设备和若干个Backup设备，各VRRP设备中的Master设备可以各不相同。通过在一个设备的一个接口上可以创建多个VRRP设备，使得该设备可以在一个VRRP设备中作为Master设备，同时在其他的VRRP设备中作为Backup设备。 在上图右边中，有三个VRRP设备存在： VRRP1：Device A作为Master设备，Device B和Device C作为Backup设备。 VRRP2：Device B作为Master设备，Device A和Device C作为Backup设备。 VRRP3：Device C作为Master设备，Device A和Device B作为Backup设备。 为了实现业务流量在Device A、Device B和Device C之间进行负载分担，需要将局域网内的主机的默认网关分别设置为VRRP1、2和3。在配置优先级时，需要确保三个VRRP设备中各路由器的VRRP优先级形成一定的交叉，使得一台路由器尽可能不同时充当2个Master路由器。 iStack交换机堆叠技术堆叠iStack（Intelligent Stack），是指将多台支持堆叠特性的交换机设备组合在一起，从逻辑上组合成一台交换设备。如下图所示，SwitchA与SwitchB通过堆叠线缆连接后组成堆叠iStack，对于上游和下游设备来说，它们就相当于一台交换机Switch。通过交换机堆叠，可以实现网络高可靠性和网络大数据量转发，同时简化网络管理。 1）高可靠性。堆叠系统多台成员交换机之间冗余备份；堆叠支持跨设备的链路聚合功能，实现跨设备的链路冗余备份。 2）强大的网络扩展能力。通过增加成员交换机，可以轻松的扩展堆叠系统的端口数、带宽和处理能力；同时支持成员交换机热插拔，新加入的成员交换机自动同步主交换机的配置文件和系统软件版本。 3）简化配置和管理。一方面，用户可以通过任何一台成员交换机登录堆叠系统，对堆叠系统所有成员交换机进行统一配置和管理；另一方面，堆叠形成后，不需要配置复杂的二层破环协议和三层保护倒换协议，简化了网络配置。 堆叠涉及以下几个基本概念： 1）角色：堆叠中所有的单台交换机都称为成员交换机，按照功能不同，可以分为三种角色： 主交换机：主交换机（Master）负责管理整个堆叠。堆叠中只有一台主交换机。 备交换机：备交换机（Standby）是主交换机的备份交换机。当主交换机故障时，备交换机会接替原主交换机的所有业务。堆叠中只有一台备交换机。 从交换机：从交换机（Slave）主要用于业务转发，从交换机数量越多，堆叠系统的转发能力越强。除主交换机和备交换机外，堆叠中其他所有的成员交换机都是从交换机。 2）堆叠ID：即成员交换机的槽位号（Slot ID），用来标识和管理成员交换机，堆叠中所有成员交换机的堆叠ID都是唯一的。 3）堆叠优先级：是成员交换机的一个属性，主要用于角色选举过程中确定成员交换机的角色，优先级值越大表示优先级越高，优先级越高当选为主交换机的可能性越大。 堆叠建立的过程包括以下四个阶段： Step1：物理连接。如下图所示，根据网络需求，选择适当的连接方式和连接拓扑，组建堆叠网络。根据连接介质的不同，堆叠可分为堆叠卡堆叠和业务口堆叠。 如上图所示，每种连接方式都可组成链形和环形两种连接拓扑。下表从可靠性、链路带宽利用率和组网布线是否方便的角度对两种连接拓扑进行对比。 连接拓扑 优点 缺点 适用场景 链形连接 首尾不需要有物理连接，适合长距离堆叠。 可靠性低：其中一条堆叠链路出现故障，就会造成堆叠分裂。堆叠链路带宽利用率低：整个堆叠系统只有一条路径。 堆叠成员交换机距离较远时，组建环形连接比较困难，可以使用链形连接。 环形连接 可靠性高：其中一条堆叠链路出现故障，环形拓扑变成链形拓扑，不影响堆叠系统正常工作。堆叠链路带宽利用率高：数据能够按照最短路径转发。 首尾需要有物理连接，不适合长距离堆叠。 堆叠成员交换机距离较近时，从可靠性和堆叠链路利用率上考虑，建议使用环形连接。 Step2：主交换机选举。确定出堆叠的连接方式和连接拓扑，完成成员交换机之间的物理连接之后，所有成员交换机上电。此时，堆叠系统开始进行主交换机的选举。在堆叠系统中每台成员交换机都具有一个确定的角色，其中，主交换机负责管理整个堆叠系统。主交换机选举规则如下（依次从第一条开始判断，直至找到最优的交换机才停止比较）： 运行状态比较，已经运行的交换机优先处于启动状态的交换机竞争为主交换机。堆叠主交换机选举超时时间为20s，堆叠成员交换机上电或重启时，由于不同成员交换机所需的启动时间可能差异比较大，因此不是所有成员交换机都有机会参与主交换机的选举：启动时间与启动最快的成员交换机相比，相差超过20s的成员交换机没有机会参与主交换机的选举，只能被动加入堆叠成为非主交换机，加入过程可参见堆叠成员加入与退出。因此，如果希望指定某一成员交换机成为主交换机，则可以先为其上电，待其启动完成后再给其他成员交换机上电。 堆叠优先级高的交换机优先竞争为主交换机。 叠优先级相同时，MAC地址小的交换机优先竞争为主交换机。 Step3：拓扑收集和备交换机选举。主交换机选举完成后，会收集所有成员交换机的拓扑信息，根据拓扑信息计算出堆叠转发表项和破环点信息下发给堆叠中的所有成员交换机，并向所有成员交换机分配堆叠ID。之后进行备交换机的选举，作为主交换机的备份交换机。除主交换机外最先完成设备启动的交换机优先被选为备份交换机。当除主交换机外其它交换机同时完成启动时，备交换机的选举规则如下（依次从第一条开始判断，直至找到最优的交换机才停止比较）： 堆叠优先级最高的设备成为备交换机。 堆叠优先级相同时，MAC地址最小的成为备交换机。 除主交换机和备交换机之外，剩下的其他成员交换机作为从交换机加入堆叠。 Step4：稳定运行。角色选举、拓扑收集完成之后，所有成员交换机会自动同步主交换机的系统软件和配置文件。堆叠具有自动加载系统软件的功能，待组成堆叠的成员交换机不需要具有相同软件版本，只需要版本间兼容即可。当备交换机或从交换机与主交换机的软件版本不一致时，备交换机或从交换机会自动从主交换机下载系统软件，然后使用新系统软件重启，并重新加入堆叠。堆叠具有配置文件同步机制，备交换机或从交换机会将主交换机的配置文件同步到本设备并执行，以保证堆叠中的多台设备能够像一台设备一样在网络中工作，并且在主交换机出现故障之后，其余交换机仍能够正常执行各项功能。 堆叠支持跨设备链路聚合技术，通过配置跨设备Eth-Trunk接口实现。用户可以将不同成员交换机上的物理以太网端口配置成一个聚合端口连接到上游或下游设备上，实现多台设备之间的链路聚合。当其中一条聚合链路故障或堆叠中某台成员交换机故障时，Eth-Trunk接口能够将流量重新分布到其他聚合链路上，实现了链路间和设备间的备份，保证了数据流量的可靠传输。 如上图左边所示，流向网络核心的流量将均匀分布在聚合链路上，当某一条聚合链路失效时，Eth-Trunk接口将流量通过堆叠线缆重新分布到其他聚合链路上，实现了链路间的备份。 如上图右边所示，流向网络核心的流量将均匀分布在聚合链路上，当某台成员交换机故障时，Eth-Trunk接口将流量重新分布到其他聚合链路上，实现了设备间的备份。 跨设备链路聚合实现了数据流量的可靠传输和堆叠成员交换机的相互备份。但是由于堆叠设备间堆叠线缆的带宽有限，跨设备转发流量增加了堆叠线缆的带宽承载压力，同时也降低了流量转发效率。为了提高转发效率，减少堆叠线缆上的转发流量，设备支持流量本地优先转发。即从本设备进入的流量，优先从本设备相应的接口转发出去；如果本设备相应的接口故障或者流量已经达到接口的线速，那么就从其它堆叠成员交换机的接口转发出去。 如上图所示，SwitchA与SwitchB组成堆叠，上下行都加入到Eth-Trunk。如果没有本地优先转发，则从SwitchA进入的流量，根据当前Eth-Trunk的负载分担方式，会有一部分经过堆叠线缆，从SwitchB的物理接口转发出去。设备支持本地优先转发之后，从SwitchA进入的流量，只会从SwitchA的接口转发，流量不经过堆叠线缆。 设备堆叠ID缺省为0。堆叠时由堆叠主交换机对设备的堆叠ID进行管理，当堆叠系统有新成员加入时，如果新成员与已有成员堆叠ID冲突，则堆叠主交换机从0～最大的堆叠ID进行遍历，找到第一个空闲的ID分配给该新成员。新建堆叠或堆叠成员变化时，如果不在堆叠前手动指定各设备的堆叠ID，则由于启动顺序等原因，最终堆叠系统中各成员的堆叠ID是随机的。因此，在建立堆叠时，建议提前规划好设备的堆叠ID，或通过特定的操作顺序，使设备启动后的堆叠ID与规划的堆叠ID一致。修改堆叠ID设备需要重启。 如上图所示，堆叠成员加入是指向已经稳定运行的堆叠系统添加一台新的交换机。堆叠成员加入分为新成员交换机带电加入和不带电加入，带电加入则需要采用堆叠合并的方式完成，此处堆叠成员加入是指不带电加入。新成员交换机加入堆叠时，建议采用不带电加入。 堆叠成员加入的过程如下： Step1：新加入的交换机连线上电启动后，进行角色选举，新加入的交换机会选举为从交换机，堆叠系统中原有主备从角色不变。 Step2：角色选举结束后，主交换机更新堆叠拓扑信息，同步到其他成员交换机上，并向新加入的交换机分配堆叠ID（新加入的交换机没有配置堆叠ID或配置的堆叠ID与原堆叠系统的冲突时）。 Step3：新加入的交换机更新堆叠ID，并同步主交换机的配置文件和系统软件，之后进入稳定运行状态。 堆叠成员退出是指成员交换机从堆叠系统中离开。根据退出成员交换机角色的不同，对堆叠系统的影响也有所不同：当主交换机退出，备份交换机升级为主交换机，重新计算堆叠拓扑并同步到其他成员交换机，指定新的备交换机，之后进入稳定运行状态。当备交换机退出，主交换机重新指定备交换机，重新计算堆叠拓扑并同步到其他成员交换机，之后进入稳定运行状态。当从交换机退出，主交换机重新计算堆叠拓扑并同步到其他成员交换机，之后进入稳定运行状态。堆叠成员交换机退出的过程，主要就是拆除堆叠线缆和移除交换机的过程。 对于环形堆叠：成员交换机退出后，为保证网络的可靠性还需要把退出交换机连接的两个端口通过堆叠线缆进行连接。 对于链形堆叠：拆除中间交换机会造成堆叠分裂。这时需要在拆除前进行业务分析，尽量减少对业务的影响。 高级STP欺骗：跨设备链路聚合M-LAGSTP会严重浪费链路资源，根源在于它会禁止冗余链路上的转发。如果通过一种办法来“欺骗”STP，让它认为物理拓扑中没有冗余链路，那么就可以解决上述问题。实际上，上面的VRRP、堆叠技术中就用到了跨设备链路聚合，不过在实现上过于复杂，因此又出现了一类技术，它们不再为控制平面做集群，只保留了跨设备链路聚合的能力，这类技术以M-LAG为代表。 定义：M-LAG（Multichassis Link Aggregation Group）即跨设备链路聚合组，是一种实现跨设备链路聚合的机制，将一台设备与另外两台设备进行跨设备链路聚合，从而把链路可靠性从单板级提高到了设备级，组成双活系统。如下图左边所示。 如上图右边所示，用户侧设备Switch（可以是交换机或主机）通过M-LAG机制与另外两台设备（SwitchA和SwitchB）进行跨设备链路聚合，共同组成一个双活系统。这样可以实现SwitchA和SwitchB共同进行流量的转发，保证网络的可靠性。 M-LAG作为一种跨设备链路聚合的技术，除了具备增加带宽、提高链路可靠性、负载分担的优势外，还具备以下优势： 更高的可靠性：把链路可靠性从单板级提高到了设备级。 简化组网及配置：可以将M-LAG理解为一种横向虚拟化技术，将双归接入的两台设备在逻辑上虚拟成一台设备。M-LAG提供了一个没有环路的二层拓扑同时实现冗余备份，不再需要繁琐的生成树协议配置，极大的简化了组网及配置。 独立升级：两台设备可以分别进行升级，保证有一台设备正常工作即可，对正在运行的业务几乎没有影响。 下表对M-LAG涉及的相关概念做了个总结，如下： 概念 说明 M-LAG主设备 部署M-LAG且状态为主的设备。 M-LAG备设备 部署M-LAG且状态为备的设备。说明：正常情况下，主设备和备设备同时进行业务流量的转发。 peer-link链路 peer-link链路是一条直连链路且必须做链路聚合，用于交换协商报文及传输部分流量。为了增加peer-link链路的可靠性，推荐采用多条链路做链路聚合。 peer-link接口 peer-link链路两端直连的接口均为peer-link接口。 M-LAG成员接口 M-LAG主备设备上连接用户侧主机（或交换设备）的Eth-Trunk接口。为了增加可靠性，推荐链路聚合配置为LACP模式。 基于M-LAG组成的双活系统提供了设备级的可靠性，如下图所示，M-LAG的建立过程有如下几个步骤： Step1：当M-LAG两台的设备完成配置后，两台的设备会通过peer-link链路定期发送M-LAG协商报文。当收到对端的M-LAG协商报文后，会判断M-LAG协商报文中的DFS Group编号是否和本端相同。如果两台的DFS Group编号相同，则这两台设备配对成功。 Step2：配对成功后，两台设备会通过比较M-LAG协商报文中的DFS Group优先级确定出主备状态。以SwitchB为例，当SwitchB收到SwitchA发送的报文时，SwitchB会查看并记录对端信息，然后比较DFS Group的优先级，如果SwitchA的DFS Group优先级高于本端的DFS Group优先级，则确定SwitchA为M-LAG主设备，SwitchB为M-LAG从设备。如果SwitchA和SwitchB的DFS Group优先级相同，比较两台设备的MAC地址，确定MAC地址小的一端为M-LAG主设备。这里的主、备不影响正常的流量转发，只有在出现故障时才起作用。 Step3：协商出主备后，两台设备之间会通过网络侧链路周期性地发送M-LAG心跳报文，也可以配置通过管理网络检测心跳，当两台设备均能够收到对端发送的报文时，双活系统即开始正常的工作。M-LAG心跳报文主要用于peer-link故障时的双主检测。 Step4：正常工作后，两台设备之间会通过peer-link链路发送M-LAG同步报文实时同步对端的信息，M-LAG同步报文中包括MAC表项、ARP表项等，这样任意一台设备故障都不会影响流量的转发，保证正常的业务不会中断。 M-LAG双活系统建立成功后即进入正常的工作，M-LAG主备设备负载分担共同进行流量的转发。如果出现故障，无论是链路故障、设备故障还是peer-link故障，M-LAG都能够保证正常的业务不受影响。按照数据中心TOR与EOR之间连接要求，我们这里仅以交换机双归接入普通以太网络和IP网络为例进行介绍。 正常工作时，如下图所示，来自非M-LAG成员端口的单播流量，按照正常的转发流程进行转发，而来自M-LAG成员端口的单播流量按照M-LAG聚合设备负荷分担方式进行转发。 而来自非M-LAG成员接口的组播/广播流量 ，在SwitchA收到组播流量后向各个下一跳转发，当流量到达SwitchB时，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量不会向S-2转发。而来自M-LAG成员接口的组播/广播流量 ，在SwitchA收到组播流量后向各个下一跳转发，当流量到达SwitchB时，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量也不会向S-2转发。如下图所示： 对于网络侧发往M-LAG成员接口的单播流量，流量会负载分担到SwitchA和SwitchB，然后发送至双活接入的设备。对于网络侧发往非M-LAG成员接口的单播流量，以发往S-1为例，流量不会进行负载分担，而是直接发到SwitchA，由SwitchA发往S-1。而网络侧的组播/广播流量不会在SwitchA、SwitchB之间采用负载分担方式转发，此处以SwitchA转发为例进行说明。 以SwitchA为例，SwitchA会发送到每一个用户侧端口，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量不会向S-2转发。如下图： 故障情况时，如下图所示，当peer-link链路故障时，按照M-LAG应用场景不同，M-LAG的设备表现也不同。 当M-LAG应用于TRILL网络的双归接入时，peer-link故障但心跳状态正常会触发状态为备的设备上M-LAG接口处于Error-Down状态。当M-LAG应用于普通以太网络、VXLAN网络或IP网络的双归接入时，peer-link故障但心跳状态正常会触发状态为备的设备上除管理网口、peer-link接口和堆叠口以外的接口处于Error-Down状态。M-LAG主设备侧Eth-Trunk链路状态仍为Up，M-LAG备设备侧Eth-Trunk链路状态变为Down，双归场景变为单归场景。一旦peer-link故障恢复，处于Error-Down状态的M-LAG接口默认将在2分钟后自动恢复为Up状态，处于Error-Down状态的其它物理接口将自动恢复为Up状态。数据中心场景中接入交换机TOR上处于Error-Down状态的M-LAG接口默认在5分钟后自动恢复为Up状态。 当M-LAG主设备故障时，M-LAG备设备将升级为主，其设备侧Eth-Trunk链路状态仍为Up，流量转发状态不变，继续转发流量。 M-LAG主设备侧Eth-Trunk链路状态变为Down，双归场景变为单归场景。M-LAG主备设备端口状态默认不回切，即当M-LAG主设备故障发生时，M-LAG由备状态升级为主状态的设备仍保持主状态，恢复故障的主设备成为M-LAG的备设备。 如果是M-LAG备设备发生故障，M-LAG的主备状态不会发生变化，M-LAG备设备侧Eth-Trunk链路状态变为Down。M-LAG主设备侧Eth-Trunk链路状态仍为Up，流量转发状态不变，继续转发流量，双归场景变为单归场景。 当下行Eth-Trunk链路发生故障，M-LAG主备状态不会变化，流量切换到另一条链路上进行转发。发生故障的Eth-Trunk链路状态变为Down，双归场景变为单归场景。 当上行链路发生故障，由于双归接入普通以太网络时，心跳报文一般是走管理网络，故不影响M-LAG主设备的心跳检测，对于双活系统没有影响，M-LAG主备设备仍能够正常转发。如图中所示，由于M-LAG主设备的上行链路故障，通过M-LAG主设备的流量均经过peer-link链路进行转发。 双归接入IP网络的M-LAG设备在正常工作时，与双归接入普通以太网类似，唯一区别是在为了保证M-LAG上三层组播功能正常，需要在M-LAG主备设备间部署一条三层直连链路，部分组播流量可以通过该三层链路进行转发，这条链路一般称为C链。在故障场景下，双归接入IP网络的M-LAG设备与接入普通以太网也只在上行链路故障时，表现有所区别。 双归接入IP网络的M-LAG设备，在上行链路故障时，M-LAG备设备由于收不到主设备的心跳报文而变为双主状态。 这时用户侧流量到达SwitchA时，不能经过peer-link链路转发且没有可用的上行出接口，SwitchA会将用户流量全部丢弃。此时，需要分别在M-LAG主备设备上配置Monitor Link关联上行接口和下行接口，当上行出接口Down时，下行接口状态也会变为Down，这样就可以防止用户侧流量被丢弃。 M-LAG特性主要应用于将服务器或交换机双归接入普通以太网络、TRILL（Transparent Interconnection of Lots of Links）、VXLAN（Virtual eXtensible Local Area Network）和IP网络。一方面可以起到负载分担流量的作用，另一方面可以起到备份保护的作用。由于M-LAG支持多级互联，M-LAG的组网可以分为单级M-LAG和多级M-LAG。 在电信云数据中心中，为了保证可靠性，服务器一般采用链路聚合的方式接入网络，如果服务器接入的设备故障将导致业务的中断。为了避免这个问题的发生，服务器可以采用跨设备链路聚合的方式接入网络，在SwitchA与SwitchB之间部署M-LAG，实现服务器的双归接入。组成M-LAG的两台TOR形成负载分担，共同进行流量转发，当其中一台设备发生故障时，流量可以快速切换到另一台设备，保证业务的正常运行。服务器双归接入时的配置和一般的链路聚合配置没有差异，必须保证服务器侧和交换机侧的链路聚合模式一致，推荐两端均配置为LACP模式。除了接入交换机TOR组成M-LAG外，汇聚交换机EOR之间也需要部署M-LAG配置，与下层TOR的M-LAG进行级联，这样不仅可以简化组网，而且在保证可靠性的同时可以扩展双归接入服务器的数量。多级M-LAG互联必须基于V-STP方式进行配置，如下图所示。 V-STP方式下，组成M-LAG的设备在二层网络中可以不作为根桥，组网灵活且支持M-LAG的级联。V-STP功能还可以解决M-LAG中错误配置或错误连线导致的环路问题，所以推荐采用V-STP方式。 VRRP技术、堆叠以及M-LAG数据中心二、层设备虚拟化技术优缺点对比及适用场景总结 优点 缺点 应用场景 堆叠 简化本地网络管理节点、易维护，同时增加系统端口密度和带宽，充分发挥设备性能。 缺点就是可靠性不高，堆叠系统分裂、系统升级都会影响业务中断 广泛应用于企业、教育 VRRP 网络设备冗余、可靠性高 配置复杂、网络建设投入成本高，不能充分发挥设备的网络性能 银行、证券、政府内网 M-LAG 网络可靠性非常高，设备控制层面独立，能单独设备升级且不影响业务，充分发挥设备性能 管理节点多（控制层面无法虚拟化） 银行、证券、数据中心（双规场景） 大二层网络的基石VxLAN大二层是个什么鬼？虽然Leaf-Spine为无阻塞传输提供了拓扑的基础，但是还需有配套合适的转发协议才能完全发挥出拓扑的能力。STP的设计哲学与Leaf-Spine完全就是不相容的，冗余链路得不到利用，灵活性和扩展性极差，废除STP而转向“大二层”成了业界的基本共识。 说到大二层的“大”，首先体现在物理范围上。虚拟机迁移是云数据中心的刚性需求，由于一些License与MAC地址是绑定的，迁移前后虚拟机的MAC地址最好不变，同时为了保证业务连续，迁移前后虚拟机的IP地址也不可以变化。迁移的位置是由众多资源因素综合决定的，网络必须支持虚拟机迁移到任何位置，并能够保持位于同一个二层网络中。所以大二层要大到横贯整个数据中心网络，甚至是在多个数据中心之间。 大二层的“大”，还意味着业务支撑能力的提升。随着公有云的普及，“多租户”成了云数据中心网络的基础能力。而传统二层网络中，VLAN最多支持的租户数量为4096，当租户间IP地址重叠的时候，规划和配置起来也比较麻烦，因此VLAN不能很好地支撑公有云业务的飞速发展。同理，对于大规模私有云而言，VLAN也难以胜任其对于网络虚拟化提出的要求。 大二层网络的实现主要依赖于网络叠加技术。网络叠加技术指的是一种物理网络架构上叠加的虚拟化技术模式，其大体框架是对基础网络不进行大规模修改的条件下，实现应用在网络上的承载，并能与其他网络业务分离，以基于IP的基础网络技术为主。其实这种模式是以对传统技术的优化而形成的。早期就有标准支持的二层Overlay技术，如RFC3378（Ethernet in IP），并且基于Ethernet over GRE的技术。H3C与思科都在物理网络基础上分别发展了私有二层Overlay技术：EVI（Ethernet Virtual Interconnection）与OTV（Overlay Transport Virtualization）。主要用于解决数据中心之间的二层互联与业务扩展问题，并且对于承载网络的基本要求是IP可达，部署上简单且扩展方便。 网络叠加技术可以从技术解决目前数据中心面临的三个主要问题： 1）解决了虚拟机迁移范围受到网络架构限制的问题。网络叠加是一种封装在IP报文之上的新的数据格式，因此，这种数据可以通过路由的方式在网络中分发，而路由网络本身并无特殊网络结构限制，具备良性大规模扩展能力，并且对设备本身无特殊要求，且具备很强的故障自愈能力、负载均衡能力。 2）解决了虚拟机规模受网络规格限制的问题。虚拟机数据封装在IP数据包中后，对网络只表现为封装后的网络参数，即隧道端点的地址，因此，对于承载网络（特别是接入交换机），MAC地址规格需求极大降低，最低规格也就是几十个（每个端口一台物理服务器的隧道端点MAC）。但是，对于核心/网关处的设备表项（MAC/ARP）要求依然极高，当前的解决方案是采用分散方式，即通过多个核心/网关设备来分散表项的处理压力。 3）解决了网络隔离/分离能力限制的问题。针对VLAN数量在4000以内的限制，在网络叠加技术中沿袭了云计算“租户”的概念，称之为Tenant ID（租户标识），用24或64比特表示，可以支持16M的虚拟隔离网络划分。针对VLAN技术下网络的TRUANK ALL（VLAN穿透所有设备）的问题，网络叠加对网络的VLAN配置无要求，可以避免网络本身的无效流量带宽浪费，同时网络叠加的二层连通基于虚拟机业务需求而创建，在云的环境中全局可控。 Overlay网络常用技术协议目前，IETF在Overlay技术领域有如下三大技术路线正在讨论：VxLAN（Virtual eXtensible Local Area Network），是由VMware、思科、Arista、Broadcom、Citrix和红帽共同提出的IETF草案，是一种将以太网报文封装在UDP传输层上的隧道转发模式，目的UDP端口号为4798。NVGRE（Network Virtualization using Generic Routing Encapsulation），是微软、Dell等提出的草案，是将以太网报文封装在GRE内的一种隧道转发模式。STT（Stateless Transport Tunneling），是Nicira公司提出的一种Tunnel技术，目前主要用于Nicira自己的NVP平台上。STT利用了TCP的数据封装形式，但改造了TCP的传输机制，数据传输遵循全新定义的无状态机制，无需三次握手，以太网数据封装在无状态TCP中。三种网络叠加技术的优缺点和数据封装格式如下： 这三种二层网络叠加技术，大体思路均是将以太网报文承载到某种隧道层面，其差异性在于选择和构造隧道的不同，而底层均是IP转发。VxLAN和STT对于现网设备的流量均衡要求较低，即负载链路负载分担适应性好，一般的网络设备都能对L2~L4的数据内容参数进行链路聚合或等价路由的流量均衡，而NVGRE则需要网络设备对GRE扩展头感知并对flow ID进行Hash，需要硬件升级；STT对于TCP有较大修改，隧道模式接近UDP性质，隧道构造技术具有革新性，且复杂度较高，而VxLAN利用了现有通用的UDP传输，成熟性极高。总体来说，VLXAN技术相对具有优势。 VxLAN的包封装格式VXLAN报文是在原始的二层报文前面再封装一个新的报文，新的报文中和传统的以太网报文类似，拥有源目mac、源目ip等元组。当原始的二层报文来到vtep节点后会被封装上VXLAN包头（在VXLAN网络中把可以封装和解封装VXLAN报文的设备称为vtep，vtep可以是虚拟switch也可以是物理switch），打上VXLAN包头的报文到了目标的vtep后会将VXLAN包头解封装，并获取原始的二层报文。 1）VXLAN头部。共计8个字节，目前使用的是Flags中的一个8bit的标识位和24bit的VNI（Vxlan Network Identifier），其余部分没有定义，但是在使用的时候必须设置为0x0000。 2）外层的UDP报头。目的端口使用4798，但是可以根据需要进行修改，同时UDP的校验和必须设置成全0。 3）IP报文头。目的IP地址可以是单播地址，也可以是多播地址。单播情况下，目的IP地址是VTEP（Vxlan Tunnel End Point）的IP地址；多播情况下引入VxLAN管理层，利用VNI和IP多播组的映射来确定VTEPs。protocol设置值为0x11，说明这是UDP数据包。Source ip是源vTEP_IP。Destination ip是目的VTEP IP。 4）Ethernet Header。Destination Address：目的VTEP的MAC地址，即为本地下一跳的地址（通常是网关MAC地址）。VLAN Type被设置为0x8100，并可以设置Vlan Id tag（这就是vxlan的vlan标签）。Ethertype设置值为0x8000，指明数据包为IPv4的。 outer mac header以及outer ip header里面的相关元组信息都是vtep的信息，和原始的二层报文没有任何关系。所在数据包在源目vtep节点之间的传输和原始的二层报文是毫无关系的，依靠的是外层的包头完成。 除此之外还有几个字段需要关注： 1、隧道终端VTEP用于多VxLAN报文进行封装/解封装，包括MAC请求报文和正常VXLAN数据报文，在一端封装报文后通过隧道向另一端VTEP发送封装报文，另一端VTEP接收到封装的报文解封装后根据被封装的MAC地址进行转发，VTEP可由支持VXLAN的硬件设备或软件来实现。 2、在UDP header里面有一个source port的字段，用于VxLAN网络节点之间ECMP的hash； 3、在VXLAN Header里的reserved字段，作为保留字段，很多厂商都会加以运用来实现自己组网的一些特性。 VxLAN的控制和转发平面1）数据平面—隧道机制 VTEP为虚拟机的数据包加上了层包头，这些新的报头只有在数据到达目的VTEP后才会被去掉。中间路径的网络设备只会根据外层包头内的目的地址进行数据转发，对于转发路径上的网络来说，一个Vxlan数据包跟一个普通IP包相比，除了个头大一点外没有区别。由于VxLAN的数据包在整个转发过程中保持了内部数据的完整，因此VxLAN的数据平面是一个基于隧道的数据平面。 2）控制平面—改进的二层协议 VxLAN不会在虚拟机之间维持一个长连接，所以VxLAN需要一个控制平面来记录对端地址可达情况。控制平面的表为（VNI，内层MAC，外层vtep_ip）。Vxlan学习地址的时候仍然保存着二层协议的特征，节点之间不会周期性的交换各自的路由表，对于不认识的MAC地址，VXLAN依靠组播来获取路径信息(如果有SDN Controller，可以向SDN单播获取)。 另一方面，VxLAN还有自学习的功能，当VTEP收到一个UDP数据报后，会检查自己是否收到过这个虚拟机的数据，如果没有，VTEP就会记录源vni/源外层ip/源内层mac对应关系，避免组播学习。 VxLAN的报文转发1）ARP报文转发，转发过程如下图： Step1：主机A向主机B发出ARP Request，Src MAC为MAC-A，Dst MAC为全F； Step2：ARP Request报文到达vtep-1后，vtep-1对其封装VxLAN包头，其中外层的Src MAC为vtep-1的MAC-1，Dst MAC为组播mac地址， Src ip为vtep-1的IP-1，Dst ip为组播ip地址，并且打上了VxLAN VNID：10。由于vtep之间是三层网络互联的，广播包无法穿越三层网络，所以只能借助组播来实现arp报文的泛洪。通常情况下一个组播地址对应一个VNID，同时可能会对应一个租户或者对应一个vrf网络，通过VNID进行租户之间的隔离。 Step3：打了VxLAN头的报文转发到了其他的vtep上，进行VxLAN头解封装，原始的ARP Request报文被转发给了vtep下面的主机，并且在vtep上生成一条MAC-A（主机A的mac）、VxLAN ID、IP-1（vtep-1的ip）的对应表项； Step4：主机B收到ARP请求，回复ARP Response，Src MAC：MAC-B、Dst MAC：MAC-A； Step5：ARP Response报文到达vtep-2后，被打上VxLAN的包头，此时外层的源目mac和ip以及VxLAN ID是根据之前在vtep-2上的MAC-A、VXLAN ID、IP-1对应表项来封装的，所以ARP Response是以单播的方式回复给主机A； Step6：打了VxLAN头的报文转发到vtep-1后，进行VxLAN头的解封装，原始的ARP Response报文被转发给了主机A； Step7：主机A收到主机B返回的ARP Response报文，整个ARP请求完成。 这种用组播泛洪ARP报文的方式是VxLAN技术早期的方式，这种方式也是有一些缺点，比如产生一些不可控的组播流量等，所以现在很多厂商已经使用了控制器结合南向协议（比如openflow或者一些私有南向协议）来解决ARP的报文转发问题。 2）单播报文转发（同一个Vxlan），转发过程如下： 在经过arp报文后，vtep-1和vtep-2上都会形成一个VxLAN二层转发表，大致如下（不同厂商表项可能略有不同，但是最主要的是以下元素）： vtep-1： MAC VNI vtep MAC-A 10 e1/1 MAC-B 10 vtep-2 ip vtep-2： MAC VNI vtep MAC-B 10 e1/1 MAC-A 10 vtep-1 ip Step1：host-A将原始报文上行送到vtep-1； Step2：根据目的mac和VNI的号（这里的VNI获取是vlan和vxlan的mapping查询出的结果），查找到外层的目的ip是vtep-2 ip，然后将外层的源目ip分别封装为vtep-1 ip和vtep-2 ip，源目mac为下一段链路的源目mac； Step3：数据包穿越ip网络； Step4：vtep-2根据VNI、外层的源目ip，进行解封装，通过VNI和目的mac查表，得到目的端口是e1/1； Step5：host-B接受此原始报文，并回复host-A，回复过程同上一致。 3）不同Vxlan之间，VxLAN与VLAN之间转发 不同VxLAN之间转发，VxLAN与VLAN之间转发，各个厂商的解决方案不大一致，一般情况下所以跨VxLAN的转发需要一个叫VxLAN网关的设备，这个VxLAN网关可以是物理交换机也可以是软件交换机。具体如下图： 另外cisco对不同vxlan之间的转发又是另外一种模式，由于cisco是一家硬件设备厂商，所以它的所有vtep都是硬件交换机，但是通常物理交换机收到的都是报文已经是Hypervisor层打了vlan tag了，所以cisco的处理方式会比较复杂，大体上是通过一个L3的VNI来完成。 VxLAN和VLAN之间的转发这部分，说实话我也想不到太多实际的应用场景，如果是一个L3的网络更是没有意义，因为VLAN的网关终结在了leaf节点上，唯一想到的场景可能就是L2网络中部分设备不支持打VxLAN只能打VLAN Tag（硬件overlay一般不会出现这个情况，软件overlay情况下，会有一些物理服务器没有vswitch来打vxlan tag，只能依靠硬件交换机来打vlan tag）。这种情况下是需要通过VxLAN网关设备来完成一个VxLAN到VLAN或者VLAN到VxLAN的mapping关系。这其实很好理解，就是比如一个打了VxLAN包头的报文要去访问一个某一个VLAN，这时候报文需要被先送到一个叫VxLAN网关设备，这个设备查找VxLAN和VLAN的对应表后，将VxLAN包头拆去，再打上内层的VLAN Tag，然后将数据包送给VLAN的网络中。同理，VLAN到VxLAN网络反之处理类似。 典型的VXLAN组网模式1）软件模式： 软件模式中vtep功能由vswitch实现，图中物理交换网络是一个L3的网络，实际软件overlay的场景下物理的交换网并不一定要是一个L3的网络，只要物理服务器的ip互相可达即可，当然L3的网络是一个比较好的选择，因为L3的网络扩展性比L2好，L2网络一大就会有各种二层问题的存在，比如广播泛洪、未知单播泛洪，比如TOR的mac问题，这个前文已经讲了很多，这里就不赘述了。 优点： 1）硬件交换机的转发平面和控制平面解耦，更加灵活，不受物理设备和厂商的限制； 2）现有的硬件网络设备无需进行替换。（如果现有物理设备是L2的互联网络也可以使用软件overlay的方式，当然L3互联更佳，overlay的网络还是建议L3的方式） 缺点： 1）vSwitch转发性能问题，硬件交换机转发基本都是由ASIC芯片来完成，ASIC芯片是专门为交换机转发而设计的芯片，而vSwitch的转发是由x86的CPU来完成，当vSwitch作为vtep后并且增加4-7层服务外加分布式路由的情况下，性能、可靠性上可能会存在问题； 2）无法实现虚拟网络和物理服务器网络的共同管理； 3）如果是用商业的解决方案，则软件层面被厂商绑死，如果选用开源或自研的vswitch，存在一定的难度和风险。 4）软件overlay容易造成管理上的混乱，特别对于传统行业来说，overlay网络到底是网络运维管还是云平台运维管，会有一些利益冲突。不过这个并不是一个技术问题，在云计算环境下，类似这种问题会越来越多。 案例：互联网公司用软件overlay的还挺多，下面附上几个的方案简介，设计一些商业解决方案和一些开源的解决方案： UCloud的“公有云SDN网络实践”分享 浙江电信云资源池引入VxLAN的部署初探（NSX） 2）硬件模式： 硬件模式中vtep功能由物理交换机来实现，硬件overlay解决方案中物理网络一般都是L3的网络。 优点： 1）硬件交换机作为vtep转发性能比较有保证； 2）虚拟化网络和物理服务器网络可以统一管理； 3）向下兼容各种的虚拟化平台（商业或开源）。 缺点： 1）各硬件交换机厂商的解决方案难有兼容性，可能会出现被一个厂商锁定的情况； 2）老的硬件交换机基本上芯片都不支持VxLAN技术，所以要使用vxlan组网，会出现大规模的设备升级； 案例：硬件overlay主要是Cisco、华为、H3C这种传统的硬件厂商提供解决方案，下面链接一个华为的的案例： 美团云携手华为SDN解决方案 3）软硬件混合模式 软硬件混合模式是一种比较折中的解决方案，虚拟化平台上依然使用vSwitch作为vtep，而对于物理服务器使其接入物理的vtep交换机。 优点：整合了软、硬件模式的优缺点，是一种比较理想的overlay网络模型； 缺点：落地起来架构比较复杂，统一性比较差，硬件交换机和软件交换机同时作为vtep，管理平台上兼容性是个问题。 案例：这个解决方案目前一些硬件的交换机厂商会提供（比如华为），还有就是一些研发能力比较强的互联网公司会选择这个方案。 网络叠加技术作为网络虚拟化在数据层的实现手段，解决了虚拟机迁移范围受到网络架构限制、虚拟机规模受网络规格限制、网络隔离/分离能力限制的问题。同时，支持网络叠加的各种协议、技术正不断演进，VxLAN作为一种典型的叠加协议，最具有代表性。Linux内核3.7已经加入了对VxLAN协议的支持。另外，由IETF工作组提出的网络虚拟化叠加（NVO3）草案也在讨论之中，各大硬件厂商也都在积极参与标准的制定并研发支持网络叠加协议的网络产品，这些都在推动着软件定义网络SDN技术在云数据中心各个业务领域逐步落地。 Spline-leaf架构下的SDN引入从 2003 年开始，随着虚拟化技术的引入，原来三层（three-tier）数据中心中，在二层以pod形式做了隔离的计算、网络和存储资源，形成一种池化的组网模式。这种技术产生了从接入层到核心层的大二层域的需求，如下图所示 。 虚拟机的引入，使得应用的部署方式越来越分布式，导致东西向流量越来越大。这些流量需要被高效地处理，并且还要保证低的、可预测的延迟。 然而，vPC只能提供两个并行上行链路，因此三层数据中心架构中的带宽成为了瓶颈。 三层架构的另一个问题是服务器到服务器延迟（server-to-server latency）随着流量路径的不同而不同。 针对以上问题，提出了一种新的数据中心设计，称作基于Clos网络的Spine-and-Leaf架构（Clos network-based Spine-and-Leaf architecture），如下图所示。事实已经证明，这种架构可以提供高带宽、低延迟、非阻塞的服务器到服务器连接，成为超大规模网络首选。 在以上两级Clos架构中，每个低层级的交换机（leaf）都会连接到每个高层级的交换机（spine），形成一个full-mesh拓扑。leaf 层由接入交换机组成，用于连接服务器等设备。spine层是网络的骨干，负责将所有的leaf连接起来。 fabric中的每个leaf都会连接到每个spine，如果一个spine挂了，数据中心的吞吐性能只会有轻微的下降。如果某个链路拥塞了，添加一个spine交换机就可以扩展每个leaf的上行链路，增大了leaf和spine之间的带宽，缓解了链路拥塞的问题。如果接入层的端口数量成为了瓶颈，那就直接添加一个新的leaf，然后将其连接到每个spine并做相应的配置即可。leaf层的接入端口和上行链路都没有瓶颈时，这个架构就实现了无阻塞。 在Spine-and-Leaf架构中，任意一个服务器到另一个服务器的连接，都会经过相同数量的设备（除非这两个服务器在同一leaf下面），这保证了延迟是可预测的，因为一个包只需要经过一个spine和另一个leaf就可以到达目的端。 在数据中心的实际应用中，Spline-and-leaf架构又分为分层解耦架构和全融合架构两种，如下图所示。 在分层解耦架构中，业务功能模块化，由不同的叶子节点分别实现。各节点可灵活合并和解耦，Pod分区具备灵活扩展能力。适用于东西流量预期有显著增长、数据中心规模大、未来高扩展的场景，比如：中国移动私有云部署场景。 在全融合架构中，业务功能集中由一对核心交换机实现，属于接入+核心的二层架构。这种架构中，网络设备少，流量模型简单，维护简单，但扩展性较差。适用于中小型数据中心、DC规划可预见/扩展性低的场景，比如：各个中小企业的私有云部署场景。 在数据中心中，到底使用大二层还是Spline-and-leaf解决方案，是由服务器的规模决定的，二者之间的区别只是服务器接入能力不同，并不存在谁比谁更先进一说。 大二层架构是由核心层+接入层共两层组成，两台核心交换机做堆叠，作为网关，FW、LB等设备旁挂在网关边上。接入层TOR两两堆叠，可提供2000+台物理服务的接入能力。 Spline-and-leaf架构是由核心层+汇聚层+接入层共三层组成，两台核心交换机做堆叠，作为网关，FW、LB等设备旁挂在网关边上。Spine层交换机数量为2的倍数，相互独立，不做堆叠。接入层TOR两两堆叠，可提供2000~5000台物理服务的接入能力。 中国移动私有云SDN解决方案技术规范中，对数据中内部的组网架构图要求如下（电信云的要求类似，多了网管CE的部署要求）。 虚拟化/裸金属服务器混合场景优先采用混合SDN组网方式，即硬件接入交换机（SDN ToR）和VSW作为VTEP。 SDN控制器或控制器插件支持南向数据一致性校验和对账。 SDN控制器支持allowed-address-pairs功能。 控制器支持OpenFlow+Evpn流表和路由相互转换。 支持IPv4/IPv6双栈网络服务的自动化部署。 支持vFW上IPv6防火墙策略的自动化部署。 Server Leaf和Border leaf采用MC-LAG方式组网，并且由控制器下发MC-LAG相关业务。 上述技术规范中，EVPN+OpenFlow双控制面是未来演进方向。相比OpenFlow单控制面来说，区别如下图所示。 在单控制面OpenFlow组网中，SDN控制器通过OpenFlow给虚拟交换机和物理交换机统一下发流表，虚拟交换机和物理交换机按照流表进行转发。优点就是OpenFlow灵活性高，SDN控制器和转发设备间可实现解耦。但是缺点很明显，也就是SDN控制器成为瓶颈，收敛性能和稳定性欠佳，不适合大规模资源池网络，最关键的是和已有IP网络的兼容性不好，如检测等特性。 在双控制面EVPN+OpenFlow组网中，SDN控制器通过OpenFlow给虚拟交换机下发流表，物理交换机之间通过BGP-EVPN协议建立转发表项。优点就是可靠性高，兼具OpenFlow的灵活性和EVPN的扩展性，符合未来演进方向，也减少现有设备投资改造费用。缺点就是对控制器要求较高，需要控制器支持EVPN协议，具备OpenFlow流表和EVPN转发表的翻译能力，而且这属于设备商的私有接口部分，只有少数几个厂家支持，比如，华为。 在实际多机房部署时，具体规划如下图所示 各个机房通过东西向互联交换机互联。 每个机房部署一套SDN控制器和云平台，采用硬件分布式或混合分布式SDN方案。 核心生产区、测试区和DMZ区的服务器统一挂在业务TOR下面。 所有的SDN控制器和云平台统一部署在管理网络区（POD1内）。 FC SAN分散部署在各个机房内，分布式存储集中部署在POD7。 在每个机房内部署东西向防火墙，Internet出口部署南北向防火墙。 以上就是数据中心层面服务器外部交换网络虚拟化的全部内容，至此网络虚拟化技术理论方面的知识点全部介绍完毕，后续会用一篇Linux原生网络虚拟化实战作为全部虚拟化专题的结束篇。可能有人会提到SDN还没讲，一方面是因为目前还在试点，另一方面是因为SDN涉及面很广，有时间的话我会写个专题，从协议、控制器部署、实际解决方案等专门来介绍SDN。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-21-KVM到底是个啥？]]></title>
    <url>%2F2019%2F06%2F22%2F2019-06-21-KVM%E5%88%B0%E5%BA%95%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[KVM的现状KVM最初是由Qumranet公司的Avi Kivity开发的，作为他们的VDI产品的后台虚拟化解决方案。为了简化开发，Avi Kivity并没有选择从底层开始新写一个Hypervisor，而是选择了基于Linux kernel，通过加载模块使Linux kernel本身变成一个Hypervisor。2006年10月，在先后完成了基本功能、动态迁移以及主要的性能优化之后，Qumranet正式对外宣布了KVM的诞生。同月，KVM模块的源代码被正式纳入Linux kernel，成为内核源代码的一部分。2008年9月4日，Redhat公司以1.07亿美元收购了Qumranet公司，包括它的KVM开源项目和开发人员。自此，Redhat开始在其RHEL发行版中集成KVM，逐步取代Xen，并从RHEL7开始，正式不支持Xen。 在KVM出现之前，Xen虚拟化解决方案已经业界比较成熟的一款开源VMM，但是KVM出现之后，很快被Linux内核社区接受，就是因为Xen不是通过Linux内核去管理系统资源（硬件/软件），而是通过自身的管理系统去完成，仅这一点就让Linux内核社区很不爽。同时，Xen在当时设计上采用半虚方式，需要修改Guest OS的内核来满足I/O驱动性能要求，从而不支持商用OS（Windows、Mac OS）的虚拟化，使得Xen相比KVM来说，在硬件辅助虚拟化的支撑上包袱更重，转型困难重重。 目前，KVM已经成为OpenStack用户选择的事实上的Hypervisor标准。OpenStack自身调查数据显示，KVM占87%以上的部署份额，详细参见http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014/。可以说，KVM已经主导公有云部署的Hypervisor市场，同时在电信云部署方面也是一枝独秀。 功能上，虚拟化发展到今天，各个Hypervisor的主要功能都差不多。KVM由于其开源性，反而商业上的限制较少。性能上，KVM和Xen都能达到原生系统95%以上的效率（CPU、内存、网络、磁盘等benchmark衡量），KVM甚至还略微好过Xen一点点。微软虽然宣布其Hype-V的性能更好，但这只是微软一家之言，并没有公开的数据支撑。 在电信云NFV领域来说，由于通信网络设备的实时性要求很高，且NFV的开源平台OPNFV选择了OpenStack。为了更好实现网络功能虚拟化的愿景，其实时性要求责无旁贷的落到了KVM头上，NFV-KVM项目也就顺理成章地诞生了，它作为OPNFV的子项目主要解决KVM在实时性方面受到的挑战。详情请参见https://wiki.opnfv.org/display/kvm/Nfv-kvm。 总的来说，虚拟化技术发展到今天已经非常成熟，再加上DPDK代码的开源化、KVM也好、其他Hypervisor也好，在转发性能的优化和硬件辅助虚拟化的支撑上都半斤八两，但由于KVM的开源性特性以及社区的热度，使得其在云计算领域解决方案一枝独秀。甚至，华为在其FusionSphere6.3版本也开始拥抱KVM而抛弃了Xen。要知道，华为在剑桥大学可是专门有一支团队在研究Xen虚拟化。 KVM虚拟化实现KVM全称是Kernel-based Virtual Machine，即基于内核的虚拟机，是采用硬件辅助虚拟化技术的全虚拟化解决方案。对于I/O设备（如硬盘、网卡等），KVM即支持QEMU仿真的全虚，也支持virtio方式的半虚。KVM从诞生开始就定位于基于硬件虚拟化支持的全虚实现，由于其在Linux内核2.6版本后被集成，通过内核加载模式使得Linux内核变成一个事实上的Hypervisor，但是硬件管理还是由Linux Kernel来完成。因此，它是一个典型Type 2型虚拟化，如下图所示。 如上图，一个KVM客户机就对应一个Linux进程，每个vCPU对应这个进程下的一个线程，还有单独处理I/O的线程，属于同一个进程组（忘了的，请回顾本站CPU虚拟化系列文章）。所以，宿主机上Linux Kernel可以像调度普通Linux进程一样调度KVM虚拟机，这种机制使得Linux Kernel的进程优化和调度功能优化等策略，都能用于KVM虚拟机。比如：通过进程权限限定功能可以限制KVM客户机的权限和优先级等。 由于KVM嵌入Linux内核中，除了硬件辅助虚拟化（如VT-d）透传的硬件设备能被虚拟机看见外，其他的I/O设备都是QEMU模拟出来的，所以QEMU是KVM的天生好基友。而在内存管理方面，由于KVM本身就是Linux Kernel中一个模块，所以其内存管理完全依赖于Linux内核。Linux系统的所有内存管理机制，如大页、零页（重复页）共享KSM、NUMA、mmap共享内存等，都可以用于KVM虚拟机的内存管理上。 在数据存储方面，由于KVM是Linux Kernel的一部分，它可以利用所有存储厂商的存储架构，支持Linux支持的任何存储和文件系统来存储数据。同时，还支持全局文件系统GFS2等共享文件系统上的虚拟机镜像，允许虚拟机在多个Host之间共享存储或使用逻辑卷共享存储。KVM虚拟机的原生磁盘格式为QCOW2，支持磁盘镜像、快照、多级快照和压缩加密等虚拟化特性。但是，有一点需要注意：基于KVM的镜像盘必须使用RAW格式（一种非精简格式盘，在华为FC解决方案中称为普通盘。它就像我们的物理硬盘一样，分配多大空间就是多大空间），否则利用镜像发布虚机会不成功。 在实时迁移方面，KVM虚拟机支持在多个Host之间热迁移，无论是OVS分布式虚拟交换机，还是Linux Bridge分布式虚拟交换机，KVM虚拟机都能完美兼容实现虚拟接入，且对用户（实际用户，APP等，也就是我们常说的租户这个概念）是透明的。同时，还支持将客户机当前状态，也就是快照保存到磁盘，并在以后恢复。 在设备驱动方面，KVM支持混合虚拟化，其中半虚拟化的驱动程序安装在虚拟机的OS中，允许虚拟机使用优化I/O接口而不用模拟设备。KVM使用的半虚拟化驱动程序是IBM和RedHat联合Linux社区开发的virtio标准，它是一个与Hypervisor独立的，构建设备驱动程序的接口，是KVM内核的另一个好基友，不仅支持KVM对其调用，还支持VMware、Hyper-V对其调用。同时，就像前面提到的，KVM也支持VT-d技术，两者如胶似漆，完美契合，通过将Host上PCI总线上的设备透传给虚拟机，让虚拟机可以直接使用原生的驱动程序来驱动这些物理设备。忘了啥是VT-d的，请回顾本站I/O虚拟化一文。同时，就像前面开篇提到的KVM在在性能方面能达到原生的95%以上，不差于Xen虚拟化，但在伸缩性方面支持拥有多达288个vCPU和4TB RAM的虚拟机，远超于Xen（Xen因为DM0的存在，其伸缩性受限，单机最大支持32个vCPU，1.5TB RAM）。 通过上面大致了解，可以说KVM就是在硬件辅助虚拟化技术之上构建起来的VMM。但并非要求所有硬件虚拟化技术都支持才能运行KVM虚拟化，KVM对硬件最低的依赖是CPU的硬件虚拟化支持（比如：Intel的VT-x技术和AMD的AMD-V技术），而其他的内存和I/O的硬件虚拟化支持，会让整个KVM虚拟化下的性能得到更多的提升。所以，我们在虚拟机部署KVM功能时，首先就是要查看宿主机Host（也就是我们实验环境的虚拟机，一般是在VMware Workstations的虚拟机打开CPU虚拟化功能。如果是真实环境，则需要物理服务器的BIOS中要开启VT-x功能，如下图所示。 然后，进入服务器后通过如下指令确认VT-x功能支持，如下图所示。 如果什么输出都没有，那说明你的系统并没有支持虚拟化的处理 ，不能使用KVM。另外Linux发行版本必须在64bit环境中才能使用KVM。 KVM软件架构拆解KVM是在硬件虚拟化支持下的全虚拟化技术，所以它能在相应硬件上运行几乎所有的操作系统，如：Linux、Windows、FreeBSD、MacOS等。KVM虚拟化的核心主要由以下两个模块组成： 1）KVM内核模块，它属于标准Linux内核的一部分，是一个专门提供虚拟化功能的模块，主要负责CPU和内存的虚拟化，包括：客户机的创建、虚拟内存的分配、CPU执行模式的切换、vCPU寄存器的访问、vCPU的执行。 2）QEMU用户态工具，它是一个普通的Linux进程，为客户机提供设备模拟的功能，包括模拟BIOS、PCI/PCIE总线、磁盘、网卡、显卡、声卡、键盘、鼠标等。同时它通过ioctl系统调用与内核态的KVM模块进行交互。 上图中，在KVM虚拟化架构下，每个客户机就是一个QEMU进程，在一个宿主机上有多少个虚拟机就会有多少个QEMU进程。客户机中的每一个虚拟CPU对应QEMU进程中的一个执行线程，一个宿主机Host中只有一个KVM内核模块，所有虚拟机都与这个内核模块进行交互。 KVM内核模块KVM内核模块是KVM虚拟化的核心模块，它在内核中由两部分组成：一个是处理器架构无关的部分，用lsmod命令中可以看到，叫作kvm模块；另一个是处理器架构相关的部分，在Intel平台上就是kvm_intel这个内核模块。如下图所示，KVM的主要功能是初始化CPU硬件，打开虚拟化模式，然后将虚拟机运行在虚拟环境下，并对虚拟机的运行提供一定的支持。 KVM仅支持硬件辅助的虚拟化，所以，“打开并初始化系统硬件以支持虚拟机的运行”是KVM模块本职工作。以Intel CPU架构服务器为例，KVM打开并初始化硬件以支持虚拟机运行的过程如下： Step1：在被内核加载的时候，KVM模块会先初始化内部的数据结构。 Step2：做好准备之后，KVM模块检测系统当前的CPU，然后打开CPU控制寄存器CR4中的虚拟化模式开关，并通过执行VMXON指令将宿主操作系统（包括KVM模块本身）置于CPU虚拟化模式中的根模式root operation，详细参见本站计算虚拟化之CPU虚拟化一文。 Step3：KVM模块创建特殊设备文件/dev/kvm并等待来自用户空间的命令。 Step4：后续虚拟机的创建和运行，其本质上就是是一个用户空间的QEMU和内核空间的KVM模块相互配合的过程。 这里面的/dev/kvm这个设备比较关键，它可以被当作一个标准的字符设备，用来缓存用户空间与内核空间切换的上下文，也就是ioctl调用上下文，是KVM模块与用户空间QEMU的通信接口。 针对/dev/kvm文件的最重要的loctl调用就是“创建虚拟机”。这里的创建虚拟机，简单理解就是KVM为了某个虚拟机创建对应的内核数据结构，并且返回一个文件句柄来代表所创建的虚拟机。针对该文件句柄的loctl调用可以对虚拟机做相应的管理，比如创建用户空间虚拟地址和客户机物理地址及真实内存物理地址的映射关系，再比如创建多个可供运行的vCPU。KVM模块同样会为每一个创建出来的vCPU生成对应的文件句柄，对其文件句柄进行相应的loctl调用，就可以对vCPU进行调度管理。 而针对vCPU的最重要的loctl调用就是“运行虚拟处理器”。通过它，虚拟机就可以在非根模式下运行，一旦执行敏感指令，就通过VMX Exit切入根模式，由KVM决定后续的操作并返回执行结果给虚拟机。 除了处理器虚拟化，内存虚拟化的实现也是由KVM内核模块完成，包括影子页表和EPT硬件辅助，均由KVM内核模块负责完成GVA—&gt;HPA的两级转换。处理器对设备的访问主要是通过I/O指令和MMIO，其中I/O指令会被处理器直接截获，MMIO会通过配置内存虚拟化来捕捉。一般情况下，除非对外设有极高性能要求，比如虚拟中断和虚拟时钟，外设均是由QEMU这个用户空间的进程来模拟实现。 QEMU用户态设备模拟QEMU原本就是一个著名的开源虚拟机软件项目，既是一个功能完整的虚拟机监控器，也在QEMU-KVM的软件栈中承担设备模拟的工作。它是由一个法国工程师独立编写的代码实现，并不是KVM虚拟化软件的一部分，但是从名字就能知道它和KVM是一辈子的好基友。 QEMU最初实现的虚拟机是一个纯软件的实现，也就是我们常说的通过二进制翻译来实现虚拟机的CPU指令模拟，所以性能比较低。但是，其优点是跨平台，甚至可以支持客户机与宿主机并不是同一个架构，比如在x86平台上运行ARM客户机。同时，QEMU能与主流的Hypervisor完美契合，包括：Xen、KVM、Hyper-v，以及VMware各种Hypervisor等，为上述这些Hypervisor提供虚拟化I/O设备。 而QEMU与KVM密不可分的原因，就是我们常说的QEMU-KVM软件协议栈。虚拟机运行期间，QEMU会通过KVM内核模块提供的系统调用ioctl进入内核，由KVM内核模块负责将虚拟机置于处理器的特殊模式下运行。一旦遇到虚拟机进行I/O操作时，KVM内核模块会从上次的系统调用ioctl的出口处返回QEMU，由QEMU来负责解析和模拟这些设备。除此之外，虚拟机的配置和创建，虚拟机运行依赖的虚拟设备，虚拟机运行时的用户操作环境和交互，以及一些针对虚拟机的特殊技术，比如动态迁移，都是由QEMU自己实现的。 QEMU除了提供完全模拟的设备以外，还支持virtio协议的设备模拟。在前端虚拟机中需要安装相应的virtio-blk、virtio-scsi、virtio-net等驱动，就能连接到QEMU实现的virtio的虚拟化后端。除此之外，QEMU还提供了virtio-blk-data-plane的高性能的块设备I/O方式，与传统virtio-blk相比，它为每个块设备单独分配一个线程用于I/O处理，不需要与原QEMU执行线程同步和竞争锁，而且使用ioeventfd/irqfd机制，利用宿主机Linux上的AIO（异步I/O）来处理客户机的I/O请求，使得块设备I/O效率进一步提高。 总之，在KVM虚拟化的软件架构中，KVM内核模块与QEMU用户态程序是处于最核心的位置，有了它们就可通过qemu命令行操作实现完整的虚拟机功能。 KVM的各类上层管理APPKVM目前已经有libvirt API、virsh命令行工具、OpenStack云管理平台等一整套管理工具，与VMware提供的商业化管理工具相比虽然有所差距，但KVM这一整套管理工具都是API化的、开源的，可以灵活使用，且能二次定制开发。 1）libvirt libvirt是使用最广泛的对KVM虚拟化进行管理的工具和应用程序接口，已经是事实上的虚拟化接口标准。作为通用的虚拟化API，libvirt不但能管理KVM，还能管理VMware、Hyper-V、Xen、VirtualBox等其他虚拟化方案。但是，在通过Docker或Kolla部署OpenStack时，由于容器镜像中集成了libvirt功能，需要关闭Host的libvirt服务，否则会发生PID调用错误。 2）virsh virsh是一个常用的管理KVM虚拟化的命令行工具，用于在单个宿主机上进行运维操作。virsh是用C语言编写的一个调用libvirt API的虚拟化管理工具，其源代码也是同步公布在libvirt这个开源项目中的。我们常用的KVM虚拟机查看指令就是virsh list –all，如下图： 3）virt-manager virt-manager是专门针对虚拟机的图形化管理软件，底层与虚拟化交互的部分仍然是调用libvirt API来操作的。virt-manager除了提供虚拟机生命周期管理的基本功能，还提供性能和资源使用率的监控，同时内置了VNC和SPICE客户端，方便图形化连接到虚拟客户机中。virt-manager在RHEL、CentOS、Fedora等操作系统上都非常流行，因其图形化操作的易用性，成为新手入门学习虚拟化操作的首选管理软件。但是，在真实服务器端由于需要安装图形化界面，所以并不常用（服务器环境实现可视化一般是通过VNC功能实现）。 4）OpenStack OpenStack是目前业界使用最广泛的功能最强大的云管理平台，它不仅提供了管理虚拟机的丰富功能，还有非常多其他重要管理功能，如：对象存储、块存储、网络、镜像、身份验证、编排服务、控制面板等。OpenStack的Nova、Cinder和Neutron，也就是计算、存储和网络管理组件仍然使用libvirt API来完成对底层虚拟化的管理，其计算、存储和网络服务组件的配置文件conf中，均有[libvirt]分类引用配置项。 以上，就是KVM这个耳熟能详的Hypervisor的全貌，作为运维人员掌握以上知识点即可，也就是理解KVM虚拟机的真正工作原理即可。如果工作层面涉及性能或管理的二次开发，必须进一步了解并掌握其官方社区的源码以及各种热点技术或BUG解决方案，这就需要自己钻研了。后面，我们会从实战的角度来介绍如何用熟KVM虚拟机的各类操作。]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Hypervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-20-SBA架构下的核心网大一统]]></title>
    <url>%2F2019%2F06%2F20%2F2019-06-20-SBA%E6%9E%B6%E6%9E%84%E4%B8%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BD%91%E5%A4%A7%E4%B8%80%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[移动通信核心网络从1G/2G程控交换、电路交换时代的大一统，到2.5G/3G/4G软交换、GPRS、EPC和IMS的专业分家，再到5G时代SBA架构统一，完美印证了那句俗得不能再俗的俗话—“天下大势，合久必分、分久必合”。 现在，很多人都以为5G的语音业务无论是初期的VoLTE、后续EPS Fallback，还是最终的VoNR仍是由IMS网络来提供，再加上现在讲解5G SBA架构的课件资料等都是以4G EPC网络架构做参考对比讲解。所以，5G 时代的核心网仍然是分成PS和IMS两大领域。其实，从业务提供层面来讲可以这样认为，毕竟不同业务的信令协议不同。但是，从网元实现和组网架构的设计理念来讲，未必还需要上述的专业分家。 业务角度的核心网专业划分移动通信网络的核心网从GSM时代的2.5G时代开始就分为电路交换域CS和分组交换域PS，分别负责移动语音业务和GPRS手机上网业务，到了3G时代，核心网在CS和PS的基础上又多了一个“小兄弟”IMS域，它的定义是“IP多媒体子系统”，在2008年的3GPP R5版本定义并冻结。在国内，只有中国移动在2009年6省市部署IMS域试点，2010年27省全面部署IMS域并全面商用，初期用于固网VOBB政企和家庭固话业务承载，而到了4G时代，由于其作为VoLTE高清语音/视频通话业务的核心网络，在移动通信核心网的地位一下由“小兄弟”蹿升为“老大哥”。 目前，4G时代，移动通信核心网主要细分为EPC、IMS和软交换三大专业，EPC仍然继承提供GPRS业务，只是带宽大了很多，速度快了许多，可以简单理解为GPRS业务的增强版。而IMS则从初期只提供固网业务，发展为开始为用户提供高清语音/视频通话业务，不仅业务种类多了手机终端侧的视频通话、视频彩铃、一号多终端（虚拟eSIM卡）等富媒体业务，且语音质量（清晰度、保真度）和接续时延等用户感知，相比传统电路交换域CS，都有质的提升。至于软交换专业，那只是2G/3G时代电路域CS的产物，最终会被时代抛弃。这一点从中国电信、中国联通纷纷宣布关闭GSM网络来看，就是最好的证明。至于中国移动，由于其组网和业务承载的复杂性（中移动目前网络是全球最复杂一张网），以及用户迁移率（VoLTE业务用户转化）等原因，暂时无法宣布关闭GSM网络时间表，但是2G网络的语音业务体验不如VoLTE，这是肯定的。如果你的终端支持VoLTE功能，且是中移动客户，那就赶紧开通VoLTE吧，资费不变，感知提升，何乐而不为呢？ 因此，在现网4G网络阶段，移动通信核心网专业从业务承载的角度来看，分为EPC专业（用户数据业务、彩信业务）、IMS专业（高清语音/视频业务、短信业务）和软交换专业（传统语音业务、短信业务）三大专业。所以，虽然大家都宣称自己是核心网专业从业人员，但是业务流程、信令协议、网元配置以及人员技能储备等方面都是不同的。 到了5G时代，由于SBA服务化核心网架构的提出（中移动首提），所有网元功能模块全部“软”化，从软件模块化设计理念来看，这必然会导致业务管理网元功能SMF不仅只继承EPC-C面功能，同时整个IMS-C面功能也是能够集成统一的。而用户面网元功能UPF不仅只继承EPC-U，同样也能集成IMS-U。 而且，基于IT领域“生产者/消费者”理念，导致SBA架构下各网元功能模块之间通过一条逻辑总线互联，松耦合且无强依赖关系。比如：鉴权解耦（AUSF）、用户数据解耦（UDM/UDR）、接入控制解耦（AMF）、业务功能解耦且接口开放（NEF、AF）、数据转发解耦（UPF）、全网路由寻址统一（NRF）等等。这个理念（生产者/消费者）与现有IMS网络架构设计理念（控制、承载、业务三分离）是相符的，这也为统一提供语音/数据类业务，不再细分IMS/CS/PS等专业打下基础。不是很理解？那我们就来掰扯下IMS网络架构如何与SBA架构融合。。。 IMS逻辑网元与SBA架构网元功能融合IMS网络架构从2008年3GPP R5版本冻结以来，一直没有变化。从这一点来看，正好说明这个架构是非常成熟的。即使到了4G时代，为了给用户提供VoLTE业务，加入了EPC网络。但是，从VoLTE业务角度来看，EPC只是一个接入网，为用户建立一条承载隧道，使其接入IMS网络，从而享受VoLTE高清类、富媒体类等业务，整体VoLTE业务及其增值业务控制都在IMS网络内，所以，IMS网络才是真正的核心网。 整个IMS网络架构也是分层的，分为接入层，承载层、控制层和业务层共4层。从VoLTE业务角度来看，接入层就是上图的EPC部分，承载层就是上图的IP专网，控制层和业务层就是上图的IMS部分（控制层功能和业务层功能由不同的物理网元实现）。其整体全貌如下，我们就按照从接入层到业务层的层次顺序，来掰扯下面这张图。。。 首先，就是核心层与接入层之间的用作边界网关的SBC网元，它主要用于用户接入控制，业务代理鉴权和数据包的路由转发。用户的业务请求从接入层首先送达SBC，然后由它负责向核心侧进行消息转发，而核心侧的响应也由它抓发给接入侧。所以，SBC网元必然是一个业务信令数据包和业务媒体数据包合一转发的网元，从数通角度来说与路由器功能一致。那么，从网元兼具的具体功能角度来看，用户接入控制功能可以卸载到SBA架构的AMF，业务代理鉴权可以卸载到SBA的AUSF，业务路由控制可以卸载到SBA的SMF，媒体面数据包转发功能可以卸载到SBA的UPF上。 从SBC往上，就到了真正的核心控制层，主要就三类网元CSCF、ENUM/DNS和HSS。先说CSCF，CSCF在3GPP定义中又分为三个逻辑功能网元P-CSCF、I-CSCF和S-CSCF。P-CSCF网元也是用于业务的接入控制，本质上它才是IMS核心控制层的入口。而现网由于涉及跨网络层对接，从安全角度考虑设置边界网关SBC，正是因为SBC的存在，使得P-CSCF变成了一个纯信令控制面网元。在固网业务网内，P-CSCF是独立设置的，与边界网关SBC采用星型拓扑连接，目的就是对不同业务区域用户接入统一集中管理。在VoLTE业务网内，P-CSCF与SBC合设。所以，从网元功能角度来看，其接入控制功能同样可以卸载SBA架构的AMF上。 I-CSCF网元主要用于跨IMS核心控制层互通，所以它有拓扑隐藏的功能。也就是说，从其他IMS核心控制层来的业务请求消息只能找本端的I-CSCF，本端核心网的其他网元对外来的业务请求消息是不可见的。这个网元的功能有个学名—“用户归属域入口”，见名知意，它本质上也是一种接入控制类网元。所以，从网元功能的角度来看，也可以卸载到SBA架构的AMF。如果考虑安全风险，也可以卸载到SBA架构的SMF。 S-CSCF网元主要用于信令的路由控制和业务逻辑的触发，从这点来看是个典型的会话控制类网元。所以，必然可以卸载到SBA的SMF上。 ENUM/DNS网元用于IMS域内全网的路由寻址，但是它不做路由转发，而是将寻址到的对端地址发给本端的S-CSCF，由本端的S-CSCF负责路由转发。由于IMS网络目前主要用于通话类语音业务，因此就涉及电话号码的翻译问题。在2G/3G时代，用户拨打电话，是通过纯号码分析功能完成全网路由寻址的，在VoLTE时代，由于语言业务承载在IP上，就涉及将电话号码翻译成IP地址的需求，这就是ENUM/DNS网元存在的意义。 如上图，ENUM/DNS从名字上就知道它是由ENUM和DNS两个逻辑网元组成，其实它还有个小名，叫ENS（初期一提ENS，很多人都懵逼了。。。我也是懵逼er之一。。。）。ENUM的功能主要完成用户电话号码到归属域域名+传输&amp;应用协议+服务端口的NAPTR翻译，然后将翻译后的结果送给DNS完成SRV和A查询的两步翻译，从而得到被叫用户归属域入口地址（也就是I-CSCF地址），实现业务从主叫端到被叫端的E2E接续。从ENUM/DNS的网元功能来看，它与SBA架构下NRF何其相似，这也是为什么有人提出5G时代DNS功能弱化的原因。唯一的区别就是现有ENUM/DNS还负责用户数据的存储，而SBA架构的NRF可没这么牛X，但是可以拉着它的“表兄弟”—UDM/UDR一起帮忙啊。。。所以，这都不是问题。 而HSS网元用于用户签约数据存储，用户接入的合法性鉴权/授权等功能，它也有个学名叫“用户数据中心”。在SBA架构下也有个类似的网元功能单元UDM/UDR，其功能与HSS也类似，同样用于用户签约数据的存储，只是它没有用户接入合法性鉴权/授权功能，这部分功能解耦在SBA架构的AUSF上面。所以，HSS网元的用户签约数据存储功能可以卸载到UDM/UDR上，而鉴权/授权功能可以卸载到AUSF上，两个网元功能单元通过SBA服务总线接口交互消息，同样也不是问题。 从核心控制层往上，就是业务层了，主要是各类业务服务器的包括基础业务提供服务器、增值业务提供服务器、短信、彩信、彩铃、彩印等等。。。这些业务服务器通过标准接口与核心控制层对接，只用于业务逻辑的生成和下发（比如：互转、彩铃播放等等），并不涉及业务路由的控制，所以它们与核心控制层是一种解耦关系，也就是说任何一家业务服务网元厂家只要按照统一接口标准开发自己的产品就可以与核心控制层完成对接，并实现特色业务提供。同样，在SBA架构下也有类似的网元功能单元，那就是AF，如果有进一步能力开放需求，还可以拉着NEF一起搞个“大事情”。 以上，通过IMS网络各网元功能的解析，阐述了从网元功能层面，IMS网元与SBA架构网元功能融合的可能性。下面，我们从网元处理逻辑的角度继续掰扯。 现网的网元种类很多，按专业细分：有软件换的、EPC的、承载网的、IMS的、增值业务的等等；按照所处的网络位置分：有边界网关、接入端局、互联互通关口局、长途汇接局、信令转发点等等。。。同样，现网各类网元的提供厂商也很多，有华为、中兴、爱立信、诺基亚等知名厂商。但是，无论什么类型的网元，无论由谁来提供，网元内部处理逻辑绕不开三大块：消息接口和分发逻辑单元、业务处理逻辑单元和数据库逻辑单元。 消息接口和分发逻辑主要用来接收各类信令/媒体消息，按照一定的过滤机制分发给内部的业务逻辑处理单元，业务逻辑处理完成处理后，将业务状态缓存到内部的数据库逻辑单元，并将处理结果转发给消息接口和分发逻辑单元，然后消息接口单元按照一定路由策略转发给外部其他网元的消息接口单元。上述各类型网元内部基本上都是类似的处理逻辑，这种逻辑处理机制同样符合软件模块化设计的思想。而5G SBA架构本身就采用软件模块化设计的思想，不仅将各类网元功能“软”化，同时将传统网络各网元的逻辑功能进行了拆分和重组，使得每个“软”化的网元功能单元能力更加清晰。因此，也就有了我们开篇提到的SMF不仅只继承EPC-C面功能，同样也能集成IMS-C面功能的观点，增加的逻辑功能点主要涉及消息接口和分发逻辑单元以及业务处理单元的开发，而接入控制功能卸载到AMF、鉴权功能卸载AUSF等，增加的逻辑功能点也主要涉及上面两处。 但是，现网网元这种处理逻辑其实是一种有状态的设计理念，涉及业务状态在本网元内部的存储，一旦本网元故障，而业务数据没有异地灾备机制的情况下，就会发生业务受损。而5G时代为了进一步提高业务可靠性，有些厂家提出了无状态的设计理念，这就需要将传统网元内部数据库逻辑单元统一进行集群化部署，而与各业务功能单元的业务处理逻辑单元采用高可靠、负载均衡对接架构，从而实现业务高可用，无损失特性。这些从软件模块设计理念来看，那都不是事儿。 了解了上面网元内部处理逻辑的概念，那么理解不同业务信令、流程在同一类SBA网元功能单元完成逻辑判断和业务处理，也就是水到渠成的事情。比如：在SMF只能处理PS业务流程基础上，在其内部业务处理逻辑单元中增加SIP信令处理单元、Diameter信令处理单元、HTTP信令处理单元，就能在SMF上同样实现CSCF的功能。同理，在AMF也能实现P-CSCF/I-CSCF的功能。而5G网络基于业务流的QoS策略，在一个PDU会话中通过识别不同业务流，从而建立不同5QI（类似现在QCI)等级的业务承载，更为这种大一统的核心网架构提供天然的基础。 以上，就是我对5G SBA架构下核心网大一统的粗浅理解。在我个人看来，结合SBA服务化网络架构理念和网元功能单元“软”化，未来核心网专业大一统是很有可能的，并且从技术角度来看，应该也不存在无法解决的问题。但是，很可能有设备厂商从网络可靠、安全的角度提出异议，或者是一些细节问题提出解决进展慢等困难，其实这都是背后的商业目的在作怪。至于人员的技能融合，那是必须要完成的，否则就算只是核心网专业，那也玩不转。]]></content>
      <categories>
        <category>5G网络架构</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-18-服务器与外部链接的网络虚拟化]]></title>
    <url>%2F2019%2F06%2F19%2F2019-06-18-%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E9%93%BE%E6%8E%A5%E7%9A%84%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[服务器与外部连接的网络虚拟化主要有基于TOR交换机的虚拟交换技术和高性能的计算网络融合技术。基于接入交换机TOR实现的虚拟交换是指某些物理交换机可通过特殊协议，感知虚拟机的存在，在物理交换机层实现虚拟交换，其代表技术就是VEPA和VN-tag。同时，高性能计算网络及其融合技术中Infiniband、FCOE、RoCE等技术旨在用一种连接线将数据中心存储网络和计算网络（互联网络）聚合起来，使服务器可以灵活的配置网络端口，简化网络部署。 基于物理交换机的分布式虚拟交换机如下图所示，基于物理交换机实现虚拟交换的思想是将虚拟机的网络流量全部导入接入交换机TOR处理，即使是同一VLAN内的流量也要绕出来，二层的广播和组播都由接入交换机TOR来模拟。 在TOR上实现虚拟交换功能的代表技术就是VEPA（Virtual Ethernet Port Aggregator）。VEPA将虚拟机之间的交换行为从服务器内部移出到上联交换机上，当两个处于同一服务器内的虚拟机要交换数据时，从虚拟机A出来的数据帧首先会经过服务器网卡送往上联交换机，上联交换机通过查看帧头中带的MAC地址（虚拟机MAC地址）发现目的主机在同一台物理服务器中，因此又将这个帧送回原服务器，完成寻址转发。该技术依赖物理交换机支持VEPA，需要虚拟交换机软件、服务器物理网口驱动作联动修改，节省了虚拟交换机查表对物理主机CPU资源消耗。物理交换机转发功能依赖芯片，功能扩展性差，目前只有H3C、HP等少数厂家宣传支持。 VEPA方案的目标是要将虚拟机之间的交换从服务器内部移出到接入硬件交换机TOR上，实现虚拟机之间的“硬交换”。采用这种方法，通常只需要对网卡驱动、VMM桥模块和外部交换机的软件做很小的改动，从而实现低成本，且对当前网卡、交换机、现有以太网报文格式和标准影响最小。 采用VEPA方案，虚拟机的流量都是通过物理接入层TOR交换机完成，它的访问控制和报文下发策略也是基于物理接入层TOR交换机，因此很好地避免了网络和服务器设备管理边界模糊的难题。VEPA标准协议VDP还能准确的感知虚拟机的工作状态，当虚拟机发生迁移时，VEPA协议会将与此相关的访问控制和报文下发等策略重新部署到新的接入交换机。 如上图所示：采用了VEPA方案之后，位于一台物理机上不同虚拟机之间的网络流量不再只是由虚拟交换机来完成，而是都要通过物理接入层TOR交换机(即使是从同一个网络端口)。有人可能会提出这样的疑问：直接在虚拟机内存中交换数据不是更快，而且节省物理网络资源吗？答案是肯定的，数据转发确实受影响。但是，H3C的解释是：虚拟交换机有助于实现虚拟网卡与服务器上物理网卡之间的转换，但也增加了原有网络结构的复杂性，产生新的“边界”给管理带来了麻烦。而且虚拟交换机在服务器宕机的情况下也同时不可用了。（个人觉得这个解释很牵强，但是VEPA对虚拟机热迁移确实很有利） VN-tag是由Cisco和VMWare共同提出的一项标准，如下图所示，其核心思想是在标准以太网帧中增加一段专用的标记—VN-Tag，用以区分不同的VIF，从而识别特定虚拟机的流量。 如上图，每个虚机对应唯一的VIF。VN-Tag中最重要的内容是一对新地址：dvif_id和svif_id，这个地址空间对应的不是交换机的端口或者IP网段，而是虚机的VIF。VN-Tag通过这一对地址说明了数据帧从何而来，到哪里去。 当数据帧从虚机流出来后，就被加上一个VN-Tag标签。基于VN-Tag的源地址dvif_id就能区分出产生于不同虚机的流量。一台具备VN-Tag协议栈的接入交换机TOR可以将VN-Tag与虚机的VIF对应起来，这样就形成了一个映射关系。物理机的出口网卡称为NIC，而上联交换机被称为TOR。 级联是VN-Tag的另一大特点，NIC对应的TOR不必是直接连接服务器的接入交换机，可以是网络内的任意IP可达的设备。这种设计的好处是，接入层设备往往比较简单，通过级联可以将虚拟机的流量拉高到高端的汇聚甚至核心交换机上，利用汇聚、核心交换机丰富的功能特性对流量进行精细化的管理。IEEE最初将VN-Tag标准称为802.1Qbh，后来改为802.1Br。 VN-Tag本质上是一种端口扩展技术，需要为以太网报文增加TAG，而对应的端口扩展设备TOR借助报文TAG中的信息，将端口扩展设备上的物理端口映射成上行物理交换机上的一个虚拟端口，并且使用TAG中的信息来实现报文转发和策略控制。 基于接入交换机TOR实现的虚拟交换，优点是：节省了虚拟交换机查表对物理主机CPU资源消耗缺点，对于对流量监管能力、安全策略部署能力要求较高的场景（如数据中心）而言，是一种优选的技术方案。缺点是：是由于流量从虚拟机上被引入到外部网络，带来了更多网络带宽开销的问题。虚拟机之间报文转发性能低于虚拟交换机，兼容性较差，需要特殊物理交换机，并且需要虚拟交换机软件、服务器物理网口驱动作联动修改，物理交换机转发功能依赖芯片，功能扩展性差。 InfiniBand技术和协议架构分析InfiniBand其实并不是什么新技术。1999年左右，当时IT界的七个大佬Compaq、HP、IBM，Dell、Intel、Microsoft、Sun联合起来成立了IBTA(InfiniBand Trade Association)，将InfiniBand定位于高速的I/O互联网络，旨在通过机外的长连接提供机内总线的I/O性能，同时保证机外互联的可扩展性。不过出于各种原因，各家巨头在IBTA成立后都纷纷离开，InfiniBand没能得到良好的市场推广，只是在超算和高性能数据库领域有所应用。Cisco在2005年收购Topspin后也拥有了InfiniBand的产品线，但是Cisco后面就没怎么继续研发了。Intel在收购QLogic之后，也没有继续研发InfiniBand，而是推出了自己的OmniPath。目前，InfiniBand的厂商主要就是Mellonax一家，基本上可以算是小圈子里面绝对的霸主了。 Infiniband大量用于FC/IP SAN、NAS和服务器之间的连接，作为iSCSI RDMA的存储协议iSER已被IETF标准化。相比FC的优势主要体现在性能是FC的3.5倍，Infiniband交换机的延迟是FC交换机的1/10，支持SAN和NAS。 InfiniBand也是一种分层协议(类似TCP/IP协议)，每层负责不同的功能，下层为上层服务，不同层次相互独立。 IB采用IPv6的报头格式。其数据包报头包括本地路由标识符LRH，全局路由标示符GRH，基本传输标识符BTH等。 InfiniBand的协议栈如上图所示。InfiniBand的传输层首部叫做BTH(Base Transport Header)，通过Queue Pair（QP）来定位远端的目标内存，通过Partition Key实现内存的访问控制，通过Sequence Number实现可靠传输。InfiniBand提供了对多种(包括可靠/不可靠、基于连接/基于报文)传输类型的支持，不同的传输类型使用不同的ETH(Extended Transport Header)，ETH紧跟在BTH的后面。传输层和传输层以下都能够卸载到InfiniBand网卡中进行硬件加速。传输层之上是InfiniBand传输层和用户应用间的映射层ULP(Upper Level Protocol)，允许用户应用在不改变程序原有语义的情况下使用底层的InfiniBand网络进行传输。比如：不同类型的应用需要不同的ULP，如TCP/IP应用可通过IPoIB这一ULP来进行适配。而InfiniBand的原生应用则可以调用InfiniBand所提供的Verbs API直接对InfiniBand网卡进行操作。 InfiniBand的超低时延得益于以下几点： 1）InfiniBand交换机都会采用直通式的转发，减小了串行化延迟； 2）使用了先进的、基于Credit的链路层流控机制，能够有效地防止丢包与死锁； 3）提供了对于RDMA的原生支持，从而大幅地提高了端点的性能。 传统服务器网卡的工作依赖于中断或者轮询，数据包从网卡到达应用程序需要经过内核协议栈的处理以及内核态与用户态间的切换，在上述过程中还要对数据包进行反复的拷贝，这些操作不仅对CPU资源造成了巨大的消耗，而且严重地制约了应用的I/O性能（详见DPDK相关介绍文章）。相比之下，RDMA首先作为一种DMA机制，能够直接在网卡的缓冲区和应用内存间进行数据交互，降低了中断或者轮询的频率，旁路掉了内核协议栈，并实现了数据的零拷贝，因此应用的I/O性能得以大幅提升，而被解放出来的CPU则可以用于处理应用本身。另外，RDMA提供了一组标准的数据操作接口，使得本端应用能够直接操作远端应用在内存中的数据，即字面所述的“远程DMA”。InfiniBand在设计之初即提供了对于RDMA的支持，是InfiniBand实现超低延时的重要基础。 InfiniBand的网卡称为CA(Channel Adaptor)，CA又可分为服务器端的HCA（Host Channel Adaptor）和交换机/存储端的TCA（Target Channel Adaptor）。CA不仅实现了InfiniBand的物理层与链路层，而且能够对InfiniBand的网络层和传输层进行硬件加速，CA的Driver工作在内核中，为ULP或者InfiniBand原生应用提供操作接口。无论是HCA还是TCA，其实质都是一个主机适配器，它是一个具备一定保护功能的可编程DMA引擎。 QP(Queue Pair)是CA提供RDMA能力的基础，它可完成应用层的虚拟地址到CA上的物理地址的自动映射，如果两端的应用需要通信，那么双方都要申请CA上的物理资源，并通过相应的QP来完成对CA的操作。如下图所示，每个QP中都包括Receive和Send两个Work Queue分别用于数据的收和发。应用在进行网络通信前，首先需要向CA请求建立一个QP，需要发送数据时通过Work Request将数据作为WQE(Work Queue Entry)投入QP的Send Queue，应用需要为WQE指定AV(Address Vector)，AV中携带着通信目标的地址信息，以及在通信路径上进行传输所需要的参数。CA根据AV开始进行L4～L2层的处理，封装好包头并从相应的物理端口送出。远端的目标CA收到数据包后，会根据传输层的QP字段将数据放入相应QP的Receive Queue中，然后将数据直接移动到相应的应用内存。每当一个WQE处理完毕之后，CA会通过Completion Queue发送CQE(Completion Queue Entry)给应用，通知其可以继续进行后面的处理。 CA将数据包从物理端口送出后，就进入到了InfiniBand Fabric。子网是InfiniBand Fabric中最重要的概念，子网内部是二层通信，InfiniBand Switch根据链路层LRH中的DLID完成转发，子网间通信需要进行三层的路由，InfiniBand Router通过网络层GRH中的DGID完成。GRH中的SGID和DGID是端到端不变的，而每经过一次路由，链路层LRH中的SLID和DLID都会逐跳发生变化。InfiniBand对子网采用了集中式的管理与控制方式，每个子网中至少有一个SM(Subnet Manager)，多个SM间可以互为备份。SM既可以硬件实现也可以软件实现，可以实现在子网的任何一个节点上，包括CA、Switch和Router。子网中每个节点上都有一个SMA(SM Agent)，SM和SMA间通过接口SMI(Subnet Management Interface)来实现子网的管理与控制，SMI主要提供Get、Set、Trap3种类型的操作。SMI信道上的控制信令称为SMP(Subnet Management Packet)，SMP的传输需要使用专用的QP0(QP0不可用于传输用户应用的数据)，并且要求放在逻辑链路VL 15上(QoS优先级最高)，保证子网的管理和控制流量能够得到优先传输，而不会被数据流量所阻塞。其子网、交换机与路由器的连接如图下图所示： 最后，对SM的主要功能及相关做一简要说明，如下： 1）获取节点信息发现并维护拓扑。SM和SMA间以带内的方式交互SMP，在拓扑发现过程中，交换机上还没有任何转发信息，SMP是没有办法进行传输的。为了解决这个问题，InfiniBand设计了一种专用的控制信令Directed Routed SMP，Directed Routed SMP携带了自身的一次性转发信息，因此可以在交换机上尚未形成LID转发表时在InfiniBand Fabric中进行传输。 2）为节点分配LID、GID。对每个CA来说，在出厂时会分配一个64位全球唯一的GUID(Global Unique Identifier)，不过这个GUID只是用来标识CA的，并不用于二层和三层的转发。LID和GID是由SM为CA集中分配的，分别用于二层和三层的转发。LID的长度为16位，只在子网本地有效，每个CA可以分配一个或者一段连续的LID(通过LMC实现)，发送数据包时CA可以采用不同的LID，以实现Fabric上的多路径转发。GID的长度为128位，由64位的子网前缀+GUID组成，实际上SM为CA分配的是子网前缀，然后CA自己在本地组合出GID。SM还会维护GUID和LID/GID间的映射关系，用于地址解析。 3）根据拓扑计算路由。InfiniBand在计算路由时通常都是以Up/Down算法为基础的，Up/Down算法会为链路指定Up或者Down两种方向，路径只允许链路方向从Up转为Down，而不允许从Down转为Up，从而可以避免形成路由环路。 4）形成LID转发表，并将转发表配置给相应的Switch。Switch会根据数据包的DLID查找LID转发表，然后找到对应的端口进行转发。由于对转发表采用集中式控制方式，为了防止子网内部产生路由环路，InfiniBand通常的做法是等到所有Switch上的转发表都形成之后，再来激活子网中流量的转发。 除了SM以外，每个子网还都需要有一个SA(Subnet Administration)，SA在逻辑上是SM的一部分(物理上两者没有必然的联系)，可以看作是子网的数据库，CA间通信所需的信息(如DLID/DGID、Path MTU等)都由SA完成解析。InfiniBand中还有一些其他的子网管理组件，比如负责维护端到端QP连接的CM（Connection Management），负责性能检测的PM（Performance Manager），负责板上器件检测的BM（Baseboard Manager）等等。除了SM以外，InfiniBand中其余的管理组件统称为GSM（General Services Manager），子网中每个节点上都有相应的GSA（General Services Agent），GSM和GSA间的接口称为GSI（General Services Interface），GSI信道上的控制信令称为GMP（General Services Management Packet），GMP的传输也需要使用专用的QP1(QP1不可用于传输用户应用的数据)，不过和SMP不同的是，GMP不可以放在逻辑链路VL 15上，也就是说GMP需要和数据流量一起接受流控。 FCOE技术原理解析FCoE采用增强型以太网作为物理网络传输架构，能够提供标准的光纤通道有效内容载荷，避免了 TCP/IP协议开销，而且FCoE能够像标准的光纤通道那样为上层软件层（包括操作系统、应用程序和管理工具）服务。 FCoE可以提供多种光纤通道服务，比如发现、全局名称命名、分区等，而且这些服务都可以像标准的光纤通道那样运作。不过，由于FCoE不使用TCP/IP协议，因此FCoE数据传输不能使用IP网络。FCoE是专门为低延迟、高性能、二层数据中心网络所设计的网络协议。 和标准的光纤通道FC一样，FCoE协议也要求底层的物理传输是无损失的。因此，国际标准化组织已经开发了针对以太网标准的扩展协议族，尤其是针对无损10Gb以太网的速度和数据中心架构。这些扩展协议族可以进行所有类型的传输。这些针对以太网标准的扩展协议族有个高大上的名字，被国际标准组织称为“融合型增强以太网（CEE）”。 FCoE的做法是使用以太网的帧头代替FC-2P和FC-2M，上层的FC-2V、FC-3和FC-4仍然保留，FCoE协议栈如下图所示。FCoE将服务器的FC节点称为ENode，FCoE交换机称为FCF。 在FCoE网络中，服务器通过一块CNA网卡同时支撑IP和FC两套协议，相当于HBA和以太网NIC的合体。FCoE的以太网类型是0x8906，其外层MAC地址的写法比较讲究，后续通过具体的通信流程会进行介绍。外层的VLAN对于FCoE来说同样非常关键，其原因主要有两个：首先，FCoE流量必须在无损无丢包的以太网链路上进行传输，这完全依赖于PFC和ETS机制，而这两种机制都需要根据VLAN标签来对流量进行分类，因此FCoE流量必须承载在特定的VLAN中。其次，FCF要想实现存储网络内部的虚拟化，需要使用不同的VLAN来承载不同VSAN(Virtual Storage Area Network)的流量。 那么封装外层以太网时具体该使用哪个VLAN呢？这由是FCoE的控制平面决定。FCoE使用FIP(FCoE Initialization Protocol)作为控制平面协议，其以太网类型为0x8914。FIP主要负责以下3个工作： 1）VLAN发现。FIP在原先VLAN中通过VLAN发现报文，并与邻居协商后续FIP信令和FCoE流量所使用的VLAN，其缺省值为1002。 2）FCF发现。FCF在所有FCoE VLAN内定期组播发现通告报文，使得当前VLAN内的所有的ENode发现自己。 3）FLOGI/PLOGI。与FC中相应过程一样，FCF作为Login Server为ENode分配FCID，同时作为Name Server记录ENode的登录信息。 经过上述3个阶段后，FCoE网络的初始化工作就完成了，FCoE流量得以无损地在以太网中传输。下面来看一个在多跳FCoE网络中典型的报文转发流程，如下图所示： 由于FCID是端到端的，因此FCoE报文在经过FCF转发时FCID不会发生变化，而外层的MAC地址会逐跳改写。我们知道IP和MAC是通过ARP协议联系在一起的，那么FCID和MAC该如何映射呢？FCoE为ENode规定了如下的映射方法：使用FC-MAP填充MAC地址的高24位，低24位填充为FCID，得到FPMA作为自己的以太网地址，而弃用CAN网卡出厂时的MAC地址。其中，FC-MAP为在FIP的FCF发现阶段中，FCF告诉ENode的信息，每个VSAN内部的ENode都使用相同的FC-MAP，不同的VSAN使用不同的FC-MAP。而对于FCF来说，不进行这种转换，直接使用本机MAC地址FCF-MAC进行外层以太网封装。同一个VSAN内的报文都在同一个VLAN内传输，FCF进行VLAN的MAC地址学习，保证了VSAN间的隔离，不同VLAN的优先级不同，通过PFC和ETS进行差异化的传输控制。 单跳FCoE的转发更为简单，负责接入的FCF收到FCoE流量后，根据FCID进行寻址，然后直接转换成FC-2的帧格式在FC网络中进行传输。 当在服务器中部署虚拟机的时候，FCF不再是FCoE接入网络的第一跳，很多FIP的交互过程就实现不了了，而FC网络也面临着这个问题。FC网络给出的解决办法是NPIV/NPV，NPIV部署在服务器中作为ENode和FCF之间的代理，为下挂多个虚拟机的ENode完成FLOGI/PLOGI过程，而NPV则将NPIV的功能放到了以太网交换机上。NPIV/NPV的示意如下图所示。同样，FCoE也可以配合NPIV的工作实现虚拟机的FCoE接入。 RoCE与RoCEv2相比于以太网，InfiniBand的优势主要在于以下几点：带宽总是能够领先一步，链路层具有流控能力，RDMA可以通过旁路内核来加速。不过，InfiniBand太贵，对运维人员要求也较高。因此，随着10GE的普及，40GE/100GE的推广，以太网的带宽资源不再是瓶颈，而DCB协议族的发展也使得以太网具备了不丢包的传输能力，如果再能够提供对RDMA的支持，那么以太网就会有能力和InfiniBand在HPC领域一较高下了。 2010年，IBTA制定了RoCE(RDMA over Converged Ethernet)，使用以太网代替了IB的链路层，保留了网络层以及传输层对于RDMA的支持，结合DCB(要求至少10G的端口速率)即可以获得微秒级的传输延迟。不过，RoCE由于在网络层仍保留着IB的GRH，因此是不能进行IP路由的。为此，IBTA在2014年又制订了RoCEv2，在RoCE的基础上将IB的网络层替换为IP，RoCEv2的流量获得了跨越广域网进行传输的能力，因此RoCEv2又称为RRoCE(Routable RoCE)。下图展示了RoCE技术的演进策略。 相比于FCoE使用FIP协议作为专有的控制平面，RoCE和RoCEv2并没有专门设计自己的控制协议。在InfiniBand中管理和控制的组件多是围绕InfiniBand子网来进行的，由于RoCE、RoCEv2中使用了以太网来替换InfiniBand的链路层，因此自然就没有了InfiniBand子网的概念，SM、SA、PM、BM等组件也就失去了存在的意义，只有传输层上用于端到端协商QP的CM保留了下来。在InfiniBand中，地址分配和解析工作是由SM和SA完成的，去掉了SM和SA后，需要由以太网/IP中相应的机制来进行地址分配和解析。三者地址分配和解析机制如下所示。 L2的生成/分配 L3的生成/分配 L2和L3的解析 IB SM分配LID和LMAC 默认为0xFE::80+GUID，SM分配GID Prefix，覆盖为GID Prefix+GUID Query HCA Verb SA RoCE 以太网MAC 默认为0xFE::80+GUID ARP RoCEV2 以太网MAC DHCP或者为静态配置 ARP 除了RoCE和RoCEv2以外，还有一种融合的方案是iWARP(RDMA over TCP/IP)，通过将TCP/IP卸载到网卡中也可以实现良好的转发性能。不过，iWARP是一种纯应用层的实现，不属于服务器与外部网络连接虚拟化范畴。 至此，服务器与外部连接的网络虚拟化典型技术介绍完毕。这一块大部分内容对于传统CT运维人员，甚至是不负责存储运维的IT人员都较有难度，涉及许多存储领域的概念和数据包转发流程等知识点。因此，大家在看博客时，需要借助搜索工具让自己对存储领域的一些知识点有个感性认识后，再来看本篇博文就显得轻松许多。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-15-服务器内部虚拟接入技术]]></title>
    <url>%2F2019%2F06%2F16%2F2019-06-15-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%86%85%E9%83%A8%E8%99%9A%E6%8B%9F%E6%8E%A5%E5%85%A5%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[在《网络虚拟化概述》一文中，我们提到了数据中心内部网络虚化技术是一种端到端的解决方案，在不同的网络层面其具体实现的技术各不相同。按照分层的架构，主要分为服务器内部的I/O虚拟化，服务器内部虚拟接入实现、服务器与外部的网络连接虚拟化和外部交换网络的虚拟化4部分。其中，服务器内部的I/O虚拟化技术可以参见本站《计算虚拟化之I/O虚拟化》一文，本文主要阐述服务器内部的虚拟接入技术。 虚拟交换机概述如下图所示，服务器内部虚拟网络的划分是以物理网卡为界，物理网卡往上为虚拟资源，物理网卡及其往下为物理资源。服务器内部的虚拟接入技术分为虚拟机网卡、虚拟交换端口和端口组、上行链路和虚拟交换机四部分。 其中，虚拟网卡就是用虚拟机中用软件模拟网络环境，提供类似真实网卡的功能，无需连接。虚拟交换端口是虚拟交换机上的端口，用来连接虚拟网卡，为虚拟机提供接入网络的服务。端口组是为了方便管理，将具有同样属性的一组虚拟交换端口称为端口组。通常情况下，一个端口组就是一个VLAN。在同一个物理服务器内部，相同端口组的虚拟机之间通信无需进过物理网络，但是不同端口组的虚拟机通信必须经过物理网络。上行链路是虚拟交换机与主机物理网卡之间之间虚拟链路，通过上行链路虚拟交换机与主机的物理网络适配器相连，使得主机中的虚拟机能够与另一主机中的虚拟机或外部网络进行通信。虚拟交换机原理与物理交换机一样，构建起虚拟机之间的网络，并提供虚拟机与外部网络互通的能力。 虚拟交换机属于二层设备，在虚拟化网络中起到承上启下的作用。虚拟交换机的交换端口连接虚拟机网卡，上行链路与物理网卡绑定，从而打通虚拟网络和物理网络；物理网卡与物理交换机相连，使虚拟机发出的数据能够转发到物理网络中。同一主机内相同端口组的虚拟机通过虚拟交换机就能通信，不需要通过物理交换机；不同主机相同端口组的虚拟机需要通过物理交换机才能互相通信。 虚拟交换机按照实现的范围分为标准虚拟交换机和分布式虚拟交换机两种，标准虚拟交换机不支持跨物理服务器，只能为同一主机内部的虚拟机之间提供二层交换功能。比如：VMware Workstations的主机网络适配器、Virtual Box的内部网络适配器、Hyper-V的内部网路适配器、VMware vSphere中的标准虚拟交换机都是这类虚拟交换机。分布式交换机支持跨物理服务器提供二层交换的功能，一台分布式虚拟交换机可以分布在多台物理服务器上。比如：华为FusionSphere中的虚拟交换机（上图所示），VMware vSphere中的分布式虚拟交换机、开源的OVS交换机、Linux原生的Linux Bridge等都是这种分布式交换机。 虚拟交换机按照I/O虚拟化的方式可以分为三种类型：普通、VMDq和SR-IOV。如下图所示： 普通模式。基于CPU实现的虚拟交换，天生会消耗一部分CPU资源。在服务器的CPU中实现完整的虚拟交换的功能，虚拟机的虚拟网卡对应虚拟交换的一个虚拟端口，服务器的物理网卡作为虚拟交换的上行链路接入物理TOR交换机。虚拟机的报文收发流程如下：虚拟交换机首先从虚拟端口/物理端口接收以太网报文，之后根据虚拟机MAC、VLAN，查找二层转发表，找到对应的虚拟端口/物理端口，然后按照具体的端口，转发报文。采用普通模式的虚拟交换机，同一服务器上的虚拟机间报文由虚拟交换机实现实现虚拟机之间报文的二层软件转发， 报文不出服务器，转发路径短，性能高。但是，跨服务器通信时，需要经物理交换机进行转发，相比物理交换机实现虚拟交换，虚拟交换模块的消耗，性能稍低于物理交换机。与物理交换机相比，由于采用纯软件实现虚拟交换，相比采用L3芯片的物理交换机，功能扩展灵活、快速，可以更好的满足云计算的网络需求扩展。同时，由于服务器内存大，相比物理交换机，在L2交换容量、ACL容量等，远大于物理交换机。 VMDq模式。相比普通模式，交换、安全、QoS等功能从服务器CPU上卸载至网卡上，CPU开销较少。采用零拷贝、VLAN、Bonding等关键技术使得虚拟机网络报文通过缓冲区到达物理网卡，并支持物理网卡的绑定操作，同时支持基于队列的VLAN和二层广播域的隔离。在ACL方面支持基于五元组的包过滤，支持状态规则，完全兼容当前的安全组功能。且ACL规则表基于队列，不受其它队列规则数的干扰。在QoS方面还支持带宽限制，可以预留最小带宽，动态调整带宽比例以及带宽优先级。VMDq采用网桥交换技术，在硬件层面完成MAC和VLAN的交换功能。最重要一点，相比SR-IOV，VMDq的支持虚拟机热迁移，华为的iNIC网卡就采用了这种技术。 SR-IOV模式。设计思想是将虚拟交换功能从服务器的CPU移植到服务器物理网卡，通过网卡硬件改善虚拟交换机占用CPU资源而影响虚拟机性能的问题，同时借助物理网卡的直通的能力，加速虚拟交换的性能。传统的SR-IOV商业网卡，可以支持简单的虚拟交换的功能，高级特性较少，并且由于自身设计以及缺乏与Hypervisor的配合存在一些缺陷，如热迁移等。 Linux原生网络设备虚拟化TAP/TUN是Linux内核实现的一对虚拟网络设备，TAP工作在二层，TUN工作在三层。Linux内核通过TAP/TUN设备向绑定该设备的用户空间程序发送数据，反之，用户空间程序也可以像操作物理网络设备那样，向TAP/TUN设备发送数据。 基于TAP驱动，即可实现虚拟机vNIC的功能，虚拟机的每个vNIC都与一个TAP设备相连，vNIC之于TAP就如同NIC之于eth。当一个TAP设备被创建时，在Linux设备文件目录下会生成一个对应的字符设备文件，用户程序可以像打开一个普通文件一样对这个文件进行读写。比如，当对这个TAP文件执行 write操作时，相当于TAP设备收到了数据，并请求内核接受它，内核收到数据后将根据网络配置进行后续处理，处理过程类似于普通物理网卡从外界收到数据。当用户程序执行read请求时，相当于向内核查询TAP设备是否有数据要发送，有的话则发送，从而完成TAP设备的数据发送。 TUN则属于网络中三层的概念，数据收发过程和TAP是类似的，只不过它要指定一段IPv4地址或IPv6 地址，并描述其相关的配置信息，其数据处理过程也是类似于普通物理网卡收到三层 IP 报文数据。 VETH设备总是成对出现，一端连着内核协议栈，另一端连着另一个设备，一个设备收到内核发送的数据后，会发送到另一个设备上去，这种设备通常用于容器中两个namespace之间的通信。 Linux Bridge详解Bridge也是 Linux内核实现的一个工作在二层的虚拟网络设备，但不同于TAP/TUN这种单端口的设备，Bridge实现为多端口，本质上是一个虚拟交换机，具备和物理交换机类似的功能。 Bridge可以绑定其他Linux网络设备作为从设备，并将这些从设备虚拟化为端口，当一个从设备被绑定到Bridge上时，就相当于真实网络中的交换机端口上插入了一根连有终端的网线。如下图所示，Bridge 设备br0绑定了实际设备eth0和虚拟设备设备tap0/tap1，当这些从设备接收到数据时，会发送给br0 ，br0会根据MAC地址与端口的映射关系进行转发。 Linux下的Bridge和vlan有点相似，它依赖于一个或多个从设备。与VLAN不同的是，它不是虚拟出和从设备同一层次的镜像设备，而是虚拟出一个高一层次的设备，并把从设备虚拟化为端口port，且同时处理各个从设备的数据收发及转发 。 Linux Bridge的功能主要在内核里实现。当一个从设备被attach到Linux Bridge上时，这时在内核程序里，netdev_rx_handler_register()被调用，一个用于接受数据的回调函数被注册。以后每当这个从设备收到数据时都会调用这个函数把数据转发到Linux Bridge上。当Linux Bridge接收到此数据时br_handle_frame()被调用，进行一个和现实世界中的交换机类似的处理过程：判断包的类别（广播/单点），查找内部MAC端口映射表，定位目标端口号，将数据转发到目标端口或丢弃，自动更新内部MAC端口映射表以自我学习。 Linux Bridge和现实世界中的二层交换机有一个区别：数据被直接发到Linux Bridge上，而不是从一个端口接受。这种情况可以看做Linux Bridge自己有一个MAC可以主动发送报文，或者说Linux Bridge自带了一个隐藏端口和宿主Linux系统自动连接，Linux上的程序可以直接从这个端口向Linux Bridge上的其他端口发数据。所以，当一个Linux Bridge拥有一个网络设备时，如bridge0加入了eth0时，实际上bridge0拥有两个有效MAC地址，一个是bridge0的，一个是eth0的，他们之间可以通讯。 通过上面的描述，也就解释了Linux Bridge为什么可以设置IP地址。通常来说IP地址是三层协议的内容，不应该出现在二层设备上。但是Linux里Bridge是通用网络设备抽象的一种，只要是网络设备就能够设定IP地址。当一个bridge0拥有IP后，Linux便可以通过路由表或者IP表规则在三层定位bridge0，此时相当于Linux拥有了另外一个隐藏的虚拟网卡和Linux Bridge的隐藏端口相连，这个网卡就是名为 virbr0的通用网络设备，如下图所示，IP可以看成是这个网卡的。 当有符合此IP的数据到达时，内核协议栈认为收到了一个目标地址为本机的数据包，此时应用程序可以通过Socket接收到它。其实，在实际中3层交换机设备，它也拥有一个隐藏的MAC地址，供设备中的三层协议处理程序和管理程序使用。设备里的三层协议处理程序，对应名为 bridge0 的通用网络设备的三层协议处理程序，即宿主Linux系统内核协议栈程序。设备里的管理程序，对应bridge0宿主Linux系统里的应用程序。 对于一个被attach到Linux Bridge上的设备来说，只有它收到数据时，此数据包才会被转发到Linux Bridge上，进而完成查表广播等后续操作。当请求是发送类型时，数据是不会被转发到Linux Bridge 上的，它会寻找下一个发送出口。我们在配置网络时经常忽略这一点从而造成网络故障。 Bridge的实现当前有一个限制：当一个设备被attach到Bridge上时，那个设备的IP会变的无效，Linux不再使用那个IP在三层接受数据。比如：如果eth0本来的IP是192.168.1.2，此时收到一个目标地址是192.168.1.2的数据，Linux的应用程序能通过Socke 操作接受到它。而当eth0被attach到一个br0 时，尽管eth0的IP还在，但应用程序是无法接受到上述数据的。此时应该把192.168.1.2赋予 br0。 借助Linux Bridge功能，同主机或跨主机的虚拟机之间能够轻松实现通信，也能够让虚拟机访问到外网，这就是我们所熟知的桥接模式，一般在装VMware虚拟机或者VirtualBox虚拟机的时候，都会提示我们要选择哪种模式，常用的两种模式是桥接和NAT。 Bridge本身是支持VLAN功能的，如下图所示，通过配置，Bridge可以将一个物理网卡设备eth0划分成两个子设备eth0.10，eth0.20，分别挂到Bridge虚拟出的两个VLAN上，VLAN id分别为VLAN 10和VLAN 20。同样，两个VM的虚拟网卡设备vnet0和vnet 1也分别挂到相应的VLAN 上。这样配好的最终效果就是VM1不能和VM2通信，达到了隔离。 OVS详解既然Linux原生的Bridge就能实现二层虚拟交换功能，为什么还有很多厂商都在做自己的虚拟交换机，比如比较流行的有 VMware virtual switch、Cisco Nexus 1000V以及 Open vSwitch。究其原因，主要有以下几点： 1）方便网络管理与监控。OVS的引入，可以方便管理员对整套云环境中的网络状态和数据流量进行监控，比如可以分析网络中流淌的数据包是来自哪个VM、哪个OS及哪个用户，这些都可以借助OVS 提供的工具来达到。 2）加速数据包的寻路与转发。相比Bridge单纯的基于MAC地址学习的转发规则，OVS引入流缓存的机制，可以加快数据包的转发效率。 3）基于 SDN 控制面与数据面分离的思想。上面两点其实都跟这一点有关，OVS控制面负责流表的学习与下发，具体的转发动作则有数据面来完成，可扩展性强。 4）隧道协议支持。Bridge只支持VxLAN，OVS支持gre/vxlan/IPsec等。 5）适用于 Xen、KVM、VirtualBox、VMware 等多种 Hypervisors。 Open vSwitch（OVS）是一个开源的虚拟交换机，遵循Apache2.0许可，其定位是要做一个产品级质量的多层虚拟交换机，通过支持可编程扩展来实现大规模的网络自动化。它的设计目标是方便管理和配置虚拟机网络，能够主动检测多物理主机在动态虚拟环境中的流量情况。针对这一目标，OVS具备很强的灵活性，可以在管理程序中作为软件交换机运行，也可以直接部署到硬件设备上作为控制层。此外，OVS还支持多种标准的管理接口，如NetFlow、sFlow、IPFIX、RSPAN、CLI、LACP、802.1ag。 对于其他的虚拟交换机设备，如VMware的vNetwork分布式交换机、思科Nexus 1000V虚拟交换机等，它也提供了较好的支持。由于OVS提供了对OpenFlow协议的支持，它还能够与众多开源的虚拟化平台（如KVM、Xen）相整合。在现有的虚拟交换机中，OVS作为主流的开源方案，发展速度很快，它在很多的场景下都可灵活部署，因此也被很多SDN/NFV方案广泛支持。 OVS在实现中分为用户空间和内核空间两个部分。用户空间拥有多个组件，它们主要负责实现数据交换和OpenFlow流表功能，还有一些工具用于虚拟交换机管理、数据库搭建以及和内核组件的交互。内核组件主要负责流表查找的快速通道。 其中，OVS最重要的组件是ovs-vswitchd，它实现了OpenFlow交换机的核心功能，并且通过Netlink协议直接和OVS的内核模块进行通信。用户通过ovs-ofctl可以使用OpenFlow协议去连接交换机并查询和控制。另外，OVS还提供了sFlow协议，可以通过额外的sFlowTrend等软件（不包含在OVS软件包中）去采样和监控数据报文。 ovs-vswitchd通过UNIX socket通信机制和ovsdb-server进程通信，将虚拟交换机的配置、流表、统计信息等保存在数据库ovsdb中。当用户需要和ovsdb-server通信以进行一些数据库操作时，可以通过运行ovsdb-client组件访问ovsdb-server，或者直接使用ovsdb-tool而不经ovsdb-server就对ovsdb数据库进行操作。 ovs-vsctl组件是一个用于交换机管理的基本工具，主要是获取或者更改ovs-vswitchd的配置信息，此工具操作的时候会更新ovsdb-server的数据库。同时，我们也可以通过另一个管理工具组件ovs-appctl发送一些内部命令给ovs-vswitchd以改变其配置。另外，在特定情况下，用户可能会需要自行管理运行在内核中的数据通路，那么也可以通过调用ovs-dpctl驱使ovs-vswitchd在不依赖于数据库的情况下去管理内核空间中的数据通路。 openvswitch.ko则是在内核空间的快速通路，主要是包括datapath（数据通路）模块，datapath负责执行报文的快速转发，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作，实现快速转发能力。通过netlink通信机制把ovs-vswitchd管理的流表缓存起来。 OVS数据流转发的大致流程如下： 1）OVS的datapah接收到从OVS连接的某个网络端口发来的数据包，从数据包中提取源/目的IP、源/目的MAC、端口等信息。 2）OVS在内核态查看流表结构（通过HASH），如果命中，则快速转发。 3）如果没有命中，内核态不知道如何处置这个数据包。所以，通过netlink upcall机制从内核态通知用户态，发送给ovs-vswitchd组件处理。 4）ovs-vswitchd查询用户态精确流表和模糊流表，如果还不命中，在SDN控制器接入的情况下，经过OpenFlow协议，通告给控制器，由控制器处理。如果没有SDN控制器接入，则进行丢弃处理。 5）如果模糊命中，ovs-vswitchd会同时刷新用户态精确流表和内核态精确流表；如果精确命中，则只更新内核态流表。 6）刷新后，重新把该数据包注入给内核态datapath模块处理。 7）datapath重新发起选路，查询内核流表，匹配；报文转发，结束。 虽然OVS作为虚拟交换机已经很好，但是它在NFV的场景下，在转发性能、时延、抖动上离商业应用还有一段距离。Intel利用DPDK的加速思想，对OVS进行了性能加速。从OVS2.4开始，通过配置支持两种软件架构：原始OVS（主要数据通路在内核态）和DPDK加速的OVS（数据通路在用户态）。所谓DPDK加速的OVS，也就是华为常用来宣传的EVS。 根据上面的描述，跟数据包转发性能相关的主要有两个组件：ovs-vswitchd（用户态慢速通路）和openvswitch.ko（内核态快速通路）。如下所示，显示了OVS数据通路的内部模块图，DPDK加速的思想就是专注在这个数据通路上。 ovs-vswitchd主要包含ofproto、dpif、netdev模块。ofproto模块实现openflow的交换机；dpif模块抽象一个单转发路径；netdev模块抽象网络接口（无论物理的还是虚拟的）。 openvswitch.ko主要由数据通路模块组成，里面包含着流表。流表中的每个表项由一些匹配字段和要做的动作组成。 OVS在2.4版本中加入了DPDK的支持，作为一个编译选项，可以选用原始OVS还是DPDK加速的OVS。DPDK加速的OVS利用了DPDK的PMD驱动，向量指令，大页、绑核等技术，来优化用户态的数据通路，直接绕过内核态的数据通路，加速物理网口和虚拟网口的报文处理速度。如下图所示，虚线框内就是DPDK加速的部分。 DPDK加速的OVS数据流转发的大致流程如下： 1）OVS的ovs-vswitchd接收到从OVS连接的某个网络端口发来的数据包，从数据包中提取源/目的IP、源/目的MAC、端口等信息。 2）OVS在用户态查看精确流表和模糊流表，如果命中，则直接转发。 3）如果还不命中，在SDN控制器接入的情况下，经过OpenFlow协议，通告给控制器，由控制器处理。 4）控制器下发新的流表，该数据包重新发起选路，匹配；报文转发，结束。 DPDK加速的OVS与原始OVS的区别在于，从OVS连接的某个网络端口接收到的报文不需要openvswitch.ko内核态的处理，报文通过DPDK PMD驱动直接到达用户态ovs-vswitchd里。 对DPDK加速的OVS优化工作还在持续进行中，重点在用户态的转发逻辑（dpif）和vhost/virtio上，比如采用DPDK实现的cuckoo哈希算法替换原有的哈希算法去做流表查找，vhost后端驱动采用mbuf bulk分配的优化，等等。 以上，就是服务器内部的虚拟接入技术实现，主要就是虚拟交换机的原理和作用。此外，我们还介绍了Linux原生的虚拟网络设备，分布式交换机Linux Bridge，以及目前被广泛应用在电信云NFV领域的开源OVS虚拟交换机的原理和数据转发加速思路。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-13-正则表达式基础入门]]></title>
    <url>%2F2019%2F06%2F14%2F2019-06-13-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[什么是正则表达式简单地说，正则表达式就是为处理大量的字符串及文本而定义的一套规则和方法。可能通过这句话大家还是不明白什么是正则表达式，这里我也就不罗列正则表达式的定义了，反正就是罗列了，你还是不明白。我们通过一个例子来说明： 首先，我们在Notepad++中建立一个文本，名字随便起，在文中添加如下三行内容： 这时，我们需要搜索这个文本文件中的kkutysllb这个单词，很简单，通过ctrl+F快捷键打开搜索栏，输入kkutysllb即可，如下： 根据搜索结果，我们找到了3个kkutysllb字符组。那么，这时我们需求变一下，需要查找以kkutysllb开头的文本，我们如何查找？眼睛不瘸的你一定发现了，Nodepad++搜索栏中有“正则表达式”字串！！！ 所以，我们开心的再次Ctrl+F打开搜索栏，选中正则表达式，在目标栏中输入^kkutysllb，点击查找所有，即可满足我们的需求。 是不是很神奇？这就是正则表达式存在的意义，我们可以把上面^kkutysllb看做一个正则表达式（本来就是），这个正则表达式的意思就是“以kkutysllb开头的行”。 到目前为止，我们已经初步接触到了正则表达式，现在让我回过头来看看开头那句话—简单地说，正则表达式就是为处理大量的字符串及文本而定义的一套规则和方法。现在，你是不是就理解一点儿了？其实，正则表达式在编写程序时经常会用到，因为我们写的代码程序主要用来处理各种数据和文本，有了正则表达式可以让程序代码足够精简且强大。同时，在Linux的文本处理三剑客中也会经常用到正则表达式（后面会专门讲三剑客工具），通过正则表达式可以将复杂的处理化繁为简，提高运维脚本编写效率，而且在Linux运维工具中只有三剑客工具支持正则表达式。 最后，我们正式给出正则表达式的官方定义：正则表达式，又称规则表达式，英文为Regular Expression，常简写为regex，regexp或RE。正则表达式是计算机科学的一个概念，通常被用来检索、替换那些符合某个模式（规则）的文本。 正则表达式入门为了让大家对Linux中使用正则表达式有个感性认识，我们需要借助一个常见的命令grep来讲述。至于grep是个啥？大家暂时把他理解成一个搜索工具即可，详细用法后面会有一篇专门介绍grep的文章。现在，大家暂时“照猫画猫”跟着我做就行。 当grep与正则结合时，可以说是如胶似漆。。。不对，应该严肃点儿说是。。。如鱼得水！！！grep会根据“正则的含义”在文本中搜索符合条件的字符串。我们首先在目录下创建一个测试文件test，写入如下内容： 123456789[root@c7-test01 ~]# cat test kkutysllbI Love kkutysllbHe is an interesting manHe is my role modelThe qq number of kkutysllb is: 31468130kkutysllb's Homepage: https://kkutysllb.cnHis common names: kkutysllb,kkutys,123kkutysllb123,kkutysllb78kkutysllb cool 如果我们想搜索出test文件中包括kkutysllb的行，可以使用如下命令： 如上图，可以看出只要包含kkutysllb字符串的行都会被搜索出来。但是如果我们只想搜索以kkutysllb字符串的开始的行呢？这时候，正则表达式就派上用场了。在正则表达式中，^符号表示以什么字符串开头，^kkutysllb就表示以kkutysllb字符串开头。因此，为了满足我们的需求，可以使用如下命令： 那么，我们如果想查找以kkutysllb字符串结尾的行呢？可以使$符号来表示以什么字符串结尾，kkutysllb$就表示以kkutysllb字符串结尾。命令如下： 我们学会了^和$，知道它们是正则表达式中分别用于锚定行首和行尾，那么如果我们把它们结合起来使用呢？比如：^kkutysllb$代表几个意思？我们先来分下下，^kkutysllb表示以kkutysllb字符串开头，紧接着kkutysllb$代表以kkutysllb字符串结尾，也就是说^kkutysllb$代表以kkutysllb开头同时以kkutysllb结尾的字符串，也就是说整行只有kkutysllb一个字符串的场景。我们不妨验证下，命令如下： 如上图，聪明如我，果然如此！那如果符号^和$之间什么都没有呢？也就是我们要匹配^$代表什么意思？其实，^$就代表空行的意思。为了显示清楚，我们将空行的的行号打印出来，命令如下： 可以看到我们的测试文件中第9行与第10行为空行，与实际情况一样。 现在，我们已经能够灵活的锚定行首和行尾了，那么，正则表达式能不能锚定词首或词尾呢？那必须滴。。。在正则表达式中，”\&lt;”表示锚定词首，”\&gt;”表示锚定词尾。比如：我们想搜索单词以kkutys开头的行，命令如下： 如上图，可以看见123kkutysllb123这个单词没有被匹配到，至于它所在行被匹配输出，是因为它前后单词都是以kkutys开头的。我们再来匹配以单词cool结尾的行，命令如下： 那么，聪明如我你一定想到了，要匹配整个单词，就将”\&lt;”和”\&gt;”结合起来使用就行了。其实，在正则表达式中，除了使用”\&lt;”和”\&gt;”去锚定词首和词尾外，还可以使用“\b”去完成同样的锚定功能。验证如下： 如上图，表示搜索以123开头，同时以123结尾，中间是任意多个字母组成的单词的行。根据搜索条件，就找到了123kkutysllb123这个单词所在的行。这里的示例主要是说明“\b”可以取代”\&lt;”和”\&gt;”来锚定词首和词尾，在实际中shell脚本中我也建议大家这样使用。至于[a-z]*的匹配规则后面会讲到，目前入门这里不是重点。 “\b”还有一个孪生兄弟“\B”，虽然它们有“血缘”，但是“性格迥异”，也就是功能完全不一样。“\b”是用来锚定词首和词尾，换句话说也就是用来锚定单词的边界。而“\B”正好相反，它是用来匹配非单词边界的，这样说可能并不容易理解，看了底下的示例你会秒懂！！！示例如下： 如上图，它的意思是匹配非kkutysllb单词所在的行，也就是说只要该行中存在不是以kkutysllb作为词首和词尾，而是作为中间内容的单词，就会匹配输出。至于“\B”匹配词首或词尾的用法，聪明如我你一定会掌握，这里不再赘述。 小结通过上面的示例，大家可能发现我们使用的都是与“位置”有关，比如“行首、行尾、词首、词尾”等，我们可以把上面是示例中用到的符号归纳为“位置匹配”有关的正则表达式符合，我们不妨做个总结，便于以后查询。这些“位置匹配”相关的符号包括：^、$、\&lt;、\&gt;、\b、\B。 ^：表示锚定行首，此字符后面的任意内容必须出现在行首，才能匹配。 $：表示锚定行尾，此字符前面的任意内容必须出现在行尾，才能匹配。 ^$：表示匹配空行，这里所描述的空行只表示回车符，而空格和tab键制表符等只能算空字符串，不能当做空行处理。 ^abc$：表示abc独占一行的场景才会被匹配到。 \&lt;或者\b：表示锚定词首，其后面的字符必须作为单词首部出现才能被匹配。 \&gt;或者\b：表示锚定词尾，其前面的字符必须作为单词尾部出现才能被匹配。 \B：用于匹配非单词边界，与\b是“性格迥异”的亲兄弟。 在正则表达式中，包含基础正则表达式和扩展正则表达式两种，大家暂时不用纠结，后面会有专门总结扩展正则表达式的文章。我们现在主要需要掌握的都是基础正则表达式，只要学会了基础正则表达式，掌握扩展正则表达式只是分分钟的事情。]]></content>
      <categories>
        <category>shell编程</category>
      </categories>
      <tags>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-12-网络虚拟化概述]]></title>
    <url>%2F2019%2F06%2F12%2F2019-06-12-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[网络作为提供数据交换的模块，是数据中心内最为核心的基础设施之一，并直接关系到数据中心的能力、规模、可扩展性和管理性。为了满足日益增长的网络服务需求，特别是移动互联网业务的爆发式增长，数据中心逐渐向大型化、自动化、虚拟化、多租户的方向发展。传统网络设备不仅部署慢、调整难、成本高，而且其二层地址表项的规模直接决定了数据中心的规模。为此，网络虚拟化的概念应运而生。 事实上，网络虚拟化这个概念由来已久。早在1990年代，世界上就出现了第一个虚拟局域网VLAN，后来逐渐出现GRE、VPN、L2TP等网络虚拟化技术。到今天，很多公司如Vmware、华为等都在使用这种不依赖物理设备的技术。最初，网络虚拟技术被设置为一个简单的开关功能，后续随着二层广播风暴的隔离和跨网络连接需求的出现，VLAN和VPN技术也应运而生。如今随着云计算技术的推动，在数据中心层面不再是简单的三层架构，而是演进出了大二层和spine-leaf架构，同时随着东西向流量的增加，I/O虚拟化技术、虚拟接入识别、物理网络可靠性以及路由网络随选SDN等技术为云数据中心提供自动化的强有力手段。 传统二、三层网络中的虚拟化网络的基础知识—OSI模型和TCP/IP模型开放式系统互联通信参考模型（Open System Interconnection Reference Model，OSI），简称为OSI模型（OSI model），如下图左边所示，是一种概念模型，由国际标准化组织提出，是一个试图使各种计算机在世界范围内互连为网络的标准框架。OSI的七层网络协议体系结构的概念清楚，理论也较为完整，但是它既复杂也不实用。 为此，互联网协议套件（Internet Protocol Suite，IPS）的概念被提出，它是一个网络通信模型，包含整个网络传输协议家族，是网络的基础通信架构，通常被称为TCP/IP协议族（TCP/IP Protocol Suite，或TCP/IP Protocols），简称TCP/IP模型。如上图右边所示，OSI模型与TCP/IP模型的对比示意图。 TCP/IP模型应用的非常广泛，它是一个四层的体系结构，包括：网络接口层、网际层（IP）、传输层（TCP或UDP）、应用层（各种应用层协议，如：TELNET、FTP、SMTP等）。通过这四层的协同工作，能够完成一些特定的任务。每一层创建在低一层提供的服务上，并且为高一层提供服务。 整个TCP/IP协议栈则负责解决数据如何通过许许多多个点对点通路（一个点对点通路，也称为一”跳”, 1 hop）顺利传输，由此不同的网络成员能够在许多”跳”的基础上创建相互的数据通路。 绕不开的二层和三层二层交换技术是发展比较成熟的技术，二层交换机属数据链路层设备，可以识别数据包中的MAC地址信息，根据MAC地址进行转发，并将这些MAC地址与对应的端口记录在自己内部的一个地址表中。三层交换技术就是将路由技术与交换技术合二为一的技术。在对第一个数据流进行路由后，它将会产生一个MAC地址与IP地址的映射表，当同样的数据流再次通过时，将根据此表直接从二层通过而不是再次路由，从而消除了路由器进行路由选择而造成网络的延迟，提高了数据包转发的效率。 二层网络就是数据链路层，只完成本地网络的互通，只识别相同的数据链路层协议，二层交换机是基于MAC地址转发，并且支持高密度以太网接口。而三层网络就是IP网络层，负责不同物理网络的连接，就像是树干一样，将物理世界与应用世界互联。可以识别多种链路层协议，使用IP协议屏蔽差异，兼容互联。三层路由器都是基于IP地址转发，可以支持ATM/SDN/以太网等多种链路层接口。 数据包在二、三层网络中转发，其封装格式示意图如下所示，数据包从四层发送到三层会被添加上IP首部然后进行转发，同样数据包到达二层会被添加上二层协议如以太首部然后进行转发，这个过程叫做封装。 数据包从二层发送到三层时，会被二层设备剥离对应的首部，露出上层设备能够识别的首部，如IP首部，同样三层向四层转发的时，也会剥离本层的协议首部露出上层设备能够识别的首部，这个过程叫做解封装。能够执行二层封装、解封装的设备为二层设备，如二层交换机。能够执行三层封装、解封装的设备为三层设备，如路由器。三层交换机既能执行二层封装解封装，也能执行三层封装解封装，为三层设备。 数据包在两个网络端点之间传输，可以有三种传送方式：单播、组播和广播。 单播方式中，发送源端明确知道目的端地址，直接使用该地址与接收者建立联系。组播方式中，发送源端将数据同时传递给一组目的地址，多个client组成接收者，接收者与源之间的路径由组播协议计算后得出。在广播方式中，发送源端不知道目的端地址，首先会发一个询问给在同一广播域的所有设备，真正的接收者收到该询问后会给源以单播的方式回一个答复，其他设备则不理会该询问。当网络很大的时候，广播包会占用一定的带宽，造成带宽的浪费，一个二层的本地网络就是广播域。 最原始的网络虚拟化VLAN的技术实现在实际的物理二层网络中，经常会有广播包的发送需求，如果不加限制，除了会带来安全性和带宽占用浪费等问题外，最重要的是会产生广播风暴。所谓广播风暴，是指由于网络拓扑的设计和连接问题，或者其他原因导致广播包在网段内大量复制传播，导致网络性能下降甚至瘫痪。如下图所示，为了解决这个问题，传统网络使用虚拟局域网技术VLAN来实现。 VLAN的主要作用就是隔离广播域，不同的VLAN之间不能直接通信。VLAN技术把用户划分成多个逻辑的网络（group），组内可以通信，组间不允许直接通信，二层转发的单播、组播、广播报文只能在组内转发。同时，VLAN技术可以很容易地实现组成员的添加或删除。也就是说，VLAN技术提供了一种管理手段，控制终端之间的互通。如上图，组1和组2的PC无法相互直接通信。 如下图所示，VLAN是通过对传统的数据帧添加tag字段来实现的。添加VLAN信息的方法最常用的就是IEEE802.1Q协议，还有一种是ISL协议。 IEEE802.1Q所附加的VLAN识别信息，位于数据帧中“发送源MAC地址”与“类别域（Type Field）”之间。具体内容为2字节的TPID和2字节的TCI，共计4字节。 在数据帧中添加了4字节的内容，那么CRC值自然也会有所变化。这时数据帧上的CRC是插入TPID、TCI后，对包括它们在内的整个数据帧重新计算后所得的值。TPID (Tag Protocol Identifier）是IEEE定义的新的类型，表明这是一个加了802.1Q标签的帧，TPID包含了一个固定的值0x8100。TCI (Tag Control Information）包括用户优先级(User Priority，3bit)、规范格式指示器(Canonical Format Indicator，1bit)和 VLAN ID（12bit）。VLAN ID 是对 VLAN 的识别字段，支持4096(2的12次方) VLAN 的识别。在4096个可能的VID 中，VID＝0 用于识别帧优先级。 4095(FFF)作为预留值，所以 VLAN 配置的最大可能值为4094，有效的VLAN ID范围一般为1-4094。 在VLAN中有以下两种链路类型：普通链路和中继链路。普通链路（Access Link）是用于连接用户主机和交换机的链路。通常情况下，主机并不需要知道自己属于哪个VLAN，主机硬件通常也不能识别带有VLAN标记的帧。因此，主机发送和接收的帧都是untagged帧。中继链路（Trunk Link）是用于交换机间的互连或交换机与路由器之间的连接。中继链路可以承载多个不同VLAN数据，数据帧在中继链路传输时，中继链路的两端设备需要能够识别数据帧属于哪个VLAN，所以在中继链路上传输的帧都是Tagged帧。 在VLAN网络中，针对上述两种不同的链路类型使用场景，分别有三种网络端口类型去适配：Access端口、Trunk端口和Hybrid端口。Access接口是交换机上用来连接用户主机的接口，它只能接入【普通链路。仅仅允许唯一的VLAN ID通过本接口，这个VLAN ID与接口的缺省VLAN ID相同，Access接口发往对端设备的以太网帧永远是不带标签的untagged帧。Trunk接口是交换机上用来和其他交换机连接的接口，它只能连接中继链路，允许多个VLAN的帧（带Tag标记）通过。Hybrid接口是交换机上既可以连接用户主机，又可以连接其他交换机的接口。Hybrid接口既可以连接普通链路又可以连接中继链路。Hybrid接口允许多个VLAN的帧通过，并可以在出接口方向将某些VLAN帧的Tag剥掉。而在虚拟交换机只用Access接口和Trunk接口。 各类型接口对数据帧的处理方式汇总如下： 接口类型 对接收不带Tag的报文处理 对接收带Tag的报文处理 发送帧处理过程 Access接口 接收该报文，并打上缺省的VLAN ID。 当VLAN ID与缺省VLAN ID相同时，接收该报文。当VLAN ID与缺省VLAN ID不同时，丢弃该报文。 先剥离帧的PVID Tag，然后再发送。 Trunk接口 打上缺省的VLAN ID，当缺省VLAN ID在允许通过的VLAN ID列表里时，接收该报文。打上缺省的VLAN ID，当缺省VLAN ID不在允许通过的VLAN ID列表里时，丢弃该报文。 当VLAN ID在接口允许通过的VLAN ID列表里时，接收该报文。当VLAN ID不在接口允许通过的VLAN ID列表里时，丢弃该报文。 当VLAN ID与缺省VLAN ID相同，且是该接口允许通过的VLAN ID时，去掉Tag，发送该报文。当VLAN ID与缺省VLAN ID不同，且是该接口允许通过的VLAN ID时，保持原有Tag，发送该报文。 Hybrid接口 打上缺省的VLAN ID，当缺省VLAN ID在允许通过的VLAN ID列表里时，接收该报文。打上缺省的VLAN ID，当缺省VLAN ID不在允许通过的VLAN ID列表里时，丢弃该报文。 当VLAN ID在接口允许通过的VLAN ID列表里时，接收该报文。当VLAN ID不在接口允许通过的VLAN ID列表里时，丢弃该报文。 当VLAN ID是该接口允许通过的VLAN ID时，发送该报文。可以通过命令设置发送时是否携带Tag。 爱折腾的VPNVPN技术起初是为了解决明文数据在网络上传输带来的安全隐患而产生的。TCP/IP协议族中的很多协议都采用明文传输，如telnet、ftp、tftp等。一些黑客可能为了获取非法利益，通过诸如窃听、伪装等攻击方式截获明文数据，使企业或者个人蒙受损失。VPN技术可以从某种程度上解决该问题。例如，它可以对公网上传输的数据进行加密，即使黑客通过窃听工具截获到数据，也无法了解数据信息的含义。VPN也可以实现数据传输双方的身份验证，避免黑客伪装成网络中的合法用户攻击网络资源。 VPN（virtual private network，虚拟专用网）就是在两个网络实体之间建立的一种受保护的连接，这两个实体可以通过点到点的链路直接相连，但通常情况下他们会相隔较远的距离。 VPN技术通过使用加密技术防止数据被窃听，并且通过数据完整性校验防止数据被破坏、篡改。通过认证机制实现通信双方身份确认，来防止通信数据被截获和回放。此外，在VPN中还可以定义何种流量需要被保护，数据被保护的机制以及数据封的过程。 VPN技术有两种基本的连接模式：隧道模式和传输模式。这两种模式实际上定义了两台实体设备之间传输数据时所采用的不同的封装过程。传输模式一个最显著的特点就是：在整个VPN的传输过程中，IP包头并没有被封装进去，这就意味着从源端到目的端数据始终使用原有的IP地址进行通信。如下图所示，而传输的实际数据载荷被封装在VPN报文中。对于大多数VPN传输而言，VPN的报文封装过程就是数据的加密过程，因此，攻击者截获数据后将无法破解数据内容，但却可以清晰地知道通信双方的地址信息。 由于传输模式封装结构相对简单（每个数据报文较隧道模式封装结构节省20字节），因此传输效率较高，多用于通信双方在同一个局域网内的情况。 隧道模式中，如下图所示VPN设备将整个三层数据报文封装在VPN数据内，再为封装后的数据报文添加新的IP包头。由于新IP包头中封装的是VPN设备的ip地址信息，所以当攻击者截获数据后，不但无法了解实际载荷数据的内容，同时也无法知道实际通信双方的地址信息。 由于隧道模式的VPN在安全性和灵活性方面具有很大的优势，在企业环境中应用十分广泛，总部和分公司跨广域网的通信、移动用户在公网访问公司内部资源等很多情况，都会应用隧道模式的VPN对数据传输进行加密。 通常情况下，VPN的类型分为站点到站点VPN和远程访问VPN。站点到站点VPN就是通过隧道模式在VPN网关之间保护两个或者更多的站点之间的流量，站点间的流量通常是指局域网之间（L2L）的通信流量。L2L的VPN多用于总部与分公司、分公司之间在公网上传输重要业务数据。比如：我们各地市VOBB固网用户接入方式就是这种类型，对于两个地市的固网终端用户来说，在VPN网关（针对现网来说可以看成一个统一的逻辑设备，包含现网中的SR、BRAS、FW、CE、SBC等设备)中间的网络是透明的，就好像通过一台路由器连接的两个局域网。各地市终端设备通过VPN连接访问核心网或其他地市接入网络资源。数据包封装的地址都是各地市规划的内网地址（一般为私有地址），而VPN网关对数据包进行的再次封装过程，客户端是全然不知的。 远程访问VPN通常用于单用户设备与VPN网关之间通信连接，单用户设备一般为一台pc或小型办公网络等。VPN连接的一端为PC，可能会让很多人误解远程访问VPN使用传输模式，但因为该种VPN往往也是从公网传输关键数据，而且单一用户更容易成为黑客的攻击对象，所以远程访问VPN对于安全性要求较高，更适用于隧道模式，如下图所示。 要想实现隧道模式的通信，就需要给远程客户端分配两个IP地址：一个是它自己的NIC地址，另一个是内网地址。也就是说远程客户端在VPN建立过程中同时充当VPN网关（使用NIC地址）和终端用户（使用内网地址）。比如：我们使用VPN代理软件从公网访问4A服务器，或者私有云远程用户从公网访问私有云资源都是这种方式。 数据中心对网络的总体要求随着电信云NFV的全面部署，以及后续5G网络架构的演进，利用虚拟化和面向服务的技术，能够为智能设备提供广泛的业务服务。虚拟化是IaaS服务的基础，计算虚拟化将一台物理服务器”分裂“成多个虚拟服务器来调度，虚拟服务器作为业务的承载者和物理服务器一样，有着网络通信需求。也就是说不仅虚拟服务器之间需要网络通信，虚拟服务器与外部也存在网络通信的需求。在这种模式下，计算和存储能力向网络中心迁移，形成云化数据中心，大量的计算请求、信息请求依托网络向数据中心发送，网络成为数据中心的和用户的纽带。因此，如下图所示，云化数据中心对网络存在业务发展弹性、虚拟感知技术、网络资源整合和共享、高效运维和能耗管理等4大要求。 业务发展弹性需求在信息化蓬勃发展的今天，业务流量增长异常迅猛。为了应对这一切，数据中心不仅需要提升服务器性能和网卡接入带宽，也需要充分利用现有的IT资源。分布式计算和虚拟化应运而生，流量模型随之从南北向流量为主向东西向流量为主转变。 从Gartner 的报告中可以看到服务器10GE TOR 接入从2011年开始迅速成为主流，所占份额不断扩大，必然会导致网络侧上行40GE/100GE互联大量应用。横向交互流量增大使网络模型从传统的三层模型向胖树架构演进，横向无阻塞和大缓存成为数据中心网络规划设计的基本需求。 在这种计算、存储、网络整合的云数据中心内，对网络的要求由连接变为服务。因为，云计算本身就是一种服务，能按需、弹性提供，且可计量。网络资源被云计算整合在基础设施资源中，自然也是一种可按需、弹性提供，且可计量的服务。 虚拟感知技术需求服务器虚拟化使得可以高效利用IT资源，降低企业运营成本成为可能。服务器内多虚拟机之间的交互流量，传统网络设备无法感知，也不能进行流量监控和必要的策略控制。虚拟机的灵活部署和动态迁移需要网络接入侧做相应的调整，在迁移时保持业务不中断。 虚拟机的大量运用以及虚拟机交互流量的出现，使网络的前沿深入到服务器内部。虚拟拓扑发现，虚拟机策略下发，接入侧交换机网络配置和动态调整对传统网络模型和管理模式提出了很大的挑战。虚拟机迁移的物理范围不应过小，否则无法充分利用空闲的服务器资源。迁移后虚拟机的IP地址不变，以保持业务不中断。因此，迁移不能跨VLAN。综合以上两点，对数据中心网络提出了大二层的需求。 网络资源的整合与共享数据中心对网络可靠性和安全性的需求是最基本的需求。可靠性设计包括：链路冗余、关键设备冗余和重要业务模块冗余。安全性设计包括物理空间的安全控制及网络的安全控制。企业多业务系统安全隔离和冗余设计导致网络资源成本高昂，利用率低，运维复杂。 在云计算时代，为了充分利用网络资源实现业务的灵活部署，需要将多业务网络纵向融合成一张物理网，通过网络设备的虚拟化实现业务隔离和冗余备份。对于多类型的网络可以横向融合，通过采用FCOE和DCB等技术降低管理复杂性以及部署扩展性的挑战。 高效的运维的电源管理数据中心内部存在网络设备和IT资源数量大，厂商多，运行配置复杂等问题。数据中心网络延伸到服务器内部，需要物理和虚拟网络拓扑完整展示，实现网络流量的精细化管理和监控，针对网络故障快速定位是对云计算时代数据中心运维的基本需求。 不断攀升的能耗成本，催高了数据中心的运营成本，绿色节能是云计算数据中心的必备条件。 数据中心层面的网络虚拟化随着云计算、大数据、物联网等技术的发展，传统的网络虚拟化技术，已经难以满足云时代下多租户的需求。例如：广泛被使用的VLAN技术，虽然可以在物理交换机上通过划分多个VLAN来隔离，并虚拟出多个逻辑网络，但是其设计和配置，通常基于固定的规划，以及网络和服务器的位置不会频繁变更为前提。而面对云化的数据中心，大量虚拟机的动态的生命周期变化，以及弹性漂移和伸缩的特点，对网络提出了更高的按需配置和随动的要求。也就说，在云化数据中心内，网络不仅仅提供连接，更是一种服务，一种像虚拟化的计算资源一样逻辑隔离，弹性且可计量的服务。 从计算虚拟化走向网络虚拟化网络虚拟化是指将网络的控制从网络硬件中脱离出来，交给虚拟化的网络层处理。这个虚拟化的网络层加载在整个物理网络之上，屏蔽掉底层的物理差异，在虚拟空间重建整个网络。如下图所示，就像计算虚拟化中将物理服务器整合抽象为计算资源池一样，物理网络也被泛化为网络资源池，通过控制面软件的集中调度，使得底层网络资源更加灵活。 逻辑网络资源池是一种逻辑资源的灵活管理抽象，是对底层物理网络的一种形象描述。每一个虚拟网络可以根据业务或部门进行灵活分配，各个虚拟网络之间逻辑隔离。因此，每个虚拟网络内的网络资源变更，不会影响其它虚拟网络。与服务器虚拟化类似，网络虚拟化可以在很短的时间（秒级）创建L2、L3到L7的网络服务，如交换，路由，防火墙和负载均衡等。虚拟网络独立于底层网络硬件，可以按照业务需求配置、修改、保存、删除，而无需重新配置底层物理硬件或拓扑。这种网络技术的革新为实现软件定义网络SDN和软件定义数据中心（SDDC）奠定了基础。 云化数据中心内部的网络要同时解决多租户资源隔离和内外互通访问的需求。多租户环境中主要存在私有云场合，一个租户就是任何一个应用，租户内的安全、资源对外隔离且排他，因此需要有网络的隔离作为基础。同时，云化的数据中心不再局限四面墙之内，需要实现地理上相互隔离的故障转移性，位于虚拟网络内的应用需要访问外部网络环境，外部网络环境也需要访问虚拟网络内的资源。因此，虚拟网络也必须具备内外互通的开放性，通过利用隧道封装技术实现跨地理资源的迁移特性。 如下所示，隧道封装技术是一种通过使用互联网络的基础设施在网络之间传递数据的方式，使用隧道传递的数据（或负载）可以是不同协议的数据帧或包。隧道协议将这些数据帧或包重新封装在新的包头中发送，新的包头提供路由信息，使封装的负载数据可以通过互联网络传递。被封装的数据包在隧道的两个端点之间通过公共互联网络进行路由，其所经过的逻辑路径就是隧道。一旦到达网络端点，数据将被解包并转发到最终的目的地。 网络虚拟化将网络的边缘从硬件交换机推到了服务器里面，将服务器和虚拟机的所有部署、管理的职能从原来的系统管理员+网络管理员的模式变成了纯系统管理员的模式，让服务器的业务部署变得简单，不再依赖于形态和功能各异的硬件交换机，一切归于软件控制，实现自动化部署。这就是网络虚拟化在数据中心中最大的价值所在，也是为什么大家明知服务器的性能远远比不上硬件交换机但还是使用网络虚拟化技术的根本原因。 数据中心网络虚拟化的层次随着越来越多的服务器被虚拟化，网络已经延伸到Hypervisor内部，网络通信的端点已经从以前的服务器变成了运行在服务器中的虚拟机，数据包从虚拟机的虚拟网卡流出，通过Hypervisor内部的虚拟交换机，在经过服务器的物理网卡流出到上联交换机。因此，虚拟化环境下的网络虚拟化需要解决端到端的问题。在整个过程中，虚拟交换机，网卡的I/O问题以及虚拟机的网络接入都是虚拟化的重点。如下图所示，每一个层面网络虚拟化的实现技术和方式均不同，我个人将其归纳为4个部分：服务器内部I/O虚拟化、服务器内部的虚拟接入、服务器与物理网络的连接、物理交换网络。 第一部分是服务器内部的IO虚拟化。多个虚拟机共享服务器中的物理网卡，需要一种机制既能保证I/O的效率，又要保证多个虚拟机对用物理网卡共享使用。I/O虚拟化的出现就是为了解决这类问题，详情可参见本站《计算虚拟化之I/O虚拟化》一文，这里不再赘述。 第二部分是服务器内部的虚拟接入识别。用于识别不同虚拟机的网络包。在传统的服务器虚拟化方案中，从虚拟机的虚拟网卡发出的数据包在经过服务器的物理网卡传送到外部网络的上联交换机后，虚拟机的标识信息被屏蔽掉了，上联交换机只能感知从某个服务器的物理网卡流出的所有流量而无法感知服务器内某个虚拟机的流量，这样就不能从传统网络设备层面来保证QoS和安全隔离。虚拟接入要解决的问题是要把虚拟机的网络流量纳入传统网络交换设备的管理之中，需要对虚拟机的流量做标识。 在解决虚拟接入的问题时，思科和惠普分别提出了自己的解决方案。思科的是VN-Tag, 惠普的方案是VEPA(VirtualEthernet Port Aggregator)。为了制定下一代网络接入的话语权，思科和惠普这两个巨头在各自的方案上都毫不让步，纷纷将自己的方案提交为标准，分别为802.1Qbh和802.1Qbg。 第三部分是服务器到网络的连接。网络连接技术一直都在追求更高的带宽中发展，比如Infiniband和10Gb以太网。在传统的企业级数据中心IT构架中，服务器到存储网络和互联网络的连接是异构和分开的。存储网络用光纤，互联网用以太网线（ISCSI虽然能够在IP层上跑SCSI，但是性能与光纤比还是差的很远）。数据中心连接技术的发展趋势是用一种连接线将数据中心存储网络和互联网络聚合起来，使服务器可以灵活的配置网络端口，简化IT部署。以太网上的FCOE技术和**Infiniband技术本身都使这种趋势成为可能。 Infiniband 技术产生于上个世纪末，是由Compaq、惠普、IBM、戴尔、英特尔、微软和Sun七家公司共同研究发展的高速先进的I/O标准。InfiniBand是一种长缆线的连接方式，具有高速、低延迟的传输特性。基于InfiniBand技术的网卡的单端口带宽可达20Gbps，为了发挥Infiniband设备的性能，需要一整套的软件栈来驱动和使用，这其中最著名的就是OFED（OpenFabrics Enterprise Distribution）,它基于Infiniband设备实现了RDMA（remote direct memoryaccess）。RDMA的最主要的特点就是零拷贝和旁路操作系统，数据直接在设备和应用程序内存之间传递，这种传递不需要CPU的干预和上下文切换。OFED还实现了一系列的其它软件栈：IPoIB（IP over Infiniband），SRP（SCSI RDMA Protocol）等，这就为Infiniband聚合存储网络和互联网络提供了基础。OFED由OpenFabrics联盟负责开发。 FCOE的出现则为数据中心互联网络和存储网络的聚合提供了另一种可能。FCOE是将光纤信道直接映射到以太网线上，这样光纤信道就成了以太网线上除了互联网网络协议之外的另一种网络协议。FCOE能够很容易的和传统光纤网络上运行的软件和管理工具相整合，因而能够代替光纤连接存储网络。虽然出现的晚，但FCOE发展极其迅猛。与Infiniband技术需要采用全新的链路相比，企业更愿意升级已有的以太网。在两者性能接近的情况下，采用FCOE方案似乎性价比更高。 第四部分是网络交换。需要将物理网络和逻辑网络有效的分离，另外网络设备如交换机、路由器等需要具备1：N和N:1的虚拟化能力。在这一层面上要解决的问题则是要对现有的互联网络进行升级，使之满足新业务的需求，网络虚拟化则是这一变革的重要方向。在这一方向上目前有两种做法，一种是在原有的基础设施上添加新的协议来解决新的问题；另一种则完全推倒重来，希望设计出一种新的网络交换模型。 当虚拟数据中心开始普及后，虚拟数据中心本身的一些特性引入了对网络新的需求。物理机的位置一般是相对固定的，虚拟化方案的一个很大的特性在于虚拟机可以迁移。当虚拟机的迁移发生在不同网络，不同数据中心之间时，对网络产生了新的要求，比如需要保证虚拟机的IP在迁移前后不发生改变，需要保证虚拟机内运行在第二层（链路层）的应用程序也在迁移后仍可以跨越网络和数据中心进行通信等等。在这方面，Cisco连续推出了OTV，LISP和VXLAN等一系列解决方案，也就是隧道封装技术。 以上就是云计算时代网络虚拟化的基本概述，不知你看明白没有，反正我是觉得我写明白了！！！]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-09-Linux原生的存储虚拟化软RAID和LVM]]></title>
    <url>%2F2019%2F06%2F09%2F2019-06-09-Linux%E5%8E%9F%E7%94%9F%E7%9A%84%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BD%AFRAID%E5%92%8CLVM%2F</url>
    <content type="text"><![CDATA[为了让大家更好理解存储虚拟化的特点，本文将讲解各个常用RAID技术方案的特性，并通过实际部署软RAID 10、RAID 5+备份盘等方案来更直观地体验RAID的效果，以便进一步了解生产环境对硬盘设备的IO读写速度和数据冗余备份机制的需求。同时，本文还将介绍LVM的部署、扩容、缩小、快照以及卸载删除的相关知识，以便让大家通过开源存储虚拟化基础对存储虚拟化的块级虚拟化和文件系统级虚拟化有个更深刻的理解。 RAID 技术这里主要介绍开源RAID技术，至于华为的RAID2.0技术详见存储虚拟化其他文章。RAID技术通过把多个硬盘设备组合成一个容量更大、安全性更好的磁盘阵列，并把数据切割成多个区段后分别存放在各个不同的物理硬盘设备上，然后利用分散读写技术来提升磁盘阵列整体的性能，同时把多个重要数据的副本同步到不同的物理硬盘设备上，从而起到了非常好的数据冗余备份效果。 RAID技术的设计初衷是减少因为采购硬盘设备带来的费用支出，但是与数据本身的价值相比较，现代企业更看重的则是RAID技术所具备的冗余备份机制以及带来的硬盘吞吐量的提升。RAID技术的几种状态如下图所示： RAID组变为降级状态后，在重建的过程中，如果还有别的成员盘出现故障，故障的成员盘的个数超过了阵列的冗余磁盘的个数，整个RAID组将为变为失效状态，此时原RAID组中的数据将会无法读取。 RAID 0RAID 0技术把多块物理硬盘设备（至少两块）通过硬件或软件的方式串联在一起，组成一个大的卷组，并将数据依次写入到各个物理硬盘中。在最理想的状态下，硬盘设备的读写性能会提升数倍，但是若任意一块硬盘发生故障将导致整个系统的数据都受到破坏。也就说，RAID 0技术能有效提升硬盘数据的吞吐率，但不具备数据备份和错误修复能力。 如上图，RAID 0使用“分条”（stripe）技术把数据分布到各个磁盘上，RAID 0至少使用两个磁盘，并将数据分成从512字节到数兆字节（一般是512Byte的整数倍）的若干块，这些数据块可以并行写到不同的磁盘中。第1块数据被写到驱动器1中，第2块数据被写到驱动器2中，如此类推，当系统到达阵列中的最后一个磁盘时，就重新回到驱动器1的下一分条进行写操作，分割数据将I/O负载平均分配到所有的驱动器。 RAID 0的数据写入是以分条形式将数据均匀分布到RAID 组的各个硬盘中。即一个分条的所有分块写满后，再开始在下一个分条上进行数据写入。如上图，现在有数据D0 ，D1 ，D2 ，D3 ，D4 ，D5需要在RAID 0中进行写入，首先将第一个数据D0写入第一块硬盘位于第一个分条的块，将第二个数据D1写入第二块硬盘位于第一个分条的块，至此，第一个分条的各个块写满了数据，当有数据D2需要写入时，就要对下一个分条进行写入，将数据D2写入第一块硬盘位于第二个分条的块中… 数据块D3，D4，D5的写入同理。写满一个分条的所有块再开始在下一个分条中进行写入。 RAID 0在收到数据读取指令后，就会在各个硬盘中进行搜索，看需要读取的数据块位于哪一个硬盘上，再依次对需要读取的数据进行读取。如上图，现在收到读取数据D0 ，D1 ，D2 ，D3 ，D4 ，D5的指令，首先从第一块磁盘读取数据块D0，再从第二块磁盘读取数据块D1…对各个数据块，从磁盘阵列读取后再由RAID控制器进行整合后传送给系统。至此，整个读取过程结束。 RAID 1RAID 1技术是把两块以上的硬盘进行绑定，在写入数据时，是将数据同时写入到多块硬盘设备上，其中某一块硬盘用作数据的备份或镜像。当其中某一块硬盘发生故障后，一般会自动切换的方式来恢复数据的正常使用。 如上图，RAID 1也被称为镜像，其目的是为了打造出一个安全性极高的RAID。RAID 1使用两组相同的磁盘系统互作镜像，速度没有提高，但是允许单个磁盘故障，数据可靠性最高。其原理为在主硬盘上存放数据的同时也在镜像硬盘上写一样的数据。当主硬盘（物理）损坏时，镜像硬盘则代替主硬盘的工作。因为有镜像硬盘做数据备份，所以RAID 1的数据安全性在所有的RAID级别上来说是最好的。 RAID 1在进行数据写入的时候，并不是像RAID 0那样将数据分条写入所有磁盘，而是将数据分别写入成员盘，各个成员磁盘上的数据完全相同，互为镜像。如上图，需要将数据块D0，D1，D2写入RAID 1，先在两个磁盘上同时写入数据块D0，再在两个磁盘上同时写入数据块D1，以此类推。 RAID 1在进行数据读取的时候，正常情况下可以实现数据盘和镜像盘同时读取数据，提高读取性能，如果一个磁盘损坏，则IO自动到存活的盘读取数据。 RAID 1的成员磁盘是互为镜像的，成员磁盘的内容完全相同，这样，任何一组磁盘中的数据出现问题，都可以马上从其它成员磁盘进行镜像恢复。比如：磁盘1损坏导致数据丢失，我们需要将故障磁盘用正常磁盘替换，再读取磁盘2的数据，将其复制到磁盘1上，从而实现了数据的恢复。 RAID 3RAID 3是带有专用奇偶位的条带化阵列，是RAID 0的一种改进模式。它也采用了奇偶校验技术，不过没有使用海明码技术而采用较为简单的异或算法。在阵列中有一个驱动器专门用来保存其它驱动器中对应分条中数据的奇偶校验信息。奇偶位是编码信息，如果某个驱动器中的数据出错或者某一个驱动器故障，可以通过对奇偶校验信息的计算来恢复出故障驱动器中的数据信息。在数据密集型环境或者是单一用户环境中，组建RAID 3对访问较长的连续记录较好。在写入数据时，RAID 3会把数据的写入操作分散到多个磁盘上进行，然而不管是向哪一个数据盘写入数据，都需要同时重写校验盘中的相关信息。因此，对于那些经常需要执行大量写入操作的应用来说，校验盘的负载将会很大，无法满足程序的运行速度，从而导致整个RAID系统性能的下降。 RAID 3是单盘容错并行传输。即采用Stripping技术将数据分块，对这些块进行异或校验，校验数据写到最后一个硬盘上。它的特点是有一个盘为校验盘，数据以位或字节的方式存于各盘（分散记录在组内相同扇区的各个硬盘上）。当一个硬盘发生故障，除故障盘外，写操作将继续对数据盘和校验盘进行操作。 RAID3的数据读取是按照分条来进行的。将每个磁盘的驱动器主轴马达做精确的控制，同一分条上各个磁盘上的数据位同时读取，各个驱动器得到充分利用，读性能较高。 RAID 3的数据读写属于并行方式。 RAID 3的数据恢复是通过对剩余数据盘和校验盘的异或计算重构故障盘上应有的数据来进行的。如上图的RAID 3磁盘结构，当磁盘2故障，其上存储的数据位A1，B1，C1丢失，我们需要经过这样一个数据恢复过程：首先恢复数据A1，根据同一分条上其它数据盘和校验盘上的数据A0，A2，P1，进行异或运算，得到应有的数据A1，再用相同方法恢复出数据B1，C1的数据，至此，磁盘2上的数据全部得到了恢复。由于校验集中在一个盘，因此在数据恢复时，校验盘写压力比较大，影响性能。 RAID 5RAID5是一种旋转奇偶校验独立存取的阵列方式，它与RAID3不同的是没有固定的校验盘，而是按某种规则把奇偶校验信息均匀地分布在阵列所属的硬盘上，所以在每块硬盘上，既有数据信息也有校验信息。这一改变解决了争用校验盘的问题，能并发进行多个写操作。所以RAID 5即适用于大数据量的操作，也适用于各种事务处理，它是一种快速、大容量和容错分布合理的磁盘阵列。当有N块阵列盘时，可用容量为N-1块盘容量。 RAID 3、RAID 5中，在一块硬盘发生故障后，RAID组从ONLINE变为DEGRADED方式，直到故障盘恢复。但如果在DEGRADED状态下，又有第二块盘故障，整个RAID组的数据将丢失。 RAID 5的数据写入也是按分条进行的，各个磁盘上既存储数据块，又存储校验信息。一个分条上的数据块写入完成后，将产生的校验信息写入对应的校验磁盘中。由于RAID 5的数据是按照数据分条存储的，在读取的时候，按照分条进行读取。 当RAID5中某一个磁盘故障，在恢复的时候，可利用其它存活成员盘数据进行异或逆运算，恢复故障盘上的数据。 RAID 6RAID 6是带有两种校验的独立磁盘结构，采用两种奇偶校验方法，需要至少N+2(N&gt;2)个磁盘来构成阵列，一般用在数据可靠性、可用性要求极高的应用场合 。常用的RAID 6技术有RAID6 P＋Q和RAID6 DP。 RAID 6实际上是在RAID 5基础上为了进一步保证数据可用性和可靠性设计的一种RAID方式。与RAID 5相比除了有通常的异或校验方式外，还增加了另一种特殊的异或校验方式和该方式校验数据存放区域，因此RAID 6的数据冗余性能相当好。但是，由于增加了一个校验，所以写入的效率比RAID 5要低，而且控制系统的设计也更为复杂，第二个校验区也减少了有效存储空间。 目前RAID 6还没有统一的标准，各家公司的实现方式都有所不同，主要有以下两种方式： RAID P+Q： 华为、HDS RAID DP： NetApp 两种技术获取校验信息的方法不同，但是都能够在两块成员盘故障的情况下读取数据，数据不丢失。 1）RAID6 P＋Q的工作原理 RAID6 P＋Q需要计算出两个校验数据P和Q，当有两个数据丢失时，根据P和Q恢复出丢失的数据。校验数据P和Q是由以下公式计算得来的： ​ P=D0⊕ D1 ⊕ D2 …… ​ Q=(α⊗D0)⊕(β⊗D1)⊕(γ⊗D2)…… 在RAID6 P＋Q中，P和Q是两个相互独立的校验值，它们的计算互不影响，都是由同一分条上其它数据磁盘上的数据依据不同的算法计算而来的。 其中，P值的获得是通过同一分条上除P和Q之外的其它所有数据盘上数据的简单异或运算得到。Q值的获得过程就相对复杂一些，它首先对同一分条其他磁盘上的各个数据分别进行一个变换，然后再将这些变换结果进行异或操作而得到校验盘上的数据。这个变换被称为GF变换，它是一种常用的数学变换方法，可以通过查GF变换表而得到相应的变换系数，再将各个磁盘上的数据与变换系数进行运算就得到了GF变换后的数据，这个变换过程是由RAID控制器来完成的。 以上图为例，P1是由分条0中的数据D0、D1、D2进行简单的异或运算而得到的。同理，P2是由分条1中的数据D3、D4、D5进行简单的异或运算而得到， P3是由分条2中的数据D6、D7、D8进行简单的异或运算而得到。Q1是由分条0中的数据D0、D1、D2分别进行GF变换之后再进行异或运算而得到的。同理，Q2是由分条1中的数据D3、D4、D5分别进行GF变换之后再进行异或运算而得到， Q3是由分条2中的数据D6、D7、D8分别进行GF变换之后再进行异或运算而得到。 当某一个分条中有一块磁盘发生故障，根本不需要Q，直接用校验值P与其他正常数据进行异或运算就可以恢复出故障盘上面的数据，数据恢复比较方便。当分条中有两块磁盘发生故障，如果其中包含Q所在的磁盘，则可以先恢复出数据盘上面的数据，再恢复出校验盘Q上的校验值；如果故障盘不包含Q所在的盘，则可以将两个校验公式作为方程组，从而可以恢复出两个故障盘上面的数据。 2）RAID6 DP的工作原理 DP就是Double Parity，就是在RAID4所使用的一个行XOR校验磁盘的基础上又增加了一个磁盘用于存放斜向的XOR校验信息。 横向校验盘中P0—P3为各个数据盘中横向数据的校验信息。比如：P0=D0 XOR D1 XOR D2 XOR D3。斜向校验盘中DP0—DP3为各个数据盘及横向校验盘的斜向数据校验信息。比如：DP0=D0 XOR D5 XOR D10 XOR D15 RAID6 DP 同样也有两个相互独立的校验信息块，但是与RAID6 P＋Q 不同的是，它的第二块校验信息是斜向的。横向校验信息和斜向校验信息都使用异或校验算法而得到，横向校验盘中的信息的获得方法非常简单：P0是由条带0中的数据D0、D1、D2、 D3进行简单的异或运算而得到的。同理， P1是由分条1中的数据D4、D5、D6、D7进行简单的异或运算而得到。 斜向校验盘中校验信息的获得依然是采用数据之间的异或运算，只是数据块的选取相对复杂一些，是一个斜向的选取过程：由分条0上面第一个磁盘上的数据D0、分条1上面第二个磁盘上的数据D5、分条2上面第三个磁盘上的数据D10、分条3上面第四个磁盘上的数据D15经过异或校验而得到校验信息DP0；由分条0上面第二个磁盘上的数据D1、分条1上面第三个磁盘上的数据D6、分条2上面第四个磁盘上的数据D11、分条3上面校验盘上面的信息P3经过异或校验而得到校验信息DP1；由分条0上面第三个磁盘上的数据D2、分条1上面第四个磁盘上的数据D7、分条2上面校验盘上面的信息P2、分条3上面第一个磁盘上的数据D12经过异或校验而得到校验信息DP2。 RAID6 DP允许阵列中同时有两个磁盘失效，我们以上图为例，假设磁盘1、2故障，则数据D0、D1、D4、D5、D8、D9、D12、D13失效，其他磁盘数据和校验信息正常。我们来看一下数据恢复是怎样一个过程：首先根据DP2和斜向校验恢复出D12， D12 =D2 ⊕ D7 ⊕ P2 ⊕DP2，然后利用P3和横向校验恢复出D13， D13 =D12 ⊕ D14 ⊕ D15 ⊕P3 ；根据DP3斜向校验恢复出D8， D8 =D3 ⊕ P1 ⊕DP3 ⊕ D13，利用P2和横向校验恢复出D9， D9 =D8 ⊕ D10 ⊕ D11 ⊕ P2；根据DP4和斜向校验恢复出D4，利用P1和横向校验恢复出D5，以此类推进而恢复出磁盘1、2上的所有数据。 RAID 10RAID 10是将镜像和条带进行组合的RAID级别，先进行RAID 1镜像然后再做RAID 0。RAID 10也是一种应用比较广泛的RAID级别。 RAID 10集RAID 0和RAID 1的优点于一身，适合应用在速度和容错要求都比较高的场合。先进行镜像，再进行分条。物理磁盘1和物理磁盘2组成RAID 1，物理磁盘3和物理磁盘4组成RAID 1，两个RAID 1再进行RAID 0。 当不同RAID1中的磁盘，如物理磁盘2和物理磁盘4发生故障导致数据失效时，整个阵列的数据读取不会受到影响，因为物理磁盘1和物理磁盘3上面已经保存了一份完整的数据。但是如果组成RAID 1的磁盘（如物理磁盘1和物理磁盘2）同时故障，数据将不能正常读取。 RAID 50RAID 50是将RAID 5和RAID 0进行两级组合的RAID级别，第一级是 RAID 5，第二级为RAID 0。 RAID 50是RAID 5和RAID 0的结合，先将3个或3个以上磁盘实现RAID 5，再把若干个RAID 5进行RAID 0分条。 RAID 50需要至少6个磁盘构成，把数据分条后分放到各个RAID 5中，在RAID 5中再进行分条和计算校验值及校验值的分布式存储。 如上图，物理磁盘1、2、3实现RAID 5，物理磁盘4、5、6实现RAID 5，再将两个RAID 5放在一起进行分条。允许不同RAID 5中的多块磁盘同时失效，但是一旦同一RAID 5中的两块磁盘故障，将会导致阵列失效。 常用RAID级别的比较和应用场景 RAID组成员盘个数不建议过多，例如超过20块成员盘的RAID,不但性能比8-9块成员盘RAID组(建议RAID5成员盘个数)低，且在运行过程中RAID组失效的概率增加。 RAID级别 RAID 0 RAID 1 RAID 3 RAID 5 /6 RAID 10 典型应用环境 迅速读写，安全性要求不高，如图形工作站等 随机数据写入，安全性要求高，如服务器、数据库存储领域 连续数据传输，安全性要求高，如视频编辑、大型数据库等 随机数据传输，安全性要求高，如金融、数据库、存储等 数据量大，安全性要求高，如银行、金融等领域 虚拟机中的RAID实操由于我们在虚拟机中搭建实验环境，所以采用Linux的软RAID来搭建，与实际生产环境硬件RAID卡相比，除了性能上不达标，一些关键特性不具备外，在数据写入、读取和恢复方面的机制类似，便于大家理解RAID的工作特性。 首先我们给虚拟机挂载4块数据盘，用于组建RAID 10，如下图所示： mdadm命令用于管理Linux系统中的软件RAID硬盘阵列，格式为： 1mdadm [模式] [选项] [成员设备名称] mdadm命令在Linux系统 中创建和管理软件RAID磁盘阵列，它涉及的理论知识的操作过程与生产环境中的完全一致。mdadm常用命令选项如下： 1）创建模式命令选项和专用选项 -C 创建RAID -l 指定级别 -n 指定设备个数 -v 显示创建过程 -a {yes|no} 自动为其创建设备文件 -c 指定数据块大小（chunk） -x 指定空闲盘（热备磁盘）个数，空闲盘（热备磁盘）能在工作盘损坏后自动顶替 注意：创建阵列时，阵列所需磁盘数为-n参数和-x参数的个数和 下面开始创建RAID过程： Step1：在系统中，通过mdadm创建一个软RAID 10，命名为/dev/md10，代码如下： 123456789[root@c7-test01 ~]# mdadm -Cv /dev/md10 -a yes -n 4 -l 10 /dev/sdb /dev/sdc /dev/sde /dev/sddmdadm: layout defaults to n2mdadm: layout defaults to n2mdadm: chunk size defaults to 512Kmdadm: size set to 20954112Kmdadm: Defaulting to version 1.2 metadatamdadm: array /dev/md10 started.[root@c7-test01 ~]# fdisk -l | grep "/dev/md10"Disk /dev/md10: 42.9 GB, 42914021376 bytes, 83816448 sectors 通过上面的校验我们发现RAID 10创建成功，且大小为42.9G，符合预期。 Step2：将创建好的md10格式化为ext4文件系统，代码如下： 123456789101112131415161718192021[root@c7-test01 ~]# mkfs.ext4 /dev/md10mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=128 blocks, Stripe width=256 blocks2621440 inodes, 10477056 blocks523852 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=2157969408320 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done Step3：创建挂载点，把md10设备进行挂载，挂载后就可以发现md10可用空间为40G，代码如下： 123456789101112[root@c7-test01 ~]# mkdir /home/data[root@c7-test01 ~]# mount /dev/md10 /home/data[root@c7-test01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 41G 2.4G 37G 7% /devtmpfs 2.0G 0 2.0G 0% /devtmpfs 2.0G 0 2.0G 0% /dev/shmtmpfs 2.0G 12M 2.0G 1% /runtmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup/dev/sda1 380M 102M 254M 29% /boottmpfs 394M 0 394M 0% /run/user/0/dev/md10 40G 49M 38G 1% /home/data Step4：查看/dev/md10磁盘阵列的详细信息，并把挂载信息写入到配置文件中，使其开机即挂载永久生效。代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@c7-test01 ~]# mdadm -D /dev/md10/dev/md10: Version : 1.2 Creation Time : Sun Jun 9 12:52:22 2019 Raid Level : raid10 Array Size : 41908224 (39.97 GiB 42.91 GB) Used Dev Size : 20954112 (19.98 GiB 21.46 GB) Raid Devices : 4 Total Devices : 4 Persistence : Superblock is persistent``` Update Time : Sun Jun 9 12:59:55 2019 State : clean Active Devices : 4``` Working Devices : 4 Failed Devices : 0 Spare Devices : 0``` Layout : near=2 Chunk Size : 512K```Consistency Policy : resync``` Name : c7-test01:10 (local to host c7-test01) UUID : 7f68ffce:e80f0c04:6c58f40c:526c5508 Events : 17Number Major Minor RaidDevice State 0 8 16 0 active sync set-A /dev/sdb 1 8 32 1 active sync set-B /dev/sdc 2 8 64 2 active sync set-A /dev/sde 3 8 48 3 active sync set-B /dev/sdd```[root@c7-test01 ~]# echo "/dev/md10 /home/data ext4 defaults 0 0" &gt;&gt; /etc/fstab[root@c7-test01 ~]# cat /etc/fstab # /etc/fstab# Created by anaconda on Sat Jun 1 20:49:28 2019## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=f078e329-6693-4fdd-89ad-0d5734b5e35f / ext4 defaults 1 1UUID=13919904-2c90-429c-b5ea-56f823a6e87c /boot ext4 defaults 1 2UUID=d3b139fc-7730-4bee-886b-1d4e0d681bae swap swap defaults 0 0/dev/md10 /home/data ext4 defaults 0 0 2）管理模式 -a(–add)：添加磁盘 -d(–del)：删除磁盘 -r(–remove)：从RAID中移除损坏的成员盘 -f(–fail)：模拟成员盘故障 -S：停止阵列工作 注意：新增加的硬盘需要与原硬盘大小一致，且如果原有阵列缺少工作磁盘（如raid1只有一块在工作，raid5只有2块在工作），这时新增加的磁盘直接变为工作磁盘，如果原有阵列工作正常，则新增加的磁盘为热备磁盘。 Step1：模拟成员盘/dev/sdb故障，代码如下： 12[root@c7-test01 ~]# mdadm /dev/md10 -f /dev/sdbmdadm: set /dev/sdb faulty in /dev/md10 Step2：查看RAID状态，此时md10仍正常工作，但出于降级状态，符合预期。如下： 在RAID 10级别的磁盘阵列中，当RAID 1磁盘阵列中存在一个故障盘时并不影响RAID 10磁盘阵列的使用，但是磁盘阵列此时出于降级使用状态。当购买了新的硬盘设备后再使用mdadm命令来予以替换即可，在此期间我们可以在/home/data目录中正常地创建或删除文件。由于我们是在虚拟机中模拟硬盘，所以先重启系统，然后再把新的硬盘添加到RAID磁盘阵列中。这个过程不是我们讨论的重点，现在/dev/sdb成员盘故障，使得md10处于降级使用状态，如果/dev/sdd故障呢？/dev/sdc故障呢？这两点才是我们要重点讨论的地方。 Step3：再次模拟成员/dev/sdd故障，代码如下： 12[root@c7-test01 ~]# mdadm /dev/md10 -f /dev/sddmdadm: set /dev/sdd faulty in /dev/md10 Step4：再次查看RAID的状态，如下： 此时阵列中虽然坏了两块成员盘/dev/sdb和/dev/sdd，因为这两块成员盘同属于不同的RAID 1，所以阵列状态仍为降级状态，也就是表示还可以使用。接下来，我们模拟同一个RAID1中两块成员盘故障，看看阵列状态是否能达到我们预期的失效态。在模拟之前，需要先恢复一块成员盘，比如：/dev/sdd。注意：需要重启系统后才能恢复。恢复磁盘后，需要等待一段时间，此时刚恢复的磁盘正在恢复数据，如下： Step5：模拟同一个RAID 1下两块成员盘同时故障，代码如下： 123[root@c7-test01 ~]# mdadm /dev/md10 -f /dev/sdb /dev/sdcmdadm: set /dev/sdb faulty in /dev/md10mdadm: set /dev/sdc faulty in /dev/md10 Step6：再次查看阵列状态，此时软RAID 10虽然显示状态为降级使用，但是只有SET-A组，也就是说SET-B组的数据已丢失。如下： RAID 5 的创建方法与此类似，不过RAID 5至少需要3块硬盘，同时，为了保证数据的可靠性，可以再增加一块热备盘。当成员盘故障时，数据盘可以立即顶上去并做数据同步和恢复操作。 Step1：创建RAID 5阵列/dev/md5，代码如下： 1234567[root@c7-test01 ~]# mdadm -Cv /dev/md5 -a yes -n 3 -l 5 -x 1 /dev/sdf /dev/sdg /dev/sdh /dev/sdimdadm: layout defaults to left-symmetricmdadm: layout defaults to left-symmetricmdadm: chunk size defaults to 512Kmdadm: size set to 20954112Kmdadm: Defaulting to version 1.2 metadatamdadm: array /dev/md5 started. Step2：格式化md5，并创建挂载目录，并挂载磁盘，代码如下： 12345678910111213141516171819202122232425262728293031323334353637[root@c7-test01 ~]# mkfs.ext /dev/md5-bash: mkfs.ext: command not found[root@c7-test01 ~]# mkfs.ext4 /dev/md5mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=128 blocks, Stripe width=256 blocks2621440 inodes, 10477056 blocks523852 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=2157969408320 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done [root@c7-test01 ~]# mkdir /home/code[root@c7-test01 ~]# mount /dev/md5 /home/code[root@c7-test01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 41G 2.4G 37G 7% /devtmpfs 2.0G 0 2.0G 0% /devtmpfs 2.0G 0 2.0G 0% /dev/shmtmpfs 2.0G 12M 2.0G 1% /runtmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup/dev/sda1 380M 102M 254M 29% /boot/dev/md10 40G 49M 38G 1% /home/datatmpfs 394M 0 394M 0% /run/user/0/dev/md5 40G 49M 38G 1% /home/code Step3：查看阵列md5的状态，如下： Step4：我们再次将成员盘/dev/sdf模拟故障，然后可以看见热备盘/dev/sdi自动顶上故障盘位置并开始同步恢复数据。如下： LVM逻辑卷管理LVM逻辑卷管理的原理浅析LVM是逻辑盘卷管理（Logical Volume Manager）的简称，它是Linux环境下对磁盘分区进行管理的一种机制，LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。 LVM的工作原理其实很简单，它就是通过将底层的物理硬盘抽象的封装起来，然后以逻辑卷的方式呈现给上层应用。在传统的磁盘管理机制中，我们的上层应用是直接访问文件系统，从而对底层的物理硬盘进行读取，而在LVM中，其通过对底层的硬盘进行封装，当我们对底层的物理硬盘进行操作时，其不再是针对于分区进行操作，而是通过一个叫做逻辑卷的东西来对其进行底层的磁盘管理操作。比如说我增加一个物理硬盘，这个时候上层的服务是感觉不到的，因为呈现给上层服务的是以逻辑卷的方式。 LVM最大的特点就是可以对磁盘进行动态管理。因为逻辑卷的大小是可以动态调整的，而且不会丢失现有的数据。如果我们新增加了硬盘，其也不会改变现有上层的逻辑卷。作为一个动态磁盘管理机制，逻辑卷技术大大提高了磁盘管理的灵活性。LVM的最大缺点就是影响磁盘I/O效率。 在一个硬盘上创建多个逻辑卷，对创建好的卷调整大小，然后将它们挂载在’/home,/var,/tmp’等目录下，如下所示： PV（Physical Volume）- 物理卷：物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区，也可以是整个物理硬盘，也可以是raid设备，是LVM的基本存储逻辑块，但和基本的物理存储介质(如分区、磁盘等)比较，却包含有与LVM相关的管理参数。 VG（Volumne Group）- 卷组：卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。 PE（physical extent）：每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是在VG过程中配置的，默认为4MB。 LVM 默认使用4MB的PE区块，而LVM的LV最多仅能含有65534个PE (lvm1 的格式)，因此默认的LVM的LV最大容量为4M*65534/(1024M/G)=256G。PE是整个LVM 最小的储存区块，也就是说，其实我们的资料都是由写入PE 来处理的。简单的说，这个PE 就有点像文件系统里面的block 角色。所以调整PE 会影响到LVM 的最大容量！不过，在 CentOS 6.x 以后，由于直接使用 lvm2 的各项格式功能，因此这个限制已经不存在了。 LV（Logical Volume）- 逻辑卷：逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。 简单来说就是：PV是物理的存储设备（整块磁盘或磁盘分区），VG是一个存放PV的仓库，将仓库中所有PV重新编号，也就是PE。LV就是用重新编号的PE组成的逻辑磁盘。 LVM 主要有三类命令行工具： pv 开头的命令用来操作 PV 物理卷 vg 开头的命令用来操作 VG 逻辑卷组 lv 开头的命令用来操作 LV 逻辑卷 功能/命令 物理卷管理 卷组管理 逻辑卷管理 扫描 pvscan vgscan lvscan 建立 pvcreate vgcreate lvcreate 显示 pvdisplay vgdisplay lvdisplay 删除 pvremove vgremove lvremove 扩展 vgextend lvextend 缩小 vgreduce lvreduce LVM逻辑卷管理实操1）安装逻辑卷管理软件，创建物理卷，并添加卷组 Step1：我们首先添加两块磁盘/dev/sdb和/dev/sdc，用来作为逻辑实验的环境，方法同RAID实验添加磁盘。 Step2：:安装lvm逻辑卷管理软件，代码如下： 1[root@c7-test01 ~]# yum install -y lvm2 Step3：对添加的两块磁盘（整盘）创建物理卷，这样这两块就可以被LVM进行管理，并有相关LVM管理参数。代码如下： 123[root@c7-test01 ~]# pvcreate /dev/sdb /dev/sdcPhysical volume "/dev/sdb" successfully created.Physical volume "/dev/sdc" successfully created. Step4：创建卷组datastorage ，并将两块物理盘加入到新创建的卷组中，代码如下： 12[root@c7-test01 ~]# vgcreate datastorage /dev/sdb /dev/sdcVolume group "datastorage" successfully created 2）逻辑卷管理实操 Step1：创建出一个150MB的逻辑卷设备lv_data01，代码如下： 123[root@c7-test01 ~]# lvcreate -n lv_data01 -L 150M datastorage Rounding up size to full physical extent 152.00 MiBLogical volume "lv_data01" created. 注意：这里切割单位的问题。在对逻辑卷进行切割时有两种计量单位。第一种是以容量为单位，所使用的参数为-L。例如，使用-L 150M生成一个大小为150MB的逻辑卷。另外一种是以基本单元的个数为单位，所使用的参数为-l。每个基本单元的大小默认为4MB。例如，使用-l 37可以生成一个大小为37×4MB=148MB的逻辑卷。我们采用的第一种切割办法。 Step2：把创建的逻辑卷格式化为ext4文件系统格式，然后挂载到/home/data01下使用。代码如下： 123456789101112131415161718192021[root@c7-test01 ~]# mkfs.ext4 /dev/datastorage/lv_data01 mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=1024 (log=0)Fragment size=1024 (log=0)Stride=0 blocks, Stripe width=0 blocks38912 inodes, 155648 blocks7782 blocks (5.00%) reserved for the super userFirst data block=1Maximum filesystem blocks=3381657619 block groups8192 blocks per group, 8192 fragments per group2048 inodes per groupSuperblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729Allocating group tables: done Writing inode tables: done Creating journal (4096 blocks): doneWriting superblocks and filesystem accounting information: done [root@c7-test01 ~]# mkdir /home/data01 &amp;&amp; mount /dev/datastorage/lv_data01 /home/data01 Linux系统会把LVM中的逻辑卷设备存放在/dev设备目录中（实际上是做了一个符号链接），同时会以卷组的名称来建立一个目录，其中保存了逻辑卷的设备映射文件（即/dev/卷组名称/逻辑卷名称）。设置开机自动挂载的方式，同上面RAID实操，这里不再赘述。 Step3：扩容逻辑卷lv_data01，在扩容前需要先卸载挂载点，否则数据会丢失。代码如下： 1234567891011121314151617181920212223242526272829# 卸载挂载点[root@c7-test01 ~]# umount /home/data01/# 扩容到290MB[root@c7-test01 ~]# lvextend -L 290M /dev/datastorage/lv_data01 Rounding size to boundary between physical extents: 292.00 MiB. Size of logical volume datastorage/lv_data01 changed from 152.00 MiB (38 extents) to 292.00 MiB (73 extents). Logical volume datastorage/lv_data01 successfully resized.# 检查磁盘完整性，并重置磁盘容量[root@c7-test01 ~]# e2fsck -f /dev/datastorage/lv_data01 e2fsck 1.42.9 (28-Dec-2013)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary information/dev/datastorage/lv_data01: 11/38912 files (0.0% non-contiguous), 10567/155648 blocks[root@c7-test01 ~]# resize2fs /dev/datastorage/lv_data01 resize2fs 1.42.9 (28-Dec-2013)Resizing the filesystem on /dev/datastorage/lv_data01 to 299008 (1k) blocks.The filesystem on /dev/datastorage/lv_data01 is now 299008 blocks long.# 重新挂载磁盘，并查看状态[root@c7-test01 ~]# mount /dev/datastorage/lv_data01 /home/data01 在前面我们的卷组是由两块硬盘设备共同组成的。用户在使用存储设备时感知不到设备底层的架构和布局，更不用关心底层是由多少块硬盘组成的，只要卷组中有足够的资源，就可以一直为逻辑卷扩容。 Step4：缩容逻辑卷lv_data01，在执行缩容操作前记得先把挂载点卸载掉，同时要先检查磁盘文件系统的完整性。代码如下： 12345678910111213141516171819202122232425262728# 取消挂载点，并检查文件系统完整性[root@c7-test01 ~]# umount /home/data01/ &amp;&amp; e2fsck -f /dev/datastorage/lv_data01 e2fsck 1.42.9 (28-Dec-2013)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary information/dev/datastorage/lv_data01: 11/75776 files (0.0% non-contiguous), 15729/299008 blocks# 把逻辑卷缩小到120M[root@c7-test01 ~]# resize2fs /dev/datastorage/lv_data01 120Mresize2fs 1.42.9 (28-Dec-2013)Resizing the filesystem on /dev/datastorage/lv_data01 to 122880 (1k) blocks.The filesystem on /dev/datastorage/lv_data01 is now 122880 blocks long.[root@c7-test01 ~]# lvreduce -L 120M /dev/datastorage/lv_data01 WARNING: Reducing active logical volume to 120.00 MiB. THIS MAY DESTROY YOUR DATA (filesystem etc.)Do you really want to reduce datastorage/lv_data01? [y/n]: y Size of logical volume datastorage/lv_data01 changed from 292.00 MiB (73 extents) to 120.00 MiB (30 extents). Logical volume datastorage/lv_data01 successfully resized.# 重新挂载，并查看挂载状态[root@c7-test01 ~]# mount /dev/datastorage/lv_data01 /home/data01/ 相较于扩容逻辑卷，在对逻辑卷进行缩容操作时，其丢失数据的风险更大。所以在生产环境中执行相应操作时，一定要提前备份好数据。另外Linux系统规定，在对LVM逻辑卷进行缩容操作之前，要先检查文件系统的完整性（当然这也是为了保证我们的数据安全）。 Step5：创建快照卷，在创建之前我们往现有挂载目录写入一个测试文件。代码如下： 12345678910# 写入测试文件[root@c7-test01 ~]# echo "Welcome to kkutysllb.cn" &gt; /home/data01/redeme.txt[root@c7-test01 ~]# cat /home/data01/redeme.txt Welcome to kkutysllb.cn# 创建快照卷，使用-s 选项创建[root@c7-test01 ~]# lvcreate -L 120M -s -n lv_data01_snp /dev/datastorage/lv_data01 Logical volume "lv_data01_snp" created. 在逻辑卷lv_data01的挂载目录中创建一个100M的测试文件，可以看见快照卷的容量同步上升，代码如下： 1234[root@c7-test01 ~]# dd if=/dev/zero of=/home/data01/test_files bs=1M count=100100+0 records in100+0 records out104857600 bytes (105 MB) copied, 1.12679 s, 93.1 MB/s LVM的“快照卷”功能类似于虚拟机软件的还原时间点功能。例如，可以对某一个逻辑卷设备做一次快照，如果日后发现数据被改错了，就可以利用之前做好的快照卷进行覆盖还原。LVM的快照卷功能有两个特点： 快照卷的容量必须等同于逻辑卷的容量； 快照卷仅一次有效，一旦执行还原操作后则会被立即自动删除。 Step6：校验快照卷的恢复效果，记得先取消逻辑卷lv_data01的挂载点。代码如下： 123456789101112131415# 取消挂载[root@c7-test01 ~]# umount /home/data01/# 使用快照卷lv_data01_snp对逻辑卷lv_data01进行还原[root@c7-test01 ~]# umount /home/data01/[root@c7-test01 ~]# lvconvert --merge /dev/datastorage/lv_data01_snp Merging of volume datastorage/lv_data01_snp started. datastorage/lv_data01: Merged: 30.56% datastorage/lv_data01: Merged: 100.00%# 重新挂载逻辑卷lv_data01 [root@c7-test01 ~]# mount /dev/datastorage/lv_data01 /home/data01/ 使用快照卷还原后，原来创建的100M垃圾文件也就同步被清空了，同时快照卷只能使用一次，因此也会被系统回收。如下： Step7：删除逻辑卷，首先需要取消挂载点，然后按照逻辑卷—卷组—物理卷的顺序依次删除。代码如下： 1234567891011121314151617181920# 取消挂载[root@c7-test01 ~]# umount /home/data01/# 删除逻辑卷lv_data01[root@c7-test01 ~]# lvremove /dev/datastorage/lv_data01 Do you really want to remove active logical volume datastorage/lv_data01? [y/n]: y Logical volume "lv_data01" successfully removed# 删除卷组[root@c7-test01 ~]# vgremove datastorage Volume group "datastorage" successfully removed# 删除物理卷[root@c7-test01 ~]# pvremove /dev/sdb /dev/sdc Labels on physical volume "/dev/sdb" successfully wiped. Labels on physical volume "/dev/sdc" successfully wiped. 完成后，通过pvdisplay、vgdisplay、lvdisplay进行校验，如下： 至此，Linux原生的存储虚拟化介绍完毕，在后续KVM和OpenStack等开源技术实操时，还会用到Linux原生的存储虚拟化LVM，网络虚拟化vSwitch等实操，所以这里还需各位好好理解并掌握。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-07-浅析5G-SBA服务网络架构]]></title>
    <url>%2F2019%2F06%2F07%2F2019-06-07-%E6%B5%85%E6%9E%905G-SBA%E6%9C%8D%E5%8A%A1%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[IMT-2020推进组提出的“三朵云“架构前文提出了5G网络架构是一种颠覆性的变化，之所以这么说，是从5G网络架构的实现方式来说的。由于“三朵云”概念的提出，导致5G的网络功能软件模块化实现，也就是我们常说的云化网络架构，其特征就是控制集中化、功能模块化和接口软件化。如下图所示： 这种云化的变化针对现网架构来说其本质上是一种重构，通过控制云实现全网资源控制和能力开放，主要是各种核心网元的虚拟化实现，将传统网元的控制面功能抽象出来通过软件模块化封装，形成一个个网络控制功能模块。而传统网元的媒体转发面被一个个虚拟交换机、白盒交换机或是支持SDN流表的专有硬件替代，构成转发云，与控制云一起完成网络数据的路由控制和转发，并构成转控分离的架构，也就是俗称的C/U分离。而接入云内部无论是无线网络还是有线网络都是按照C/U分离的架构部署，同时将CDN和MEC等下沉到接入云控制面，一方面满足低时延的业务需求，同时与控制云配合实现端到端的切片网络。 为了实现上述云化网路的重构，3GPP推荐的电信云化网络的基础架构就是NFV，全球各大运营的选择也是NFV，NFV的全称就是网络功能虚拟化，通过NFV架构实现5G网络需求的SBA服务架构、网络服务的无状态设计、C/U分离和切片管理。如下图左边所示： 而NFV网络架构的提出并不是3GPP定义的，是ETSI定义的。ETSI作为NFV的发起标准组织，于2015年初发布了NFV参考架构，如上图右边所示。ETSI定义的NFV参考架构，与传统网络架构相比，进行了“三纵两横”的划分。纵向与传统网络类似，分为基础设施层、虚拟网元层和运营支撑层共3层，通过这三层实现网络功能虚拟化和网络服务的运营管理。而横向分为两个域：业务网络域和管理编排域。 整个NFV架构的关键就是管理编排域MANO，MANO只是个逻辑功能，并没有实体。其本质上通过NFVO、VNFM和VIM通过一定的管理流程来创建各类虚拟网元，提供各类电信业务，从而实现对左边的业务网络域编排和管理。比如：我们需要扩容一个vMME网元，NFVO会将容量需求和业务接口需求等构建一个网元描述文件发送给VNFM，VNFM会根据这些需求生成资源清单并向NFVO申请资源，包括：需要多个虚拟机、划分为几组虚拟机组，每个虚机的CPU/内存/磁盘/IP/通信端口的规划、以及每个虚机要加载的操作系统镜像等。而NFVO接收申请后，会将资源清单发送给VIM，由VIM去筹备底层的虚拟化资源，完成后NFVO会通知VNFM资源准备好了，VNFM从而完成虚拟网元的实例化部署。 上述这种调度方式，是一种间接调度方式，资源的编排和控制统一由NFVO完成，VNFM不能直接对VIM发起资源编排需求，类似我们每个部门的领导一样要掌控一切信息。还有一种直接调度方式，就是由VNFM去直接控制VIM来完成资源的编排。从性能角度来看，直接模式更好，但是从部署落地以及后续演进角度来看，间接模式更好。目前，中国移动就是采用的这种方式。详见电信云落地若干问题一文。 而5G网络为了实现切片功能，必须在NFV架构基础上引入SDN，那么上图ETSI的NFV架构就演进成上图最右边部分，在NFVO层面引入SDNO，实现对SDNC的全局编排管理，而在VIM层面引入SDNC（主要是在OpenStack的neutron组件下挂接各厂家SDN控制器实现），实现对底层虚拟网元资源和物理网络资源的统一管理。由SDNO去编排SDNC，从而实现全网网络资源的灵活编排和部署，并实现各切片网络需求的网络隔离性。 SBA服务化网络架构因此，从网络架构实现角度来说，5G网络架构相比传统2G/3G/4G架构而言，是一种颠覆性的的变化。但是，从网络功能来说，5G网络相比传统2G/3G/4G还是由一定集成性。因为，通信网络的各代演进从来都不是一种割裂的状态，而是一种螺旋式递进的规律，每一代通信网络都会继承前一代一些特征，并演变出一些的新的特征。 比如：2G到2.5G，无线接入网没有变化，核心网在电路域的基础上引入分区域，实现GPRS功能呢。到了3G，无线接入网变为NodeB+RNC的架构模式，核心网在电路域和分组域的基础上引入了IMS域。而到了4G网络时代，无线接入网少了RNC这一层，NodeB变胖了变成了eNodeB，集成了原NodeB、RNC和SGSN的部分功能，实现接入控制、移动性管理、路由控制和无线资源控制，同时各个eNodeB之间多了个X2接口，因此4G网络时代，无线接入网的变化可以总结为“少一层、多一口、胖基站”。而核心网层面电路域和IMS域没有变化，但是分组域为了学习IMS域业务-控制-转发分离的架构，也拆分为MME、S-GW和P-GW，同时引入QoS的集中控制功能PCRF。但是4G网络在分组域的这种变化并不彻底，S-GW和P-GW上还存在部分控制功能。 为了实现5G网络的功能模块化、无状态设计和C/U分离需求，3GPP在网络功能进行从新定义，将传统2G/3G/4G的网元重新定位网络功能模块，就是上图右边各种命名以F结尾的功能块。与4G网络功能相比，主要有9点变化，前6点针对4G网络功能拆分和整合，7,8,9三点是5G网络功能的新增。 AMF就相当于4G网络的MME，但是只负责用户接入控制和移动性管理，而MME的会话管理部分被拆分整合进SMF，此外SMF还整合原S-GW/P-GW的路由控制，地址分配和合法监听等控制功能。PCF与4G的PCRF相比没有多大变化，仍然负责QoS的控制，但是5G网络的QoS的是基于流的，这一点与4G网络基于承载的QoS相比，变化还是较大的。UDM与传统HSS网元类似，负责用户数据和签约信息的存储，但是UDM不具备传统HSS/HLR的鉴权功能，这一部分被拆分至AUSF功能中，其目的是为了便于互联网业务鉴权的整合，实现5G网络的统一鉴权功能。UPF就是4G网络的S-GW/P-GW的纯转发面，也就是用户面功能。 至于第七点变化的NEF功能，属于能力开放功能，便于与各类互联网和物联网应用对接和扩展。其实，在4G网络中也有能力开放，目前在杭研集中一点实现，主要实现了PCC和智能网的能力开放。而NSSF功能主要是5G网络中为了实现切片管理新增的功能，而NRF功能用于5G网络各种网元功能的注册管理，这也是实现SBA服务网络架构的关键。 基于SBA服务网络架构是中国移动提出的一种架构，借鉴了IT领域生产者和消费者的概念。所谓生产者就是只产生网络服务能力，至于谁去使用并不关心。而消费者就是使用网路服务能力去实现网络服务，至于网络服务能力由谁产生并不关心。 通过这种理念引进，整个网络功能实现松耦合，各个网络功能挂载一条总线上（上图右边画圈部分），各个网络功能既是生产者，也是消费者，不仅自身产生网络服务能力，同时也消费其他网络功能的能力，这种角色的转变就是通过NRF网络功能实现。各个网络功能都向NRF注册，将自己IP地址、域名和能力集等信息注册到NRF上，NRF统一监控总线上各类网络功能的变更，并及时更新已变更的信息。当某个网络功能需要与另一个网络功能通信时，会去NRF上查询对端的信息，NRF会将对端的IP、域名和能力集进行反馈。然后，该网络功能就能通过总线寻址到对端，从而完成通信。因此，这种服务化的架构不仅实现了网络功能松耦合，而且将原来架构中的固定转发路径变为点到点转发。NRF的作用类似现网DNS，VoLTE的ENS，用作全网路由寻址。 还有一点变化就是在5G网络架构中，接入网是一种固移融合的组网形式，统一由AMF完成接入控制。同时，在5G网络用户的移动性附着与承载会话建立是全解耦的，也就是说用户在5G网络附着后，无需建立PDU会话承载，当有业务需求时再按需发起PDU会话承载的建立。而不像4G网络，用户附着网络必须建立一条默认PDN会话承载。这种改变一定程度上减少了全网资源消耗。 这种服务化架构在云原生的应用就如上图所示，每一朵云都是基于这种SBA的组网形式，不仅云内各服务松耦合，云与云之间也是一种松耦合状态，通过全网统一的编排和管理实现对接。目前，中国移动内部规划通过OSS4.0实现全网资源的统一编排和管理。 中国移动电信云&amp;5GC资源池部署演进策略前面提到，5G网络架构的实现基于NFV/SDN的一种云化网络架构。因此，各大运营商在部署5G之前必须对传统网络进行一种云化改造。中国移动在2015年底就启动了NFV云化试点工作，经过4年多省市连续试点，目前已经解决大网云化改造中80%的问题。因此，在2019年中国移动启动电信云资源池的建设，全国分为8个大区（西北、西南、华北、华南、东北、华东1、华东2、中部），按照“三步走，两级演进”的策略，最终全网核心控制云8大区共部署10万台服务器，仅西北大区就需部署20000+服务器。如下图所示： 如上图所示，三步走策略中目前已基本完成第一步8大区电信云资源池的建设，预计在今年底8大区电信云进入商用阶段。在目前第一阶段中，首先完成大网核心网的云化改造，涉及IMS/EPC两大专业，并在今年完成各省市大网业务的上云割接，同时随着资源池规模的逐步加大，最终预计在2022年完成全网业务云化搬迁。 同时，在今年9月同步开启二阶段资源池建设，并且在一阶段电信云资源池的基础上引入5GC资源池，同步引入SDN技术。由于SDN的引入需要对已有资源池从虚层开始完成改造和重部署，考虑到已割接上云业务安全和网络稳定性，因此5GC资源池与一阶段电信云资源池采用“共址不共池”策略部署。 后续，在明年年中同步启用三阶段资源池扩容部署，主要目的是满足全网用户资源规模需求。并在明年年中同步开始一阶段4G云化核心网向5GC升级演进。 而所谓的“二级架构”是根据业务需求来逐步部署，总体原则是先“先核心后边缘”。在上述二阶段5GC资源池引入后，如果有30ms~10ms的业务需求，在各省省会城市部署边缘云资源池，主要完成CDN下沉、vBRAS改造、MEC部署以及试点CU池部署。如果有低于10ms业务需求，可考虑在地市/区县层面部署计算/存储一体化的站点计算域资源池，主要涉及C-RAN的CU/DU就近接入改造，从而满足低时延业务需求。该层面的资源池不部署管理节点，有省端边缘云统一纳管调度，同时各站点资源池之间不涉及跨池迁移特性。接入站点资源池可考虑三种部署模式“AZ、Cell和轻量化资源池”，目前在接入站点不考虑MEC需求时，统一采用AZ方式部署，如果后续部署MEC，考虑到MEC可能有运营管理需求，可考虑轻量化资源池方式部署。（AZ和Cell是OpenStack NOVA组件中的概念，因此需要大家具备OpenStack的一些基本知识，也可看我公众号的系列文章，公众号ID：CloudSupermanLLB） 而在8大区核心控制云资源池中，目前采用“二层解耦、逐步演进到三层解耦”的策略部署。如下图示所示： 所谓二层解耦，就是硬件和软件解耦，目前8大区电信云资源池硬件采用中兴/浪潮服务器、华为/中兴TOR和EOR、迈普的管理TOR，西北大区除服务器为浪潮外，其余硬件配置与其他大区一致。除了硬件外，虚层VIM、虚拟网元层VNF/VNFM、编排层NFVO都是同厂商部署，由于软件层面目前因未开始正式集采招标，因此厂商还未确定。但是可以确定的是—“每个大区两个软件厂商”，由于电信云采用分布式存储，因此存储的招标与软件一起完成，目前也未确定。 二层解耦部署策略主要考虑集成交付快、不涉及不同厂商软件兼容性问题，因此问题较少，但是同样也会带来资源利用率不高的缺点。比如：厂商A的池内资源不够，即使厂商B池内由空闲资源也无法提供给厂商A使用，因为这里涉及虚层VIM、虚拟网元层VNF/VNFM、编排层NFVO和分布式存储等软件重新部署，网络改动量大，不利于大网稳定。同时，由于软件采用双厂商部署，也会导致运维人员必须掌握两个厂家软件机制、部署流程和运维管理流程等，对运维人员的要求较高。 而边缘资源池和接入站点也是采用精简化的两级部署架构，主要以业务需求来驱动。如下图所示： VIM的集中管理部署在省端，采用3节点高可用部署策略，每个节点资源池可纳管不多余256个接入站点资源池，如果后续省内业务规模较大导致接入站点资源池扩容，省端的VIM集中管理节点可考虑按照3、5、7。。。等节点规划扩容（必须为奇数才能实现高可用策略）。同时，在接入站点资源池除MEC部署需求外，统一部署计算/存储一体化资源池，实现“VIM集中、AZ拉远”的部署要求，不仅利于节约投资，也方便划分省/地两端的维护界面。在接入站点，由于采用计算/存储一体化部署策略，因此存储层面必须采用分布式存储，每个计算服务器部署存储软件“机头”，每个资源池内所有服务器的硬盘（除系统盘外）构成OSD存储池，整体的管理调度由省端VIM层面部署集群控制软件来完成。详见本站分布式存储介绍文章。 按照这种“两级部署”策略，后续随着5G业务的发展，整体维护界面较清晰，同时也利于省端维护人员转型积极性的提升。整体5G电信云资源池的维护界面如下图所示： 核心网控制云维护由大区负责，涉及从动环到NFVO各层面维护职责，主要全网资源监控、业务质量的分析，以及自动化手段的开发部署。随着业务需求发展，省/地两端负责边缘云资源池和接入站点的维护，同样涉及从动环到NFVO各层面维护职责。整体界面划分清晰。 在对运维人员转型要求方面主要按照NFV云化网络各层来进行专业划分，传统烟囱式的专业划分逐渐淡化和模糊化。基础设施运维层面主要负责硬件故障处理、Host OS和Hypervisor的运维管理，因此运维人员必须具备IT硬件及Linux运维实践的知识技能。业务层面的维护主要涉及传统CT网元运维管理、云管平台的运维管理，因此运维人员除掌握传统CT技能外，还需掌握云计算和虚拟化相关知识技能，并具备一定运维经验。在业务编排层面，除了要掌握各类开发语言，如：Linux shell、Python、yaml和xml等，还需具备下两层的运维经验，具备云化网络端到端拉通能力，否则开发的脚本在部署加载会引发各类不可预知的问题。]]></content>
      <categories>
        <category>5G网络架构</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-07-2G~5G网络架构的演进]]></title>
    <url>%2F2019%2F06%2F07%2F2019-06-07-2G-5G%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[5G将渗透到未来社会的各个领域，以用户为中心构建全方位的信息生态系统。面对极致的体验、效率和性能要求，以及“万物互联” 的愿景，5G的网络架构设计将面临极大挑战。相较于2G/3G/4G时代，5G的网络结构会发生颠覆性的变化。 移动通信网络架构的演进包括两个方面，即无线接入网（RAN，Radio Access Network）的演进和核心网（CN，Core Network）的演进。 2G/3G/4G网络架构的演变从GSM网络（2G）演进到GPRS网络（2.5G），最主要的变化是引入了分组交换业务。原有的GSM网络是基于电路交换技术，不具备支持分组交换业务的功能。因此，为了支持分组业务，在原有GSM 网络结构上增加 了几个功能实体，相当于在原有网络基础上叠加了一个小型网络，共同构成GPRS网络。 在接入网方面，在BSC上增加了分组控制单元（PCU，Packet Control Unit），用以提供分组交换通道；在核心网方面，增加了服务型GPRS支持节点（SGSN，Service GPRS Supported Node）和网关型GPRS支持节点（GGSN，Gateway GPRS Supported Node），功能方面与MSC和GMSC一致，区别在于处理的是分组业务，外部网络接入IP网；从GPRS叠加网络结构开始，引入了两个概念。一个是电路交换域，一个是分组交换域，即CS域与PS域。 到了3G时代，在速率方面有了质的飞跃，而在网络结构上，同样发生巨大变化。首先，就是空中接口的改变，以往网络结构中的Um接口变成Uu接口，而接入网和核心网的接口也换成了Iu接口，不再是A接口。在接入网方面，不再包含BTS和BSC，取而代之的是基站NodeB与无线网络控制器（RNC，Radio Network Controller），功能方面与以往保持一致，核心网方面基本与原有网络共用，无太大区别。 NodeB的功能：主要完成射频处理和基带处理两大类工作。射频处理主要包括发送或接收高频无线号，以及高频无线信号和基带信号的相互转换功能；基带处理主要包括信道编/译码、复用/解复用、扩频调制及解扩/解调功能。 RNC的功能：主要负责控制和协调基站间配合工作，完成系统接入控制、承载控制、移动性管理、宏分集合并、无线资源管理等控制工作。 到4G时代，整个LTE网络从接 入网和核心网方面分为E-UTRAN和EPC。在接入网方面，网络扁平化，不再包含两种功能实体，整个网络只有一种基站eNodeB，它包含整个NodeB和部分RNC的功能，演进过程可以概括为“少一层，多一口，胖基站”。这样做降低了呼叫建立时延和用户数据传输时延，并且随着网络逻辑节点的减少，可以降低建设和维护成本，满足低时延、低复杂度和低成本的求。 “少一层”：4层组网架构变为3层，去掉了RNC，减少了基站和核心网之间信息交互的多节点开销，用户平面时延大大降低，系统复杂性降低。 “多一口”：以往无线基站之间是没有连接的，而eNodeB直接通过X2接口有线连接，实现无线侧IP化传输，使基站网元之间可以协调工作。eNodeB互连后，形成类似于“Mesh”的网络，避免某个基站成为孤点，这增强了网络的健壮性。 “胖基站”：eNodeB的功能由3G阶段的NodeB、RNC、SGSN、GGSN的部分功能演化而来，新增了系统接入控制、承载控制、移动性管理、无线资源管理、路由选择等。 核心网侧也发生了重大变革。在GPRS/UMTS中，服务GPRS支持节点（SGSN）主要负责鉴权、移动性管理和路由选择，而网关GPRS支持节点（GGSN）负责IP地址分配、数据转发和计费。到了LTE时代，EPC（Evolved Packet Core）对之前的网络结构能够保持前向兼容，但自身结构方面不再有3G时的各种实体部分，主要由移动管理实体（MME，Mobile Management Entity）、服务网关S-GW和分组数据网关（P-GW）构成，外部网络只接入IP网。其中，MME主要负责移动性管理，包括承载的建立和释放、用户位置更新、鉴权、加密等，这些笼统地被称为控制面功能，而S-GW和P-GW更主要的是处理用户面的数据转发，但还保留内容过滤、数据监控与计费、接入控制以及合法监听等控制面功能。可以看到，从GPRS到EPC的演进中，有着相似的体系架构和接口，并朝着控制与转发分离的趋势演进，但这种分离并不彻底。比如MME相当于SGSN的控制面功能，S-GW则相当于SGSN的用户面。 此外， LTE核心网新增了一个网元PCRF（图中未画出），即策略与计费执行功能单元，可以实现对用户和业务态服务质量（QoS）进行控制，为用户提供差异化的服务，并且能为用户提供业务流承载资源保障以及流计费策略。 5G架构设计需求分析5G的架构设计主要需要满足关键性能需求和网络运营需求。3GPP定义了5G应用的三大场景：eMBB、mMTC和uRLLC。 对于eMBB场景，又可进一步细分为连续广覆盖场景和热点高容量场景。在连续广覆盖场景下，要随时随地提供100Mbps ~1Gbps的高体验速率，并支持在高速移动如500km/h过程中的基本服务能力和业务的连续性。在LTE网络结构中，基站间虽然可以通过X2接口实现南北向数据交互，但无线资源管理、移动性管理和站间协同能力较弱。 另外，4G主要是通过核心网实现多种无线接入的统一控制，不同的接入技术在无线侧控制面各异，无法统一，互操作复杂。在热点高容量场景下，核心网网关部署的实际位置较高，且数据转发模式单一，回传网络的容量压力较大。 对于mMTC场景，当海量的5G差异化的物联网终端接入时，由于LTE采用的是与移动互联网场景相同的单一移动性和连接管理机制，承载物联网少量数据仍需消耗较大的基于隧道的GTP-C报头开销， 不仅效率低，还极有可能造成信令拥堵。 对于uRLLC场景，现有网络架构的控制面功能逻辑上分布在MME/SAE-GW等多个网元中，无法实现集中控制，同一网络控制功能可能需要多个网元通过接口协议协商完成，端到端通信时需经历较长的传输时延，且可能由于某些原因存在路由迂回现象。这既无法满足5G高可靠性前提下的低时延要求（现网端到端时延与 5G的时延要求约存在两个数量级的差距），又无法满足特定业务如车联网的安全性要求。 而网络运营方面的需求主要是运营商在部署新网络时，一般都会考虑建设和运营的可行性，便利性。因此，对5G网络提出了灵活部署、覆盖与容量兼容、精细化控制、能力开放和异构兼容等需求，这些需求在当前LTE网络只能实现很少一部分。因此， 5G网络架构的设计需要满足转控分离、集中控制、分布式部署、资源池化和服务模块化等要求。 综上，对于5G接入网，要设计一个满足多场景的以用户为中心的多层异构网络，以支持宏微结合，统一容纳多种接入技术，提升小区边缘协同处理效率，提高无线和回传资源利用率。对于5G核心网的设计，一方面要将转发功能进一步简化和下沉，将业务存储和计算能力从网络中心下移至网络边缘，以支持高流量和低时延业务要求，以及灵活均衡的流量负载调度功能；另一方面通过能力开放、资源编排、集中控制等更高效地支持差异化的业务需求。 5G网络架构的提出在5G逻辑架构的设计上，业界遵循先继承、后创新的思路，参照现有成熟的LTE网络架构，引入SDN和NFV等关键技术对网络功能进行解析和重构，以逐步适应5G架构的演进需要。 为了便于大家理解，我们从LTE的网络结构分析入手，逐步引出5G的网络逻辑架构演进思路。我们将LTE非漫游网络架构从逻辑上划分为3个部分，如下图所示。 第1部分是LTE为了满足网络的后向兼容性所引入的，在此不做赘述。第2部分是接入网，接入网（空口）的演进几乎是历代移动通信网络架构演进中最为关键的部分。第3部分是核心网，是一个并不彻底的转控分离架构，且是5G时代优先需要重构的重点。因此，我们优先聚焦该部分的重构。 Step1：为了解决控制与转发分离不彻底的问题，需要首先对兼具控制和转发功能的网关进行解耦。LTE架构中S-GW和P-GW实际上是物理网元功能合一的，在逻辑上我们可以将其视为统一的SAE-GW，然后引入SDN技术进行网络功能解耦，用户面功能由新定义的网元UPF承载，控制面功能则交由新网元SMF进行统一管理。相应地，将原本已是纯控制面网元的MME、HSS和PCRF分别定义为AMF、UDM和PCF，但对网元的实际功能只做微小的变更或整合。核心网第一步重构后的网络逻辑架构如下图所示。 Step2：为了满足网络资源充分灵活共享的需求，实现基于实际业务需求的网络自动部署、弹性伸缩、故障隔离和自愈等，需要引入NFV对网络功能进行虚拟化。因此，我们定义新网元NF以适应新的需要。考虑到NF面向的是用户差异化的服务，我们将其置于网元PCF，并定义新的接口以便NF能够按需获取PCF的策略控制等参数。核心网第二步重构后的网络逻辑架构如下图所示。 Step3：需要将业务平台下沉到网络边缘，为用户就近提供业务计算和数据缓存能力，实现网络从接入管道向信息化服务使能平台的关键跨越。因此，需要增强UDM的功能，以承担业务平台下沉后相应的数据管理工作。同时，引入网元AUSF承担数据访问的鉴权和授权工作。此外，考虑到5G面向的是极端差异化的业务场景，传统的“竖井式”单一网络体系架构无法满足多种业务的不同QoS保障需求，还需引入网元NSSF以实现网络切片选择的功能，使网络本身具备弹性和灵活扩展的能力。核心网第三步重构后的网路逻辑架构如下图所示。 Step4：接入网侧的网元编排。从1G到4G，无线通信系统经历了迅猛的发展，现实网络逐步形成了包含多种无线制式、频谱利用和覆盖范围的复杂现状。在5G时代，同一运营商将面临多张不同制式网络长期共存的局面。因此，多网络融合也将成为5G网络架构设计不可规避的因素。对此，需要改变原有网络单一的eNodeB接入形式，对接入网侧做进一步的优化和增强。需要重新定义新的网元为（R）AN，以表示接入侧不再是单一的无线接入，而是固移融合。第四步重构后的网络逻辑架构图如下图所示。 Step5：完成各个网元之间逻辑接口的定义，终端UE与AMF实体之间的N1接口是新定义的空口，使得低时延、高可靠、超密连接等愿景具备落地的可能。同时，原来的NF被定义为运营商授信、自行部署的应用功能单元AF。 通过五步重构，我们就得到了3GPP所确定的5G网络架构，这是一个点到点网络架构，主要包含以下网元功能： AMF：接入和移动性管理功能单元，负责控制面的注册和连接管理、移动性管理、信令合法监听以及上下文的安全性管理。相比4G的MME，AMF将漫游控制、承载管理以及网关选择等功能剥离，是一个“瘦身版”的MME。 SMF：会话管理功能单元，相当于是4G的MME和SAE-GW控制面整合后集中控制单元，主要负责会话管理，包括会话建立、变更和释放，以及AN节点和UPF间的承载维持。SMF同时也继承了4G MME的漫游控制功能、UPF选择和控制功能和P-GW的UE-IP地址分配功能。 UPF：数据转发功能单元，保留了4G SAE-GW的数据转发功能，包括本地移动性锚点、包路由和转发、上下行传输级包标记、包过滤和用户面策略控制功能执行，相当于SDN架构中纯转发面，无自主控制权，只能执行来自SMF的控制指令。 PCF：策略控制功能单元，负责制定统一的策略框架来管理网络行为。一方面结合自定义信息作出决策并强制控制面执行，另一方面也为前端提供连接用户数据库获取用户订阅信息的渠道。PCF与4G的PCRF功能几乎相同。 UDM：统一数据管理单元，类似4G的HSS分为FE和BE两个功能实体，UDM同样也包含两个功能实体UDM和UDR。UDM类似FE属于应用前端接口单元，负责鉴权、授权、位置管理和订阅管理。UDR类似BE属于用户数据库单元，负责用户数据的存储、包括订阅表示、安全认证、移动性数据、会话数据等。与HSS相比，是一个增强版的HSS。 AUSF：认证服务器功能单元，负责业务层面的认证和授权，相当于将4G时代业务服务器AS中对访问进行鉴权和授权的功能单独剥离出来。作为网络准入的裁决者，AUSF联合UDM对通过AMF来访的UE进行准入授权，认证通过的UE可以凭借AUSF授权的专用秘钥token实现数据的访问和获取。 NSSF：网络切片选择功能单元，主要根据网络配置，为合法的UE选择可提供特定服务的网络切片示例。其机制是通过切片需求辅助信息的匹配，为UE选择一个或一组特定的AMF提供网络服务。需要注意一点，NSSF只是完成核心网层面的切片选择，至于端到端的切片建立，是通过NSSF选择的AMF来实现接入网切片的确认。 AF：应用功能单元，通过与核心网交互对外提供专用服务。AF是运营商自行部署的授信的应用，可以直接访问网络的相关应用功能，无需经过其它外部接口。 为契合IMT-2020推进组提出的“三朵云”5G网络架构，我们将上述重构后的网络架构进行平面的切割，如下图所示。 “三朵云”5G网络是一个可依业务场景灵活部署的融合网络。 控制云完成全局的策略控制、会话管理、移动性管理、策略管理、信息管理等，并支持面向业务的网络能力开放，实现定制网络与服务， 满足不同新业务的差异化需求，扩展新的网络服务能力。接入云将支持用户在多种应用场景和业务需求下的智能无线接入，并实现多种无线接入技术的高效融合，无线组网可基于不同部署条件要求，进行灵活组网，并提供边缘计算能力。转发云配合接入云和控制云，实现业务汇聚转发功能，基于不同新业务的带宽和时延等需求，转发云在控制云的路径管理与资源调度下，实现eMBB、uRLLC和mMTC等不同业务数据流的高效转发与传输，保证业务端到端质量要求。“三朵云” 不可分割，协同配合，并且是基于SDN/NFV技术实现。]]></content>
      <categories>
        <category>5G网络架构</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-07-华为分布式存储FusionStorage原理详解]]></title>
    <url>%2F2019%2F06%2F07%2F2019-06-07-%E5%8D%8E%E4%B8%BA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8FusionStorage%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[华为FusionStorage中的基础概念FusionStorage采用全分布式DHT架构，将所有元数据按规则分布在各存储节点，不存在跨节点的元数据访问，彻底避免了元数据瓶颈。该核心架构设计保证了FusionStorage系统相比分布式文件系统具备更大规模的线性扩展能力。 所谓DHT架构，就是分布式哈希表，Distributed Hash Table。如下图所示，它是FusionStorage中指数据路由算法。 DHT机制可以保证上层应用对数据的IO操作会均匀分布在不同服务器的不同硬盘上，不会出现局部的热点，实现全局复负载均衡。DHT采用的是开源对象存储swift中一致性哈希算法（详见OpenStack的swift介绍），每个存储节点负责存储一小部分数据，基于DHT实现整个系统的寻址和存储。 上图中，Partition代表了一块数据分区，在DHT环上映射为固定Hash段代表的数据区Pn（n=1,2,3…)。Key-Value键值对表示底层磁盘上的数据组织索引，每个Value代表一个块存储空间。当我们要查找某一个数据的存放位置时，通过key值计算散列值（哈希值），根据计算的散列值找到DHT换上某一个Partition，然后根据Partition与disk的映射关系找到对应的磁盘，再通过Value值查找磁盘上的具体数据。具体如下所示 Volume代表应用卷，也就是虚拟机能看见并挂载的虚拟磁盘，它本质上代表了应用看到的一个LBA连续编址。而Volume的LBA连续编址就是上述用于数据查找的key值。 在华为FusionStorage中还有一个基础概念就是资源池，它是由上述一组分区Partition构成的存储池。如下图所示，每个资源池对应一个DHT环。 在上图中，由两个资源池，分别对应两个DHT环，左边三个硬盘对应DHT1的三个Partition分区（P1、P2和Px），右边三块硬盘对应DHT2的三个Partition分区（P1、P2和Py）。当虚拟机挂载上面的Volume1/2/3时，将对应的LBA散列后，计算的结果分别对用DHT1的P1/P2/Px；当虚拟机挂载上面的Volume10/11时，将对应的LBA散列后，计算的结果分别对应DHT2的P1/P2/Py。 华为FusionStorage的资源池类似于SAN存储中的RAID组概念，但是与RAID相比，其优点是主要有三点：一是条带宽度大，最大支持96块盘（2份拷贝），提供超大存储空间，而且系统自动将每个卷的数据块打散存储在不同服务器的不同硬盘上，冷热不均的数据会均匀分布在不同的服务器上，不会出现集中的热点，避免高I/O应用导致热点瓶颈；二是具有动态热备功能，资源池内所有硬盘都可用作资源池的热备盘；三是结构简单，通过资源池和Volume二层结构，使服务器直接看到Volume，消除了中间的LUN逻辑存储，提升I/O读写性能。 FusionStorage的数据路由原理如下所示，FusionStorage数据路由采取分层处理方式：1）VBS通过计算确定数据存放在哪个服务器的哪块硬盘上。2）OSD通过计算确定数据存放在硬盘的具体位置。 Step1：系统初始化时，FusionStorage将哈希空间（0~2^32）划分为N等份，每一等份是1个分区（Partition），这N等份按照硬盘数量取整除并进行均分。例如：二副本场景下，系统N默认为3600，假设当前系统有36块硬盘，则每块硬盘承载100个分区。上述“分区-硬盘”的映射关系在系统初始化时会分配好，后续会随着系统中硬盘数量的变化会进行调整。该映射表所需要的空间很小，FusionStorage系统中的节点会在内存中保存该映射关系，用于进行快速路由。 Step2：FusionStorage会对每个LUN在逻辑上按照1MB大小进行切片，例如1GB的LUN则会被切成1024*1MB分片。当应用侧访问FusionStorage时候，在SCSI命令中会带上LUN ID和LBA ID以及读写的数据内容，OS转发该消息到本节点VBS模块，VBS根据LUN ID和LBA ID组成一个key，该key会包含LBA ID对1MB的取整计算信息。通过DHT Hash计算出一个整数(范围在0~2^32内)，并落在指定Partition中；根据内存中记录的“分区-硬盘”映射关系确定具体硬盘，VBS将IO操作转发到该硬盘所属的OSD模块。 Step3：每个OSD会管理一个硬盘，系统初始化时，OSD会按照1MB为单位对硬盘进行分片管理，并在硬盘的元数据管理区域记录每个1MB分片的分配信息。OSD接收到VBS发送的I/O操作后，根据key查找该数据在硬盘上的具体分片信息，获取数据后返回给VBS。从而完成整个数据路由过程。 比如：应用需要访问LUN1+LBA1地址起始的4KB长度的数据，首先构造key=LUN1+LBA1/1M，对该key进行HASH计算得到哈希值，并对N取模，得到partition号，根据内存中记录的“分区-硬盘“映射表可得知数据归属的硬盘。 总结：VBS将操作系统的SCSI命令中的key提取出来（Key值的计算方式：Key=LUN ID+LBA ID/1MB），通过哈希运算，确定访问数据内容落到DHT环上的哪块Partition上，根据Partition和Disk的对应关系（系统初始化时形成），确定数据存放在那个服务器上的哪块盘上，再通过OSD通过计算确定数据存放在硬盘的具体位置并将获取的数据返回给VBS。 FusionStorage的内部组件功能和相互关系FusionStorage VBS模块及处理流程VBS模块作为FusionStorage系统存储功能的接入侧，负责完成两大类业务：一是卷和快照的管理功能；二是I/O的接入和处理。VBS模块内部如下图所示，主要由：VBM、VBP、CLIENT、DATANET、SCSI协议控制（SCSI Initiator和SCSI Target）和心跳控制模块HeartBeat组成。 VBM模块是VBS中的控制管理模块，主要负责完成卷和快照的管理功能。比如：创建卷、挂载卷、卸载卷、查询卷、删除卷、创建快照、删除快照、基于快照创建卷等。 I/O数据流在VBS进程中需要经过三个模块的处理，分别为**SCSI、VBP、CLIENT**： Step1：SCSI启动器模块SCSI Initiator负责从内核（VSC.KO）中将I/O引入VBS进程，SCSI目标模块SCSI Target接收到的I/O数据包是标准SCSI协议格式的I/O请求，通过SCSI协议的四元组（host_id/channel_id/target_id/lun_id）和该I/O数据包在块设备上的偏移地址offset，读写的数据长度len共三个参数标识符，唯一标识一个I/O数据包，SCSI目标模块SCSI Target将收到的I/O信息交给VBP(Virtual Block Process)模块。 Step2：VBP内部将通用块格式的I/O数据包转换为FusionStorage内部Key-Value格式的I/O数据包下发给client，其中KEY的组成为：tree_id（4Byte）+block_no（4Byte）+ branch_id（2Byte）+snap_id（2Byte），tree_id/branch_id/snap_id是FusionStorage内部对卷、快照的唯一标识；block_no是将卷按照1M的块划分，本次I/O落在哪一个1M块上的编号。（**block_no对1M取整后与tree_id/branch_id/snap_id求和就是key值，对1M取余后得到的余数就是offset**） Step3：I/O数据包请求到达client模块后，client根据KEY中的tree_id/branch_id进行hash计算，确定本次I/O发给哪一个OSD进程处理，确定后将I/O通过DATANET模块发给对应的OSD处理。 Step4：OSD完成I/O数据收取后在物理硬盘上执行真正的读写I/O操作，然后将操作结果逐层返回返回给内核VSC.KO模块。 备注：卷和快照的通用属性信息（如卷大小、卷名等）及卷和快照在DSware系统内部的一些私有属性（如用于定位卷和快照数据在系统中存储位置的tree_id/branch_id/snap_id）保存在DSware内部的一个私有逻辑卷中，该卷我们就称为元数据卷。 FusionStorage OSD模块及处理流程FusionStorage存储池管理的每个物理磁盘对应一个OSD进程，OSD进程作为FusionStorage中读写磁盘I/O的执行进程，主要实现3大功能：一是磁盘的管理；二是I/O数据流的复制；三是I/O数据流的Cache处理。 如上图所示，OSD进程是一种主备方式部署的进程，由MDC模块实时监控主备OSD进程的状态。当指定Partition所在的主OSD故障时，存储服务会实时自动切换到备OSD，保证了业务的连续性。OSD进程内部主要分为RSM、SNAP、CACHE、AIO和SMIO等子进程。各类子进程的作用的如下： RSM：采用复制协议实现I/O数据流的复制。 SNAP：实现卷与快照的I/O功能、磁盘空间的管理。 CACHE：实现cache功能。 AIO：实现异步I/O数据流下发到底层SMIO模块，并且通过调用SMIO接口来监控介质故障。 SMIO：**下发I/O数据流到实际的物理介质、监控物理介质故障、获取磁盘信息。** 每个OSD会管理一个硬盘，系统初始化时，OSD会按照1MB为单位对硬盘进行分片管理，并在硬盘的元数据管理区域记录每个1MB分片的分配信息。OSD接收到VBS发送的I/O数据流操作后，根据key查找该数据在硬盘上的具体分片信息，获取数据后返回给VBS，从而完成整个数据路由过程。 对于写请求，OSD根据分区-主磁盘-备磁盘1-备磁盘2映射表，通知各个备磁盘的OSD进行写操作，主与备OSD进程都完成写后返回VBS。（在多副本场景下，备OSD数据由主OSD数据同步写入，避免调用VBS进程占用计算节点资源） 在OSD内核部分还有有一个VDB子进程（上图中VDL，新版本已改为VDB），也就是Key-Value DB数据库。如下图所示： 磁盘的每一个1M空间都固定的分配给一个key，一定数量连续的key组成一个chunk。所谓的Chunk就是组成Partition的基本单位，一个Partition的存储空间由1个或多个Chunk构成。 FusionStorage MDC模块功能MDC（Metadata Controller）是一个高可靠集群，通过HA(High Availability)机制保证整个系统的高可用性和高可靠性，如下图所示： MDC模块部署在Zookeeper集群中，通过ZooKeeper集群，实现元数据（如Topology、OSD View、Partition View、VBS View等）的可靠保存。ZK盘的配置原则为：在32台服务器内，默认选择3个独立的ZK盘（HDD）；在32~128台服务器内，默认选择5个独立ZK盘（HDD）；在多于128台服务器，默认选择5个独立ZK盘（SSD）；当使用SSD卡做主存时，选择3个ZK分区。 ### MDC模块内部通过Partition分配算法，实现数据多份副本的RAID可靠性。并且，作为FusionStorage的主控制模块，通过与OSD、VBS间的消息交互，实现对OSD、VBS节点的状态变化的获取与通知。 通过与Agent间的消息交互，MDC实现系统的扩减容、状态查询、维护等。通过心跳检测机制，MDC模块完成对OSD、VBS的状态监控。 Zookeeper(简称ZK) 分布式服务框架主要用来解决分布式应用中经常遇到的，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等，ZK主要工作包括三项： MDC主备管理： MDC采用一主两备部署模式；在MDC模块进程启动后，各个MDC进程会向ZK注册选主，先注册的为主MDC；运行过程中，ZK记录MDC主备信息，并通过心跳机制监控MDC主备健康状况，一旦主MDC进程故障，会触发MDC重新选主。 数据存储：在MDC运行过程中，会生成各种控制视图信息，包括目标视图、中间视图、IO视图信息等，这些信息的保存、更新、查询、删除操作都通过ZK提供的接口实现。 数据同步：数据更新到主ZK，由主ZK自动同步到两个备ZK，保证主备ZK数据实时同步。一旦ZK发生主备切换，业务不受影响 在FusionStorage典型部署中，为了保证系统可靠性，ZK采用一主两备部署模式，每个管理节点部署一个ZK进程，ZK主备管理由ZK内部机制保证。 MDC进程与ZK进程采用C/S模式，通过TCP协议通信；MDC可以连接到任意一个ZK服务器，并且维持TCP连接。如果这个TCP连接中断，MDC能够顺利切换到另一个ZK服务器。 FusionStorage中的视图如下图所示，FusionStorage中有三种视图最为关键，分别是：OSD View、IO View和Partition View。 其中，OSD View主要记录OSD进程的ID和状态；而IO View中记录了主Partition分区ID和对应的OSD节点之间的映射关系；Partition View中记录主备Partition与OSD的对应关系。IO View是Partition View的子集。 MDC通过心跳感知OSD的状态，OSD每秒上报给MDC特定的消息（比如：OSD容量等），当MDC连续在特定的时间内（比如：当前系统为5s）没有接收到OSD的心跳信息，则MDC认为该OSD已经出故障（比如：OSD进程消失或OSD跟MDC间网络中断等），MDC则会发送消息告知该OSD需要退出，MDC更新系统的OSD视图信息，并给每台OSD发送视图变更通知，OSD根据新收到的视图，来决定后续的操作对象。 多副本复制取决于MDC的视图；两副本情况下，当client发送一个写请求到达该OSD的时候，该OSD将根据Partition视图的信息，将该写请求复制一份到该Partition的备OSD。多副本情况下，则会复制发送多个写请求到多个备OSD上。 FusionStorage 主要模块交互关系 Step1：**系统启动时，MDC与ZK互动决定主MDC**。主MDC与其它MDC相互监控心跳，主MDC决定某MDC故障后接替者。其它MDC发现主MDC故障又与ZK互动升任主MDC。 Step2：**OSD启动时向主MDC查询归属MDC,向归属MDC报告状态，归属MDC把状态变化发送给VBS。当归属MDC故障，主MDC指定一个MDC接管，最多两个池归属同一个MDC。** Step3：**VBS启动时查询主MDC，向主MDC注册（主MDC维护了一个活动VBS的列表，主MDC同步VBS列表到其它MDC，以便MDC能将OSD的状态变化通知到VBS），向MDC确认自己是否为leader；VBS从主MDC获取IO View，主VBS向OSD获取元数据，其它VBS向主VBS获取元数据**。 FusionStorage系统中会存在多个VBS进程，如果多个VBS同时操作元数据卷，会引起数据被写坏等问题。为避免该问题，FusionStorage系统对VBS引入了主备机制，只有主VBS可操作元数据卷，所有的备VBS不允许操作元数据卷，一套FusionStorage系统中只存在一个主VBS；VBS的主备角色由MDC进程确定，所有VBS通过和MDC间的心跳机制保证系统中不会出现双主的情况。 只有主VBS能够操作元数据，所以备VBS收到的卷和快照管理类命令需要转发到主VBS处理，对于挂载、卸载等流程，主VBS完成元数据的操作后，还需要将命令转到目标VBS实现卷的挂载、卸载等操作。 FusionStorage的I/O读写流程FusionStorage Cache读机制FusionStorage的读缓存采用分层机制，第一层为内存cache，内存cache采用LRU机制缓存数据。第二层为SSD cache，SSD cache采用热点读机制，系统会统计每个读取的数据，并统计热点访问子，当达到阈值时，系统会自动缓存数据到SSD中，同时会将长时间未被访问的数据移出SSD。FusionStorage预读机制，统计读数据的相关性，读取某块数据时自动将相关性高的块读出并缓存到SSD中。 如上图所示，OSD收到VBS发送的读I/O操作的步骤处理： Step 1：从“内存读cache”中查找是否存在所需I/O数据，存在则直接返回，并调整该I/O数据到“读cache”LRU队首，否则执行Step 2； Step 2：从“SSD的读cache”中查找是否存在所需I/O数据，存在则直接返回，并增加该I/O数据的热点访问因子，否则执行Step 3； Step 3：从“SSD的写cache”中查找是否存在所需I/O数据，存在则直接返回，并增加该I/O数据的热点访问因子；如果热点访问因子达到阈值，则会被缓存在“SSD的读cache”中。如果不存在，执行Step 4； Step 4：从硬盘中查找到所需I/O数据并返回，同时增加该I/O数据的热点访问因子，如果热点访问因子达到阈值，则会被缓存在“SSD的读cache”中； 读修复：在读数据失败时，系统会判断错误类型，如果是磁盘扇区读取错误，系统会自动从其他节点保存的副本读取数据，然后重新写入该副本数据到硬盘扇区错误的节点，从而保证数据副本总数不减少和副本间的数据一致性。 FusionStorage Cache写机制OSD在收到VBS发送的写I/O操作时，会将写I/O缓存在SSD cache后完成本节点写操作。OSD会周期性地将缓存在SSD cache中的写I/O数据批量写入到硬盘，写Cache有一个水位值，未到刷盘周期超过设定水位值也会将Cache中数据写入到硬盘中。 FusionStorage支持将服务器部分内存用作读缓存，NVDIMM和SSD用作写缓存。并且支持大块直通，按缺省配置大于256KB的块直接落盘不写Cache，这个配置可以人为修改。 FusionStorage 读I/O流程 Step1：APP下发读IO请求到OS，OS转发该IO请求到本服务器的VBS模块；VBS根据读I/O信息中的LUN和LBA信息，通过数据路由机制确定数据所在的Primary OSD；如果此时Primary OSD故障，VBS会选择secondary OSD读取所需数据。 Step2：Primary OSD接收到读I/O请求后，按照Cache机制中的“Read cache机制”获取到读I/O所需数据，并返回读I/O成功给VBS。 FusionStorage 写I/O流程 Step1：APP下发写I/O请求到OS，OS转发该I/O请求到本服务器的VBS模块；VBS根据写I/O信息中的LUN和LBA信息，通过数据路由机制确定数据所在的Primary OSD。 Step2：Primary OSD接收到写I/O请求后，同时以同步方式写入到本服务器SSD cache以及数据副本所在其他服务器的secondary OSD，secondary OSD也会同步写入本服务器SSD cache。Primary OSD接收到两个都写成功后，返回写I/O成功给VBS；同时，SSD cache中的数据会异步刷入到硬盘。 Step3：VBS返回写I/O成功，如果是3副本场景，primary OSD会同时同步写I/O操作到secondary OSD和third OSD。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-06-03-华为分布式存储FusionStorage概述]]></title>
    <url>%2F2019%2F06%2F03%2F2019-06-03-%E5%8D%8E%E4%B8%BA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8FusionStorage%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[分布式存储基于通用的x86服务器，通过分布式的存储软件来构建存储系统，开放丰富灵活的软件义 策略和接口，允许管理员和租户进行自动化的系统管理和资源调度发放。业界的分布式存储，有多种形态，包括分布式块存储、分布式文件存储、分布式对象存储。各大存储厂商都由自己的解决方案，而开源的代表的就是ceph。考虑到电信云NFV领域的应用特性和相关设备商分布，我们这里主要介绍华为的分布式存储系统FusionStorage。 传统SAN架构在传统的IT行业和电信云早期时候，计算和存储是分离的，计算由各种x86服务器组成计算集群，存储由各种SAN/NAS组成存储集群，这种分离架构虽然通过多个计算实例共享相同存储，在计算节点发生故障后，无须耗时的外存 数据迁移，即可快速在其他计算节点上恢复故障应用。但是，这种传统存储资源缺乏共享。因为，传统存储设备和资源往往由不同厂家提供，之间无法进行资源共享，数据中心看到的是一个个孤立的存储资源。如下图所示。 而且，SAN机头成为制约系统扩展的单点瓶颈。因为，随着计算集群规模的不断扩展，由于存储资源池集中式控制机头的存在，使得共享存储系统的扩展性、性能加速、可靠性受到制约。比如：采用多个SAN系统，单个SAN系统的机头最多由双控扩展到16控，不同的SAN系统独立管理无法组成集群。各计算节点只能依赖于集中式机头内的缓存实现I/O加速，但由于各节点cache有限，且不共享导致cache的I/O缓存加速只能达到GB级别。尽管SAN控制机头自身具有主备机制，但依然存在异常条下主备同时故障的可能性。 在传统SAN存储系统中，一般采用集中式元数据管理方式，元数据中会记录所有LUN中不同偏移量的数据在硬盘中的分布，例如LUN1+LBA1地址起始的4KB长度的数据分布在第32块硬盘的LBA2上。每次I/O操作都需要去查询元数据服务，随着系统规模逐渐变大，元数据的容量也会越来越大，系统所能提供的并发操作能力将受限于元数据服务所在服务器的能力，元数据服务将会成为系统的性能瓶颈。 什么是Server SAN?为了解决上述问题，业界针对大多数企业事务型IT应用而言，提出信息数据的“ 即时处理” 理念，通过计算和存储融合，引入scale-out存储机制，也就出现了Server SAN的概念。所谓Server SAN就是由多个独立服务器自带的存储组成一个存储资源池，同时融合了计算和存储资源。scale-out的存储架构示意图如下： 以数据中心级资源共享为例，一个数据中心内可以构建一个很大的存储资源池，如下图所示，满足数据中心内各类应用对存储容量，性能和可靠性的需求，实现资源共享和统一管理。 通过在各个计算节点引入分布式控制器VBS，这是一种软件控制机头，线性扩展能力最大到4096个节点。而且，利用各个计算节点的cache组成分布式cache集群，满配情况下可达到TB级别的cache缓存能力。同时，各计算节点存储平面的存储网卡可采用专用IB接口卡，可以实现P2P的无阻赛传输网络，可达到100Gbps的数据传输能力。 最重要的是，采用分布式软件存储方式属于软件定义存储SDS的范畴，通过将控制面数据抽象和解耦，可与厂商的专有存储硬件解耦，底层的专有存储设备变为通用设备，实现简单统一管理和线性扩展。以华为FusionStorage为例，由于其采用的DHT算法（分布式哈希表结构，后面会详细讲），不仅数据能够尽可能分布到所有的节点中，可以使得所有节点负载均衡，现有节点上的数据不需要做很大调整实现数据均衡性，而且当有新节点加入系统中，系统会重新做数据分配，数据迁移仅涉及新增节点，从而实现数据修改的单调性。 分布式存储池的概念分布式存储系统把所有服务器的本地硬盘组织成若干个资源池，基于资源池提供创建/删除应用卷（Volume）、创建/删除快照等接口，为上层软件提供卷设备功能。分布式存储系统资源池示意图如下，具有以下特点。 每块硬盘分为若干个数据分片（Partition），每个Partition只属于一个资源池，Partition是数据多副本的基本单位，也就是说多个数据副本指的是多个Partition，如上图中的Pn色块。 系统自动保证多个数据副本尽可能分布在不同的服务器上（服务器数大于数据副本数时）。系统自动保证多个数据副本间 的数据强一致性（所谓强一致性是指无论读还是写I/O，都要进行数据一致性检查）。Partition中的数据以Key-Value的方式存储。对上层应用呈现并提供卷设备（Volume），没有LUN的概念，使用简单。系统自动保证每个硬盘上的主用Partition和备用Partition数量相当，避免出现集中的热点。所有硬盘都可以用做资源池的热备盘，单个资源池最大支持数百上千块硬盘。 分布式存储系统采用分布式集群控制技术和分布式Hash数据路由技术，提供分布式存储功能特性。这里的概念比价抽象，所以下面结合华为FusionStorage的具体情况进行介绍。 华为FusionStorage分布式存储概述FusionStorage通过创新的架构把分散的、低速的SATA/SAS机械硬盘组织成一个高效的类SAN存储池设备，提供比SAN设备更高的I/O，如下图所示： 集群内各服务器节点的硬盘使用独立的I/O带宽，不存在独立存储系统中大量磁盘共享计算设备和存储设备之间有限带宽的问题。其将服务器部分内存用做读缓存，NVDIMM用做写缓存，数据缓存均匀分布到各个节点 上，所有服务器的缓存总容量远大于 采用外置独立存储的方案。即使采用大容量低成本的SATA硬盘，分布式存储系统仍然可以发挥很高的I/O性能，整体性能提升1～3倍，同时提供更大的有效容量。 由于其采用无状态的分布式软件机头，机头部署在各个服务器上，无集中式机头的性能瓶颈。单个服务器上软件机头只占用较少的CPU资源，提供比集中式机头更高的IOPS和吞吐量。例如：假设系统中有20台服务器需要访问FusionStorage提供的存储资源，每台服务器提供给存储平面的带宽为210Gb，我们在每台服务器中部署1个VBS模块（相当于在每台服务器中部署1个存储机头），20台服务器意味着可部署20个存储机头，所能获取到的总吞吐量最高可达202*10Gb=400Gb，随着集群规模的不断扩大，可以线性增加的存储机头，突破了传统的双控或者多控存储系统集中式机头的性能瓶颈。 华为FusionStorage是一种软件定义存储SDS技术，将通用X86服务器的本地HDD、SSD等介质通过分布式技术组织成大规模存储资源池。同时，以开放API的方式对非虚拟化环境的上层应用和虚拟机提供工业界标准的SCSI和iSCSI接口。关键一点是，华为官方自己吹牛逼的的口号—-FusionStorage是目前唯一商用支持PB级数据吞吐/存储能力的Server SAN产品。 华为FusionStorage主要应用于两种场景，如下图所示，而中国移动的电信云NFV领域的应用属于第一种场景。 场景一：与主流云平台的集成，构建云资源池中的存储资源池。可以和各种云平台集成，如华为FusionSphere、VMware、开源Openstack等，按需分配存储资源。 场景二：提供传统的存储应用，如各种业务应用(如SQL、Oracle RAC、Web、行业应用等等)。 因此，华为FusionStorage是一种开发兼容的系统，可以兼容各种主流数据库、各种主流虚拟化平台以及各种主流服务器。同时，支持虚拟化平台和数据库资源池融合部署，即共用一个数据中心内的FusionStorage存储资源池。华为FusionStorage的软硬件兼容性总结如下： 硬件兼容性主要包括：服务器、各种HDD盘，SSD盘，PCIE SSD卡/盘以及各种RAID卡，以太网卡， Infiniband卡等。 软件兼容性主要包括：各种虚拟化平台、各种操作系统以及各种数据库软件。 而且，FusionStorage支持使用SSD替代HDD作为高速存储设备，支持使用Infiniband网络替代GE/10GE网络提供更高的带宽，为对性能要求极高的大数据量实时处理场景提供完美的支持千万级IOPS。InfiniBand（直译为“无限带宽”技术，缩写为IB）是一个用于高性能计算的计算机网络通信标准，它具有极高的吞吐量和极低的延迟，用于计算机与计算机之间的数据互连。InfiniBand也用作服务器与存储系统之间的直接或交换互连，以及存储系统之间的互连。硬件接口图如下： inifiniband接口可以实现链路可聚合：大多数系统使用一个4X聚合。12X链路通常用于计算机集群和超级计算机互连，以及用于内部网络交换器连接。而且，InfiniBand也提供远程直接内存访问（RDMA）能力以降低CPU负载。除了板式连接，它还支持有源和无源铜缆（最多30米）和光缆（最多10公里）。通过Inifiniband Association可以指定CXP铜连接器系统，用于通过铜缆或有源光缆达到高达传输速率120Gbps的能力。 FusionStorage的技术规格参数表如下： 集群指标 规格 卷规格指标 规格 单集群存储服务器数量 4,096个 集群最大卷数量 1,280,000个 单集群硬盘数量 49,152个 单资源池最大卷数量 65,000个 单集群支持的计算节点数量 10,240个 卷容量 64MB～256TB 单集群最大资源池数量 128个 卷最大共享主机数量 128个 资源池规格指标 规格 每个主机最大挂载卷数量 512个 单资源池的硬盘数量 两副本（HDD或SSD）：12个～96个三副本（HDD或SSD）：12个～2,048个 共享卷最大数量 20,000个 单资源池的存储服务器数量 两副本（HDD或SSD）：3个～16个三副本（HDD或SSD）：4个～256个 单个卷最大快照数量 无限制，快照总数不超过1,280,000个 单资源池的机柜数量 非跨机柜数据安全：1个～12个跨机柜数据安全：3个～12个 单个卷最大链接克隆数量 2,048个 单个服务器最多可划分的资源池数量 3个 同步复制卷的最大数量 4,096个 iSCSI接口协议指标 规格 同步复制卷的最大容量 2TB iSCSI CHAP用户最大数量 1,024个 每个主机支持的同步复制卷的总容量 64TB iSCSI卷最大扩容容量 256TB FusionStorage逻辑架构在中国移动电信云NFV中，主要使用分布式块存储，华为FusionStorage Block的解决方案将通用X86存储服务器池化，建立大规模块存储资源池，提供标准的块存储数据访问接口（SCSI和iSCSI等）。支持各种虚拟化Hypervisor平台和各种业务应用(如SQL、Web、行业应用等等)，可以和各种云平台集成，如华为FusionSphere、VMware、开源Openstack等，按需分配存储资源。如下图所示： 同时，FusionStorage Block通过及SSD做Cache，构建内存—缓存（SSD)—主存（HDD/SAS/SATA)三级存储等关键技术，将存储系统的性能和可靠性得到极大的提高。 如下图所示，华为FusionStorage的逻辑架构中，按照管理角色和代理角色分为FSM和FSA两类进程，在代理进程内部按照数据管理、I/O控制和数据读写进一步细分为MDC、VBS和OSD三个子进程，各类软件逻辑模块的功能解释如下： FSM（FusionStorage Manager）：FusionStorage管理模块，提供告警、监控、日志、配置等操作维护功能。一般情况下FSM通过管理虚机主备节点部署，不考虑资源利用率的场景下，也可以使用物理服务器主备部署。 FSA（FusionStorage Agent）：代理进程，部署在各节点上，实现各节点与FSM通信。FSA包含MDC、VBS和OSD三类不同的子进程。根据系统不同配置要求，分别在不同的节点上启用不同的进程组合来完成特定的功能。比如：单纯存储节点只需要部署OSD子进程，用于构建虚拟存储池。计算和存储融合节点，不仅需要部署OSD进程，还需部署VBS进程，用于控制数据读写I/O。 MDC（MetaData Controller）：负责整个分布式存储集群的的元数据控制，实现对分布式集群的状态控制，以及控制数据分布式规则、数据重建规则等。 MDC是整个分布式存储集群的核心，通过MDC模块才能找到数据的具体存储位置，知悉存储副本的数量，分布和大小。因此，MDC子进程必须采用高可用HA的方式部署，默认情况下，是部署在3个节点的ZK(Zookeeper)盘上，形成MDC集群。 VBS（Virtual Block System）：虚拟块存储管理组件，负责卷元数据的管理，也就是我们俗称的”软件机头“。VBS子进程提供分布式集群接入点服务，使计算资源能够通过VBS访问分布式存储资源。VBS上存储的是虚拟机虚拟磁盘卷的元数据，用于描述虚拟卷在OSD的位置，分布和大小等。凡是融合计算资源的节点，每个节点上默认部署一个VBS进程，形成VBS集群，由于VBS从OSD读取数据时可以并发，所以节点上也可以通过部署多个VBS来提升IO性能。 OSD（Object Storage Device）：对象存储设备服务子进程，执行具体的I/O操作。在每个存储节点服务器上可以部署多个OSD进程。一般情况下，存储服务器上的一块本地磁盘默认对应部署一个OSD进程。如果使用大容量SSD卡作主存，为了充分发挥SSD卡的性能，可以在1张SSD卡上部署多个OSD进程进行管理，例如2.4TB的SSD卡可以部署6个OSD进程，每个OSD进程负责管理400GB存储空间。 FusionStorage部署方式在中国移动电信云NFV解决方案中，无论软件由哪家厂商提供，虚拟化Hypervisor都是采用KVM来实现，区别只是不同厂商在开源KVM中私有增强或加固。华为FusionStorage Block支持XEN/KVM之类的Linux开放体系的Hypervisor场景，包括华为基于XEN/KVM增强的FusionSphere UVP虚拟化平台，和非华为的XEN/KVM虚拟化平台。在XEN/KVM的虚拟化场景下，FusionStorage Block既支持计算存储融合的部署模式，又支持计算存储分离的部署模式。 所谓融合部署，就是指计算与存储的融合部署方式，也就是说将应用或虚拟机与存储在集群范围内部署在同一台服务器上。如下图所示： 融合部署对应到FusionStorage的逻辑架构中，就是指的是将VBS和OSD部署在同一台服务器中。这种部署方式提升了资源利用率，虽然一定意义上增加了计算节点服务器的CPU开销，但是通过分布式存储软件策略，可以满足数据就近读写，提升I/O转发性能，同时数据副本异地存放，满足数据存储可靠性。在电信云NFV中，对时延较敏感的业务控制数据应用虚机推荐采用融合部署的方式部署。 所谓分离部署，顾名思义就是将计算与存储的分开部署，也就是将业务虚拟机与存储部署在不同的服务器上。如下图所示： 分离部署对应到FusionStorage的逻辑架构中，就是将VBS和OSD分别部署在不同的服务器中，其中存储节点服务器只部署OSD子进程，计算节点服务器只部署VBS子进程。计算节点通过存储网络完成存储节点上数据的I/O读写，一般高性能数据库应用因为需要对数据进行计算，推荐采用分离部署的方式。但是，这种部署方式在I/O转发性能略差于融合部署。采用分离部署时，存储节点服务器Host OS可以采用通用的Linux OS操作系统，这就为后续分布式存储解耦提供了基础。 FusionStorage数据可靠性集群管理华为分布式存储FusionStorage的分布式存储软件采用集群管理方式，规避单点故障，一个节点或者一块硬盘故障自动从集群内隔离出来，不影响整个系统业务的使用。集群内选举进程Leader，Leader负责数据存储逻辑的处理，当Leader出现故障，系统自动选举其他进程成为新的Leader。 多数据副本华为分布式存储FusionStorage中没有使用传统的RAID模式来保证数据的可靠性（实际采用的RAID2.0+技术），而是采用了多副本备份机制，即同一份数据可以复制保存多个副本。在数据存储前，对数据打散进行分片，分片后的数据按照一定的规则保存集群节点上。如下图所，FusionStorage采用数据多副本备份机制来保证数据的可靠性，即同一份数据可以复制保存为2~3个副本。 FusionStorage针对系统中的每个卷，默认按照1MB进行分片，分片后的数据按照DHT算法保存集群节点上。如上图所示，对于服务器Server1的 磁盘Disk1上的数据块P1，它的数据备份为服务器Server2的磁盘Disk2上P1’，P1和P1’构成了同一个数据块的两个副本。当P1所在的硬盘故障时，P1’可以继续提供存储服务。 数据分片分配算法保证了主用副本和备用副本在不同服务器和不同硬盘上的均匀分布，换句话说，每块硬盘上的主用副本和备副本数量是均匀的。扩容节点或者故障减容节点时，数据恢复重建算法保证了重建后系统中各节点负载的均衡性。 ### 数据一致性和弹性扩展数据一致性的要求是：当应用程序成功写入一份数据时，后端的几个数据副本必然是一致的，当应用程序再次读取时，无论在哪个副本上读取，都是之前写入的数据，这种方式也是绝大部分应用程序希望的。保证多个数据副本之间的数据一致性是分布式存储系统的重要技术点。华为分布式存储FusionStorage采用强一致性复制技术确保各个数据副本的一致性，一个副本写入，多个副本读取。 如下图所示， 存储资源池支持多种安全级别，采用强一致性复制协议，冗余策略按需灵活配置、部署。当采用机柜级冗余时，能同时容忍2或3个机柜失效；当采用节点级冗余时，能同时容忍2或3个节点失效。 FusionStorage还支持Read Repair机制。Read Repair机制是指在读数据失败时，会判断错误类型，如果是磁盘扇区读取错误，可以通过从其他副本读取数据，然后重新写入该副本的方法进行恢复，从而保证数据副本总数不减少。 同时，FusionStorage的分布式架构具有良好的可扩展性，支持超大容量的存储。如下图所示，扩容存储节点后不需要做大量的数据搬迁，系统可快速达到负载均衡状态。 FusionStorage支持灵活扩容方式，不仅支持计算节点、硬盘、存储节点单类性扩容，而且支持同时进行扩容。由于软件机头、存储带宽和Cache都均匀分布到各个节点上，系统IOPS、吞吐量和Cache随着节点扩容线性增加，理论上计算节点和存储节点越多，FusionStorage的性能越强。 快速数据重建分布式存储系统内部需要具备强大的数据保护机制。数据存储时被分片打散到多个节点上，这些分片数据支持分布在不同的存储节点、不同的机柜之间，同时数据存储时采用多副本技术，数据会自动存多份，每一个分片的不同副本也被分散保存到不同的存储节点上。在硬件发生故障导致数据不一致时，分布式存储系统通过内部的自检机制，通过比较不同节点上的副本分片，自动发现数据故障。发现故障后启动数据修复机制，在后台修复数据。如下图所示为华为FusionStorage的数据重建过程示意图。 FusionStorage中的每个硬盘都保存了多个DHT分区（Partition），这些分区的副本按照策略分散在系统中的其他节点。当FusionStorage检测到硬盘或者节点硬件发生故障时，自动在后台启动数据修复。 由于分区的副本被分散到多个不同的存储节点上，数据修复时，将会在不同的节点上同时启动数据重建，每个节点上只需重建一小部分数据，多个节点并行工作，有效避免单个节点重建大量数据所产生的性能瓶颈，对上层业务的影响做到最小化。 ### 掉电保护分布式存储系统运行过程中可能会出现服务器突然下电的情况，此时在内存中的元数据和写缓存数据会随着掉电而丢失，需要使用非易失存储介质来保存和恢复元数据和缓存数据。 华为FusionStorage支持的保电介质为NVDIMM内存条或SSD。如下图所示，程序运行过程中会把元数据和缓存数据写入保电介质中，节点异常掉电并重启后，系统自动恢复保电介质中的元数据和缓存数据。 部署FusionStorage时，要求每一台服务器配备NVDIMM内存条或SSD盘，服务器掉电时会把元数据和缓存数据写入NVDIMM的Flash或SSD盘中，上电后又会把Flash中的数据还原到内存中。FusionStorage能够自动识别出系统中的NVDIMM内存，并把需要保护的数据按照内部规则存放在NVDIMM中，用于提供掉电保护功能。 FusionStorage的特性FusionStorage分布式存储软件总体框架如下所示，由存储管理模块、存储接口层、存储服务层和存储引擎层4类组成。 FusionStorage块存储功能 - SCSI/iSCSI块接口FusionStorage通过VBS以SCSI或iSCSI方式提供块接口。当采用SCSI方式时，安装VBS的物理服务器、FusionSphere或KVM等采用SCSI接口（带内方式）；当采用iSCSI方式时，安装VBS以外的虚拟机或主机提供存储访问，VMware、MS SQL Server集群采用iSCSI模式（带外方式）。 使用SCSI存储接口时，支持快照、快照备份、链接克隆功能，iSCSI暂不支持前述特性。对于iSCSI协议的支持是通过VBS提供iSCSI Target，块存储使用方通过本机的Initiator与iSCSI Target联接来访问存储。FusionStorage支持以下两种安全访问的标准：CHAP身份验证和LUN MASKING给Host对Lun的访问进行授权。 FusionStorage精简配置功能相比传统方式分配物理存储资源，精简配置可显著提高存储空间利用率。FusionStorage天然支持自动精简配置，和传统SAN相比不会带来性能下降。 当用户对卷进行写操作时，系统才分配实际物理空间，FusionStorage Block仅处理虚拟卷空间和实际物理空间之前的映射关系，对性能无影响。 FusionStorage快照功能FusionStorage快照机制，将用户卷数据在某个时间点的状态保存下来，可用作导出数据、恢复数据之用。FusionStorage Block快照数据基于DHT机制，数据在存储时采用ROW（Redirect-On-Write）机制，快照不会引起原卷性能下降。比如：针对一块容量为2TB的硬盘，完全在内存中构建索引需要24MB空间，通过一次Hash查找即可判断有没有做过快照，以及最新快照的存储位置，效率很高。 无限次快照：快照元数据分布式存储，水平扩展，无集中式瓶颈，理论上可支持无限次快照。 卷恢复速度快：无需数据搬迁，从快照恢复卷1S内完成（传统SAN在几小时级别）。 FusionStorage链接克隆功能FusionStorage支持一个卷快照创建多个克隆卷，对克隆卷修改不影响原始快照和其它克隆卷。克隆卷继承普通卷所有功能，即克隆卷可支持创建快照、从快照恢复及作为母卷再次克隆操作。 支持批量进行虚拟机卷部署，在1秒时间内创建上百个虚拟机卷。 支持1:2048的链接克隆比，提升存储空间利用率。 FusionStorage Block卷迁移为了存储池之间的容量平衡，FusionStorage支持将容量满的池迁移到一个空闲的池。或者，为了改变卷的性能，FusionStorage支持卷在不同性能的池之间的迁移，从低性能的池向高性能的池迁移。这就是卷迁移使用的场景，在FusionStorage中，由于迁移过程中，源卷不能有写数据的操作，所以这种迁移属于冷迁移。 步骤： 【创建目标卷】&gt;&gt;&gt;【卷数据复制】&gt;&gt;&gt;【删除源卷】&gt;&gt;&gt;【目标卷改名】&gt;&gt;&gt;【完成】 FusionStorage Block双活 基于AB两个数据中心的两套FusionStorage Block集群构建双活容灾关系，基于两套FusionStorage的卷虚拟出一个双活卷，两数据中心业务的主机能同时进行读写服务。任意数据中心故障，数据零丢失，业务能迅速切换到另外一个站点运行，保证业务连续型。 在原有基础服务基础上，引入复制集群，按服务化的架构提供双活业务。支持物理部署和虚拟机部署，可以做到独立安装和升级，独立扩展，按卷粒度提供双活服务。且支持优先站点仲裁和第三方仲裁的双仲裁模式，故障自动倒换，无需人工介入。同时，能跟上层Oracle RAC、VMWare等应用形成端到端双活容灾解决方案。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-31-PXE批量部署原理及kickstart工具实践]]></title>
    <url>%2F2019%2F05%2F31%2F2019-05-31-PXE%E6%89%B9%E9%87%8F%E9%83%A8%E7%BD%B2%E5%8E%9F%E7%90%86%E5%8F%8Akickstart%E5%B7%A5%E5%85%B7%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[PXE服务器简介PXE(preboot execute environment)是由Intel公司开发的最新技术，工作于C/S的网络模式，支持工作站通过网络从远端服务器下载映像，并由此支持来自网络的操作系统的启动过程，其启动过程中，终端要求服务器分配IP地址，再用TFTP（trivial file transfer protocol）或MTFTP(multicast trivial file transfer protocol)协议下载一个启动软件包到本机内存中并执行，由这个启动软件包完成终端基本软件设置，从而引导预先安装在服务器中的终端操作系统。 在Linux中操作系统有多种的安装方式：HDD、USB、CDROM、PXE及远程管理卡等。在系统运维中，经常要批量安装操作系统，一般的企业服务器数量都在几十、几百、几千、甚至上万台。这么多的机器，如果人工的一台一台去安装，那运维人员可能要把大部分时间都花费在了安装系统上，所以，一般都会建立一个PXE服务器，通过网络来批量部署系统。PXE部署的逻辑图如下图所示： 无人值守部署系统流程Step1：部署PXE需要的环境。首先在pxe服务器端需要有一个DHCP服务器，需要有tftp服务器和一个文件服务器，其中文件服务器可以是ftp，http，nfs等文件服务器，如果服务器性能好或者流量不是太大，这些服务器完全可以放在一台服务器上面。pxe启动需要网卡支持这样的功能，现在的绝大部分的网卡已经支持。 Step2：pex功能的客户端在主机开机启动项为网络启动，一般默认都此选项，如果没有可自行设置bios启动项。 Step3：客户端开机之后进入网络启动，此时客户端没有IP地址需要发送广播报文（pxe网卡内置dhcp客户端程序），dhcp服务器响应客户端请求，分配给客户端相应的IP地址与掩码等信息。 Step4： 客户端得到IP地址之后，与tftp通信，下载pxelinux.0，default文件，根据default指定的vmlinuz，initrd.img启动系统内核，并下载指定的ks.cfg文件。 Step5：根据ks.cfg文件去文件共享服务器（http/ftp/nfs）上面下载RPM包开始安装系统，注意此时的文件服务器是提供yum服务器的功能的。 部署各功能服务器（整合在一套服务器上）由于我们实验环境验证10台以内服务器批量部署，网络接口的流量不是很大，因此将所有功能服务器部署在一台虚拟机上。如果是实际生产环境，应根据部署规模估算网络接口流量，根据流量大小将上述各功能服务器部署在不同的物理服务器上，采用分布式批量部署的方式。 tftp服务的安装tftp的服务器需要安装tftp-server包，tftp工作在udp 69号端口。启动服务稍有不同，在CentOS7需要启动tftpd.socket ，而在CentOS6的版本中需要保证服务开机启用，并且重新启动xinetd。因为，在CentOS7是将所有进程托管给systemd进程，只需启动tftp.socket，打开监听的端口套接字即可，而在CentOS6中则是将不常用的服务统一托管给了xinetd进程，由xinetd进程统一进行管理，所以重启xinetd即可。 tftp-server默认没有配置文件，直接启用服务，就可以使用（当然可以手动建立，但是没有必要）。 Step1：安装tftp-server服务 1[root@C7-Server01 ~]# yum install -y tftp-server tftp xinetd Step2：配置xinetd.conf文件 1234567891011121314151617181920212223# vim /etc/xinetd.d/tftp# description: The tftp server serves files using the trivial file transfer \# protocol. The tftp protocol is often used to boot diskless \# workstations, download configuration files to network-aware printers, \# and to start the installation process for some operating systems.service tftp&#123; socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd server_args = -s /var/lib/tftpboot disable = no #这里默认是yes，改成no per_source = 11 cps = 100 2 flags = IPv4&#125; Step3：启用tftp、tftp-server服务 123[root@C7-Server01 ~]# systemctl enable tftp.socket &amp;&amp; systemctl start tftp.socket[root@C7-Server01 ~]# systemctl enable tftp.service &amp;&amp; systemctl start tftp.service[root@C7-Server01 ~]# systemctl enable xinetd &amp;&amp; systemctl start xinetd 启动完成后，进行服务启动检验，提示active就表示服务启用成功. Step4：tftp客户端测试 由于本机同时是Server端，也是Client端（参见上面第一步安装的软件包tftp），所以本机不需要再次安装tftp软件包，直接测试即可。 12345678910111213141516171819202122232425262728# 进入tftp服务的根目录[root@C7-Server01 ~]# cd /var/lib/tftpboot/[root@C7-Server01 tftpboot]# pwd/var/lib/tftpboot# 创建一个tftp的测试文件[root@C7-Server01 tftpboot]# touch tftp.test[root@C7-Server01 tftpboot]# ls -l tftp.test -rw-r--r-- 1 root root 0 May 30 12:07 tftp.test# 切换到root用户家目录[root@C7-Server01 tftpboot]# cd[root@C7-Server01 ~]# pwd/root# 开始测试[root@C7-Server01 ~]# tftp 192.168.101.3tftp&gt; get tftp.testtftp&gt; quit# 校验文件是否下载成功[root@C7-Server01 ~]# ll tft*-rw-r--r-- 1 root root 0 May 30 12:09 tftp.test DHCP服务器的安装DHCP使用udp的67号端口，使用ss -unl 可以查看到监听的67号端口。 Step1：安装DHCP服务器包 1[root@C7-Server01 ~]# yum install -y dhcp Step2：因为DHCP默认配置文件为空，如下图，官方建议复制配置示例文件进行替换。 1[root@C7-Server01 ~]# cp /usr/share/doc/dhcp-4.2.5/dhcpd.conf.example /etc/dhcp/dhcpd.conf Step3：编辑配置文件，如下： 1234567891011# vim /etc/dhcp/dhcpd.conf#---------可用最简配置-------------------next-server 192.168.101.3; #tftp服务器地址filename "pxelinux.0"; #启动文件#subnet 192.168.101.0 netmask 255.255.255.0 &#123;range 192.168.101.110 192.168.101.131; #ip地址池option routers 192.168.101.2; #网关option domain-name-servers 192.168.101.2; #DNS &#125; Step4：启用dhcp服务，查询监听端口状态。 123[root@C7-Server01 etc]# systemctl enable dhcpd &amp;&amp; systemctl start dhcpd[root@C7-Server01 etc]# ss -nlu|grep 67UNCONN 0 0 *:67 *:* 需要注意一点：如果是多网卡，默认监听eth0，指定DHCP监听eth1网卡方法如下 12345# vim /etc/sysconfig/dhcpd 在这个文件中，不是/etc/dhcp/dhcpd.conf# Command line options hereDHCPDARGS=eth1 # 指定监听网卡 安装并配置HTTP服务器我们通过http远程安装，同样也可以配置为ftp、NFS方式安装。 Step1：安装httpd软件包 1[root@CentOS7 ~]# yum -y install httpd Step2：启动httpd服务 1[root@CentOS7 ~]# systemctl enable httpd &amp;&amp; systemctl start httpd ftp服务器安装配置ftp可以进行许多安全方面的配置，但是在做一个内网的服务没有必要做许多安全方面的配置，只需要保证能正常使用即可，ftp的默认文件共享路径为：/var/ftp/pub/，需要共享文件只需放在该目录即可，安装系统可以直接将光盘挂载至该共享目录的一个子目录即可。 Step1：安装软件包 1[root@C7-Server01 ~]# yum install -y vsftpd Step2：创建挂载目录 1[root@C7-Server01 ~]# mkdir -p /var/ftp/pub/centos7/ Step3：启用ftp服务 1[root@C7-Server01 ~]# systemctl enable vsftpd &amp;&amp; systemctl start vsftpd 配置ks.cfg文件Anaconda是RedHat、CentOS、Fedora等Linux的安装管理程序。它可以提供文本、图形等安装管理方式，并支持Kickstart等脚本提供自动安装的功能。该程序的功能是把位于光盘或其他源上的数据包，根据设置安装到主机上。为实现该定制安装，它提供一个定制界面，可以实现交互式界面供用户选择配置（如选择语言，键盘，时区等信息）。 Anaconda支持的管理模式： Kickstart提供的自动化安装、 对一个RedHat实施upgrade和Rescuse模式对不能启动的系统进行故障排除。要进入安装步骤，需要先有一个引导程序引导启动一个特殊的Linux安装环境系统。引导有多种方式： （可用的安装方式：本地CDROM、硬盘驱动器、网络方式（NFS、FTP、HTTP）等等） 基于网络方式的小型引导镜像，需要提供小型的引导镜像； U盘引导，通过可引导存储介质中的小型引导镜像启动安装过程； 基于PXE的网络安装方式，要提供PXE的完整安装环境； 其他bootloder引导（如GRUB）。 Step1：拷贝ks.cfg模板文件 1[root@CentOS7 ~]# cp /root/anaconda-ks.cfg /var/www/html/ks.cfg Step2：编辑ks.cfg文件（以下是我自己常用的设置） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103[root@CentOS7 ~]# cat /var/www/html/ks.cfg #version=DEVEL# System authorization informationauth --enableshadow --passalgo=sha512# Use Network installation mediaurl --url=http://192.168.101.3/centos7/# Firewall configurationfirewall --disableselinux --disable# Use graphical install# graphicaltext# Run the Setup Agent on first bootfirstboot --disableignoredisk --only-use=sda# Keyboard layoutskeyboard --vckeymap=us --xlayouts='us'# System languagelang en_US.UTF-8# Network informationnetwork --bootproto=dhcp --device=eth0 --onboot=yesnetwork --bootproto=dhcp --device=eth1 --onboot=nonetwork --bootproto=dhcp --device=eth2 --onboot=nonetwork --hostname=CentOS7# Reboot after installationreboot# Root passwordrootpw --iscrypted $6$j/GfiKGObTLP91Om$02u.tAD0W7dPLs1PIDWebq8AiIHCCZ16h.3unIzeKp75io2PKln.g3T.zlra.Jzd.1wAZ2xmwqnUk7kcbsbh.1# System servicesservices --enabled="chronyd"# System timezonetimezone Asia/Shanghai --isUtc# System bootloader configurationbootloader --location=mbr --boot-drive=sda# Partition clearing informationclearpart --none --initlabel# Disk partitioning informationpart /boot --fstype="ext4" --ondisk=sda --size=400part swap --fstype="swap" --ondisk=sda --size=8192part / --fstype="ext4" --ondisk=sda --size=1 --grow%packages@base@compat-libraries@debugging@developmentgccglibcgcc-c++openssl-developensshtreenmapsysstatlrzszvimntpdatenet-toolswgetlibffi-develgitchronybridge-utilsqemu-kvmlibvirtvirt-install%end%addon com_redhat_kdump --disable --reserve-mb='auto'%end Step3：更改ks.cfg权限 1[root@CentOS7 ~]# chmod +r /var/www/html/ks.cfg 最后生成的配置文件如下：其中以#开头的行是注释行，其它部分开头是%开头，%end结尾。%packages是系统要安装的包，@开头是软件包组，@^是环境包组开头，以-开头是排除在外的包名或组名，除非必须的依赖性包则会安装，否则不会安装。%pre，%post是脚本，%pre是在任何磁盘分区之前进行，%post是在系统安装之后进行的系统配置。 复制内核文件 内核文件、虚拟根文件以及菜单文件，都是通过tftp服务来提供的，由于系统及版本的不同，对于一个比较复制集群来说，需要准备不同系统，不同版本的内核文件，initrd.img文件。菜单文件只需要一份即可。/var/lib/tftpboot/目录规划如下： Step1：挂载光驱，虚拟机方式一定要先在连接光驱打上钩 123456789101112[root@CentOS7 ~]# ls -l /dev|grep cdromlrwxrwxrwx. 1 root root 3 May 31 00:53 cdrom -&gt; sr0crw-rw----. 1 root cdrom 21, 1 May 31 00:53 sg1brw-rw----. 1 root cdrom 11, 0 May 31 00:53 sr0# 创建挂载目录[root@CentOS7 ~]# mkdir /var/www/html/centos7# 由于我的镜像文件已上传到linux中，所以通过以下方式挂载[root@CentOS7 ~]# mount -o loop /mydata/img/CentOS-7-x86_64-DVD-1804.iso /var/www/html/centos7/ Step2：将镜像中的启动文件COPY到tftp server的根目录中 12[root@CentOS7 ~]# cp /var/www/html/centos7/images/pxeboot/&#123;vmlinuz,initrd.img&#125; /var/lib/tftpboot/[root@CentOS7 ~]# cp /usr/share/syslinux/menu.c32 /var/lib/tftpboot/ vmlinuz是可引导的、压缩的内核文件，是可执行的Linux内核。 initrd是“initial ram disk”的简写，用来临时的引导硬件被vmlinuz接管并继续引导。 Step3：复制pxelinux.0文件到tftp目录下 1234567# 首先安装syslinux，因为pxelinux.0文件由syslinux包提供[root@CentOS7 ~]# yum install -y syslinux# 复制文件[root@CentOS7 ~]# cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/ Step4：编辑pxelinux.cfg菜单文件,即isolinux.cfg 1234567891011121314151617181920212223# 创建pxelinux.cfg目录[root@CentOS7 ~]# mkdir /var/lib/tftpboot/pxelinux.cfg# 复制菜单文件[root@CentOS7 ~]# cp /var/www/html/centos7/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default# 复制启动信息文件，可以编辑[root@CentOS7 ~]# cp /var/www/html/centos7/isolinux/boot.msg /var/lib/tftpboot# 编辑default文件[root@CentOS7 ~]# cat /var/lib/tftpboot/pxelinux.cfg/default default menu.c32timeout 600menu title CentOS Linux PXE Installlabel centos7 menu label ^Auto Install CentOS Linux 7 kernel vmlinuz append initrd=initrd.img inst.repo=http://192.168.101.3/centos7 ks=http://192.168.101.3/ks.cfg net.ifnames=0 biosdevname=0 ksdevice=eth0 测试安装在Vmware中建立一个空的虚拟机，设置开机启动为网卡启动，启动后出现如下图示，表示pxe安装配置成功。ps：我的系统装了两个版本的CentOS批量安装，大家可以参照上述方法自行添加第二、第三系统的配置。下一篇我们讲述cobbler自动化批量部署系统，与kickstart相比，配置稍简单点儿。但是cobbler是kickstart的升级版，所以理解并掌握cobblerl前必须掌握kickstart。]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-29-华为存储虚拟化解决方案]]></title>
    <url>%2F2019%2F05%2F29%2F2019-05-29-%E5%8D%8E%E4%B8%BA%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[在前文《存储虚拟化概述》中，我们提到存储虚拟化与软件定义存储SDS的区别就是其控制平面数据和存储平面数据是一种紧耦合的关系，因此各厂商的存储虚拟化解决方案是一种“私有”方案，不仅不同厂商之间不能兼容，甚至通常不同产品之间有可能不兼容。因此，考虑到电信云NFV的应用场景，以及设备商的分布情况，我们这里主要讨论华为的虚拟化存储解决方案和华为的分布式存储Fusion Storage的情况。 华为存储虚拟化的存储模型华为对存储虚拟化的定义：存储虚拟化是将存储设备抽象为数据存储，虚拟机在数据存储中作为一组文件存储在自己的目录中。而数据存储是逻辑容器，类似于文件系统，它将各个存储设备的特性隐藏起来，并提供一个统一的模型来存储虚拟机文件。同时，存储虚拟化技术可以更好的管理虚拟基础架构的存储资源，使系统大幅提升存储资源利用率和灵活性，提高应用的正常运行时间。 能够封装为数据存储的存储单元包括： SAN存储（包括iSCSI或光纤通道的SAN存储）上划分的LUN； NAS存储上划分的文件系统共享目录； 华为分布式存储FusionStorage上的存储池； 主机的本地硬盘以及主机的本地内存盘； 数据存储可支持文件系统格式： 华为私有的虚拟镜像管理系统（VIMS），为存储虚拟机而优化的高性能文件系统，主机可以将虚拟镜像管理系统数据存储部署在任何基于SCSI的本地或联网存储设备上，包括光纤通道、以太网光纤通道和iSCSI SAN设备。 网络文件系统（NFS），NAS设备上的文件系统。FusionSphere支持NFS V3协议，可以访问位于NFS服务器上制定的NFS磁盘，挂载该磁盘并满足任何存储需求。 EXT4，FusionSphere支持服务器的本地磁盘虚拟化。 从上面华为存储虚拟化的定义可以看出，主要涉及三个概念：存储资源、存储设备和数据存储。华为的这些基本概念定义的比较奇葩，从我个人理解来看是一种反人类的定义，如下图所示。按照正常人的理解，存储设备指的是真实物理存储，存储资源指的是抽象化后逻辑存储，但是它的定义正好是相反的。 华为存储虚拟化模型中，存储概念的定义如下： 存储资源：表示物理存储设备，例如IPSAN、Advanced SAN、NAS等，其中，Advance SAN是不支持虚拟化的。一个存储资源可以包含多个物理存储。主机访问存储资源时，先需要添加存储资源，再选定主机关联存储资源。如下所示： 存储设备：表示存储资源中的管理单元，包括本地磁盘、LUN、 Advanced SAN存储池、Fusion Storage存储池、NAS共享目录等。多个存储设备可以归属在一个存储资源中，且存储设备必须被添加为数据存储才能使用。其中，LUN在使用前需要在存储侧或者交换机侧进行配置，该配置根据不同的厂家会不一样，具体需要参照相关厂家的技术文档。存储设备需要通过主机探测的方式进行扫描来发现：主机需要链接存储资源后才能扫描存储资源所包含的存储设备，而且每个主机都能发现各自的存储设备，也能发现共享的存储设备。如下所示： 数据存储：表示在存储设备上创建的逻辑管理单元，需要创建在指定的存储设备上，且一个存储设备只能创建一个数据存储，数据存储和存储设备一一对应，且数据存储大小依赖于存储设备的大小。数据存储和主机关联，为主机提供资源，数据存储可以关联到多个主机，一个主机也可以使用多个数据存储。数据存储承载了具体的虚拟机业务，例如创建磁盘等。对于SAN存储上的LUN，也可以作为数据存储直接供虚拟机使用，而不再创建虚拟磁盘，此过程称为裸设备映射，目前仅支持部分操作系统的虚拟机使用，用于搭建数据库服务器等对磁盘空间要求较大的场景。如果使用裸设备部署应用集群服务（如 Oracle RAC等），建议不要使用虚拟机的快照、快照恢复功能，快照恢复后，会导致应用集群服务异常。数据存储映射的示意图如下： 比如：裸设备映射方式可以将一个SAN设备【存储资源】分配的一个LUN【存储设备】接入到FusionCompute环境成为一个数据存储【数据存储】，可以在该数据存储上创建运行业务的虚拟机，对外提供服务。 在华为的FusionCompute中配置数据存储时，首先需要完成存储接口的设置，然后可以通过以下流程接入和使用存储资源，如下图所示： 在FusionCompute界面上首先添加存储资源（如：IPSAN等等），并在存储设备上进行主机启动器的配置。 主机关联存储资源后，执行“扫描存储设备”动作，将IPSAN上的LUN扫描到主机。 主机选择存储设备，执行“添加数据存储”动作，并选择“虚拟化”，存储配置完成。 随后，可以在数据存储上可以进行创建卷、创建快照等行为。 所谓存储接口，是指主机与存储设备连接所用的端口。可以将主机上的一个物理网卡，或者多个物理网卡的绑定设置为存储接口。当使用iSCSI存储时，一般使用主机上两个物理网卡与存储设备多个存储网卡相连，组成存储多路径，此时不需要绑定主机存储平面的物理网卡；当使用NAS存储时，为保证可靠性，建议将主机的存储平面网卡以主备模式进行绑定设置为存储接口与NAS设备连接。 同时，在存储接口设置还有个存储多路径的概念，主要指存储设备通过多条链路与主机一个或多个网卡连接，通过存储设备的控制器控制数据流的路径，实现数据流的负荷分担，保证存储设备与主机连接的可靠性。一般情况下，iSCSI存储和光纤通道存储（如IP SAN存储设备、FC SAN存储设备、OceanStor 18000系列存储）均支持存储多路径。而且在华为的解决方案中，存储多路径包含华为多路径与通用多路径两种模式，通用多路径下虚拟机采用裸设备映射的磁盘时，不支持Windows Server操作系统的虚拟机搭建MSCS集群。 华为虚拟化存储的链接FC-SAN：存储区域网络(Storage Area Networks，SAN)是一个用在服务器和存储资源之间的、专用的、高性能的网络体系。 SAN是独立于LAN的服务器后端存储专用网络。 SAN采用可扩展的网络拓扑结构连接服务器和存储设备，每个存储设备不隶属于任何一台服务器，所有的存储设备都可以在全部的网络服务器之间作为对等资源共享。SAN主要利用Fibre Channel protocol（光纤通道协议），通过FC交换机建立起与服务器和存储设备之间的直接连接，因此我们通常也称这种利用FC连接建立起来的SAN为FC-SAN。如下图所示，FC特别适合这项应用，原因在于一方面它可以传输大块数据，另一方面它能够实现较远距离传输。SAN主要应用在对于性能、冗余度和数据的可获得性都有很高的要求高端、企业级存储应用上。 SAN架构中常用的三种协议： FC 协议 (Fibre Channel) ，使用该种协议的SAN架构，称为FC-SAN。 iSCSI 协议 (Internet SCSI)，使用该种协议的SAN架构，称为IP-SAN。 FCoE 协议(Fibre Channel over Ethernet)，FC 协议通常和iSCSI协议用于现代的SAN架构中，而FCoE协议在服务器需要融合SAN和LAN业务时，也是用得越来越多。 IP-SAN：以TCP/IP协议为底层传输协议，采用以太网作为承载介质构建起来的存储区域网络架构。 实现IP-SAN的典型协议是iSCSI，它定义了SCSI指令集在IP中传输的封装方式。IP-SAN把SCSI指令集封装在了TCP/IP上。类似于不管我们是选择哪家快递公司，最终都是把我们想要发送的东西发送至目的地，都是由我们发起寄送请求，快递公司进行响应，差别只在于快递公司不同而已。iSCSI是全新建立在TCP/IP和SCSI指令集的基础上的标准协议，所以其开放性和扩展性更好。如下图所示，IP-SAN具备很好的扩展性、灵活的互通性，并能够突破传输距离的限制，具有明显的成本优势和管理维护容易等特点。 IP-SAN典型组网方式有： 直连：主机与存储之间直接通过以太网卡、TOE卡或iSCSI HBA卡连接，这种组网方式简单、经济，但较多的主机分享存储资源比较困难； 单交换：主机与存储之间由一台以太网交换机，同时主机安装以太网卡或TOE卡或iSCSI HBA卡实现连接。这种组网结构使多台主机能共同分享同一台存储设备，扩展性强，但交换机处是一个故障关注点； 双交换：同一台主机到存储阵列端可由多条路径连接，扩展性强，避免了在以太网交换机处形成单点故障。 IP-SAN是基于IP网络来实现数据块传输的网络存储形态，与传统FC SAN的最大区别在于传输协议和传输介质的不同。目前常见的IP-SAN协议有iSCSI、FCIP、iFCP等，其中iSCSI是发展最快的协议标准，大多时候我们所说的IP-SAN就是指基于iSCSI实现的SAN。 NAS：网络附加存储，是一种将分布、独立的数据进行整合，集中化管理，以便于对不同主机和应用服务器进行访问的技术。如下图所示，NAS和SAN最大的区别就在于NAS有文件操作和管理系统，而SAN却没有这样的系统功能，其功能仅仅停留在文件管理的下一层，即数据管理。 SAN和NAS并不是相互冲突的，是可以共存于一个系统网络中的，但NAS通过一个公共的接口实现空间的管理和资源共享，SAN仅仅是为服务器存储数据提供一个专门的快速后方存储通道。NAS文件共享功能有点儿类似FTP文件服务，但是两者是完全不同的。FTP只能将文件传输到本地的目录之后才能执行，而网络文件系统NAS可以允许直接访问源端的文件，不需要将数据复制到本地再访问。 华为存储虚拟化的原理存储虚拟化技术可以将不同存储设备进行格式化，屏蔽存储设备的能力、接口协议等差异性，将各种存储资源转化为统一管理的数据存储资源，可以用来存储虚拟机磁盘、虚拟机配置信息、快照等信息，使得用户对存储的管理更加同质化。华为的虚拟化存储栈示意图如下所示，其实现存储虚拟化的关键的就是中间的文件系统那一层。 华为虚拟化存储栈中文件系统作用就是提供文件操作接口，屏蔽底层存储设备的差异，并且为虚拟化卷文件提供存放空间。当前FusionCompute所支持的文件系统格式有：VIMS、EXT4、NFS。它们在应用场景，虚拟化特性支持方面的差异如下： 所需存储**设备** 是否创建**文件系统** 是否支持**共享** 是否支持**延迟置零卷** VIMS LUN 是 是 是 EXT4 本地磁盘 是 否 是 NFS 共享目录 否 是 否 其中，NFS不支持创建文件系统是因为NFS本身就具备文件系统功能，而且NFS是以共享目录的方式提供数据存储，如果采用延迟置零卷的分配方式，容易造成数据丢失。EXT4主要用于服务器本地硬盘，如果要支持共享，需要在存储I/O通道上通过锁机制来避免竞争，目前方案不支持。上述的三个文件系统中，VIMS是华为自研的文件系统，其余两个都是标准文件系统类型。 VIMS (Virtual Image Manage System)虚拟镜像管理系统，属于华为自研的一种文件系统，是一种高性能集群文件系统。在华为存储虚拟化解决方案中，它是实现是自动精简置备磁盘、快照、存储迁移等高级特性的技术基础。如下图所示，VIMS使虚拟化技术的应用超出了单个存储系统的限制，其设计、构建和优化针对虚拟服务器环境，可让多个虚拟机共同访问一个整合的集群式存储池，从而显著提高了资源利用率。 VIMS兼容FC SAN、IPSAN、NAS、本地磁盘，支持建立固定空间磁盘、动态空间磁盘、差分磁盘等。主要应用于需要存储迁移、快照、链接克隆等高级存储特性虚拟机。 虚拟磁盘的类型一个虚拟机在虚拟化计算节点文件系统上表现为一个或多个虚拟磁盘文件，也就是说，一个虚拟机磁盘体现为一个或多个虚拟磁盘文件。在华为存储虚拟化解决方案中，虚拟磁盘的格式为VHD镜像格式。它是FusionCompute实现精简卷、快照等功能的基本载体，实现了FusionCompute虚拟机镜像数据的基本储存功能。如下所示，虚拟机的一个虚拟磁盘对应一个VHD文件。 在华为存储虚拟化解决方案中，虚拟机的虚拟磁盘类型分为三种：固定空间磁盘、动态空间磁盘和差分磁盘。 固定空间磁盘：创建时需要将磁盘文件对应的存储块空间全部进行初始化成”0”，创建速度慢，但是IO性能最佳，适用于对IOPS要求较高的场景。如下图所示，磁盘大小恒定，创建后使用空间和预留空间相等。 数据区主要用来存放虚拟机业务数据，未写满的时候内部空间包含大量0，数据冗余度很高。最后一个扇区用来存放磁盘的元数据，也就是虚拟磁盘的大小、块的个数，位于物理存储的位置等数据，固定空间的磁盘主要应用于系统中的普通卷。 动态空间磁盘：创建时只需写头和结束块，创建速度块，IO性能较差，适用于应用于精简磁盘和普通延迟置零磁盘。如下图所示，磁盘大小会随着用户写入数据而增长，但不会随着用户删除数据而缩减，只能通过磁盘空间回收来手动缩减应用在系统中的精简磁盘空间。 前面和后面共m+1个扇区用来存放虚拟磁盘的元数据信息，其余扇区用来存放虚拟机业务数据。当用来建立精简磁盘时，首次创建只建立头和尾共m+1扇区的元数据区，后续随着虚拟机业务数据的增加逐渐增加数据区的大小，因此在一些IOPS要求不高的场景可以节省物理存储空间。动态空间磁盘通过工具可以和固定空间磁盘互相转换，例如，可以用一个精简磁盘的模板部署一个普通磁盘的虚拟机。 差分磁盘：差分磁盘的结构和动态磁盘一模一样，只是文件头中会记录它的父文件路径，因此差分卷不能独立存在，必须能够访问到父文件才能正常工作，如下图所示，主要用于链接克隆场景。 前m个扇区和最后一个扇区存放虚拟磁盘自身的元数据信息，从m+1到k扇区之间存储父文件的元数据信息，数据区存放相对于父文件数据的增量业务数据，所以被称为差分磁盘。差分卷也可以成为父文件，在此场景下，差分卷不仅集成自身父文件的数据，还要向其子卷提供数据，类似祖孙三代中父亲的角色。 差分磁盘的特性和动态盘类似，但是很多业务有限制。差分磁盘以块为单位记录相对于父文件的修改。配合快照、非持久化磁盘、链接克隆等功能被使用，起到保护源盘不被修改，并可以跟踪虚拟机磁盘差异数据的作用。 华为存储虚拟化特性华为存储虚拟化提供基于磁盘的RAID 2.0+、基于虚拟磁盘的精简置备和空间回收、快照、连接克隆、存储热迁移等特性，适用于各类不同业务场景下的虚拟机。 RAID 2.0+我们在了解华为RAID2.0+技术原理之前，可以先了解一下我们的传统RAID技术。我们传统的RAID技术是一种盘级虚拟化的技术，有RAID0 RAID1 RAID5 RAID10等常见的RAID技术，如下图所示，是一个RAID 50的组合。 传统RAID的状态主要有几种：创建RAID 、RAID组正常工作 、RAID组降级、 RAID组失效 。传统RAID技术热备方式主要是通过固定的盘来进行数据的恢复。 在传统的RAID技术中，是将几块小容量廉价的磁盘组合成一个大的逻辑磁盘给大型机使用。后来硬盘的容量不断增大，组建RAID的初衷不再是构建一个大容量的磁盘，而是利用RAID技术实现数据的可靠性和安全性，以及提升存储性能。如下图所示，由于单个容量硬盘都已经较大了，数据硬盘组建的RAID容量更大，然后再把RAID划分成一个一个的LUN映射给服务器使用。 随着硬盘技术的发展，单块硬盘的容量已经达到数T，传统RAID技术在重建的过程中需要的时间越来越长，也增加了在重构过程中其它硬盘再坏掉对数据丢失造成的风险，为了解决这一问题，块虚拟化技术应运而生，将以前以单块硬盘为成员盘的RAID技术再细化，将硬盘划分成若干的小块，再以这些小块为成员盘的方式构建RAID,也就是现在业界所说的RAID2.0+技术。 RAID2.0+是一种块级的虚拟化技术，如下图所示。 它由不同类型的硬盘组成硬盘域，把硬盘域内每个硬盘切分为固定64MB的块（CK），硬盘域内同种类型的硬盘被划分为一个个的Disk Group（DG），从同一个DG上随机选择多个硬盘，每个硬盘选取CK按照RAID算法组成Chunk Group（CKG），CKG被划分为固定大小的Extent，Thick LUN以Extent为单位映射到LUN（上图上半部分），也可以采用Grain在Extent的基础上进行更细粒度的划分，Thin LUN以Grain 为单位映射到LUN（上图下半部分）。 为了进一步说明RAID 2.0+各类逻辑概念以及相互之间的关系，通过下图RAID 2.0+的软件逻辑架构进行说明。 Disk Domain（磁盘域）：一个硬盘域上可以创建多个存储池（Storage Pool）一个硬盘域的硬盘可以选择SSD、SAS、NL-SAS中的一种或者多种，不同硬盘域之间是完全隔离的，包括故障域、性能和存储资源等。 Storage Pool（存储池）&amp; Tier：一个存储池基于指定的一个硬盘域创建，可以从该硬盘域上动态的分配Chunk（CK）资源，并按照每个存储层级（Tier）的“RAID策略”组成Chunk Group（CKG）向应用提供具有RAID保护的存储资源。 Disk Group（DG）：由硬盘域内相同类型的多个硬盘组成的集合，硬盘类型包括SSD、SAS和NL-SAS三种。 LD（逻辑磁盘）：是被存储系统所管理的硬盘，和物理硬盘一一对应。 Chunk（CK）：是存储池内的硬盘空间切分成若干固定大小的物理空间，每块物理空间的大小为64MB，是组成RAID的基本单位。 Chunk Group（CKG）：是由来自于同一个DG内不同硬盘的CK按照RAID算法组成的逻辑存储单元，是存储池从硬盘域上分配资源的最小单位。 Extent：是在CKG基础上划分的固定大小的逻辑存储空间，大小可调，是热点数据统计和迁移的最小单元（数据迁移粒度），也是存储池中申请空间、释放空间的最小单位。 Grain：在Thin LUN模式下，Extent按照固定大小被进一步划分为更细粒度的块，这些块称之为Grain。 Volume &amp; LUN：Volume即卷，是存储系统内部管理对象；LUN是可以直接映射给主机读写的存储单元，是Volume对象的对外体现。 RAID2.0+ 优秀性能主要体现在负载均衡优、数据重构时间短和提升单卷（LUN）的读写性能等几个方面。 1）RAID2.0+的负载均衡更优，如下图所示，数据在存储池中硬盘上的自动均衡分布，避免了硬盘的冷热不均，从而降低了存储系统整体的故障率。 RAID 2.0+的负载均衡方式主要有两种：第一种是根据crush算法，在创建CKG的时候选择CK，保证硬盘被选中的概率与硬盘剩余容量成正比；第二种就是smartmotion，主要用于数据迁移发生在DG层次。当有新盘加入硬盘域的时候，会触发smartmotion，查出待均衡的原CKG，然后给该CKG分配一个目标CKG，该CKG包含来源于新盘的CK，如果原CKG和目标CKG中对应位置的CK落在不同的盘，就会触发实现均衡。那么有数据的原CKG迁移到目标CKG中，没有数据的只需要改变CKG的映射关系即可。 2）RAID2.0+的数据重构时间短，如下图所示，相较传统RAID重构数据流串行写入单一热备盘的方式，RAID2.0+采用多对多的重构，重构数据流并行写入多块磁盘，极大缩短数据重构时间，1TB数据仅需30分钟。 RAID2.0+的数据重构方式主要有三种：第一种是全盘重构，就是当一块盘故障或者被拔出之后进行数据恢复；第二种是局部重构，就是硬盘上出现坏块，通过RAID算法将上面的数据重构到热备CK中；第三种是恢复重构，就是在某块硬盘正常访问期间的写操作无法完成，只能处于降级写状态，会在系统上记录相关的日志并且更新校验值，待硬盘恢复后，将故障期间的数据根据RAID算法计算之后更新数据。 3）提升单卷（LUN）的读写性能，如下图所示，RAID2.0+的热备采用的是空间的形式，每个盘上面的CK都可以作为磁盘热点，这种条块化的分割极大提升单卷（LUN）的读写IOPS。 传统存储的RAID通常是以单个磁盘为粒度来建立RAID，RAID被限制在有限的几个磁盘上，所以当主机对一个较小的卷进行密集访问时，只能访问到有限的几个磁盘，这就造成磁盘访问瓶颈，导致磁盘热点。 而RAID2.0+技术基于Chunk而非物理磁盘构成RAID。一个物理磁盘上的不同CK可以用于构成不同RAID类型的卷。对于HVS阵列而言，即使是很小的卷也可以通过CK的方式分布到很多磁盘上。宽条带化技术使得小的卷不再需要额外的大容量即可获得足够的高性能，且避免了磁盘热点。物理磁盘上剩余的CK还可以用于其它的卷。 在使用RAID 2.0+ 技术时，首先创建硬盘域，指定该硬盘域使用的硬盘类型和每种类型硬盘的数量；另外还要指定针对不同类型的硬盘，使用其上的CK创建CKG时采取的RAID算法，不同类型的硬盘可以使用不同的RAID算法，比如针对故障率高的SATA硬盘采用可靠性更高的RAID 6算法等。接下来创建存储池，一个存储池是基于一个硬盘域的，创建存储池时可以设置该存储池所使用的Extent大小，设置后不能更改。然后在存储池内创建LUN，只需要指定LUN的容量大小即可，系统会根据事先定义的规则（数据分层、自动迁移等）选择适当的Extent完成创建。最后将创建好的LUN映射给需要的主机，主机在使用LUN时RAID 2.0+ 技术就会在后台发挥作用。 由此可以看出，当一块硬盘损坏时，只会影响该硬盘域上的存储池中的CKG，对于其它硬盘域上的CKG无影响，因此可以实现故障隔离。 精简磁盘和空间回收华为存储虚拟化支持创建精简磁盘，可以随着用户使用而自动分配空间。但是，后续膨胀的精简磁盘不会随着用户删除数据而缩小，必须使用空间回收工具可以将用户删除的数据空间释放到数据存储。如下图所示，创建虚拟机可以选择精简磁盘模式，提高磁盘使用率，增加虚拟机部署密度。 精简磁盘可应用于局点运行初期，用户磁盘使用率低的情况。能够降低初始存储投资及维护成本，存储设备只保存有效数据，不保存预留空间，可以提高存储资源利用率。 在FusionCompute中，选择“存储池”。 进入“存储池”页面。在左侧导航树，选择“站点名称 &gt; 数据存储名称”。 显示“入门”页签。单击“磁盘”。 显示磁盘信息列表。单击“创建磁盘”。 弹出“创建磁盘”对话框，如图所示。 配置模式： 普通：根据磁盘容量为磁盘分配空间，在创建过程中会将物理设备上保留的数据置零。这种格式的磁盘性能要优于其他两种磁盘格式，但创建这种格式的磁盘所需的时间可能会比创建其他类型的磁盘长，且预留空间和实际占用空间相等，建议系统盘使用该模式。 精简：该模式下，系统首次仅分配磁盘容量配置值的部分容量，后续根据使用情况，逐步进行分配，直到分配总量达到磁盘容量配置值为止。数据存储类型为“FusionStorage”或“本地内存盘”时，只支持该模式；数据存储类型为“本地硬盘”或“SAN存储”时，不支持该模式。 普通延迟置零：根据磁盘容量为磁盘分配空间，创建时不会擦除物理设备上保留的任何数据，但后续从虚拟机首次执行写操作时会按需要将其置零。创建速度比“普通”模式快；IO性能介于“普通”和“精简”两种模式之间。只有数据存储类型为“虚拟化本地硬盘”、“虚拟化SAN存储”或版本号为V3的“Advanced SAN存储”时，支持该模式。 磁盘模式： 从属：快照中包含该从属磁盘，默认选项。 独立-持久：更改将立即并永久写入磁盘，持久磁盘不受快照影响。即对虚拟机创建快照时，不对该磁盘的数据进行快照。使用快照还原虚拟机时，不对该磁盘的数据进行还原。 独立-非持久：关闭电源或恢复快照后，丢弃对该磁盘的更改。 快照和快照链虚拟机快照记录了虚拟机在某一时间点的内容和状态，通过恢复虚拟机快照使虚拟机多次快速恢复到这一时间点，比如我们初次手动安装OpenStack服务时，每安装一个服务都会创建一个快照，这样当后续出现错误且无法恢复时，可以通过前面快照恢复这一步安装前的状态，而不用从头再来。虚拟机快照包含磁盘内容、虚拟机配置信息、内存数据，多次快照之间保存差量数据，节约存储空间。主要适用于虚拟机用户在执行一些重大、高危操作前，例如系统补丁，升级，破坏性测试前执行快照，可以用于故障时的快速还原。 虚拟机快照的创建、恢复和删除都是由用户手动触发的，系统并不会自动执行。如下图所示，创建快照时会生成一个新的差分卷，创建的方式一般包括COW（写时拷贝）、ROW（写时重定向）和WA（随机写），一般都是写时重定向ROW方式创建。 当创建快照采用了ROW方式时，快照虚拟机会挂载这个差分卷，快照创建后的写操作会进行重定向，所有的写IO都被重定向到新卷中，所有旧数据均保留在只读的源卷中。 当用户对一个虚拟机进行多次快照操作，可以形成快照链，如下图所示，一个虚拟机生成快照的数量不能超过32个，也是快照链的最大长度。 SNAP1是基于源卷的第一次差分卷，SNAP2是基于SNAP2的第二次差分卷，且虚拟机源卷始终挂载在快照链的最末端。用户可以将虚拟机从当前状态恢复到快照链中的某个状态，且快照链中任意一个快照都可以删除而不影响其余快照。 链接克隆链接克隆在桌面云解决方案里面有重要的地位，在电信云NFV领域目前暂时没有应用，因此只需要了解链接克隆特性以及与快照的区别即可。 链接克隆技术是一种通过将源卷和差分卷组合映射为一个链接克隆卷，提供给虚拟机使用的技术。如下图所示，一个链接克隆模板可以创建多个链接克隆差分卷，对应创建多个链接克隆虚拟机。 上图中黄色部分为虚拟机源卷，VM1和VM2两个虚拟机是基于源卷+各自差分卷创建出来的链接克隆虚拟机。链接克隆虚拟机新创建的差分卷初始占用空间很小，随着虚拟机的使用，空间会逐渐膨胀。 与快照相同的是，链接克隆虚拟机的写IO操作也只会更新到差分卷中，且创建数量和创建时间不限。不同的是，链接克隆虚拟机可以和源虚拟机同时运行，且能同时处于同一网络，但是快照与源虚拟机不能同时运行，自然也就不能处于同一网路。而且，快照主要用于记录源虚拟机某一时间的状态，链接克隆虚拟机主要用于同质业务虚拟机多拷贝分发。虚拟机快照可以在源虚拟机运行时创建，但是链接克隆虚拟机必须在源虚拟机关闭时才能创建。 存储热迁移华为存储虚拟化解决方案支持将虚拟机的磁盘从一个数据存储迁移到另一个数据存储。当需要对数据存储空间进行减容时，这时我们需要将源数据存储上的虚拟机磁盘进行迁移，如下如所示，可以将虚拟机的所有磁盘整体迁移，也可以单个磁盘分别迁移。 在迁移虚拟机虚拟磁盘文件时，虚拟机的快照可以一起迁移（但只支持关机状态下冷迁移），且无论虚拟机是开启或者关闭状态，都可以迁移。当虚拟机为关机状态时，这种数据存储间的迁移称为冷迁移，数据存储冷迁移前后性能对比如下： 源存储类型（源配置模式） 目的存储类型 配置模式是否变化 迁移后模式 是否支持带**快照迁移** 虚拟化存储（普通，延迟置零，精简） 虚拟化存储（非NAS） 否 保持不变 是 虚拟化存储（延迟置零） 虚拟化存储（NAS） 是 精简 是 虚拟化存储（延迟置零，精简） 块存储 是 普通 否 虚拟化存储（普通） 块存储 否 普通 否 块存储 虚拟化存储 否 保持不变 否 块存储 块存储 否 保持不变 否 当虚拟机为开机状态时，这种迁移就称为存储热迁移。数据存储冷迁移前后性能对比如下： 源存储类型（源配置模式） 目的存储类型 配置模式是否变化 迁移后模式 块存储 虚拟化存储 是 迁移时可以选择为普通延迟置零（NAS不支持）或者精简 虚拟化存储（普通卷） 虚拟化存储 是 迁移时可以选择为普通延迟置零（NAS不支持）或者精简 虚拟化存储（延迟置零卷） 虚拟化存储（非NAS） 否 保持不变 虚拟化存储（延迟置零卷） 虚拟化存储（NAS） 是 精简 虚拟化存储（精简卷） 虚拟化存储 否 保持不变 当发生存储热迁移时，同时需迁移虚拟机磁盘镜像和系统内存状态，也就说存储热迁移一般和虚拟机热迁移同步进行。存储热迁移的示意图如下所示： 在存储热迁移场景下，华为的解决方案并不完美，仍然受以下条件限制：不支持迁移已挂载为“共享”类型的磁盘和链接克隆虚拟机的磁盘。当虚拟机为“运行中”时，不支持非持久化磁盘、带快照虚拟机磁盘和开启iCache功能虚拟机磁盘的迁移，可将虚拟机关闭后迁移；当虚拟机为“已停止”时，如果目标数据存储为块存储，不支持非持久化磁盘、带快照虚拟机磁盘的迁移。 根据上图，华为存储虚拟化解决方案中存储热迁移的步骤如下： Step1：在目的存储上创建一个与源相同的空镜像文件。 Step2：将目的存储的镜像文件设置为源镜像文件的mirror，使虚拟机的I/O写也能落盘在目的存储上，保证了脏块数据的同步。 Step3：通过迭代迁移的技术，将源镜像的数据迁移到目的镜像中，保证了基线数据的同步。 Step4：在基线数据同步完成后，短暂的时间内暂停虚拟机的I/O请求，将虚拟机的存储文件从源镜像切换到目的镜像上，这样就完成了存储的迁移。 而且，在华为存储虚拟化解决方案中，可以通过界面设置3种不同热迁移速率，应对不同的业务场景： 适中 （迁移速率不高于20M/s，用于存储IO压力较大场景，缓解迁移操作对用户虚拟机的影响） 快速 （迁移速率不高于30M/s，用于存储IO压力正常场景，在保证迁移速度的同时可以适当减少对用户虚拟机的影响） 不限 （迁移速率不高于1024M/s，用于用户虚拟机业务优先级很低的场景）]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-27-5G：看起来很美，任重而道远]]></title>
    <url>%2F2019%2F05%2F27%2F2019-05-27-5G%EF%BC%9A%E7%9C%8B%E8%B5%B7%E6%9D%A5%E5%BE%88%E7%BE%8E%EF%BC%8C%E4%BB%BB%E9%87%8D%E8%80%8C%E9%81%93%E8%BF%9C%2F</url>
    <content type="text"><![CDATA[5G：第五代移动通信技术的简称 ，4G技术的延伸，弥补了4G技术的不足，在吞吐率、时延、连接数量、能耗等方面进一步提升系统性能。 5G致力于应对2020年后多样化差异化业务的巨大挑战，满足超高速率、超低时延、高速移动、高能效和超高流量与连接数密度等多维能力指标。 5G的总体愿景5G网络就是第五代移动通信网络。 5G将通信的作用从人与人之间的连接扩展到各行各业、万事万物之间的互相连接，形成崭新的数字化社会、物联网世界新格局。5G将渗透到未来社会的各个领域，以用户为中心构建全方位的信息生态系统。5G将使信息突破时空限制，提供极佳的交互体验，为用户带来身临其境的信息盛宴；5G将拉近万物的距离，通过无缝融合的方式，便捷地实现人与万物的智能互联。5G将为用户提供光纤般的接入速率，“零”时延的使用体验，千亿设备的连接能力，超高流量密度、超高连接数密度和超高移动性等多场景的一致服务，业务及用户感知的智能优化，同时将为网络带来超百倍的能效提升和超百倍的比特成本降低，最终实现“信息随心至，万物触手及”的总体愿景。 在最近几年里，普通人了解到 的5G特征往往是速度快。 当前，4G LTE网络服务传输速率实际仅为75Mbps。 而在5G网络中，最早三星电子利用64个天线单元的自适应阵列传输技术，成功地在28GHz波段下达到1Gbps的传输速率，实现了新的突破。 未来5G网络的传输速率可达到10Gbps，这意味着手机用户在不到1s时间内即可下载一部高清电影。 在通信业内人士眼里，5G网络的主要目标是让终端用户始终处于联网状态。5G网络将来支持的设备远远不止是智能手机和平板电脑，它还要承载个人智能通信工具、可穿戴设备等。5G网络将是4G网络的颠覆性升级版，它的基本要求并不仅仅体现在无线网络上，还有为实现5G功能而搭建的核心网、承载网以及接入网。 5G概念提出的背景移动互联网和物联网是推进未来移动通信网络发展的两大驱动力。移动互联网颠覆了传统移动通信业务模式，为用户提供前所未有的、多样化的使用体验，深刻影响着人们工作生活的方方面面。物联网扩展了移动通信的服务范围，从人与人通信延伸到物与物、人与物智能互联，使移动通信技术渗透至更加广阔的行业和领域。2014年到2019年全球IMT流量将进一步快速增长，总的流量上涨倍数达到几十到100倍。中国国内由国家技术创新委员会、工信部和发改委组织的IMT-2020（5G推进组）于2014年发布的《 5G愿景与需求白皮书》中预计，2010~2020年全球移动数据流量增长将超过200倍，2010~2030年 将增长近20000倍。中国的移动数据流量增速高于全球平均水平，预计2010~2020年将增长300倍上，2010~2030年将增长超过40000倍。发达城市及热点地区的移动数据流量增速更快，2010~2020 年上海移动数据流量的增长率可达原来的600倍，北京热点区域移动数据流量的增长率可达原来的1000 倍。 另外，我国“互联网+”国家战略需求中明确指出：未来电信基础设施和信息服务要在国民经济中下沉，满足农业、医疗、金融、交通、流通、制造、教育、生活服务、公共服务、教育和能源等垂直行业的信息化需求，改变传统行业，促生跨界创新。因此，未来5G网络不仅需要继续面对移动互联网业务带来的挑战，例如：频谱效率和用户体验速率的提升，时延的减少，移动性的增强等，同时还需要满足物联网多样化的业务需求。 从信息交互对象不同的角度出发，目前5G应用分为三大类场景：增强移动宽带（eMBB）、海量机器类通信（mMTC）和超可靠低时延通信（uRLLC）。eMBB 场景是指在现有移动宽带业务场景的基础上， 对于用户体验等性能的进一步提升，主要还是追求人与人之间极致的通信体验。mMTC 和 uRLLC都是物联网的应用场景，但各自侧重点不同。mMTC 主要是人与物之间的信息交互，而 uRLLC 主要体现物与物之间的通信需求。未来全球移动通信网络连接的设备总量将达到千亿规模。预计到2020年，全球移动终端（不含物联网设备）数量将超过100 亿，其中中国将超过20亿。全球物联网设备连接数也将快速增长， 2020年将接近全球人口规模达到70亿，其中中国将接近15亿。 5G的三大业务模式为了更好的面向数字化的世界，服务数字化的社会，全球范围内的运营商都在进行数字化转型。运营商数字化转型的目标在于为企业客户、消费者提供 ROADS (Real- time, On-Demand, All online, DIY, Social) 体验，这需要通过端到端协同整体架构才能够实现，需要在各个环节都实现敏捷，自动化和智能化。运营商的网络、运营系统、业务的全面云化是必要条件和实现手段。 5G 时代将以一张物理的基础网络支撑多种不同的商业需求，云化的端到端网络架构通过以下几个方面实现上述需求： 在同一套物理基础设上基于不同的业务需求生成逻辑隔离的独立运行的网络切片，通过基于数据中心的云化架构支撑多种应用场景。 利用CloudRAN对无线接入网络进行重构，满足5G时代多技术连接以及RAN功能按需部署的需求。 通过控制面和用户面（C/U）分离，功能模块化以及统一的数据库管理技术简化核心网络架构，实现网络功能的按需配置。 基于应用驱动来自动的生成，维护，终止网络切片服务， 利用敏捷的网络运维降低运营商的运营成本。 5G时代新的通信需求对现有网络提出了包括技术上的，商业模式上的种种挑战，需要下一代移动网络来满足。ITU将5G时代的主要移动网络业务划分为三 类：eMBB（Enhanced Mobile Broadband）, uRLLC(Ultra-reliable and Low-latency Communications) 以 及 mMTC（Massive Machine Type Communications）。 eMBB 聚焦对带宽有极高需求的业务，例如高清视频，虚拟现实/增强现实等等，满足人们对于数字化生活的需求。比如：移动的环境中，网联无人机对大带宽、低时延的需求，将会引爆众多高价值创新行业应用。初期可以采用4G拓展应用，后续随着5G引入业务体验更好、创新应用更多。 uRLLC 聚焦对时延极其敏感的业务，例如自动驾驶/辅助驾驶，远程控制等，满足人们对于数字化工业的需求。比如：5G技术可满足车联网低时延、高速、高可靠性的业务需求，可以重新定义汽车安全，促使车联网创新应用成为现实。 mMTC 则覆盖对于联接密度要求较高的场景，例如智慧城市，智能农业，满足人们对于数字化社会的需求。比如：奠基国家工业互联网，助力中国制造2025，采用授权频率，使用4G/5G及有线网络等技术，基于运营商的泛在网络，为工业互联网工厂内外数据应用提供连接服务，主要包含采集类、控制类、监测类等应用。 5G的技术定义5G（第五代移动通信）是IMT（国际移动通信）的下一阶段，ITU（国际电信联盟）将其正式命名为IMT-2020。目前，ITU正在对IMT-2020进行初步的规划。此外，端到端系统的大多数其他变革（既包括核心网络内的，又包括无线接入网络内的）也将会成为未来5G系统的一部分。在移动通信市场中，IMT-Advanced（包括LTE-Advanced与WMAN-Advanced）系统之后的系统即为“5G”。 在大力研发5G潜在“候选技术”的同时，全球移动通信行业对于5G技术研发驱动的理解也逐步达成了共识。ITU-R（国际电信联盟无线电通信局）确定未来的5G具有以下三大主要的应用场景：1）增强型移动宽带；2）超高可靠与低延迟的通信；3）大规模机器类通信。具体包括吉比特每秒移动宽带数据接入、智慧家庭、智能建筑、语音通话、智慧城市、三维立体视频、超高清晰度视频、云工作、云娱乐、增强现实、行业自动化、紧急任务应用、自动驾驶汽车等。 一项技术创新可以分为渐进式创新、模块创新、架构创新和彻底创新4类。从2G到4G是频谱效率和安全性等逐步提升的渐进式创新，也是在维持集中式网络构架下的模块式创新，还有从网络构架向扁平化和分离化演进的架构创新。但到了5G时代，除了网络能力以外，还必须面向各种新的行业服务，提供随时需要的、高质量的连接服务，这也要求5G网络的建设是多方位的、彻底的创新。 移动网络架构架主要包括核心网和无线接入网，到了5G时代，移动网络按循序渐进的方式引入5G网元设备。 Step1：5G NR（新无线）先行， 5G基站（gNodeB）与4G基站（eNodeB）以双连接的方式共同接入4G核心 网。 Step2：5G基站独立接入5G核心网（NGCN，下一代核心网）。 Step3：5G基站和4G基站统统接入5G核心网，4G核心网退出历史舞台。 以上5G网络架构演进看似整体一致，实际上，我们把核心网和无线接入网分开来看，其内部架构发生了颠覆性的改变。核心网的网元由4G时代的MME/SAE-GW变为5G时代的AMF/UPF（AMF/UPF是由中国移动牵头提出的SBA 5G核心网基础架构）。 AMF（Mobility Management Function）负责控制面的移动性和接入管理，代替了MME的功能。 UPF（User Plane Function） 负责用户面，它代替了原来4G中执行路由和转发功能的SGW和PGW。 另外一个概念是5G系统服务架构，这是一个基于云原生设计原则的架构，不仅要对传统4G核心网网元NFV虚拟化，网络功能还将进一步软件模块化，实现从驻留于云到充分利用云的跨越，以实现未来以软件化、模块化的方式灵活、快速地组装和部署业务应用。 为了灵活应对智慧城市、车联网、物联网等多样化的服务，使能网络切片，核心网基于云原生构架设计，面临毫秒级时延、海量数据存储与计算等挑战，云化的C-RAN构架和实时的移动边缘计算（MEC）应运而生。 从核心网到接入网，未来5G网络将分布式部署巨量的计算和存储于云基础设施之中，核心数据中心和分布式云数据中心构成网络拓扑的关键节点。这是一场由海量数据引发的从量变到质变的数据革命，是一场由技术创新去推动社会进步的革命，因此，5G需广泛地与各行业深入合作，共同激发创新，从而持续为社会创造价值。 无线接入网发生的主要改变是分离，首先是控制面和用户面的分离，其次是基站被分离为AAU、DU和CU这3个部分。 5G无线关键技术有微基站（Small Cell）和Massive MIMO。 5G的容量需求是4G的1000倍，峰值速率10～20Gbps，要提升容量和速率无非就是增大频谱带宽、提升频谱效率和增加小区数量“三板斧”操作。 首先是频谱带宽，高频段的频率资源丰富，同时，目前小于3GHz的低频段基本被2G/3G/4G占用（中国移动的5G目前规划为2.6G整频段，电联需要退出目前4G的占用），所以，5G必然要向高频段3.5～30GHz（甚至更高）扩展。那么如何解决频段越高，穿透能力越差，覆盖范围越小的问题就引出了 5G的两大关键技术—Massive MIMO和微基站。 微基站已成为未来解决网络覆盖和容量的关键。未来城市路灯、广告牌、电杆等各种街道设施都将成为微基站挂靠的地方。Massive MIMO就是在基站侧配置远多于现有系统的大规模天线阵列的MU-MIMO，来同时服务多个用户。它可以大幅提升无线频谱效率，增强网络覆盖和系统容量，简言之， 就是通过分集技术提升传输可靠性、空间复用提升数据速率、波束赋形提 覆盖范围。 MU-MIMO将多个终端联合起来空间复用，同时使用多个终端的天线，这样一来，大量的基站天线和终端天线形成一个大规模的、虚拟的MIMO信道系统。这是从整个网络的角度更宏观地去思考提升系统容量。 波束赋形是指大规模多天线系统可以控制每一个天线单元的发射（或接收）信号的相位和信号幅度，产生具有指向性的波束，消除来自四面八方的干扰，增强波束方向的信号。它可以补偿无线传播损耗。 国家政策驱动下，“企业上云”的进程将进一步加快。云业务的发展，对网络需求自内生而建，使两者从独立走向融合。5G超高速上网和万物互联将产生呈指数级上升的海量数据，这些数据需要云存储和云计算，并通过大数据分析和人工智能产出价值。与此同时，为了面向未来多样化和差异化的5G服务，一场基于虚拟化、云化的ICT融合技术革命正在推动着网络重构与转型。 引入新的组件： 编排器和网络控制器，地市城域网实现互连、地市PTN网络实现互连、新增云专线网关，最终实现“云+专线” 融合业务的自动化发放。 关于5G的标准ITU和3GPP5G最重要的标准化组织有ITU和3GPP。其中，ITU是联合国负责国际电信事务的专业机构，其下分为电信标准化部门（ITU-T）、无线电通信部门（ITU-R）和电信发展部门（ITU-D），每个部门下设多个研究组，每个研究组下设多个工作组，5G的相关标准化工作是在ITU-R WPSD 下进行的。ITU-R WPSD是专门研究和制订移动通信标准IMT（包括IMT-2000和IMT-Advanced）的组织，根据ITU的工作流程，每一代移动通信技术国际标准的制订过程包括业务需求、频率规划和技术方案3个部分，当前对5G的时间表已经确定了3个阶段： 第一个阶段截至2015年底，完成IMT-2020国际标准前期研究，重点是完成5G宏观描述，包括5G的愿景、5G的技术趋势和ITU的相关决议，并在2015年世界无线电大会上获得必要的频率资源； 第二个阶段是2016~2017年底，主要完成5G性能需求、评估方法研究等内容； 第三个阶段是收集5G的候选方案。 而3GPP是一个产业联盟，其目标是根据ITU的相关需求，制订更加详细的技术规范与产业标准，规范产业行为。3GPP（the 3rd Generation Partnership Project）是领先的3G技术规范机构，是由欧洲的ETSI、日本的ARIB和TTC、韩国的TTA、美国的T1和中国的无线通信组CWTS共6个标准化组织伙伴组成。3GPP的会员包括组织伙伴、市场代表伙伴和个体会员3类。3GPP市场代表伙伴不是官方的标准化组织，它们是向3GPP提供市场建议和统一意见的机构组织。 5G的几个3GPP阶段性标准根据3GPP此前公布的5G网络标准制订过程，5G整个网络标准分几个阶段完成，如下图所示。 2017年12月21日，在国际电信标准组织3GPP RAN第78次全体会议上，5G NR（New Radio）首发版本正式发布，这是全球第一个可商用部署的5G标准。 非独立组网的NSA 5G标准被冻结，但这只是一种过渡方案，仍然依托4G基站和网络，只是空口采用5G，算不上真正的5G标准。 非独立组网标准的确立，可以让一些运营商在已有的4G网络上进行改造，在不进行大规模设备替换的前提下，将移动 网速提升到5G网络，即1Gbps速率。 R15阶段重点满足增强移动宽带（eMBB）和低时延高可靠（uRLLC）应用需求，该阶段又分为两个子阶段：第一个子阶段，5G NR非独立组网特性已于2017年12月完成，2018年3月冻结；第二个子阶段，5G NR独立组网标准于2018年6月14日冻结。2018年6月，已经完成了5G独立组网（SA）标准，支持增强移动宽带和低时延高可靠物联网，完成了网络接口协议定义。现在的R15 5G标准只能算是第一阶段，重点满足增强移动宽带（eMBB）和低时延高可靠（uRLLC）应用需求，可用于设计制造专业5G设备以及网络建设，单独建立一张全新的5G网络，可以满足超高视频、VR直播等对移动带宽需求大的业务，而无人驾驶、工业自动化等需要高可靠连接的业务也有了网络保证。 5G第二个标准版本R16计划于2019年12月完成，2020年3月冻结，全面满足eMBB、uRLLC、大连接低功耗场景mMTC 等各种场景的需求。可以说，预计2020年3月形成的5G标准才是完整的5G标准。5G技术标准由3GPP确定之后，还需要经过ITU认定。 解读3GPP R152018年6 月14日， 3GPP 全会批准了首个5G独立组 网（SA）标准，这意味着3GPP首个完整的5G标准R15正式落地，5G产业链进入商用阶段。3GPP正式最终确定5G第二阶段标准（R16）的15个研究方向。 序号 研究方向 研究内容 1 MIMO的进一步演进 多用户MU-MIMO、Mutil-TRP和波束管理增强。 2 52.6GHz以上的新空口 将对5G系统使用 52. 6GHz 以上的频谱资源进行研究。 3 5G NR双链接完善 新增异步NR-NR双链接方案研究。 4 无线接入/无线回传一体化 3GPP将在R16阶段继续研究并考虑无线接入/无线回传一体化设计。 5 工业物联网 5G第二阶段标准（R16）将进一步研究URLLC（超高可靠与低时延信）增强来满足诸如“工业制造”“电力控制” 等更多的5G工业物联网应用场景。 6 5G新空口移动性增强 包括提高移动过程的可靠性、缩短由移动导致的中断时间。 7 基于5G新空口的V2X 研究基于5G新空口的V2X技术，使得其满足由SA1定义的“高级自动驾驶”应用场景，与LTE V2X形成“互补”。 8 5G新空口的新型定位方式 研究更精确的定位技术，包括“RAT-dependent” 以及混合定位技术。 9 非正交多址接入NOMA 面向5G的NOMA有多种候选技术。而R16将研究潜在的技术方案并完成标准化工作。 10 5G NR-U 在5G第二阶段标准（R16）中，5G NR-U需可利用非授权频谱提升5G系统容量。 11 非地面5G网络 研究面向“非地面5G网络”的物理层的控制机制、随机接入和HARQ切换、系统架构等。 12 远程干扰管理+交叉链路干扰抑制 5G第二阶段标准（R16）将研究如何识别造成强干扰的远端5G基站，以及如何进行干扰抑制。 13 5G终端能力 5G第二阶段标准（R16）将研究5G终端上报“终端能力”并降低5G终端上报信令开销的方法。 14 5G新空口以无线接入网为中心的数据收集与利用 5G第二阶段标准（R16）将研究SON、MDT等技术。 15 5G新空口终端功耗 5G第二阶段标准（R16）将研究5G终端工作在“CONNECTED”模式下如何降低功耗。 2018年6月发布的SA标准完成了5G核心网架构，实现了5G独立组网。此次独立组网标准的冻结，让5G确定了全新的网络架构和核心网组网方式，让网络向IT化、互联网化、极简化、服务化转变。 在IT化方面，全软件化的核心网实现了统一的IT基础设施和调度。功能软件化、计算和数据分离是代表性的技术。传统“网元”重构为5G的“网络功能”，以“软件”的形式部署，充分发挥云化、虚拟化技术的 优势。将处理逻辑和数据存储分离，更便于提升系统的可靠性、动态性、大数据分析的能力。 在互联网化方面，从固定网元、固定连接的刚性网络到动态调整的柔性网络。服务化架构（SBA，Service- based Architecture）、新一代核心网协议体系（基于HTTP2. 0/JSON）是其代表性技术。SBA的设计是由模块化、可独立管理的“服务”来构建的。服务可灵活调用、灰度发布，实现网络能力的按需编排和快速升级。传统电信特有的接口协议代之以互联网化的API调用，使得5G网络更加开放、灵活。 在极简化方面，极简的转发面提高性能，集中灵活的控制面提升效率。C/U分离（控制面和用户面离）、新型移动性及会话管理是其代表性技术。通过C/U分离，一方面实现控制面集中部署、集中管控、集中优化，另一方面实现用户面功能简化，实现高效、低成本、大流量的数据转发。移动性管理和会话管理解耦，使得终端可以按需建立会话连接，节省了网络地址和存储资源。同时，针对不同的终端类型定义了多种类型的移动性管理，简化了终端和网络的状态。 在服务化方面，从通用化服务到个性化、定制化服务。网络切片、边缘计算是其代表性技术。网络切片提供定制化、逻辑隔离、专用的端到端虚拟移动络（包括接入网、核心网），是5G面向垂直行业、实现服务可保障的基本技术形式。而边缘计算将网络的功能应用靠近用户部署，使得极致的低时延、本地特色应用成为可能，是5G满足如智能工厂等垂直行业业务需求的重要基础。 同时，在无线侧，5G NR为设计、架构、频段、天线4个方面带来新变化。 在设计上，与以往通信系统不同，通信行业和垂直行业的跨界融合是5G发展的关键之一。为满足垂直行业的各种差异性需求，并应对部署场景的多样性与复杂性，5G在帧结构等方面提出了全新的设计。与4G相比，5G提供了更多可选择的帧结构参数，可根据5G基础通信业务、物联网和车联网等多样化应用场景，以及宏基站、小基站等不同网络部署需求灵活地配置，通过“软件定义空口”的设计理念使无线信号“量体裁衣”，通过同一个空口技术来满足5G多样化的业务需求，大幅提升5G网络部署的效率。 在架构上，为了使组网方式更加灵活并提升网络效率，5G引入了接入网CU/DU分离的无线接入网架构，可将基站的功能分成实时处理的DU部分和非实时处理的CU部分，从而使得中心单元CU可以部署到集中的物理平台，以承载更多的小区和用户，提升了小区间协作和切换的效率。 在频段上，5G系统需要不同频段来共同满足其覆盖、容量、连接数密度等关键性能指标要求。因此，与4G不同的是，5G通过灵活的参数设计（子载波间隔和CP长度等），可支持更大范围的频率部署，包括6GHz以下以及6GHz以上的毫米波频段。其中，6GHz以下频段主要用于实现5G系统的连续广域覆盖，保证高移动性场景下的用户体验以及海量设备的连接；而6GHz以上频段能够提供连续较大宽，可满足城市热点、郊区热点与室内场景极高的用户体验速率和极高容量需求。 在天线上，5G支持大规模天线大幅度提升系统效率。大规模天线实现三维的波束赋形，形成能量更集中、覆盖更立体、方向更精准的波束。在大规模天线的架构下，波束扫描与波束管理等多个5G先进技术成为可能，网络覆盖及用户体验的顽健性可得到进一步的提升，实现更好的控制信道和业务信道的覆盖平衡。 我国提出的5G目标随着IMT-2020（5G）推进组发布5G试验第三阶段规范，5G预商用开始进入倒计时。此前在2017年12月1日，3GPP的5G第一个标准冻结，打响了全球5G市场竞赛的发令枪。在全球产业链的共同推动下，5G商用时间点不断被提前。 我们移动通信领域在经历了“2G追赶，3G突破，4G并进”的进阶之后，在即将到来的5G时代，我国通信业正在酝酿一出更加精彩的大戏—“5G引领”，这是中国移动通信产业提出的新目标。要实现5G引领，从2017年年底到今年年初各方面的一系列动作看，我国正在政策引导、频率规划和技术创新等多方面协同发力。 当前，全球多国正积极筹备5G试商用。日前，美国运营商AT＆T已经明确宣布：“2018年将会在十余个美国城市首先推出5G服务，其部署的5G将是3GPP不久前刚刚批准的5G标准。”亚洲其他国家也已经宣布5G商用时间表，其中，韩国将于2018年平昌冬奥会期间实现5G预商用，而日本预计将于2020年为东京奥运会提供5G商用服务。 目前，国内布局5G的步伐还在不断加快，中国5G第三阶段试验大幕已经拉开。第三阶段的重点是面向5G商用前的产品研发、验证和产业协同，开展商用前的设备单站、组网、互操作，及系统、芯片、仪表等产业链上下游的互联互通测试，全面推进产业链主要环节基本达到预商用水平。目前，根据国内三大运营商规划，2018年已经开始陆续在主要城市进行5G试验，2019年则进行规模试商用，2020年正式开始商用部署。中国移动前期预计在若干城市建设每城20个基站的预商用试验网，中国电信表示将在2018年之前完成原型无线组网的验证阶段，目前在广东深圳、成都、兰州、江苏苏州、上海、河北雄安六地启动中国电信5G示范网试验。中国联通目前正在加快推进相关研究工作，计划2018年在多个城市启动5G外场试验工作，2019年进一步扩大试验规模。 作为5G发展的基础性资源，频谱对5G商用进展有至关重要的作用。2017年11月，我国率先发布了5G系统在中频段频谱使用规划，明确将3300-3400MHz（原则上限室内使用）、3400-3600MHz和4800-5000MHz频段作为5G系统的工作频段。与之前2G、3G、4G相比，5G具备远超以往的带宽、更高的速率，且同时支持千亿级物联网设备的连接，5G所需频谱数量也远超之前几代移动通信之和。与此同时，为了实现移动宽带、低时延、超大规模组网三大应用场景，5G系统在规划之初就确定了“全频段”，需要从高频、中频、低频统筹规划。 在低频段大多为现有2G、3G和4G占用的情况下，在中频段上，3.5GHz频段因为有利于信号覆盖，被全球多个国家视为5G网络的先锋频段。目前，我国已为IMT分配522MHz，低频段频谱需求808-1078MHz，频谱缺口300-500MHz。 在中国IMT-2020（5G）推进组的领导下，以中国移动为代表的中国企业发挥了重要的作用，贡献的文稿数占整个项目文稿数的半壁江山。5G系统架构（5GS）项目由中国移动担任报告人主导完成，并得到全球超过67家合作伙伴的大力支持，是中国人首次牵头设计新一代移动网络的系统架构。在全球运营商中，中国移动的文稿贡献数和通过数都排在第一位。 还有，华为在5G核心技术上作出了与其市场体量相匹配的创新贡献，在5G编码技术、多址技术、空口技术、天线技术、网络架构、物联网接入、用户体验保证上都有原创型技术，这些原创技术在3GPP前一阶段的5G关键技术“选美”中获选，随着标准的冻结，固化为国际标准。 中兴近年来也一直致力于对包括Massive MIMO、MUSA（多用户共享接入）、FB-OFDM（滤波器组OFDM）、虚拟和网络分片等在内的核心5G技术进行研发，并携手产业链合作伙伴，共同推动5G研发进程。 从2G到5G，中国实现了从追赶走向引领，在通信领域前所未有地接近世界大格局的中央。5G引领，符合国家“强国战略，中华民族伟大复兴”的战略目标，需要政府、运营商、设备商以及社会各行各业发生共振效应，共同努力才能实现。整个过程任重而道远，需要我们不忘初心，砥砺前行。]]></content>
      <categories>
        <category>5G解决方案</category>
      </categories>
      <tags>
        <tag>5G</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-27-存储虚拟化概述]]></title>
    <url>%2F2019%2F05%2F27%2F2019-05-27-%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[存储虚拟化概述存储虚拟化（StorageVirtualization）最通俗的理解就是对存储硬件资源进行抽象化表现。这种虚拟化可以将用户与存储资源中大量的物理特性隔绝开来，就好像我们去仓库存放或者提取物品一样，只要跟仓库管理员打交道，而不必关心我们的物品究竟存放在仓库内的哪一个角落。对于用户来说，虚拟化的存储资源就像是一个巨大的“存储池”，用户不会看到具体的存储磁盘、磁带，也不必关心自己的数据经过哪一条路径通往哪一个具体的存储设备。 存储虚拟化减少了物理存储设备的配置和管理任务，同时还能够充分利用现有的存储资源。存储虚拟化的方式是将整个云系统的存储资源进行统一整合管理，为用户提供一个统一的存储空间。如下图所示： 传统存储面临的挑战传统的数据中心里，存储的类型大致可分为以下几种：服务器内置磁盘、直接附加存储、存储区域网络及网络附加存储。 服务器内置磁盘包括SCSI、SATA及IDE磁盘等，这些磁盘可能直接由操作系统管理，也可能通阵列卡等RAID管理器进行配置使用（常见的有RAID 1、RAID 10、RAID 5、RAID 6等）。内置磁盘作为最简单直接的存储方式，在目前数据中心里仍然到处可见。服务器磁盘RAID阵列数据组织模式如下： 直接附加存储（Directed Attached Storage，DAS）作为一种最简单的外接存储方式，通过数据线接连接在各种服务器或客户端扩展接口上。它本身是硬件的堆叠，不带有任何存储操作系统，因而也不能独立于服务器对外提供存储服务。DAS常见的形式是外置磁盘阵列，通常的配置就是RAID控制器+一堆磁盘。DAS安装方便、成本较低的特性使其特别适合于对存储容量要求不高、服务器数量较少的中小型数据中心。如下图所示： 存储区域网络（Storage Area Network，SAN）是一种高速的存储专用网络，通过专用的网络交换术连接数据中心里的所有存储设备和服务器。在这样的存储网络中，存储设备与服务器是多对多的服务关系：一台存储设备可以为多台服务器同时提供服务，一台服务器也可以同时使用来自多台存储设备的存储服务。不同于DAS，SAN中的存储设备通常配备智能管理系统，能够独立对外提供存储服务。SAN存储网络系统结构图如下： 典型的 SAN利用光纤通道（Fiber Channel，FC）技术连接节点，并使用光纤通道交换机（FC Switch）提供网络交换。不同于通用的数据网络，存储区域网络中的数据传输基于FC协议栈。在FC协议栈之上运行的SCSI协议提供存储访问服务。与之相对的iSCSI存储协议，则提供了一种低成本的替代方式 即将SCSI协议运行于TCP/IP协议栈之上。为了区别这两种存储区域网络，前者通常称为FC SAN，后者称为IP SAN。由于SAN存储采用网络架构和光纤传输，使它具有易部署、扩展强、传输快等优势，传输速率可达8~16Gbps。数据中心常见的SAN交换机如下图所示： 网络附加存储（Network Attached Storage，NAS）提供了另一种独立于服务器的存储设备访问方（相对于内置存储与DAS）。类似于SAN，NAS也是通过网络交换的方式连接不同的存储设备与服务器。同样，存储设备与服务器之间也一种多对多的服务关系。NAS服务器通常也具有智能管理系统，能够独立对外提供服务。与SAN不同的是，NAS基于现有的企业网络（即TCP/IP网络），不需要额外搭建昂贵的专用存储网络（FC）。此外NAS通过文件I/O的方式提供存储，这点也不同于SAN的块 I/O访问方式。NAS存储的架构示意图如下： 常见的NAS访问协议有NFS（Network File System）和CIFS（Common Internet File System），因此允许多台服务器以共享的方式访问同一个数据存储单元LUN。而且，由于NAS内部嵌入了一个精简的专门用于存储的操作系统，集成了网络传输和I/O访问等协议，因此NAS存储独立于用户操作系统，可在线扩展，且部署容易，管理成本低（相对SAN存储）。实际中的常见的NAS产品，比如群晖NAS产品实物图如下： 随着云计算与软件定义数据中心的出现，对存储管理有了更高的要求，传统存储也面临着诸多前所未有的挑战： 对于服务器内置磁盘和DAS来说，单一磁盘或阵列的容量与性能都是有限的，而且也很难对其进行扩展。另外这两种存储方式也缺乏各种数据服务，例如数据保护、高可用性、数据去重等。最大的麻烦在于这样的存储使用方式导致了一个个的信息孤岛，这对于数据中心的统一管理来说无疑是一个噩梦。 对于SAN和NAS来说，目前的解决方案首先存在一个厂商绑定的问题。与服务器的标准化趋势不同，存储产品的操作系统（或管理系统）仍然是封闭的。不仅不同厂商之间的系统互不兼容，而且一家厂商的不同产品系列之间也不具有互操作性，这必然导致高价和技术壁垒。此外，管理孤岛的问题依旧存在，相对于DAS来说只是岛大一点，数量少一点而已。最后，SAN与NAS的扩展性也仍然是个问题。 另外，一些全新的需求，比如：对多租户（Multi-Tenancy）模式一致性支持、云弹性（Cloud-Scale）的动态迁移服务支持、动态定制的数据服务（Data Service）以及直接服务虚拟网络的应用等。这些需求并不是通过对传统存储架构的简单修修补补就可以满足的。 存储虚拟化的定义为了解决上述挑战，存储虚拟化和软件定义存储SDS的概念日趋火热，但是需要注意一点—存储虚拟化并不等于软件定义存储SDS，准确点讲存储虚拟化是软件定义存储的一个具体实现。存储虚拟化的本质是存储整合的一个重要组成部分，它能减少管理问题，而且能够提高存储利用率，这样可以降低新增存储的费用。权威机构SNIA（存储网络工业协会）给出的定义为：通过将存储系统/子系统的内部功能从应用程序、计算服务器、网络资源中进行抽象、隐藏或隔离，实现独立于应用程序、网络的存储与数据管理。 总结起来就是：存储虚拟化技术将底层存储设备进行抽象化统一管理，向服务器层屏蔽存储设备硬件的特殊性，而只保留其统一的逻辑特性，从而实现了存储系统的集中、统一、方便的管理。 与传统存储相比，虚拟化存储的优点主要体现在： 磁盘利用率高，传统存储技术的磁盘利用率一般只有30－70%，而采用虚拟化技术后的磁盘利用率高达70－90%； 存储灵活，可以适应不同厂商、不同类别的异构存储平台，为存储资源管理提供了更好的灵活性； 管理方便，提供了一个大容量存储系统集中管理的手段，避免了由于存储设备扩充所带来的管理方面的麻烦； 性能更好，虚拟化存储系统可以很好地进行负载均衡，把每一次数据访问所需的带宽合理地分配到各个存储模块上，提高了系统的整体访问带宽。 如下图所示为华为存储虚拟化的解决方案架构图。 在华为的存储虚拟化解决方案中，有三个概念，分别是：存储资源、存储设备和数据存储。 存储资源：表示物理存储设备，例如IPSAN、Advanced SAN、NAS等。 存储设备：表示存储资源中的管理单元，类似LUN、 Advanced SAN存储池、NAS共享目录等。 数据存储：表示虚拟化平台中可管理、操作的存储逻辑单元。 各类存储资源的特性对比如下，其中，存储卸载是指将部分存储操作（模板部署、删除清零等操作）下移到存储侧进行，这样做可以不浪费主机侧资源，同时也可以提升操作效率 存储资源类型 底层协议 存储设备类型 是否支持虚拟化 是否支持存储卸载 IPSAN TCP/IP LUN 是 否 FCSAN 光纤 LUN 是 否 NAS TCP/IP 共享目录 是 否 本地磁盘 本地连接 本地磁盘 是 否 AdvancedSAN TCP/IP 存储池 否 是 FusionStorage TCP/IP 存储池 是 是 华为的虚拟化存储栈全景图如下，可以将不同存储设备进行格式化，屏蔽存储设备的能力、接口协议等差异性，将各种存储资源转化为统一管理的数据存储资源，可以用来存储虚拟机磁盘、虚拟机配置信息、快照等信息，使得用户对存储的管理更加同质化。 存储虚拟化的技术实现分类虚拟化存储有多种分类方法，从大的方面可以分为：根据在I/O路径中实现虚拟化的位置不同进行分类和根据控制路径和数据路径的不同进行分类。 根据在I/O路径中实现虚拟化的位置不同，虚拟化存储可以分为主机的虚拟存储、网络的虚拟存储、存储设备的虚拟存储。根据控制路径和数据路径的不同，虚拟化存储分为对称虚拟化与不对称虚拟化。 1）基于主机的虚拟存储 基于主机的虚拟存储完全依赖存储管理软件，无需任何附加硬件。基于主机的存储管理软件，在系统和应用级上，实现多机间的共享存储、存储资源管理（存储媒介、卷、文件管理）、数据复制和数据迁移、远程备份、集群系统、灾难恢复等存储管理任务。 基于主机的虚拟存储又可分为数据块以上虚拟层和数据块存储虚拟层：数据块以上虚拟层（ViAualizationaboveBlock）它是存储虚拟化的最顶层，通过文件系统和数据库给应用程序提供一个虚拟数据视图，屏蔽了底层实现。数据块存储虚拟层（BlockStorageVirtualzation）通过基于主机的卷管理程序和附加设备接口，给主机提供一个整合的存储访问视图。卷管理程序为虚拟存储设备创建逻辑卷，井负责数据I/O请求的路由。如下图所示，为基于主机的虚拟化存储示意图，主要应用于服务器的存储空间可以跨越多个异构的磁盘阵列场景，常用于在不同磁盘阵列之间做数据镜像保护。 一般由操作系统下的逻辑卷管理软件完成（安装客户端软件），不同操作系统的逻辑卷管理软件也不相同。优点是：支持异构的存储系统。缺点是：占用主机资源，转发性能差，与主机OS兼容性差，数据迁移复杂。 2）基于存储设备的虚拟存储 基于存储设备的存储虚拟化方法依赖于提供相关功能的存储模块。如果没有第三方的虚拟软件，基于存储的虚拟化经常只能提供一种不完全的存储虚拟化解决方案。对于包含多厂商存储设备的SAN存储系统，这种方法的运行效果并不是很好。利用这种方法意味着最终将锁定某一家单独的存储厂商。如下图所示，为基于存储设备的虚拟化示意图，主要应用于在同一存储设备内部，进行数据保护和数据迁移的场景。 通过在存储控制器上添加虚拟化功能实现，只有中高端存储设备具备此功能。优点是：与主机无关，不占用主机资源，数据管理功能丰富。缺点是：只能对本设备内的磁盘虚拟化，厂商绑定不能异构，多套存储设备软件不兼容，需多部署，成本高。 比较常见的基于设备的存储虚拟化应用就是精简配置虚拟磁盘，能够提供远远大于物理磁盘容量的虚拟空间。不管虚拟机磁盘分配了多少空间，如果没有数据写到虚拟磁盘上，就不会占用任何物理磁盘空间。精简配置虚拟磁盘的示意图如下所示，可以用小的物理容量为操作系统提供超大容量的虚拟存储空间。并且，随着应用数据量的增长，实际存储空间也可以及时扩展，而无须手动扩展。总之，自动精简配置提供的是“运行时空间”，可以显著减少已分配但是未使用的存储空间。 如果采用传统的磁盘分配方法，如上图左边。需要用户对当前和未来业务发展规模进行正确的预判，提前做好空间资源的规划。在实际中，由于对应用系统规模估计得不准确，往往会造成容量分配的浪费。 自动精简配置磁盘，如上图右边。有效地解决了存储资源的空间分配难题，提高了资源利用率。采用自动精简配置技术的数据卷分配给用户的是一个逻辑的虚拟磁盘，而不是一个固定的物理空间，只有当用户真正向该逻辑资源写数据时，才按照预先设定好的策略从物理空间分配实际空间容量。 3）基于网络的虚拟存储 网络虚拟层包括了绑定管理软件的存储服务器和网络互联设备。基于网络的虚拟化是在网络设备之间实现存储虚拟化功能，它将类似于卷管理的功能扩展到整个存储网络，负责管理Host视图、共享存储资源、数据复制、数据迁移及远程备份等，并对数据路径进行管理避免性能瓶颈。如下图所示，基于网络的虚拟存储示意图，主要用应用与异构存储系统的整合和统一管理场景。 通过在存储域网（SAN）中添加虚拟化引擎实现。优点是：与主机无关，性能好、能够异构主机和存储设备、管理统一、功能丰富。缺点是：各厂商产品品质参差不齐，部分厂商产品想能差，兼容性差。 基于网络的虚拟存储可采用对称或非对称的虚拟存储架构。在非对称架构中，虚拟存储控制器处于系统数据通路之外，不直接参与数据的传输。服务器可以直接经过标准的交换机对存储设备进行访问。虚拟存储控制器对所有存储设备进行配置，并将配置信息提交给所有服务器，如下图所示，服务器在访问存储设备时，不再经过虚拟存储控制器，而是直接访问存储设备并发工作，同样达到了增大传输带宽的目的，这种架构也称为存储虚拟化的带外虚拟引擎（out-of-band）。 一般用于不同存储设备之间的数据复制。优点是：虚拟化设备发生故障，整个系统将不会中断。缺点是：主机资源占用大，数据配置、同步复杂，缺乏统一管理。 而对称式架构中，虚拟存储控制设备直接位于服务器与存储设备之间，利用运行其上的存储管理软件来管理和配置所有存储设备，组成一个大型的存储池，其中的若干存储设备以一个逻辑分区的形式，被系统中所有服务器访问，这种架构也称为带内存储虚拟化引擎（in-band）。如下图所示，虚拟存储控制设备有多个数据通路与存储设备连接，多个存储设备并发工作，所以系统总的存储设备访问效率可达到较高水平。 主要用于异构存储系统整合，统一数据管理，在业务运行同时完成复制、镜像、CDP等各种数据管理功能。优点是：兼容性好，不占主机资源，配置简单，功能丰富。缺点是：虚拟化设备发生故障，整个系统将中断。 软件定义存储SDS上面提到过存储虚拟化并不等于软件定义存储SDS，其实存储虚拟化也可以归入软件定义存储的类别，实际上很多虚拟化存储厂商也是这么做的。但是严格意义上来说，这两者又略有不同，存储虚拟化一般只能在专门的硬件设备上使用，它的控制平面和存储数据平面是紧耦合的，很多厂商都要使用专门量身定做的设备才能实现存储虚拟化，而软件定义存储则没有设备限制，其控制平面和存储数据平面是松耦合，其本质上只定义控制平面的“软化”，而存储数据平面仍由各厂商去各自实现。可以简单理解成就是一个存储的管理程序。 软件定义存储实际上存在更大的适用范围，它的目标是从存储硬件中分离出存储控制功能和服务并提供编程接口，如 OpenStack Cinder， EMC ViPR，华为的Fusion Sphere和Fusion Storage都是这个范畴。如下图所示： 再具体一点，软件定义存储是把存储硬件或软件提供的控制能力抽象出来，并与数据层面的能力(数据访问)分开，这些控制能力包括卷管理，RAID，QoS，数据复制，监控，快照和备份等等，这个举动意义在于这些控制能力抽象出来以后，任何厂商提供的存储能力控制都是接近的，避免对厂商的绑定。 然后它通过这些控制能力进一步为管理员提供自定义、基于策略的虚拟存储层，这些策略可以是基于空间、性能、费用等等因素。它的优势在于与存储虚拟化相比更加轻量，通常可以保留底层存储系统如SAN，阵列的特性并仍然发挥作用，而且部署和实现难度都大幅度下降，可以采用更小的代价实现管理存储基础设施的能力，如下图所示： OpenStack Cinder是一个典型的软件定义存储产品，它目前支持大量的存储厂商设备，它定义了一些卷管理，快照，备份，简单统计等特性，用户可以使用Cinder提供的接口来获得不同存储设备提供的相似能力。 而Ceph可以看作一个典型的存储虚拟化产品，它将大量的通用存储设备联合起来提供一个存储池，并实现了一般存储厂商产品的能力。而Ceph的块存储能力使得它成为了Cinder的一个Driver。同时Cinder通过了这些基本API进行扩展，可以定义出不同的存储池，智能化的存储区域等等。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-27-Linux系统命令-第十篇《系统管理命令》]]></title>
    <url>%2F2019%2F05%2F27%2F2019-05-27-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E5%8D%81%E7%AF%87%E3%80%8A%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[lsof：查看进程打开的文件lsof全名为list open files，也就是列举系统中已经被打开的文件，通过lsof命令，就可以根据文件找到对应的进程信息，也可以根据进程信息找到进程打开的文件。 语法格式：lsof [option] 重要选项参数 【使用示例】 1）显示使用文件的进程 12345# 显示使用/var/log/message文件的进程信息[root@C7-Server01 ~]# lsof /var/log/messages COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMErsyslogd 1039 root 6w REG 8,3 782990 101157938 /var/log/messages 如果想知道文件是被哪个进程所使用，直接lsof+文件名就可以查询。 上述结果的说明如下： COMMAND：命令，进程的名称。 PID：进程号。 USER：进程的所有者。 FD：文件描述符，它又包含如下内容。 ​ 0：表示标准输出。 ​ 1：表示标准输入。 ​ 2：表示标准错误。 ​ u：表示该文件被打开并处于读取/写入模式。 ​ r：表示该文件被打开并处于只读模式。 ​ w：表示该文件被打开并处于写入模式。 TYPE：文件类型，REG（regular）为普通文件。 DEVICE：指定磁盘的标识。 SIZE/OFF：文件的大小。 NODE：索引节点。 NAME：文件名称。进程号。 USER：进程的所有者。 FD：文件描述符，它又包含如下内容。 ​ 0：表示标准输出。 ​ 1：表示标准输入。 ​ 2：表示标准错误。 ​ u：表示该文件被打开并处于读取/写入模式。 ​ r：表示该文件被打开并处于只读模式。 ​ w：表示该文件被打开并处于写入模式。 TYPE：文件类型，REG（regular）为普通文件。 DEVICE：指定磁盘的标识。 SIZE/OFF：文件的大小。 NODE：索引节点。 NAME：文件名称。 2）显示指定进程所打开的文件 12345678910111213141516171819202122232425262728293031323334353637383940414243# 通过-c选项指定进程名[root@C7-Server01 ~]# lsof -c crondCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEcrond 671 root cwd DIR 8,3 224 64 /crond 671 root rtd DIR 8,3 224 64 /crond 671 root txt REG 8,3 70128 462795 /usr/sbin/crondcrond 671 root mem REG 8,3 62184 1900 /usr/lib64/libnss_files-2.17.socrond 671 root mem REG 8,3 106070960 100665267 /usr/lib/locale/locale-archivecrond 671 root mem REG 8,3 144792 1908 /usr/lib64/libpthread-2.17.socrond 671 root mem REG 8,3 23968 2361 /usr/lib64/libcap-ng.so.0.0.0crond 671 root mem REG 8,3 402384 2006 /usr/lib64/libpcre.so.1.2.0crond 671 root mem REG 8,3 2173512 1882 /usr/lib64/libc-2.17.socrond 671 root mem REG 8,3 127184 2359 /usr/lib64/libaudit.so.1.0.0crond 671 root mem REG 8,3 19776 1888 /usr/lib64/libdl-2.17.socrond 671 root mem REG 8,3 61672 3998 /usr/lib64/libpam.so.0.83.1crond 671 root mem REG 8,3 155784 190119 /usr/lib64/libselinux.so.1crond 671 root mem REG 8,3 164240 1875 /usr/lib64/ld-2.17.socrond 671 root 0r CHR 1,3 0t0 1028 /dev/nullcrond 671 root 1u unix 0xffff9fd574eca800 0t0 23970 socketcrond 671 root 2u unix 0xffff9fd574eca800 0t0 23970 socketcrond 671 root 3uW REG 0,20 4 23971 /run/crond.pidcrond 671 root 4u unix 0xffff9fd574e3f800 0t0 25637 socketcrond 671 root 5r a_inode 0,10 0 8261 inotify# 通过-p选项指定进程号[root@C7-Server01 ~]# lsof -p 1105COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEhttpd 1105 apache cwd DIR 8,3 224 64 /httpd 1105 apache rtd DIR 8,3 224 64 /httpd 1105 apache txt REG 8,3 523672 5947365 /usr/sbin/httpdhttpd 1105 apache mem REG 8,3 62184 1900 /usr/lib64/libnss_files-2.17.sohttpd 1105 apache mem REG 8,3 27808 68422763 /usr/lib64/httpd/modules/mod_cgi.sohttpd 1105 apache mem REG 8,3 68192 2275 /usr/lib64/libbz2.so.1.0.6httpd 1105 apache mem REG 8,3 157424 2265 /usr/lib64/liblzma.so.5.2.2httpd 1105 apache mem REG 8,3 99944 2264 /usr/lib64/libelf-0.170.sohttpd 1105 apache mem REG 8,3 19896 1993 /usr/lib64/libattr.so.1.1.0httpd 1105 apache mem REG 8,3 88720 84 /usr/lib64/libgcc_s-4.8.5-20150702.so.1httpd 1105 apache mem REG 8,3 297360 193704 /usr/lib64/libdw-0.170.sohttpd 1105 apache mem REG 8,3 20032 1998 /usr/lib64/libcap.so.2.22httpd 1105 apache mem REG 8,3 44448 1912 /usr/lib64/librt-2.17.so。。。 3）监听指定的协议、端口和主机等信息，显示符合条件的网络服务进程 语法格式：lsof -i [46] [protocol][@hostname][:service|port] 说明如下： 46：4代表IPv4，6代表IPv6。 protocol：传输协议，可以是TCP或UDP。 hostname：主机名称或者IP地址。 service：进程的服务名，例如NFS、SSH和FTP等。 port：系统中与服务对应的端口号。例如HTTP服务默认对应的端口号为80，SSH服务默认对应的端口号为22。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 查看所有网络服务进程[root@C7-Server01 ~]# lsof -iCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsystemd 1 root 48u IPv6 42537 0t0 TCP *:telnet (LISTEN)chronyd 645 chrony 1u IPv4 22341 0t0 UDP localhost:323 chronyd 645 chrony 2u IPv6 22342 0t0 UDP localhost:323 sshd 1038 root 3u IPv4 26920 0t0 TCP *:ssh (LISTEN)sshd 1038 root 4u IPv6 26929 0t0 TCP *:ssh (LISTEN)httpd 1042 root 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1101 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1102 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1103 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1104 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1105 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)master 1310 root 13u IPv4 24294 0t0 TCP localhost:smtp (LISTEN)master 1310 root 14u IPv6 24295 0t0 TCP localhost:smtp (LISTEN)docker-pr 1648 root 4u IPv6 24517 0t0 TCP *:terabase (LISTEN)sshd 1736 root 3u IPv4 29038 0t0 TCP C7-Server01:ssh-&gt;192.168.101.1:servicetags (ESTABLISHED)# 查看所有tcp连接进程[root@C7-Server01 ~]# lsof -i tcpCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsystemd 1 root 48u IPv6 42537 0t0 TCP *:telnet (LISTEN)sshd 1038 root 3u IPv4 26920 0t0 TCP *:ssh (LISTEN)sshd 1038 root 4u IPv6 26929 0t0 TCP *:ssh (LISTEN)httpd 1042 root 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1101 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1102 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1103 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1104 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1105 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)master 1310 root 13u IPv4 24294 0t0 TCP localhost:smtp (LISTEN)master 1310 root 14u IPv6 24295 0t0 TCP localhost:smtp (LISTEN)docker-pr 1648 root 4u IPv6 24517 0t0 TCP *:terabase (LISTEN)sshd 1736 root 3u IPv4 29038 0t0 TCP C7-Server01:ssh-&gt;192.168.101.1:servicetags (ESTABLISHED)# 显示端口号为22的网络服务进程[root@C7-Server01 ~]# lsof -i :22COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 1038 root 3u IPv4 26920 0t0 TCP *:ssh (LISTEN)sshd 1038 root 4u IPv6 26929 0t0 TCP *:ssh (LISTEN)sshd 1736 root 3u IPv4 29038 0t0 TCP C7-Server01:ssh-&gt;192.168.101.1:servicetags (ESTABLISHED)# 显示所有支持ipv6的进程[root@C7-Server01 ~]# lsof -i 6COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsystemd 1 root 48u IPv6 42537 0t0 TCP *:telnet (LISTEN)chronyd 645 chrony 2u IPv6 22342 0t0 UDP localhost:323 sshd 1038 root 4u IPv6 26929 0t0 TCP *:ssh (LISTEN)httpd 1042 root 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1101 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1102 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1103 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1104 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)httpd 1105 apache 4u IPv6 27020 0t0 TCP *:http (LISTEN)master 1310 root 14u IPv6 24295 0t0 TCP localhost:smtp (LISTEN)docker-pr 1648 root 4u IPv6 24517 0t0 TCP *:terabase (LISTEN) 3）显示指定用户使用的文件 123456789101112131415161718192021222324252627282930313233# 使用-u选项，显示chrony用户使用的文件[root@C7-Server01 ~]# lsof -u chronyCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEchronyd 645 chrony cwd DIR 8,3 224 64 /chronyd 645 chrony rtd DIR 8,3 224 64 /chronyd 645 chrony txt REG 8,3 261024 466082 /usr/sbin/chronydchronyd 645 chrony mem REG 8,3 547568 1239 /usr/lib64/libfreeblpriv3.sochronyd 645 chrony mem REG 8,3 68192 2275 /usr/lib64/libbz2.so.1.0.6chronyd 645 chrony mem REG 8,3 157424 2265 /usr/lib64/liblzma.so.5.2.2chronyd 645 chrony mem REG 8,3 90248 190122 /usr/lib64/libz.so.1.2.7chronyd 645 chrony mem REG 8,3 99944 2264 /usr/lib64/libelf-0.170.sochronyd 645 chrony mem REG 8,3 88720 84 /usr/lib64/libgcc_s-4.8.5-20150702.so.1chronyd 645 chrony mem REG 8,3 297360 193704 /usr/lib64/libdw-0.170.sochronyd 645 chrony mem REG 8,3 44448 1912 /usr/lib64/librt-2.17.sochronyd 645 chrony mem REG 8,3 86544 193707 /usr/lib64/libnss_myhostname.so.2chronyd 645 chrony mem REG 8,3 106848 1910 /usr/lib64/libresolv-2.17.sochronyd 645 chrony mem REG 8,3 31824 1898 /usr/lib64/libnss_dns-2.17.sochronyd 645 chrony mem REG 8,3 62184 1900 /usr/lib64/libnss_files-2.17.sochronyd 645 chrony mem REG 8,3 19896 1993 /usr/lib64/libattr.so.1.1.0chronyd 645 chrony mem REG 8,3 19776 1888 /usr/lib64/libdl-2.17.sochronyd 645 chrony mem REG 8,3 2173512 1882 /usr/lib64/libc-2.17.sochronyd 645 chrony mem REG 8,3 144792 1908 /usr/lib64/libpthread-2.17.sochronyd 645 chrony mem REG 8,3 266680 86207 /usr/lib64/libseccomp.so.2.3.1chronyd 645 chrony mem REG 8,3 20032 1998 /usr/lib64/libcap.so.2.22chronyd 645 chrony mem REG 8,3 11464 1237 /usr/lib64/libfreebl3.sochronyd 645 chrony mem REG 8,3 1139680 1890 /usr/lib64/libm-2.17.sochronyd 645 chrony mem REG 8,3 164240 1875 /usr/lib64/ld-2.17.sochronyd 645 chrony 0u unix 0xffff9fd57f8d7000 0t0 22339 socketchronyd 645 chrony 1u IPv4 22341 0t0 UDP localhost:323 chronyd 645 chrony 2u IPv6 22342 0t0 UDP localhost:323 chronyd 645 chrony 3r CHR 1,9 0t0 1033 /dev/urandomchronyd 645 chrony 5u unix 0xffff9fd57f8d6c00 0t0 22347 /var/run/chrony/chronyd.sock 4）显示所有socket文件 12345678910111213141516171819202122232425262728293031# 使用-U选项可以显示所有socket文件[root@C7-Server01 ~]# lsof -UCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsystemd 1 root 12u unix 0xffff9fd375ba5800 0t0 10700 /run/systemd/privatesystemd 1 root 14u unix 0xffff9fd56b38ec00 0t0 42422 socketsystemd 1 root 20u unix 0xffff9fd577436400 0t0 31 /run/systemd/notifysystemd 1 root 21u unix 0xffff9fd577436800 0t0 33 /run/systemd/cgroups-agentsystemd 1 root 22u unix 0xffff9fd375ba1800 0t0 18449 /run/systemd/journal/stdoutsystemd 1 root 23u unix 0xffff9fd5717e5c00 0t0 19070 /run/systemd/journal/stdoutsystemd 1 root 24u unix 0xffff9fd3fb00f800 0t0 17293 /run/systemd/journal/stdoutsystemd 1 root 25u unix 0xffff9fd3fbb68800 0t0 17326 /run/systemd/journal/stdoutsystemd 1 root 26u unix 0xffff9fd5733cec00 0t0 19182 /run/systemd/journal/stdoutsystemd 1 root 28u unix 0xffff9fd3fbb7fc00 0t0 19185 /run/systemd/journal/stdoutsystemd 1 root 29u unix 0xffff9fd574eca400 0t0 24785 /run/systemd/journal/stdoutsystemd 1 root 30u unix 0xffff9fd574e38000 0t0 24108 /run/systemd/journal/stdoutsystemd 1 root 31u unix 0xffff9fd376117000 0t0 26054 /run/systemd/journal/stdoutsystemd 1 root 32u unix 0xffff9fd3fbb74400 0t0 26904 /run/systemd/journal/stdoutsystemd 1 root 33u unix 0xffff9fd375bec800 0t0 26905 /run/systemd/journal/stdoutsystemd 1 root 34u unix 0xffff9fd376116c00 0t0 26908 /run/systemd/journal/stdoutsystemd 1 root 35u unix 0xffff9fd3fb8f5800 0t0 14396 /run/systemd/shutdowndsystemd 1 root 36u unix 0xffff9fd5733c9000 0t0 22337 socketsystemd 1 root 37u unix 0xffff9fd375be8800 0t0 22569 /var/run/docker.socksystemd 1 root 38u unix 0xffff9fd37612d800 0t0 22571 /run/dbus/system_bus_socketsystemd 1 root 41u unix 0xffff9fd3fb8f5000 0t0 14401 /run/udev/controlsystemd 1 root 42u unix 0xffff9fd577437800 0t0 41 /run/systemd/journal/stdoutsystemd 1 root 43u unix 0xffff9fd577437c00 0t0 44 /run/systemd/journal/socketsystemd 1 root 45u unix 0xffff9fd577436000 0t0 46 /dev/logsystemd-j 462 root 3u unix 0xffff9fd577437800 0t0 41 /run/systemd/journal/stdoutsystemd-j 462 root 4u unix 0xffff9fd577437c00 0t0 44 /run/systemd/journal/socket。。。 uptime：显示系统的运行时间及负载uptime命令可以输出当前系统时间、系统开机到现在的运行时间、目前有多少用户在线和系统平均负载等信息。 语法格式：uptime 【使用示例】 显示系统的运行时间及负载信息 12[root@C7-Server01 ~]# uptime 18:56:22 up 2:59, 1 user, load average: 0.00, 0.01, 0.05 free：查看系统内存信息free命令用于显示系统内存状态，具体包括系统物理内存、虚拟内存、共享内存和系统缓存等。 语法格式：free [option] 重要参数选项 【使用示例】 1）查看系统内存 1234567891011121314151617181920# 不加任何参数使用free命令，结果是以字节为单位显示，很难看懂[root@C7-Server01 ~]# free total used free shared buff/cache availableMem: 7992344 243632 259960 12084 7488752 7369040Swap: 8388604 0 8388604# 使用-m选项以MB为单位显示，但是可能不是很精确[root@C7-Server01 ~]# free -m total used free shared buff/cache availableMem: 7805 238 253 11 7313 7196Swap: 8191 0 8191# 使用-h选项，以人类可读的方式显示[root@C7-Server01 ~]# free -h total used free shared buff/cache availableMem: 7.6G 238M 253M 11M 7.1G 7.0GSwap: 8.0G 0B 8.0G Linux系统的特性是将不用的物理内存缓存起来，因此253MB不是系统的真实剩余内存，系统真正可用的内存为7.0G。buffers为写入数据的缓冲区，cache为读取数据的缓冲区。 2）定时查询内存 123456789101112131415# 使用-s选项设定每10秒刷新一次内存[root@C7-Server01 ~]# free -h -s 10total used free shared buff/cache availableMem: 7.6G 236M 7.1G 11M 327M 7.1GSwap: 8.0G 0B 8.0Gtotal used free shared buff/cache availableMem: 7.6G 236M 7.1G 11M 327M 7.1GSwap: 8.0G 0B 8.0Gtotal used free shared buff/cache availableMem: 7.6G 237M 7.1G 11M 327M 7.1GSwap: 8.0G 0B 8.0Gtotal used free shared buff/cache availableMem: 7.6G 237M 7.1G 11M 327M 7.1GSwap: 8.0G 0B 8.0G iftop：动态显示网络接口流量信息iftop是一款实时流量监控工具，可用于监控TCP/IP连接等，必须以root用户的身份运行。一般最小化安装系统都是没有这个命令的，需要使用yum命令额外安装， 12345# 安装iftop[root@C7-Server01 ~]# yum list | grep iftopiftop.x86_64 1.0-0.14.pre4.el7 epel [root@C7-Server01 ~]# yum install -y iftop 语法格式：iftop [option] 重要参数选项 【使用示例】 1）不使用任何选项启动iftop命令监控流量 123# 默认监听系统的第一块网卡，可以使用-i选项指定网卡[root@C7-Server01 ~]# iftop 上图说明如下： 界面上显示的是类似刻度尺的刻度范围，是以标尺的形式显示流量图形的长条。 中间的&lt;=或=&gt;这两个左右箭头，表示的是流量的方向。 TX：发送流量。 RX：接收流量。 TOTAL：总流量。 Cum：运行iftop到目前时间的总流量。 peak：流量峰值。 rates：分别表示过去2s、10s、40s的平均流量。 2）常用命令组合 12345# -nNBP用于显示各服务端口号的流量情况，单位为byte# 默认显示第一块网卡的流量，可以通过-i选项指定网卡[root@C7-Server01 ~]# iftop -nNBP vmstat：虚拟内存统计vmstat是Virtual Memory Statistics（虚拟内存统计）的缩写，利用vmstat命令可以对操作系统的内存信息、进程状态和CPU活动等进行监视。但是只能对系统的整体情况进行统计，无法对某个进程进行深入分析。 语法格式：vmstat [option] [delay [ count]]，delay表示两次输出之间的间隔时间。count表示按照delay指定的时间间隔统计的次数。 重要参数选项 【使用示例】 1）显示虚拟内存的使用情况 12345678910111213141516171819202122232425262728293031323334# 如果省略间隔时间和次数选项，则仅显示最后一次报告就退出[root@C7-Server01 ~]# vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 7267600 2076 479564 0 0 79 4 55 57 0 0 100 0 0# 使用delay选项，可以每隔n秒显示一次，直到ctrl+c退出[root@C7-Server01 ~]# vmstat 5procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 7267436 2076 479652 0 0 75 4 53 56 0 0 100 0 0 1 0 0 7267412 2076 479652 0 0 0 0 95 104 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 84 101 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 87 106 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 88 99 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 91 102 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 83 100 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 90 105 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 85 95 0 0 100 0 0^C # 加上count选项，可以按照指定次数显示，完成后自动退出[root@C7-Server01 ~]# vmstat 5 6procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 7267312 2076 479652 0 0 70 3 51 54 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 102 110 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 85 95 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 85 97 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 90 98 0 0 100 0 0 0 0 0 7267288 2076 479652 0 0 0 0 89 97 0 0 100 0 0 上面输出的结果说明： 第1列：procs。 ​ r列表示运行和等待CPU时间片的进程数。 ​ b列表示正在等待资源的进程数。 第2列：memory。 ​ swpd列表示使用虚拟内存的大小。 ​ free列表示当前空闲的物理内存数量。 ​ buff列表示buffers的内存数量。 ​ cache列表示cache的内存数量。 第3列：swap。 ​ si（swap in）列表示由磁盘调入内存，也就是磁盘读入内存交换区的数量。 ​ so（swap out）列表示由内存调入磁盘，也就是内存交换区写入磁盘的数量。 第4列：I/O项显示磁盘读写状况。 ​ bi列表示从块设备读入数据的总量（即读磁盘）（块/s）。 ​ bo列表示写入块设备的数据总量（即写磁盘）（块/s）。 第5列：system显示采集间隔内发生的中断数。 ​ in列表示在某一时间间隔中观测到的每秒设备中断数。 ​ cs列表示每秒产生的上下文切换次数。 第6列：CPU项显示了CPU的使用状态。 ​ us列显示了用户进程消耗的CPU时间百分比。 ​ sy列显示了系统（内核）进程消耗的CPU时间百分比。 ​ id列显示了CPU处在空闲状态的时间百分比。 ​ wa列显示了I/O等待所占用的CPU时间百分比。 ​ st列显示了虚拟机占用的CPU时间的百分比。 2）显示活跃和非活跃内存 12345678910# 每2秒刷新一次，共刷新5此，通过-a选项显示系统活跃和非活跃内存[root@C7-Server01 ~]# vmstat -a 2 5procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free inact active si so bi bo in cs us sy id wa st1 0 0 7266764 244696 250212 0 0 57 3 45 48 0 0 100 0 00 0 0 7266648 244696 250168 0 0 0 0 98 104 0 0 100 0 00 0 0 7266648 244696 250168 0 0 0 0 81 93 0 0 100 0 00 0 0 7266648 244696 250168 0 0 0 0 82 93 0 0 100 0 00 0 0 7266648 244696 250168 0 0 0 0 114 135 0 0 100 0 0 inact：表示非活跃内存的大小。 active：表示活跃内存的大小。 3）查看内存使用的详细信息 12345678910111213141516171819202122232425262728293031# 通过-s选项可以查看内存使用的详细信息# 这些信息来自/proc/meminfo、/proc/stat和/proc/vmstat等内存映射文件[root@C7-Server01 ~]# vmstat -s 7992344 K total memory243984 K used memory250216 K active memory244696 K inactive memory7266392 K free memory2076 K buffer memory479892 K swap cache8388604 K total swap0 K used swap8388604 K free swap390 non-nice user cpu ticks0 nice user cpu ticks1466 system cpu ticks812275 idle cpu ticks229 IO-wait cpu ticks0 IRQ cpu ticks52 softirq cpu ticks0 stolen cpu ticks411934 pages paged in20492 pages paged out0 pages swapped in0 pages swapped out348908 interrupts371812 CPU context switches1556980554 boot time2038 forks 4）查看磁盘的读写信息 123456789# 使用-d可以查看所有磁盘的读写信息# 这些信息主要来自/proc/diskstats文件，merged表示合并写/读请求次数[root@C7-Server01 ~]# vmstat -ddisk- ------------reads------------ ------------writes----------- -----IO------total merged sectors ms total merged sectors ms cur secsr0 18 0 2056 28 0 0 0 0 0 0sda 10037 2 821813 7396 2986 132 40987 1497 0 4 5）查看指定分区的读写信息 12345# 通过-p选项可以指定分区查看读写信息[root@C7-Server01 ~]# vmstat -p /dev/sda3sda3 reads read sectors writes requested writes8008 771194 931 36850 这些信息主要来自于/proc/diskstats，输出结果各列的说明具体如下： reads：来自于该分区的读的次数。 read sectors：来自于该分区的读扇区的次数。 writes：来自于该分区的写的次数。 requested writes：来自于该分区的写请求次数。 mpstat：CPU信息统计mpstat是Multiprocessor Statistics的缩写，是一种实时系统监控工具。mpstat命令会输出CPU的一些统计信息，这些信息存放在/proc/stat文件中。在多CPU的系统里，此命令不但能用来查看所有CPU的平均状况信息，而且还能够用来查看特定CPU的信息。mpstat命令的最大特点是：可以查看多核心CPU中每个计算核心的统计数据，而类似命令vmstat只能查看系统整体的CPU情况。 语法格式：mpstat [option] [delay [ count]]，delay和count的含义同vmstat。 重要选项参数 【使用示例】 1）显示CPU信息统计 12345678910111213141516171819202122# 不加任何选项，默认显示所有CPU信息# 省却delay和count，仅显示最后一次报告就退出[root@C7-Server01 ~]# mpstat Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)11:26:00 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle11:26:00 PM all 0.05 0.00 0.15 0.02 0.00 0.00 0.00 0.00 0.00 99.77# 每2秒刷新一次，共刷新5次[root@C7-Server01 ~]# mpstat 2 5Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)11:27:09 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle11:27:11 PM all 0.00 0.00 0.12 0.00 0.00 0.00 0.00 0.00 0.00 99.8811:27:13 PM all 0.00 0.00 0.12 0.00 0.00 0.00 0.00 0.00 0.00 99.8811:27:15 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0011:27:17 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0011:27:19 PM all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00Average: all 0.00 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.00 99.95 命令输出结果的说明如下： 第1列：11：27：09 PM，表示当前时间。 第2列：CPU，all表示所有CPU，0表示第一个CPU…… 后面9列的含义分别如下。 ％usr：用户进程消耗的CPU时间百分比。 ％nice：改变过优先级的进程占用的CPU时间百分比。 ％sys：系统（内核）进程消耗的CPU时间百分比。 ％iowait：IO等待所占用的CPU时间百分比。 ％irq：硬中断占用的CPU时间百分比。 ％soft：软中断占用的CPU时间百分比。 ％steal：虚拟机强制CPU等待的时间百分比。 ％guest：虚拟机占用CPU时间的百分比。 ％idle：CPU处在空闲状态的时间百分比。 2）显示指定CPU的信息统计 123456# 系统有4个cpu，通过-P选项指定显示第二个CPU的信息[root@C7-Server01 ~]# mpstat -P 1Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)11:30:31 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle11:30:31 PM 1 0.06 0.00 0.22 0.02 0.00 0.01 0.00 0.00 0.00 99.70 注意：这里的CPU指的是逻辑CPU，也就是超线程。 iostat：I/O信息统计iostat是I/O statistics（输入/输出统计）的缩写，其主要功能是对系统的磁盘I/O操作进行监视。它的输出主要是显示磁盘读写操作的统计信息，同时也会给出CPU的使用情况。同vmstat命令一样，iostat命令也不能对某个进程进行深入分析，仅会对系统的整体情况进行分析。 语法格式：iostat [option] [ interval [ count ] ]，interval同delay。 重要选项参数 【使用示例】 1）显示所有设备的负载情况 1234567891011# 默认不接任何选项，显示所有设备的负载情况# 省略时间间隔和次数，仅显示最后一次报告就退出[root@C7-Server01 ~]# iostat Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)avg-cpu: %user %nice %system %iowait %steal %idle0.05 0.00 0.13 0.02 0.00 99.80Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnscd0 0.01 0.29 0.00 1028 0sda 3.86 115.53 8.72 411970 31087 命令输出结果的说明如下： 第1～2行中各列的含义具体如下： ％user：用户进程消耗的CPU时间百分比。 ％nice：改变过优先级的进程占用的CPU时间百分比。 ％system：系统（内核）进程消耗的CPU时间百分比。 ％iowait：IO等待所占用的CPU时间百分比。 ％steal：虚拟机强制CPU等待的时间百分比。 ％idle：CPU处在空闲状态的时间百分比。 第3～4行中各列的含义如下： tps：表示该设备每秒的传输次数，“一次传输”的意思是“一次I/O请求”，多个逻辑请求可能会被合并为“一次I/O请求”，“一次传输”请求的大小是未知的。 kB_read/s：表示每秒读取的数据块数。 kB_wrtn/s：表示每秒写入的数据块数。 kB_read：表示读取的所有块数。 kB_wrtn：表示写入的所有块数。 3）只显示磁盘统计信息 1234567# 通过-d选项直线磁盘的统计信息[root@C7-Server01 ~]# iostat -dLinux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnscd0 0.00 0.27 0.00 1028 0sda 3.63 108.57 8.20 411970 31124 4）查看扩展信息 1234567# 使用-x选项可以查看扩展信息[root@C7-Server01 ~]# iostat -d -xLinux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilscd0 0.00 0.00 0.00 0.00 0.27 0.00 114.22 0.00 1.56 1.56 0.00 1.11 0.00sda 0.00 0.05 2.63 0.93 106.63 8.06 64.38 0.00 0.68 0.74 0.54 0.33 0.12 命令输出结果说明如下： rrqm/s：每秒进行merge的读操作数目。 wrqm/s：每秒进行merge的写操作数目。 r/s：每秒完成的读I/O设备次数。 w/s：每秒完成的写I/O设备次数。 rkB/s：每秒读入的千字节数。 wkB/s：每秒写入的千字节数。 avgrq-sz：设备平均每次进行I/O操作的数据大小（扇区）。 avgqu-sz：平均I/O队列长度。 await：设备平均每次I/O操作的等待时间（毫秒）。 svctm：设备平均每次I/O操作的服务时间（毫秒）。 ％util：每秒钟用于I/O操作的百分比。 5）只查看CPU的统计信息 123456# 通过-c选项可以产看CPU的统计信息[root@C7-Server01 ~]# iostat -cLinux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)avg-cpu: %user %nice %system %iowait %steal %idle0.04 0.00 0.12 0.02 0.00 99.82 iotop：动态显示磁盘I/O统计信息iotop命令是一款实时监控磁盘I/O的工具，但必须以root用户的身份运行。使用iotop命令可以很方便地查看每个进程使用磁盘I/O的情况。最小化安装系统一般是没有这个命令的，需要使用yum命令额外安装，安装命令如下： 1yum -y install iotop 语法格式：iotop [option] 重要选项参数 【使用示例】 不使用任何参数 123# 通过iotop指令启动服务，类似top指令，不过它只显示磁盘io的使用情况[root@C7-Server01 ~]# iotop 命令显示结果的说明如下： Total DISK READ：总的磁盘读取速度。 Total DISK WRITE：总的磁盘写入速度。 TID：进程pid值。 PRIO：优先级。 USER：用户。 DISK READ：磁盘读取速度。 DISK WRITE：磁盘写入速度。 SWAPIN：从swap分区读取数据占用的百分比。 IO：I/O占用的百分比。 COMMAND：消耗I/O的进程名。 sar：收集系统信息通过sar命令，可以全面地获取系统的CPU、运行队列、磁盘I/O、分页（交换区）、内存、CPU中断和网络等性能数据。 语法格式：sar [option] [ interval [ count ] ] 重要选项参数 【使用示例】 1）查看系统CPU的负载 123456789# 使用-u选项只显示系统CPU的负载情况[root@C7-Server01 ~]# sar -u 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)11:55:12 PM CPU %user %nice %system %iowait %steal %idle11:55:14 PM all 0.00 0.00 0.00 0.00 0.00 100.0011:55:16 PM all 0.00 0.00 0.12 0.00 0.00 99.8811:55:18 PM all 0.00 0.00 0.00 0.00 0.00 100.00Average: all 0.00 0.00 0.04 0.00 0.00 99.96 2）显示运行队列的大小 123456789# 使用-q选项，只显示运行队列大小[root@C7-Server01 ~]# sar -q 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)11:56:26 PM runq-sz plist-sz ldavg-1 ldavg-5 ldavg-15 blocked11:56:28 PM 0 248 0.00 0.01 0.05 011:56:30 PM 0 248 0.00 0.01 0.05 011:56:32 PM 1 248 0.00 0.01 0.05 0Average: 0 248 0.00 0.01 0.05 0 命令输出结果说明如下： runq-sz：运行队列的长度（等待运行的进程数）。 plist-sz：进程列表中进程（process）和线程（thread）的数量。 ldavg-1：最后1分钟的系统平均负载（system load average）。 ldavg-5：过去5分钟的系统平均负载。 ldavg-15：过去15分钟的系统平均负载。 3）只显示系统内存的使用情况 123456789# 使用-r选项，只显示系统内存的使用[root@C7-Server01 ~]# sar -r 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/04/2019 _x86_64_ (4 CPU)11:59:34 PM kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty11:59:36 PM 7258452 733892 9.18 2076 390608 558672 3.41 268816 230800 011:59:38 PM 7258452 733892 9.18 2076 390608 558672 3.41 268816 230800 011:59:40 PM 7258452 733892 9.18 2076 390608 558672 3.41 268824 230800 0Average: 7258452 733892 9.18 2076 390608 558672 3.41 268819 230800 0 命令输出结果说明如下： kbmemfree：空闲物理内存量。 kbmemused：使用中的物理内存量。 ％memused：物理内存量的使用率。 kbbuffers：内核中作为缓冲区使用的物理内存容量。 kbcached：内核中作为缓存使用的物理内存容量。 kbcommit：应用程序当前使用的内存大小。 ％commit：应用程序当前使用的内存大小占总大小的使用百分比。 4）显示缓冲区的使用情况 123456789# 使用-b选项，只显示缓冲区的使用情况[root@C7-Server01 ~]# sar -b 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/05/2019 _x86_64_ (4 CPU)12:01:59 AM tps rtps wtps bread/s bwrtn/s12:02:01 AM 0.00 0.00 0.00 0.00 0.0012:02:03 AM 0.00 0.00 0.00 0.00 0.0012:02:05 AM 0.00 0.00 0.00 0.00 0.00Average: 0.00 0.00 0.00 0.00 0.00 命令输出结果如下： tps：每秒钟物理设备的I/O传输总量。 rtps：每秒钟从物理设备读入的数据总量。 wtps：每秒钟向物理设备写入的数据总量。 bread/s：每秒钟从物理设备读入的数据量，单位为块/s。 bwrtn/s：每秒钟向物理设备写入的数据量，单位为块/s。 5）显示网络的运行状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# 使用-n选项，只显示网络运行状态# DEV表示网络接口[root@C7-Server01 ~]# sar -n DEV 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/05/2019 _x86_64_ (4 CPU)12:04:55 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s12:04:57 AM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:57 AM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:57 AM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:57 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:57 AM vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:57 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:57 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s12:04:59 AM eth0 0.50 0.50 0.03 0.37 0.00 0.00 0.0012:04:59 AM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:59 AM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:59 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:59 AM vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:59 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:04:59 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s12:05:01 AM eth0 0.50 0.50 0.03 0.37 0.00 0.00 0.0012:05:01 AM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:05:01 AM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:05:01 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:05:01 AM vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:05:01 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/sAverage: eth0 0.33 0.33 0.02 0.25 0.00 0.00 0.00Average: eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00# 显示网络接口错包统计# EDEV表示网络接口错误信息[root@C7-Server01 ~]# sar -n EDEV 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/05/2019 _x86_64_ (4 CPU)12:06:49 AM IFACE rxerr/s txerr/s coll/s rxdrop/s txdrop/s txcarr/s rxfram/s rxfifo/s txfifo/s12:06:51 AM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:51 AM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:51 AM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:51 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:51 AM vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:51 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:51 AM IFACE rxerr/s txerr/s coll/s rxdrop/s txdrop/s txcarr/s rxfram/s rxfifo/s txfifo/s12:06:53 AM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:53 AM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:53 AM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:53 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:53 AM vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:53 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:53 AM IFACE rxerr/s txerr/s coll/s rxdrop/s txdrop/s txcarr/s rxfram/s rxfifo/s txfifo/s12:06:55 AM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:55 AM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:55 AM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:55 AM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:55 AM vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:06:55 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: IFACE rxerr/s txerr/s coll/s rxdrop/s txdrop/s txcarr/s rxfram/s rxfifo/s txfifo/sAverage: eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: vethc27967f 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00# 通过SOCK关键字，可以只显示套接字信息[root@C7-Server01 ~]# sar -n SOCK 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/05/2019 _x86_64_ (4 CPU)12:08:42 AM totsck tcpsck udpsck rawsck ip-frag tcp-tw12:08:44 AM 979 4 1 0 0 012:08:46 AM 979 4 1 0 0 012:08:48 AM 979 4 1 0 0 0Average: 979 4 1 0 0 0 命令输出结果的说明： IFACE：网络接口。 rxpck/s：每秒钟接收的数据包。 txpck/s：每秒钟发送的数据包。 rxkB/s：每秒钟接收的字节数。 txkB/s：每秒钟发送的字节数。 rxcmp/s：每秒钟接收的压缩数据包。 txcmp/s：每秒钟发送的压缩数据包。 rxmcst/s：每秒钟接收的多播数据包。 6）显示磁盘的读写性能 12345678910111213141516# 通过-d选项，可以只显示磁盘的读写状态[root@C7-Server01 ~]# sar -d 2 3Linux 3.10.0-862.el7.x86_64 (C7-Server01) 05/05/2019 _x86_64_ (4 CPU)12:10:13 AM DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util12:10:15 AM dev11-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:10:15 AM dev8-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:10:15 AM DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util12:10:17 AM dev11-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:10:17 AM dev8-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:10:17 AM DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %util12:10:19 AM dev11-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:10:19 AM dev8-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: DEV tps rd_sec/s wr_sec/s avgrq-sz avgqu-sz await svctm %utilAverage: dev11-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Average: dev8-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 输出结果说明如下： DEV：表示磁盘的设备名称。 tps：表示该设备每秒的传输次数，“一次传输”的意思是“一次I/O请求”，多个逻辑请求可能会被合并为“一次I/O请求”，“一次传输”请求的大小是未知的。 rd_sec/s：表示每秒从设备读取的扇区数。 wr_sec/s：表示每秒写入设备的扇区数目。 avgrq-sz：设备平均每次I/O操作的数据大小（扇区）。 avgqu-sz：平均I/O队列长度。 await：设备平均每次I/O操作的等待时间（毫秒）。 svctm：设备平均每次I/O操作的服务时间（毫秒）。 ％util：每秒钟用于I/O操作的百分比。 ntsysv：管理开机服务ntsysv命令提供了一种基于文本界面的菜单操作方式，以设置不同运行级别下的系统服务启动状态。 此工具在最小化系统安装时没有安装，需要手动安装，安装命令如下： 12345678# 查询ntsysv的yum包，结果显示属于基础包[root@C7-Server01 ~]# yum list | grep ntsysvntsysv.x86_64 1.7.4-1.el7 base# 执行yum install进行安装[root@C7-Server01 ~]# yum install -y ntsysv 语法格式：ntsysv [option] 重要选项参数 【使用示例】 配置系统服务 123# 在命令行直接输入ntsysv，进入交互式菜单界面[root@C7-Server01 ~]# ntsysv 进入后，就可以通过键盘的上下左右键和tab键选择你需要配置的开机启动服务，如果需要确认某个选项，按空格键完成，取消确认按两次空格键即可。 rpm：RPM包管理器rpm命令的全称是Red Hat Package Manager（Red Hat包管理器），几乎所有的Linux发行版本都使用了这种形式的命令管理、安装、更新和卸载软件。概括地说，rpm命令包含了五种基本功能（不包括创建rpm包）：安装、卸载、升级、查询和验证。 语法格式：rpm [option] 重要选项参数 1234567891011# 下载一个软件包作为测试[root@C7-Server01 ~]# wget https://mirrors.aliyun.com/centos/6/os/x86_64/Packages/lrzsz-0.12.20-27.1.el6.x86_64.rpm--2019-05-05 00:35:39-- https://mirrors.aliyun.com/centos/6/os/x86_64/Packages/lrzsz-0.12.20-27.1.el6.x86_64.rpmResolving mirrors.aliyun.com (mirrors.aliyun.com)... 112.19.3.240, 112.19.3.241, 112.19.3.182, ...Connecting to mirrors.aliyun.com (mirrors.aliyun.com)|112.19.3.240|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 72436 (71K) [application/x-redhat-package-manager]Saving to: ‘lrzsz-0.12.20-27.1.el6.x86_64.rpm’100%[=====================================================&gt;] 72,436 --.-K/s in 0.07s 2019-05-05 00:35:42 (974 KB/s) - ‘lrzsz-0.12.20-27.1.el6.x86_64.rpm’ saved [72436/72436] 【使用示例】 1）查看rpm包信息 1234567891011121314151617181920212223242526272829303132# 显示rpm包概要信息# 使用-q选项查询软件包# 使用-p选项指定某一个rpm包# 使用-i选线关于-qp联用，表示查看某个rpm包的概要信息[root@C7-Server01 ~]# rpm -qpi lrzsz-0.12.20-27.1.el6.x86_64.rpm warning: lrzsz-0.12.20-27.1.el6.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID c105b9de: NOKEYName : lrzszVersion : 0.12.20Release : 27.1.el6Architecture: x86_64Install Date: (not installed)Group : Applications/CommunicationsSize : 162901License : GPLv2+Signature : RSA/SHA256, Sun 03 Jul 2011 12:43:30 PM CST, Key ID 0946fca2c105b9deSource RPM : lrzsz-0.12.20-27.1.el6.src.rpmBuild Date : Thu 19 Aug 2010 02:20:40 PM CSTBuild Host : c6b3.bsys.dev.centos.orgRelocations : (not relocatable)Packager : CentOS BuildSystem Vendor : CentOSURL : http://www.ohse.de/uwe/software/lrzsz.htmlSummary : The lrz and lsz modem communications programsDescription :Lrzsz (consisting of lrz and lsz) is a cosmetically modifiedzmodem/ymodem/xmodem package built from the public-domain version ofthe rzsz package. Lrzsz was created to provide a working GNUcopylefted Zmodem solution for Linux systems. 2）查询软件包内容 1234567891011121314151617# 使用-q选项表示查询软件包# 使用-p选项表示查询某一个rpm包# 使用-l选项表示查看某一个rpm包里的软件列表[root@C7-Server01 ~]# rpm -qpl lrzsz-0.12.20-27.1.el6.x86_64.rpm warning: lrzsz-0.12.20-27.1.el6.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID c105b9de: NOKEY/usr/bin/rb/usr/bin/rx/usr/bin/rz/usr/bin/sb/usr/bin/sx/usr/bin/sz/usr/share/locale/de/LC_MESSAGES/lrzsz.mo/usr/share/man/man1/rz.1.gz/usr/share/man/man1/sz.1.gz 3）查询软件包的依赖 12345678910111213141516171819202122# 使用-q选项表示查询软件包# 使用-p选项表示查询某一个rpm包# 使用-R选项表示查看某一个rpm包依赖关系[root@C7-Server01 ~]# rpm -qpR lrzsz-0.12.20-27.1.el6.x86_64.rpm warning: lrzsz-0.12.20-27.1.el6.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID c105b9de: NOKEYlibc.so.6()(64bit)libc.so.6(GLIBC_2.11)(64bit)libc.so.6(GLIBC_2.2.5)(64bit)libc.so.6(GLIBC_2.3)(64bit)libc.so.6(GLIBC_2.3.4)(64bit)libc.so.6(GLIBC_2.4)(64bit)libc.so.6(GLIBC_2.7)(64bit)libnsl.so.1()(64bit)rpmlib(CompressedFileNames) &lt;= 3.0.4-1rpmlib(FileDigests) &lt;= 4.6.0-1rpmlib(PartialHardlinkSets) &lt;= 4.0.4-1rpmlib(PayloadFilesHavePrefix) &lt;= 4.0-1rtld(GNU_HASH)rpmlib(PayloadIsXz) &lt;= 5.2-1 4）安装软件包 1234567891011121314151617# 单独使用-i选项，表示安装某一个rpm包# 使用-v选项，表示安装时显示详细信息# 使用-h选项，表示用‘#’符号显示安装进度条[root@C7-Server01 ~]# rpm -ivh lrzsz-0.12.20-27.1.el6.x86_64.rpm warning: lrzsz-0.12.20-27.1.el6.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID c105b9de: NOKEYPreparing... ################################# [100%] package lrzsz-0.12.20-36.el7.x86_64 (which is newer than lrzsz-0.12.20-27.1.el6.x86_64) is already installed file /usr/bin/rb from install of lrzsz-0.12.20-27.1.el6.x86_64 conflicts with file from package lrzsz-0.12.20-36.el7.x86_64 file /usr/bin/rx from install of lrzsz-0.12.20-27.1.el6.x86_64 conflicts with file from package lrzsz-0.12.20-36.el7.x86_64 file /usr/bin/rz from install of lrzsz-0.12.20-27.1.el6.x86_64 conflicts with file from package lrzsz-0.12.20-36.el7.x86_64 file /usr/bin/sb from install of lrzsz-0.12.20-27.1.el6.x86_64 conflicts with file from package lrzsz-0.12.20-36.el7.x86_64 file /usr/bin/sx from install of lrzsz-0.12.20-27.1.el6.x86_64 conflicts with file from package lrzsz-0.12.20-36.el7.x86_64 file /usr/bin/sz from install of lrzsz-0.12.20-27.1.el6.x86_64 conflicts with file from package lrzsz-0.12.20-36.el7.x86_64 file /usr/share/man/man1/sz.1.gz from install of lrzsz-0.12.20-27.1.el6.x86_64 conflicts with file from package lrzsz-0.12.20-36.el7.x86_64 注意：我的机器因为已经安装过lrzsz工具，所以有上面的提示。 5）查询系统是否已经安装某个软件包 12345678# 使用-q选项，表示查询软件包# 使用-a选项，表示查询系统是否已经安装某个软件包# 如果没有输出，表示没有安装，如果有输出表示已经安装[root@C7-Server01 ~]# rpm -qa lrzszlrzsz-0.12.20-36.el7.x86_64 6）卸载软件包 123# 使用-e选项，可以卸载指定的rpm包[root@C7-Server01 ~]# rpm -e lrzsz 注意：这个参数比较危险，因为通过rpm是强依赖卸载，因为很有可能会误删除一些系统必备的文件，最后导致系统损坏。如果非要卸载，可以通过yum工具完成。 7）插叙文件属于哪个软件包 123456# 使用-q选项，表示查询软件包# 使用-f选项，表示查询文件属于哪个软件包[root@C7-Server01 ~]# rpm -qf $(which ifconfig)net-tools-2.0-0.24.20131004git.el7.x86_64 有时候会发现系统没有某些文件或者命令，但是又不知道这个文件或命令是属于哪个软件包，这时就可以使用-f参数来查询（在有这个文件的系统上查询）。比如本例查询ifconfig命令属于net-tools软件包。 yum：自动化RPM包管理工具yum（Yellow dog Updater Modified）是多个Linux发行版的软件包管理器，例如Redhat RHEL、CentOS和Fedora。yum主要用于自动安装、升级rpm软件包，它能自自动查找并解决rpm包之间的依赖关系。如下图： 通过执行yum install -y lrzsz命令后，它会自动发现系统老版本的软件包，然后进行更新，更新后会自动删除老版本的软件包。 语法格式：yum [option] [command] [package] 重要选线参数 【使用示例】 1）安装httpd软件包 123456789101112# 使用-y选项，自动确认，无需交互式手动 [root@C7-Server01 ~]# yum install -y httpdLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile - base: mirrors.aliyun.com- epel: mirrors.njupt.edu.cn- extras: mirrors.huaweicloud.com- updates: mirrors.huaweicloud.comPackage httpd-2.4.6-89.el7.centos.x86_64 already installed and latest versionNothing to do ​ 2）查看已安装和未安装的包组 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 使用grouplist命令，可以查看所有包组情况[root@C7-Server01 ~]# yum grouplistLoaded plugins: fastestmirror There is no installed groups file. Maybe run: yum groups mark convert (see man yum) Loading mirror speeds from cached hostfile - base: mirrors.aliyun.com - epel: mirrors.njupt.edu.cn - extras: mirrors.huaweicloud.com - updates: mirrors.huaweicloud.com Available Environment Groups: Minimal Install Compute Node Infrastructure Server File and Print Server Cinnamon Desktop MATE Desktop Basic Web Server Virtualization Host Server with GUI GNOME Desktop KDE Plasma Workspaces Development and Creative Workstation Available Groups: Cinnamon Compatibility Libraries Console Internet Tools Development Tools Educational Software Electronic Lab Fedora Packager General Purpose Desktop Graphical Administration Tools Haskell Legacy UNIX Compatibility MATE Milkymist Scientific Support Security Tools Smart Card Support System Administration Tools System Management TurboGears application framework Xfce Done - base: mirrors.aliyun.com - epel: mirrors.njupt.edu.cn - extras: mirrors.huaweicloud.com - updates: mirrors.huaweicloud.com Available Environment Groups: Minimal Install Compute Node Infrastructure Server File and Print Server Cinnamon Desktop MATE Desktop Basic Web Server Virtualization Host Server with GUI GNOME Desktop KDE Plasma Workspaces Development and Creative Workstation Available Groups: Cinnamon Compatibility Libraries Console Internet Tools Development Tools Educational Software Electronic Lab Fedora Packager General Purpose Desktop Graphical Administration Tools Haskell Legacy UNIX Compatibility MATE Milkymist Scientific Support Security Tools Smart Card Support System Administration Tools System Management TurboGears application framework Xfce Done 注意：此例只是示例，只是表示可以这样使用，并不代表真的存在相关描述的软件包。因为从包组中查找安装软件包非常少用，几乎用不到。如果想安装某个软件包，可以使用yum list | grep “软件包名或通配”来完成。 至此，Linux常用的系统管理命令已经更新完毕，其实，在Linux系统管理中最常用的还是一些管理工具，会在Linux常用工具分类中系统管理工具有总体介绍。 随着Linux系统管理命令更新结束，Linux系统核心命令部分全部更新完毕，共分为10篇，约70-80个常用命令，与Linux系统总体命令600余个相比还差很远。但是，这些常用命令以及后续介绍常用工具是一个运维人员必须掌握的，至于其它命令和工具在实际中碰到了再查询使用帮助即可。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-25-Linux系统命令-第九篇《网络管理命令》]]></title>
    <url>%2F2019%2F05%2F25%2F2019-05-25-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%B9%9D%E7%AF%87%E3%80%8A%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[ifconfig：配置或显示网络接口信息ifconfig命令用于配置网卡IP地址等网络参数或显示当前网络的接口状态，其类似于Windows下的ipconfig命令，这两个命令很容易混淆，读者需要区分一下。此外，ifconfig命令在配置网卡信息时必须以root用户的身份来执行。使用ifconfig命令配置网卡信息仅会临时生效，重启网络或服务器配置就会失效。要想配置永久生效，正确的姿势是“编辑配置网卡配置文件”。 如果系统中没有ifconfig命令，那就需要安装一下，安装命令为yum-y install net-tools。 interface为网络接口名，Linux下的网络接口名类似于eth0、eth1和lo等，分别表示第1块网卡、第2块网卡和回环接口。这是个可选项，如果不添加此选项，则显示系统中所有的网卡信息；如果添加此选项，则显示指定的网卡信息。 语法格式：ifconfig [interface] [option] 重要参数选项 【使用示例】 1）显示当前系统开启的所有网络接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 使用ifconfig不带任何选项显示所有活动的网络接口[root@C7-Server01 ~]# ifconfigdocker0: flags=4163 mtu 1500inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255inet6 fe80::42:56ff:fe71:5647 prefixlen 64 scopeid 0x20ether 02:42:56:71:56:47 txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 8 bytes 648 (648.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eth0: flags=4163 mtu 1500inet 192.168.101.81 netmask 255.255.255.0 broadcast 192.168.101.255inet6 fe80::20c:29ff:fe17:f09e prefixlen 64 scopeid 0x20ether 00:0c:29:17:f0:9e txqueuelen 1000 (Ethernet)RX packets 84 bytes 9437 (9.2 KiB)RX errors 0 dropped 0 overruns 0 frame 0TX packets 112 bytes 11835 (11.5 KiB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eth1: flags=4163 mtu 1500inet 172.0.2.81 netmask 255.255.255.0 broadcast 172.0.2.255inet6 fe80::20c:29ff:fe17:f0a8 prefixlen 64 scopeid 0x20ether 00:0c:29:17:f0:a8 txqueuelen 1000 (Ethernet)RX packets 1 bytes 243 (243.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 14 bytes 1008 (1008.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eth2: flags=4163 mtu 1500inet 10.0.5.81 netmask 255.255.255.0 broadcast 10.0.5.255inet6 fe80::20c:29ff:fe17:f0b2 prefixlen 64 scopeid 0x20ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet)RX packets 1 bytes 243 (243.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 14 bytes 1008 (1008.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73 mtu 65536inet 127.0.0.1 netmask 255.0.0.0inet6 ::1 prefixlen 128 scopeid 0x10loop txqueuelen 1000 (Local Loopback)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 0 bytes 0 (0.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0veth1f3cf35: flags=4163 mtu 1500inet6 fe80::483c:5fff:fe03:30fa prefixlen 64 scopeid 0x20ether 4a:3c:5f:03:30:fa txqueuelen 0 (Ethernet)RX packets 0 bytes 0 (0.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 16 bytes 1296 (1.2 KiB)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 2）显示指定网卡信息 1234567891011# 显示网卡eth2的信息[root@C7-Server01 ~]# ifconfig eth2eth2: flags=4163 mtu 1500inet 10.0.5.81 netmask 255.255.255.0 broadcast 10.0.5.255inet6 fe80::20c:29ff:fe17:f0b2 prefixlen 64 scopeid 0x20ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet)RX packets 2 bytes 501 (501.0 B)RX errors 0 dropped 0 overruns 0 frame 0TX packets 14 bytes 1008 (1008.0 B)TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 3）启动/关闭指定网卡 1234567891011121314151617181920212223242526272829# 关闭eth2网卡[root@C7-Server01 ~]# ifconfig eth2 down# 使用-a选项查询关闭状态下的网卡信息# 网卡flags没有UP项，表示网卡关闭[root@C7-Server01 ~]# ifconfig -a eth2eth2: flags=4098 mtu 1500 inet 10.0.5.81 netmask 255.255.255.0 broadcast 10.0.5.255 ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet) RX packets 12 bytes 1572 (1.5 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14 bytes 1008 (1008.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 再次启动网卡eth2[root@C7-Server01 ~]# ifconfig eth2 up[root@C7-Server01 ~]# ifconfig -a eth2eth2: flags=4163 mtu 1500 inet 10.0.5.81 netmask 255.255.255.0 broadcast 10.0.5.255 inet6 fe80::20c:29ff:fe17:f0b2 prefixlen 64 scopeid 0x20 ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet) RX packets 12 bytes 1572 (1.5 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 21 bytes 1586 (1.5 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 4）配置网卡ip地址 1234567891011121314151617181920212223242526# 查询网卡eth2的ip地址信息# 显示eth2网卡的ip为10.0.5.81[root@C7-Server01 ~]# ifconfig eth2eth2: flags=4163 mtu 1500 inet 10.0.5.81 netmask 255.255.255.0 broadcast 10.0.5.255 inet6 fe80::20c:29ff:fe17:f0b2 prefixlen 64 scopeid 0x20 ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet) RX packets 13 bytes 1830 (1.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 22 bytes 1656 (1.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 修改eth2网卡的ip地址为10.0.5.181[root@C7-Server01 ~]# ifconfig eth2 10.0.5.181[root@C7-Server01 ~]# ifconfig eth2eth2: flags=4163 mtu 1500 inet 10.0.5.181 netmask 255.0.0.0 broadcast 10.255.255.255 inet6 fe80::20c:29ff:fe17:f0b2 prefixlen 64 scopeid 0x20 ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet) RX packets 13 bytes 1830 (1.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 22 bytes 1656 (1.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 在实际运维中，除非测试环境，否则不建议这样修改网卡ip地址，因为会引起生产环境网络连接异常。如果要增加对应ip，可以给对应网卡增加子接口的方式实现，见下面示例。 5）添加网卡子接口，并配置ip地址 1234567# 网卡子接口的格式eth2:0、eth2:1.。。。[root@C7-Server01 ~]# ifconfig eth2:0 192.168.101.181/24 up[root@C7-Server01 ~]# ifconfig eth2:0eth2:0: flags=4163 mtu 1500inet 192.168.101.181 netmask 255.255.255.0 broadcast 192.168.101.255ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet) 6）修改网卡MAC地址 123456789101112131415161718192021222324# 查询eth2的MAC地址为ether 00:0c:29:17:f0:b2[root@C7-Server01 ~]# ifconfig eth2eth2: flags=4163 mtu 1500 inet 10.0.5.181 netmask 255.0.0.0 broadcast 10.255.255.255 inet6 fe80::20c:29ff:fe17:f0b2 prefixlen 64 scopeid 0x20 ether 00:0c:29:17:f0:b2 txqueuelen 1000 (Ethernet) RX packets 22 bytes 4077 (3.9 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 22 bytes 1656 (1.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 使用hw选项和网卡类型关键ether修改MAC地址[root@C7-Server01 ~]# ifconfig eth2 hw ether 00:AA:BB:CC:DD:EE[root@C7-Server01 ~]# ifconfig eth2eth2: flags=4163 mtu 1500 inet 10.0.5.181 netmask 255.0.0.0 broadcast 10.255.255.255 inet6 fe80::20c:29ff:fe17:f0b2 prefixlen 64 scopeid 0x20 ether 00:aa:bb:cc:dd:ee txqueuelen 1000 (Ethernet) RX packets 23 bytes 4320 (4.2 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 22 bytes 1656 (1.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 总结：上述修改在服务器重启后就失效了，要想永久生效，只能修改网卡的配置文件。CentOS系统的网卡配置文件在/etc/sysconfig/network-scripts/下，Ubuntu系统在/etc/network/interfaces中。 ifup：激活网络接口ifup命令用于激活指定的网络接口。ifup命令其实是一个Shell脚本，有Shell基础的读者可以使用which命令来找到这个脚本并读一读。ifup命令可读取配置文件/etc/sysconfig/network和/etc/sysconfig/network-scripts/ifcfg-对网络接口进行相应的操作。 语法格式：ifup [iface] 【使用示例】 激活网络接口 12345678910111213141516171819202122# 用ifconfig down关闭网络接口eth2，再使用ifup激活[root@C7-Server01 ~]# ifconfig eth2 down[root@C7-Server01 ~]# ifconfig eth2eth2: flags=4098 mtu 1500 inet 10.0.5.181 netmask 255.0.0.0 broadcast 10.255.255.255 ether 00:aa:bb:cc:dd:ee txqueuelen 1000 (Ethernet) RX packets 23 bytes 4320 (4.2 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 22 bytes 1656 (1.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0[root@C7-Server01 ~]# ifup eth2[root@C7-Server01 ~]# ifconfig eth2eth2: flags=4163 mtu 1500 inet 10.0.5.181 netmask 255.0.0.0 broadcast 10.255.255.255 inet6 fe80::2aa:bbff:fecc:ddee prefixlen 64 scopeid 0x20 ether 00:aa:bb:cc:dd:ee txqueuelen 1000 (Ethernet) RX packets 23 bytes 4320 (4.2 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 35 bytes 2634 (2.5 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 除了激活网卡用ifup外，还有个关闭网卡指令ifdown，用法和ifup类似，请大家自行练习。 route：显示或管理路由表route命令可以显示或管理Linux系统的路由表，route命令设置的路由主要是静态路由。Linux上配置的路由都属于静态路由。静态路由规则是系统管理员使用route命令加入的，也就是通过手动输入的方式来加入的路由规则。动态路由无需手动配置，其路由规则是本机与不同的机器彼此经过路由程序（Routing daemon）相互交换路由规则而来的。 语法格式：route [option] 重要选项参数 【使用示例】 1）查看当前系统的路由表信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 直接使用route命令就可以查看当前系统的路由表信息[root@C7-Server01 ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault gateway 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth210.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2link-local 0.0.0.0 255.255.0.0 U 1002 0 0 eth0link-local 0.0.0.0 255.255.0.0 U 1003 0 0 eth1link-local 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0# 使用-ee选项可以查看更详细的路由表信息[root@C7-Server01 ~]# route -eeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface MSS Window irttdefault gateway 0.0.0.0 UG 0 0 0 eth0 0 0 010.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth2 0 0 010.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2 0 0 0link-local 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 0 0 0link-local 0.0.0.0 255.255.0.0 U 1003 0 0 eth1 0 0 0link-local 0.0.0.0 255.255.0.0 U 1004 0 0 eth2 0 0 0172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1 0 0 0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker 0 0 0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 0 0 0# 使用-n选项可以不进行DNS解析，这样会加快显示[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.101.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth210.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 命令结果说明如下： Destination：表示网络号，也就是network的意思。 Gateway：连出网关地址，也就是说该网络是通过该IP连接出去的，如果显示0.0.0.0，则表示该路由是直接由本机传送出去的。如果有IP显示，则表示本条路由必须经过该IP的转接才能连接出去。 Genmask：表示子网掩码地址，也就是netmask。Destination和Genmask将组合成一个完整的网络地址段。 Flags：路由标记信息，通常会有下面几种不同的标记： ​ U（route is up）：表示此路由当前为启动状态。 ​ H（target is a host）：目标路由是一个主机（IP）而非网络。 ​ R（reinstate route for dynamic routing）：使用动态路由时，恢复路由信息标识。 ​ G（use gateway）：表示需要通过外部的主机（gateway）来转接传递数据。 ​ M（modified from routing daemon or redirect）：表示路由已经被修改了。 ​ D（dynamically installed by daemon or redirect）：已经由服务设定为动态路由。 ​ ！（reject route）：这个路由将不会被接受（用来抵挡不安全的网络）。 Metric：需要经过几个网络节点（hops）才能到达路由的目标网络地址。 Ref：学习到此路由规则的数目。 Use：有几个转发数据包学习到了此路由规则。 Iface：路由对应的网络设备接口。 命令输出结果的第一行是系统的默认网关信息，表示去任何地方（0.0.0.0）都发给192.168.101.2，因为是默认网关，所以放在了第一条，如果不符合任何一条规则就交给默认网关来处理。 2）删除和添加默认网关 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 显示路由信息[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.101.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth210.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0# 第一种方式删除默认网关[root@C7-Server01 ~]# route del default[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth210.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0# 第一种方式添加默认网关[root@C7-Server01 ~]# route add default gw 192.168.101.2[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.101.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth210.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0# 第二种方式删除默认网关[root@C7-Server01 ~]# route del default gw 192.168.101.2[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth210.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0# 第二种方式添加默认网关[root@C7-Server01 ~]# route add default gw 192.168.101.2 dev eth0[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.101.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 eth210.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 3）配置网络路由 一般多网段之间互相通信，会希望建立一条优先路由，而不是通过默认网关，这时就可以配置网络路由。还是拿房子作比喻，你现在不是要出门，而是要去卧室或卫生间，去卧室就要经过卧室的门，去卫生间就要经过卫生间的门，这里的卧室和卫生间的门就可以认为是去往某一网段的路由，而不是默认路由（即房子的大门）。在实际工作中也会有类似的需求，即两个不同的内部网络之间互相访问，而不是出网访问。 由于我们是VMware虚拟机实验环境，因此，我们主要验证虚拟机主机之间互通。主机的网段是192.168.100.0/24，虚拟机网段是192.168.101.0/24。在虚拟机安装完成后，天生就具备和主机进行互通，这是因为VMware虚拟网卡的网关功能实现。为了验证我们的配置，首先需要删除虚拟机上的默认网关，然后添加到主机的静态路由来验证。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 不删除默认网关，虚拟机可以ping主机（我的主机地址是192.168.100.199）[root@C7-Server01 ~]# ping -c 5 192.168.100.199PING 192.168.100.199 (192.168.100.199) 56(84) bytes of data.64 bytes from 192.168.100.199: icmp_seq=1 ttl=128 time=0.277 ms64 bytes from 192.168.100.199: icmp_seq=2 ttl=128 time=0.566 ms64 bytes from 192.168.100.199: icmp_seq=3 ttl=128 time=0.497 ms64 bytes from 192.168.100.199: icmp_seq=4 ttl=128 time=2.19 ms64 bytes from 192.168.100.199: icmp_seq=5 ttl=128 time=0.524 ms--- 192.168.100.199 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4004msrtt min/avg/max/mdev = 0.277/0.811/2.193/0.698 ms# 删除虚拟机默认路由后再次尝试ping主机[root@C7-Server01 ~]# route del default gw 192.168.101.2[root@C7-Server01 ~]# ping -c 5 192.168.100.199connect: Network is unreachable# 添加到主机网段的静态路由[root@C7-Server01 ~]# route add -net 192.168.100.0/24 gw 192.168.101.2[root@C7-Server01 ~]# ping -c 5 192.168.100.199PING 192.168.100.199 (192.168.100.199) 56(84) bytes of data.64 bytes from 192.168.100.199: icmp_seq=1 ttl=128 time=0.736 ms64 bytes from 192.168.100.199: icmp_seq=2 ttl=128 time=0.466 ms64 bytes from 192.168.100.199: icmp_seq=3 ttl=128 time=0.690 ms64 bytes from 192.168.100.199: icmp_seq=4 ttl=128 time=0.416 ms64 bytes from 192.168.100.199: icmp_seq=5 ttl=128 time=0.467 ms--- 192.168.100.199 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4002msrtt min/avg/max/mdev = 0.416/0.555/0.736/0.131 ms# 查询路右边，发现多了一条道192.168.100.0/24网段的静态路由，从eth0网口转发[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.100.0 192.168.101.2 255.255.255.0 UG 0 0 0 eth0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 上述的路由配置只是临时的，要想永久生效可以有多种方法，但是建议大家采用如下方式实现： 12345678910111213141516171819202122232425# 新建一个路由配置文件route-eth0[root@C7-Server01 ~]# touch /etc/sysconfig/network-scripts/route-eth0# 添加如下内容到配置文件中[root@C7-Server01 ~]# echo "192.168.100.0/24 via 192.168.101.2" &gt;&gt; /etc/sysconfig/network-scripts/route-eth0[root@C7-Server01 ~]# cat /etc/sysconfig/network-scripts/route-eth0 192.168.100.0/24 via 192.168.101.2# 重启网卡或重启系统后永久生效[root@C7-Server01 ~]# systemctl restart network[root@C7-Server01 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.101.2 0.0.0.0 UG 0 0 0 eth010.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1004 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.100.0 192.168.101.2 255.255.255.0 UG 0 0 0 eth0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 4）配置/删除到主机的路由 1234567891011# 添加到C7-Server02主机（192.168.101.82）的静态路由，由eth2网口转发[root@C7-Server01 ~]# route add -host 192.168.101.82 dev eth2[root@C7-Server01 ~]# route -n|tail -1192.168.101.82 0.0.0.0 255.255.255.255 UH 0 0 0 eth2# 尝试trace，查看配置是否生效[root@C7-Server01 ~]# traceroute -In 192.168.101.82traceroute to 192.168.101.82 (192.168.101.82), 30 hops max, 60 byte packets 1 192.168.101.82 0.434 ms 0.484 ms 0.498 ms 在keepalived或HA高可用服务器对之间使用单独的网卡接心跳线通信时，就会用到以上的主机路由。 arp：管理系统的arp缓存arp命令用于操作本机的arp缓存区，它可以显示arp缓存区中的所有条目、删除指定的条目或者添加静态的IP地址与MAC地址的对应关系。APR主要功能是根据IP地址获取物理地址（MAC地址）。 语法格式：arp [option] 重要参数选项 【使用示例】 1）显示arp缓存区的所有条目 123456789101112131415[root@C7-Server01 ~]# arpAddress HWtype HWaddress Flags Mask Iface192.168.101.82 ether 00:0c:29:d3:54:c1 C eth2gateway ether 00:50:56:f8:93:68 C eth0192.168.101.1 ether 00:50:56:c0:00:08 C eth010.0.5.82 ether 00:0c:29:d3:54:c1 C eth2# 使用-n选项，以数字ip的形式显示缓存区所有条目[root@C7-Server01 ~]# arp -nAddress HWtype HWaddress Flags Mask Iface192.168.101.82 ether 00:0c:29:d3:54:c1 C eth2192.168.101.2 ether 00:50:56:f8:93:68 C eth0192.168.101.1 ether 00:50:56:c0:00:08 C eth010.0.5.82 ether 00:0c:29:d3:54:c1 C eth2 输出结果说明： Address：主机地址。 Hwtype：硬件类型。 Hwaddress：硬件地址。 Flags Mask：记录标志，“C”表示arp高速缓存中的条目，“M”表示静态的arp条目。 Iface：网络接口。 2）绑定静态ip地址和MAC地址 1234567891011121314151617181920# 使用-s选项可以绑定主机ip和固定mac地址的映射关系[root@C7-Server01 ~]# arp -s 192.168.101.254 00:AA:BB:CC:DD:EE[root@C7-Server01 ~]# arp -nAddress HWtype HWaddress Flags Mask Iface192.168.101.82 ether 00:0c:29:d3:54:c1 C eth2192.168.101.2 ether 00:50:56:f8:93:68 C eth0192.168.101.254 ether 00:aa:bb:cc:dd:ee CM eth0192.168.101.1 ether 00:50:56:c0:00:08 C eth010.0.5.82 ether 00:0c:29:d3:54:c1 C eth2# 删除绑定关系可以使用选项-d[root@C7-Server01 ~]# arp -d 192.168.101.254[root@C7-Server01 ~]# arp -nAddress HWtype HWaddress Flags Mask Iface192.168.101.82 ether 00:0c:29:d3:54:c1 C eth2192.168.101.2 ether 00:50:56:f8:93:68 C eth0192.168.101.1 ether 00:50:56:c0:00:08 C eth010.0.5.82 ether 00:0c:29:d3:54:c1 C eth2 当局域网有arp病毒时，就可以用上述方法绑定MAC地址，以防止中毒。 netstat：查看网络状态netstat命令用于显示本机网络的连接状态、运行端口和路由表等信息。 语法格式：netstat [option] 重要选项参数 【使用示例】 1）常用选项组合 1234567891011121314151617181920212223242526# -an选项组合以ip地址的形式显示所有socket的监听信息[root@C7-Server01 ~]# netstat -anActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN tcp 0 52 192.168.101.81:22 192.168.101.1:6481 ESTABLISHEDtcp6 0 0 :::4000 :::* LISTEN tcp6 0 0 :::80 :::* LISTEN tcp6 0 0 :::22 :::* LISTEN tcp6 0 0 ::1:25 :::* LISTEN udp 0 0 127.0.0.1:323 0.0.0.0:* udp6 0 0 ::1:323 :::* Active UNIX domain sockets (servers and established)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ACC ] STREAM LISTENING 19221 /var/run/vmware/guestServicePipeunix 2 [ ACC ] STREAM LISTENING 27162 /var/run/docker.sockunix 3 [ ] DGRAM 31 /run/systemd/notifyunix 2 [ ] DGRAM 33 /run/systemd/cgroups-agentunix 2 [ ACC ] STREAM LISTENING 22569 /var/run/docker.sockunix 2 [ ACC ] STREAM LISTENING 41 /run/systemd/journal/stdoutunix 2 [ ACC ] STREAM LISTENING 22571 /run/dbus/system_bus_socketunix 6 [ ] DGRAM 44 /run/systemd/journal/socketunix 12 [ ] DGRAM 46 /dev/log。。。 12345678910111213# 使用-lntup选项组合，查看所有监听连接的端口和进程PID[root@C7-Server01 ~]# netstat -lntupActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1038/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1310/master tcp6 0 0 :::4000 :::* LISTEN 1648/docker-proxy tcp6 0 0 :::80 :::* LISTEN 1042/httpd tcp6 0 0 :::22 :::* LISTEN 1038/sshd tcp6 0 0 ::1:25 :::* LISTEN 1310/master udp 0 0 127.0.0.1:323 0.0.0.0:* 645/chronyd udp6 0 0 ::1:323 :::* 输出结果说明如下： Active Internet connections (servers and established) ：表示活动的TCP/IP网络连接。 Active UNIX domain sockets (servers and established)：表示活动的unix socket连接。 Proto：表示socket使用的协议（TCP、UDP、RAW） Recv-Q：接收到但还未处理的字节数。 Send-Q：已经发送，但是还未收到远程主机响应的自己数。 Local Address：本地主机地址和端口信息。 Foreign Address：远程主机的地址和端口信息。 State：socket的状态，通常仅有TCP的状态信息，状态值有ESTABBLISHED、SYN_SENT、SYN_RECV、FIN_WAIT、FIN_WAIT2、TIME_WAIT等。 2）显示当前系统的路由信息 123456789101112131415# 使用-r选项显示系统路由表信息，使用-n选项显示ip地址，不进行DNS解析[root@C7-Server01 ~]# netstat -rnKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface0.0.0.0 192.168.101.2 0.0.0.0 UG 0 0 0 eth010.0.5.0 0.0.0.0 255.255.255.0 U 0 0 0 eth2169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth2172.0.2.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.100.0 192.168.101.2 255.255.255.0 UG 0 0 0 eth0192.168.101.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0192.168.101.82 0.0.0.0 255.255.255.255 UH 0 0 0 eth2 3）显示网卡状态 1234567891011# 使用-i选项显示网络接口状态信息[root@C7-Server01 ~]# netstat -iKernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgdocker0 1500 0 0 0 0 8 0 0 0 BMRUeth0 1500 8480 0 0 0 4404 0 0 0 BMRUeth1 1500 24 0 0 0 54 0 0 0 BMRUeth2 1500 41 0 0 0 60 0 0 0 BMRUlo 65536 10 0 0 0 10 0 0 0 LRUvethe5480a8 1500 0 0 0 0 16 0 0 0 BMRU 输出结果说明如下： Iface：表示网络设备的接口名称。 MTU：表示最大传输单元，单位为字节。 RX-OK/TX-OK：表示已经准确无误地接收/发送了多少数据包。 RX-ERR/TX-ERR：表示接收/发送数据包时产生了多少错误。 RX-DRP/TX-DRP：表示接收/发送数据包时丢弃了多少数据包。 RX-OVR/TX-OVR：表示由于误差而遗失了多少数据包。 Flg：表示接口标记，其中各标记含义具体如下。 ​ L：表示该接口是个回环设备。 ​ B：表示设置了广播地址。 ​ M：表示接收所有数据包。 ​ R：表示接口正在运行。 ​ U：表示接口处于活动状态。 ​ O：表示在该接口上禁用arp。 ​ P：表示一个点到点的连接。 正常情况下，RX-ERR/TX-ERR、RX-DRP/TX-DRP和RX-OVR/TX-OVR的值都应该为0，如果这几个选项的值不为0，并且很大，那么网络质量肯定有问题，网络传输性能也一定会下降。 ping：测试主机之间网络的连通性ping命令可用于测试主机之间网络的连通性。执行ping命令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而可得知该主机运作正常。但是，如果ping不通，不一定是网络问题，也可能是远端主机设置了禁ping功能，就是收到ICMP包不会回应。 语法格式：ping [option] [destination] 重要参数选项 【使用示例】 1）测试目标主机的网络连通性 123456789101112131415161718192021222324252627# 测试openstack官网的连通性[root@C7-Server01 ~]# ping www.openstack.orgPING www.openstack.org.cdn.cloudflare.net (104.20.111.33) 56(84) bytes of data.64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=1 ttl=128 time=41.8 ms64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=2 ttl=128 time=40.7 ms64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=3 ttl=128 time=41.0 ms64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=6 ttl=128 time=49.0 ms64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=7 ttl=128 time=40.2 ms64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=8 ttl=128 time=40.9 ms64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=9 ttl=128 time=40.9 ms64 bytes from 104.20.111.33 (104.20.111.33): icmp_seq=10 ttl=128 time=41.7 ms。。。。# 此时不使用ctrl+c就会一直ping下去# 使用-c选项，设定ping的次数[root@C7-Server01 ~]# ping -c 3 www.openstack.orgPING www.openstack.org.cdn.cloudflare.net (104.20.110.33) 56(84) bytes of data.64 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=1 ttl=128 time=63.3 ms64 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=2 ttl=128 time=54.0 ms64 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=3 ttl=128 time=54.0 ms--- www.openstack.org.cdn.cloudflare.net ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 5517msrtt min/avg/max/mdev = 54.017/57.164/63.387/4.404 ms ping命令会显示一个时间作为衡量网络延迟的参数，以判断源主机与目标主机之间网络的质量。ping命令的输出信息中含有TTL值。TTL（Time To Life）称为生存期，它是ICMP报文在网络上的存活时间。不同的操作系统发出的ICMP报文的生存期各不相同，常见的生存期为32、64、128和255等。TTL值反映了ICMP报文所能够经过的路由器数目，每经过一个路由器，路由器都会将其数据包的生存期减去1，如果TTL值变为0，则路由器将不再转发此报文。 2）用1k的大包测试 123456789101112# 使用-s选项可以这是ICMP包的大小[root@C7-Server01 ~]# ping -c 5 -s 1024 www.openstack.orgPING www.openstack.org.cdn.cloudflare.net (104.20.110.33) 1024(1052) bytes of data.1032 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=1 ttl=128 time=55.2 ms1032 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=2 ttl=128 time=55.6 ms1032 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=3 ttl=128 time=55.2 ms1032 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=4 ttl=128 time=55.0 ms1032 bytes from 104.20.110.33 (104.20.110.33): icmp_seq=5 ttl=128 time=63.0 ms--- www.openstack.org.cdn.cloudflare.net ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 7575msrtt min/avg/max/mdev = 55.084/56.875/63.085/3.122 ms telnet：远程登录主机telnet命令以前是用于登录远程主机，对远程主机进行管理的。但是因为telnet是采用明文传送报文的，其安全性不好，因此现在很多Linux服务器都不开放telnet服务，而是改用更安全的SSH服务了。现在，只有在交换机等网络设备还可能呢会采用telnet登录的方式。但是，使用telnet命令还可以判断远端服务器的端口是否开放，这是目前telnet命令使用最主要场景。 语法格式：telnet [option] [host] [port] 【使用示例】 测试SSH服务端口是否开放 1234567891011121314151617181920# 测试192.168.101.83的ssh服务是否开放[root@C7-Server01 ~]# telnet 192.168.101.83 22 # 22端口就是ssh服务的端口Trying 192.168.101.83...Connected to 192.168.101.83.Escape character is '^]'.SSH-2.0-OpenSSH_7.4 # &lt;==看到这种结果，就证明SSH服务的22端口已经开放了。# 此时命令行已经挂起了，不能再进行其他操作，Ctrl+C也无法退出。根据提示输入“Ctrl+]”，然后进入telnet命令行，输入quit就能退出。SSH-2.0-OpenSSH_7.4^]telnet&gt; quitConnection closed.# 出现下列情况表示服务未开启或端口被屏蔽无法访问[root@C7-Server01 ~]# telnet 192.168.101.83 23Trying 192.168.101.83...telnet: connect to address 192.168.101.83: Connection refused wget：命令行下载工具wget命令用于从网络上下载某些资料，该命令对于能够连接到互联网的Linux系统，作用非常大，可以直接从网络上下载自己所需要的文件。wget的特点如下： 支持断点下载功能。 支持FTP和HTTP下载方式。 支持代理服务器。 非常稳定，它在带宽很窄的情况下或不稳定的网络中有很强的适应性。如果是由于网络的原因下载失败，wget会不断地尝试，直到整个文件下载完毕。如果是服务器打断了下载过程，它会再次连接到服务器上从停止的地方继续下载。这对那些从限定了连接时间的服务器上下载大文件非常有用。 语法格式：wget [option] [url] 重要选项参数 【使用示例】 1）使用wget下载单个文件 1234567891011# 从阿里云镜像服务器上下载epel源[root@C7-Server01 ~]# wget http://mirrors.aliyun.com/repo/epel-6.repo--2019-05-04 17:46:56-- http://mirrors.aliyun.com/repo/epel-6.repoResolving mirrors.aliyun.com (mirrors.aliyun.com)... 183.201.217.232, 183.201.229.102, 183.201.217.227, ...Connecting to mirrors.aliyun.com (mirrors.aliyun.com)|183.201.217.232|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 664 [application/octet-stream]Saving to: ‘epel-6.repo’100%[======================================================&gt;] 664 --.-K/s in 0s 2019-05-04 17:46:58 (191 MB/s) - ‘epel-6.repo’ saved [664/664] 2）指定保存文件名下载 1234567891011# 使用-O选项指定本地保存文件名[root@C7-Server01 ~]# wget -O ~/MyCentOS6-epel.repo http://mirrors.aliyun.com/repo/epel-6.repo--2019-05-04 17:48:38-- http://mirrors.aliyun.com/repo/epel-6.repoResolving mirrors.aliyun.com (mirrors.aliyun.com)... 183.201.217.230, 183.201.217.233, 111.48.28.118, ...Connecting to mirrors.aliyun.com (mirrors.aliyun.com)|183.201.217.230|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 664 [application/octet-stream]Saving to: ‘/root/MyCentOS6-epel.repo’100%[======================================================&gt;] 664 --.-K/s in 0s 2019-05-04 17:48:40 (123 MB/s) - ‘/root/MyCentOS6-epel.repo’ saved [664/664] wget默认会以最后一个符合“/”的后面的字符来命名，对于动态链接的下载文件名通常会不正确。为了解决这个问题，我们可以使用参数-O来指定一个文件名。 3）限速加载 123456789101112# 使用--limit-rate选项限定下载速度为10K[root@C7-Server01 ~]# wget -O ~/mydata/myico.ico --limit-rate=10k https://kkutysllb.cn/favicon.ico--2019-05-04 17:52:22-- https://kkutysllb.cn/favicon.icoResolving kkutysllb.cn (kkutysllb.cn)... 185.199.110.153Connecting to kkutysllb.cn (kkutysllb.cn)|185.199.110.153|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 97780 (95K) [image/vnd.microsoft.icon]Saving to: ‘/root/mydata/myico.ico’100%[======================================================&gt;] 97,780 10.0KB/s in 9.5s 2019-05-04 17:52:34 (10.0 KB/s) - ‘/root/mydata/myico.ico’ saved [97780/97780][root@C7-Server01 ~]# 4）断点续传 123456789101112# 从阿里云镜像仓库下载centos6.10的iso镜像# 使用-c选项支持断点续传，当下载到5%时，我们手动通过ctrl+c打断[root@C7-Server01 ~]# wget -c https://mirrors.aliyun.com/centos/6/isos/x86_64/CentOS-6.10-x86_64-bin-DVD1.iso--2019-05-04 17:58:23-- https://mirrors.aliyun.com/centos/6/isos/x86_64/CentOS-6.10-x86_64-bin-DVD1.isoResolving mirrors.aliyun.com (mirrors.aliyun.com)... 183.203.69.11, 183.201.217.227, 183.203.69.12, ...Connecting to mirrors.aliyun.com (mirrors.aliyun.com)|183.203.69.11|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 3991928832 (3.7G) [application/octet-stream]Saving to: ‘CentOS-6.10-x86_64-bin-DVD1.iso’5% [==&gt; ] 223,244,572 11.3MB/s eta 5m 21s ^C 再次下载，发现继续从5%开始下载，再次在11%打断，再次继续上次地方下载 5）后台下载 123456789# 使用-b选项后台下载，同时也可组合-c选项，支持断点续传[root@C7-Server01 ~]# wget -bc https://mirrors.aliyun.com/centos/7.6.1810/isos/x86_64/CentOS-7-x86_64-DVD-1810.isoContinuing in background, pid 3333.Output will be written to ‘wget-log’.# 查看下载进度日志文件wget-log[root@C7-Server01 ~]# tail -f wget-log 6）伪装下载 1234567891011# 使用--user-agent选项伪装客户端下载[root@C7-Server01 ~]# wget --user-agent="Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16" https://kkutysllb.cn/favicon.ico--2019-05-04 18:11:26-- https://kkutysllb.cn/favicon.icoResolving kkutysllb.cn (kkutysllb.cn)... 185.199.110.153Connecting to kkutysllb.cn (kkutysllb.cn)|185.199.110.153|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 97780 (95K) [image/vnd.microsoft.icon]Saving to: ‘favicon.ico’100%[======================================================&gt;] 97,780 24.0KB/s in 4.0s 2019-05-04 18:11:33 (24.0 KB/s) - ‘favicon.ico’ saved [97780/97780] 有些网站会根据判断代理名称不是浏览器而拒绝你的下载请求，不过你可以通过——user-agent参数进行伪装。 7）监控网站URL是否正常 12345678910111213# 使用-T选项设置超时时间，单位为秒# 使用-q关闭尝试连接的输出# 使用--tries选项设置重试次数# 使用--spider选项模拟爬虫方式访问网站# 最后通过echo $?输出结果判断，如果结果为0就是表示URL正常，否则异常[root@C7-Server01 ~]# wget -q -T 3 --tries=3 --spider www.sina.com.cn[root@C7-Server01 ~]# echo $?0 上述语句常用自动化运维脚本中。 结论：截止目前Linux系统核心命令中网络管理命令基本介绍这么多，还有一些常用的命令工具，如：抓包（tcpdump），监听（nmap）、追踪（traceroute）、配置（ip、nc）、邮件（mail、mailq）等等会放在Linux常用网络工具一文中介绍。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-25-计算虚拟化之I/O虚拟化]]></title>
    <url>%2F2019%2F05%2F25%2F2019-05-25-%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B9%8BI-O%E8%99%9A%E6%8B%9F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[IO虚拟化概述现实中的外设资源是有限的，为了提高资源的利用率，满足多个虚拟机操作系统对外部设备的访问需求，VMM必须通过I/O虚拟化的方式来实现资源的复用，让有限的资源能被多个虚拟机共享。如何将服务器上的物理设备虚拟化为多个逻辑设备，并将物理设备与逻辑设备解耦，使虚拟机可以在各个虚拟化平台间无缝迁移，正是I/O虚拟化的目标。I/O虚拟化的基本要求主要有以下的三点： (1) 保真性，要求在虚拟化平台下进行的I/O访问与非虚拟化条件下的I/O访问除了完成时间差别外，其它效果相同； (2) 安全性，要求各虚拟机操作系统只能访问VMM分配给其的I/O资源，而各I/O设备也只能与属于同一个虚拟机的其它资源进行交互，如存储空间和CPU，而不能越界访问属于其它虚拟机的资源； (3) 性能，要求虚拟化系统下I/O访问的性能与非虚拟化系统下的I/O访问性能接近。 在这三个基本要求之中最为重要的就是安全性方面的要求，这是保证虚拟化环境可以正常运行的根本，只有首先满足这一要求才能保证虚拟化I/O的保真性，对于性能的要求才会有意义。虚拟化环境下之所以会出现安全性的问题是因为虚拟机操作系统可见的地址并不是实际的机器物理地址，而是客户物理地址，设备若无法获知客户物理地址和机器物理地址间的转换关系，直接使用客户物理地址进行内存访问，这会导致非法地址访问、破坏正常数据。 正是由于I/O虚拟化对于I/O空间安全隔离性方面的要求，所以对于虚拟机操作系统的I/O访问操作一般都需要VMM的介入，而不允许虚拟机操作系统与I/O设备直接进行交互，导致虚拟机操作系统的I/O访问会受到VMM干预，导致性能与非虚拟化环境有较大差距。而VMM介入之所以会导致性能严重损失的根本原因就是发生了上下文切换。 上下文切换分为两种，虚拟机与虚拟机之间发生的上下文切换以及虚拟机与VMM之间发生的上下文切换。在I/O虚拟化中影响性能的就是虚拟机和VMM之间的上下文切换。虚拟机操作系统产生的I/O请求会被VMM截获，VMM将I/O请求交由驱动域去处理，驱动域完成I/O请求要返回执行结果，这些过程都会造成上下文切换。虚拟机和VMM之间的上下文切换的开销主要分为以下三个部分： （1）直接开销，这个部分主要由CPU的切换造成，CPU需要停滞虚拟机切换到VMM去执行，之后又要从VMM转回虚拟机执行下一条指令。 （2）间接开销，两种环境的切换导致需要保存上下文环境。 （3）同步开销，这个部分主要是VMM处理VM EXIT造成的。 因此，一旦发生大量的上下文切换，将严重影响I/O虚拟化的性能，尤其在I/O密集型的任务中，上下文切换导致的开销往往是无法忍受的。为了解决性能的问题，通过DMA重映射、IOMMU等硬件辅助，实现了域间隔离和设备地址转换，保证被分配给虚拟机的设备不会访问不属于该虚拟机的存储器空间。 I/O虚拟化在具体实现上与CPU和内存虚拟化一样，分为软件与硬件虚拟化；在被虚拟机访问的方式上，又分为共享模式与直接访问模式。 软件I/O虚拟化技术软件I/O虚拟化通过软件模拟设备的方式，使得I/O设备资源能够被多个虚拟机共享，该方式可使I/O设备的利用率得到极大的提高，并且可以做到物理设备与逻辑设备分离，具有良好的通用性，但由于该方式需要VMM的介入而导致多次上下文切换，使得I/O性能受到影响。 其本质是VMM需要截获虚拟机操作系统对外部设备的访问请求，通过软件的方式模拟出真实的物理设备的效果，这样，虚拟机看到的实际只是一个虚拟设备，而不是真正的物理设备，这种模拟的方式就是I/O虚拟化的一种实现，下图所示就是一个典型的虚拟机I/O模型。 为了达到虚拟化I/O的目的，VMM截获客户操作系统对设备的访问请求，然后通过软件的方式来模拟真实设备的效果。I/O虚拟化中的设备对软件来说，就是一堆的寄存器（I/O端口）和I/O内存，以及中断和DMA。而设备虚拟化的过程，就是模拟设备的这些寄存器和内存，然后截获Guest OS里面对I/O端口和寄存器的访问，然后通过软件的方式来模拟真实的硬件。软件I/O虚拟化需要解决3个问题： 1）设备发现: 需要控制各虚拟机能够访问的设备。 所谓设备发现就是VMM必须采取某种方式，使得虚拟机能够发现虚拟设备。这样，虚拟机才能够加载相应的驱动程序。 在没有虚拟化的系统中，由BIOS或者操作系统通过遍历PCI总线上的所有设备完成设备发现过程，而在虚拟化系统中，则由VMM决定向虚拟机呈现哪些设备。具体过程要根据设备是否存在于物理总线上来进行。对于一个真实存在于物理总线的设备，如果是不可枚举的类型，例如PS/2键盘，由于这类设备是硬编码固定的，驱动程序会通过其特定的访问方式来检查设备是否存在，因此VMM只要在相应端口上模拟出该设备，虚拟机即可成功检测到它；如果是可枚举的类型，譬如PCI设备或者PCIe设备，这种设备通常定义了完整的设备发现方法，并允许BIOS或者操作系统在设备枚举过程中通过PCI配置空间对其资源进行配置。因此VMM不仅要模拟这些设备本身的逻辑，还要模拟PCI总线的一些属性，包括总线拓扑关系及相应设备的PCI配置空间，以便虚拟机OS在启动时能够发现这些设备。 VMM不仅可以模拟真实设备，还可以实际并不存在的虚拟设备。由于这类设备并没有现实中的规范和模型与之相对应，因此完全由VMM决定它们的总线类型。这类虚拟设备可以挂载在已有PCI总线上，也可以完全自定义一套新的总线。若使用自定义的新总线，则必须在虚拟机中加载特殊的总线驱动程序，且影响虚拟机在不同虚拟化平台的迁移性。 2）访问截获: 通过I/O端口对设备的访问。 所谓访问截获就是虚拟机操作系统发现虚拟设备之后，虚拟机上的驱动程序就会按照特定接口访问这个虚拟设备。驱动程序对于接口的调用必须能够被VMM截获，并能按照真实设备的行为进行模拟。 以分配到端口I/O资源的设备为例，由于CPU对于端口I/O资源的控制与指令流所处的特权级别和相关 I/O位图有关，而在虚拟化环境中虚拟机OS又被降级到非特权级别，因此OS能否访问设备的I/O端口就完全由I/O位图决定。对于没有直接分配给虚拟机的设备，VMM就可以把它的I/O端口在I/O位图中关闭，当 虚拟机操作系统在访问该端口时，就会抛出一个保护异常，VMM即可获得异常原因，并进而将请求发送给底层设备模拟器进行模拟，并异常访问处理结果返回给虚拟机；如果该设备是直接分配给虚拟机，那么VMM就可以在I/O位图中打开它的I/O端口，这样虚拟机驱动程序就可以直接访问该设备。 同理，对于分配到MMIO（Memory Mapped I/ O）资源的设备，比如某些PCIe设备，由于MMIO是系统物理地址空间的一部分，物理地址空间由页表管理，因此VMM同样可以通过控制MMIO相应的页表项是否有效来截获虚拟机操作系统对于MMIO资源的访问。 3）设备模拟：通过软件的方式模拟真实的物理设备。 所谓设备模拟就是模拟设备的功能，内容十分多样且复杂。对于像PS/2键盘、鼠标这样的设备，VMM需要根据设备的接口规范模拟设备的所有行为，才能够无需修改驱动就在虚拟机上展现出设备应有的效果。而对于磁盘存储系统，则不必受限于实际的磁盘控制器以及具体磁盘类型和型号。比如，对IDE硬盘其I/O端口虚拟化时，底层可以是一块磁盘，可以是一个分区，也可以是不同格式的文件；然后在其上实现一个专门的块设备抽象层；最后在块设备上使用文件系统，并引入一些真实硬件没有的高级特性，例如：加密、备份、增量存储等。 上述三个环节仅是VMM处理一个虚拟 机所发出I/O请求的流程。在实际中，系统的物理设备需要同时接受来自多个虚拟 机的I/O请求。因此，VMM还要将多个虚拟机的I/O请求合并为单独一个I/O数据流发送给底层设备驱动。当VMM收到来自底层设备驱动完成I/O请求的中断时，VMM还要能够将中断响应结果转发给正确的虚拟机，以通知其I/O操作结束。同时VMM在调度各个虚拟机发送来的I/O请求处理时，必须依据一定的算法确保虚拟机I/O的QoS与设备共享的公平性。 在实现架构上，软件I/O虚拟化技术主要包括Hypervisor全虚架构和前端/后端的半虚架构来说实现。 1）全虚拟化—模拟模型**，即完全使用软件来模拟真实硬件，模拟通常硬件，例如键盘鼠标。** 该架构使用最为广泛的I/O设备虚拟化模型，采用软件的方式模拟设备行为，为虚拟机模拟出与底层硬件完全一致的虚拟化环境，保证虚拟机操作系统的行为与非虚拟化环境下完全一致。在模拟模型中，虚拟设备必须以某种方式让虚拟机可以发现，导致虚拟机被“欺骗”。当虚拟机访问虚拟设备时，访问请求被VMM截获，然后VMM将I/O请求交由domain0来模拟完成，最后将结果返回给虚拟机。如下图所示是一个基于设备模拟的xen的I/O虚拟化模型。 这种架构最大的优点在于不需要对虚拟机操作系统内核做修改，也不需要为改写其原生驱动程序，因此，这种架构是可移植性与兼容性最佳的一种I/O设备虚拟模型，这也是它被如此广泛使用的主要原因，除了Xen架构的全虚模型外，VMware的Workstations和ESX都有类似的全虚模型，且是全虚模型的典型代表。 但是，全虚模型有一个很大的不足之处，即性能不够高，主要原因有两方面：第一、模拟方式是用软件行为进行模拟，这种方式本身就无法得到很高的性能；第二、这种模型下I/O请求的完成需要虚拟机与VMM多次的交互，产生大量的上下文切换，造成巨大开销。模拟IO虚拟化方式的最大开销在于处理器模式的切换：包括从Guest OS到VMM的切换，以及从内核态的VMM到用户态的IO模拟进程之间的切换。 在全虚拟化，由于VMM实现模式不同，采用的设备虚拟化方式也不同。比如，全虚拟化最有代表性的VMware ESX和VMWare Workstattion。 在VMware ESX中，VMM直接运行在物理硬件之上，直接操作硬件设备，而Guest OS看到的则是一组统一的虚拟IO设备。Guest OS对这些虚拟设备的每一个IO操作都会陷入VMM 中，由VMM对IO指令进行解析并映射到实际的物理设备，然后直接控制硬件完成。 而VMWare WorkStation采用了不同的方式。ＶＭＭ实际上运行在一个传统的操作系统之上，这类VMM无法获得对硬件资源的完全控制，因此采用软件模拟的方式来模拟IO设备。Guest OS的IO操作会被VMM捕获，并转发给宿主机（host OS）的一个用户态进程，该进程通过对宿主机操作系统的系统调用来模拟设备的行为。 以下是VMware的ESX架构，VMkernel负责管理虚拟机对于网络和存储设备的访问。通过设备模拟术，不同的物理设备对于虚拟机可以呈现为某种特定的虚拟设备，甚至并不存在的虚拟设备。 对于存储，物理服务器上部署的可能是某种SCSI设备、磁盘阵列甚至是SAN存储网络，但是ESX能够模拟出BusLogic或者LSILogic的SCSI适配器，因此对于虚拟机总是呈现为SCSI设备。而对于网络ESX则模拟为AMD Lance适配器或者一个并不存在的自定义接口vmxnet，来帮助虚拟机对网络的问。上图显示了在ESXi服务器内部经过的I/O路径。其中，虚拟机分别使用vmxnet虚拟网络适配器与LSI Logic虚拟SCSI适配器对网络和存储进行访问，而物理服务器则使用Intel e1000网卡连接到SAN网络的QLogic光纤HBA卡。 第1步所示，虚拟机中的某个应用程序通过操作系统发起I/O访问，比如发送一个网络数据包或者向磁盘写入一个文件； 第2步表示操作系统对其进行处理，并调用设备驱动处理相应的I/O请求； 第3步表示当设驱动试图访问外设时，VMM拦截到该操作并将控制权切换到Vmkernel； 第4步为VMkernel获得控制权后，I/O请求会被转发到与设备无关的网络或者存储抽象层进行处理； 第5步表示VMkernel还会同时接收到来自其他虚拟机的多个I/O请求，并对这些I/O请求按照特定算法进行优先级调度处理。 I/O请求最终会被转发到具有物理设备驱动程序的硬件接口层进行处理。当I/O请求的完成中断到达时，I/O处理过程则与上述路径完全相反。VMkernel中设备驱动会将到达的中断保护起来，并调用VMkernel来处理该中断；接下来VMkernel会通知相应虚拟机的VMM进程，VMM进程会向虚拟机发起中断，以通知I/O请求处理完毕；同时VMkernel还要确保I/O处理完毕的相关信息与其他虚拟机的数据相互隔离。 由于上述过程需要VMkernel处理I/O请求，因此从虚拟机到VMkernel上下文的切换过程会导致一定的开销。为了降低开销，vmxnet在收发数据包之前，会收集一组数据包再转发给各个虚拟机或者一起发送出去。对于多个虚拟机之间的设备资源管理方面，对于网络ESX采用了流量整形的方式限制每个虚拟机的总带外流量；对于存储ESX实现了比例公平的算法平衡多个虚拟机之间磁盘访问带宽。 2）半虚拟化—泛虚拟化模型**，即属于前后端驱动模型的IO虚拟化，也称为分离驱动模型。** 泛虚拟化模型是被广泛使用的另一种I/O设备虚拟化模型。相比于全虚模型而言，泛虚拟化模型在性能上有很大的提升。主要有以下两个原因：一是该模型采用了I/O环机制（一种大块多队列聚合传输技术，支持I/O环适配功能的虚拟机操作系统，只有安装了Tools才能使用到IO环适配功能），减少了虚拟机与VMM之间的切换；二是该模型摒弃了传统的中断机制，而采用事件或回调机制来实现设备与客户机间的通信。进行中断处理时，传统的中断服务程序需要进行中断确认和上下文切换，而采用事件或回调机制，无需进行上下文切换。如下图所示是一个基于泛虚拟化的Xen的I/O虚拟化模型。 前端/后端架构也称为“Split I/O”，即将传统的I/O驱动模型分为两个部分，一部分是位于客户机OS内部的设备驱动程序（前端），该驱动程序不会直接访问设备，所有的I/O设备请求会转发给位于一个特权虚机的驱动程序（后端），后端驱动可以直接调用物理I/O设备驱动访问硬件。前端驱动负责接收来自其他模块的I/O操作请求，并通过虚拟机之间的事件通道机制将I/O请求转发给后端驱动。后端在处理完请求后会异步地通知前端。相比于全虚模型中VMM需要截获每个I/O请求并多次上下文切换的式，这种基于请求/事务的方式能够在很大程度上减少上下文切换的频率，并降低开销。 但是这种I/O模型有一个很大的缺点，要修改操作系统内核以及驱动程序，因此会存在移植性和适用性方面的问题，导致其使用受限。**下面以Xen架构的模型为例说明：** 在Xen架构的半虚拟化模型中，通过修改Guest OS的内核，将原生的设备驱动从Guest OS移出，放到一个特殊的设备虚拟机中Dom0了，其余虚拟机中的I/O请求都由设备虚拟机处理。而在Guest OS内部，为每个虚拟设备安装一个特殊的驱动程序，由该驱动程序负责I/O请求的传递，设备虚拟机经过VMM授权，解析收到的请求并映射到实际物理设备，最后交给设备的原生驱动来完成IO。实际上在这种情况下，Guest OS的驱动是消息代理的作用，把I/O事件转换为消息，发送给设备虚拟机处理。具体前后端驱动配合实现物理设备驱动功能，需要通过以下几步实现： Step1：如何实现设备发现？ a）所有VM的设备信息保存在Domain0的XenStore中。 b）VM中的XenBus (为Xen开发的半虚拟化驱动)通过与Domain0的XenStore通信，获取设备信息。 c）加载设备对应的前端驱动程序。 Step2：如何实现设备数据截获？ a）前端设备驱动将数据通过VMM提供的接口全部转发到后端驱动。 b）后端驱动VM的数据进行分时分通道进行处理。 Step3：如何模拟使用IO设备 Domain U中虚拟机程序使用IO设备时，通过前端驱动Front-End Driver由XenBus总线访问Domain 0中的Back-End Driver，Back-End Driver通过XenStor中记录的IO设备信息，找到真实的设备驱动Native Driver去访问真实IO设备。 需要注意一点：这种前后端驱动架构的瓶颈就是Domain 0，因为Domain 0通过XenBus总线采用时分复用的策略与前端多个DomainU联动。 硬件I/O虚拟化技术为了改善I/O性能，旨在简化I/O访问路径的设备直通访问方式又被提了出来。代表技术为Intel公司出 的VT-d与AMD公司的IOMMU技术。尽管这两种技术在一定程度上提高了I/O访问性能，但代价却是限制了系统的可扩展性。目前PCI-SIG提出的SR-IOV与MR-IOV是平衡I/O虚拟化通用性、访问性能与系统可扩展性的很好的解决方案。 IO透传—设备直接分配模型**，即直接分配给虚拟机物理设备** 软件实现I/O虚拟化的技术中，所有的虚拟机都共享物理平台上的硬件设备。如果物理条件好，有足够的硬件，就可以考虑让每个虚拟机独占一个物理设备，这样无疑会提高系统的性能。把某一个设备直接分配给一个虚拟机，让虚拟机可以直接访问该物理设备而不需要通过VMM或被VMM截获，这就是设备直通技术。如下图所示为设备直接分配的I/O模型。 在设备直接分配模型中，虚拟机操作系统可直接拥有某一物理设备的访问控制权限，VMM不再干涉其访问操作。因此，该模型可以较大地改善虚拟化设备的性能，降低VMM程序的复杂性，易于实现，并且不需要修改操作系统，保证了高可用性。 设备直接分配模型虽然在性能上相比软件方式的两种I/O设备虚拟化模型有着很大的提升，但是该模型的使用也是有一定限制的。因为该模型将一件物理设备直接分配给了一个虚拟机，其它虚拟机是无法使用该设备的，所产生的一个问题就是如果其它虚拟机需要访问该设备则无法满足需求，解决办法就是物理资源充分满足需求或者通过硬件虚拟化技术虚拟出多个IO设备（与物理设备性能极为接近）供多个虚拟机使用（硬件必须支持）。 Intel的设备硬件虚拟化技术—VT-dVT-d，即VT for Direct I/O，主要在芯片组中实现，允许虚拟机直接访问I/O设备，以减少VMM和CPU的负担，如下图画红框部分。 Intel公司提出的VT系列技术中VT-d其目的就是让虚拟机直接访问物理机底层I/O设备，使虚拟机能够使用自己的驱动直接操作I/O设备，而无需VMM的介入和干涉。通过引入DMA重映射，VT-d不仅可以使虚拟机直接访问设备，同时还提供了一种安全隔离机制，防止其他虚拟机或者VMM访问分配给指定虚拟机的物理内存。VT-d中DMA重映射原理如下图所示：（在北桥也就是现在CPU封装中实现） 具体来说，VT-d技术在北桥引入了DMA重映射技术，并通过两种数据结构（Root Entry 和 Context Entry）维护了设备的I/O页表。设备上的DMA操作都会被DMA重映射硬件截获，并根据对应的I/O页表对DMA中的地址进行转换，同时也会对要访问的地址空间进行控制。 在具体实现上，VT-d使用PCI总线中的设备描述符BDF（Bus Device Function）来标示DMA操作发起者的标示符。其次，VT-d使用两种数据结构来描述PCI总线结构，分别是根条目（Root Entry）和上下文条目（Context Entry）。如下图所示，其中根条目用于描述PCI总线。由于PCI总线个数可以达到256个，因此根条目的范围是0~255，其中每个根条目的一个指针字段都指向该总线的所有PCI设备的上下文条目表指针（Context Table Pointer，CTP）。由于一个PCI总线可以包含256个设备，因此上下文条目表的范围也是0~255 。在每个上下文条目中都包含两个重要字段：地址空间根（Address Space Root，ASR） 指向该设备的I/O页表；域标示符（Domain ID，DID）可以理解为唯一标示一个虚拟机的标示符。 基于这种方式，当某一个设备发起DMA操作及被DMA重映射硬件截获时，通过该设备的BDF中的Bus字段，可以找到其所在的根条目。根据device和function字段，可以索引到具体设备的上下文条目。这样就可根据上下文条目中的ASR字段找到该设备的I/O页表，再通过DMA重映射硬件将I/O请求的GPA转换为HPA，从而达到设备直接访问虚拟机内存的目的。 使用VT-d将设备直接分配给虚拟机的I/O访问性能十分接近无虚拟化环境下的I/O访问性能，然而VT-d事实上是一种I/O设备被虚拟机独占的方式，这种方式牺牲了虚拟化平台中的设备共享能力，设备用率大大降低，而且系统的可扩展性受到物理平台插槽个数的限制。比如，假定一个服务器配置4个CPU，每个CPU为8核，按照平均分配方式每个虚拟机一个核，则可以创建32个虚拟机，如果按照备 直接访问的方式分配网卡，则需要32个物理插槽，这是不现实的。 VT-d还为DMA重映射提供了安全隔离的保障。上图是没有VT-d技术与有VT-d技术的虚拟化平台的对比。可以看出，图（a）部分没有VT-d虚拟化技术的平台中，物理设备的DMA操作可以访问整个系统内存。而图（b）有VT-d技术的平台中，对设备DMA操作的地址范围进行了限制，只能访问指定的地址空间。 除了DMA重映射外，VT-d还提供了中断重映射功能，有兴趣的可以参考Intel官网的VT技术手册。 VMDq和VMDc技术在集群和数据中心这类环境中，每台主机通常同时运行大量的虚拟机。由于主机的网络设备数目有限，多个虚拟机不得不复用同一个网络设备，从而导致性能下降。Intel VT-c 技术可针对虚拟化进一步优化网络性能。VT-c 包括如下两个关键技术： 1）虚拟机设备队列（Virtual Machine Device Queues，VMDq）。收到一个数据包时，VMM必须将其分类以确定应该转发给哪个虚拟机，这占用了大量宝贵的处理器资源。如果以太网控制器支持 VMDq 技术，VMM可以为虚拟机使用不同的数据包队列，以太网控制器自动分类数据包并投放到相应的队列中，大大减轻VMM的负担，提高了I/O吞吐量，如下图所示。 而Intel所使用的VMDq就是通过网卡芯片內建的 Layer 2 classifier / sorter 以加速网络数据传送,它可以先行將不同的虚拟机所需的网络数据包，直接在芯片里规划好，然后再通过 receive queue直送给虚拟机。这样就不用送过虚拟交换机转发数据包减少网络的负载与CPU的开销，有VMDq和没VMDq的对比如下： 2）虚拟机直接连接（Virtual Machine Direct Connect，VMDc）。这是Intel借鉴了SR-IOV技术的特点。与SR-IOV技术一样，支持该技术的网络设备能够对外展现出多个虚拟功能接口VF（Virtual Function）。每个功能接口相当于一个网络设备，VMM可将其直接分配给某个虚拟机，从而“避免”了网络设备的复用。例如，VMM仅用单个英特尔万兆位服务器网卡，可为10个客户机操作系统分配独立受保护的1Gb/秒的专用链路，VMM无需继续管理这些直接通信链路，进一步提升I/O性能并减少主机CPU的负载 。 SR-IOV与MR-IOV：PCIe的虚拟化如前所述，软件设备模拟尽管实现了物理与逻辑的分离，但是性能受到影响；VT-d或者IOMMU技术则以牺牲系统扩展性为代价获得近似于直接访问设备的I/O性能，而且其中任何一种都不是基于现有的工业标准。因此，业界希望重新设计一种可以原生共享的设备。具有原生共享特性的设备可同时为多个虚拟机提供单独的内存空间、工作队列、中断与命令处理，使得设备的资源能够在多个虚拟机之间共享。同时，这些设备能够从多个源端同时接收命令，并将其合并再一起发送出去。因此，原生共享设备不需要VMM模拟设备，同时也在硬件层次上使得多个虚拟机同时访问设备，很好地兼顾了虚拟化系统的性能与可扩展性。 PCI-SIG组织提出了一个新的技术规范：SR-IOV（Single Root I/O Virtualization）。该规范定义了一个单根设备（如 一个以太网卡端口）如何呈现为多个虚拟设备。在SR-IOV中，定义了两个功能类型：一是物理功能类型PF，负责管理SR-IOV设备的特殊驱动，其主要功能是提供设备访问功能和全局共享资源配置的功能，虚拟机所有影响设备状态的操作均需通过通信机制向PF发出请求完成。二是虚拟功能类型VF是轻量级的PCIe功能，包含三个方面：向虚拟机操作系统提供的接口；数据的发送、接收功能；与PF进行通信，完成全局相关操作。由于VF的资源仅是设备资源的子集，因此VF驱动能够访问的资源有限，对其它资源的访问必须通过PF完成。 上图是一个支持SR-IOV的网卡配置。其中左侧三个VM是通过VF可以直接分配到的网卡资源，而该网卡的PF所具有的完整资源仍能用于最右侧两个虚拟机对模拟设备的访问路径。一个具备SR-IOV特性的设备通过VMM配置可以在PCI配置空间中呈现为多个VF，每个VF都配置了基地址寄存器（Base Address Register，BAR）的完整配置空间。VMM通过将一个或多个VF的配置空间映射到虚拟机的PCI配置空间中实现VF的分配。结合VT-d内存映射等技术，虚拟机可以直接访问VF的内存空间，这就 能绕过VMM直接访问I/ O设备。如下图所示： SR-IOV还实现了地址转换服务（Address Translation Service，ATS）来提供更好的性能。通过缓存TLB到本地，I/O设备可以在发起PCI事务之前直接对DMA地址进行转换，这样就避免了在IOMMU中进行地址转换时可能发生的缺页情况。通过这种方式，绑 定到VF的虚拟机可 获得与基于硬件I/O虚拟 化虚拟机接近的性能。但是与基于硬件I/O虚拟化较低的可扩展性相比，一个SR-IOV设备可以具有几百个VF，因此SR-IOV具有更好的可扩展性。 MR-IOV（Multiple Root I/O Virtualization）扩展了SR-IOV规范。MR-IOV允许PCIe设备在多个有独立PCI根的系统之间共享，这些系统通过基于PCIe转换器的拓扑结构与PCIe设备或者PCIe-PCI桥相接。MR-IOV与SR-IOV相比，每个VH（Virtual Hierarchy，一个VH就是一个虚拟独立的SR-IOV设备）拥有独立的PCI Memory，IO，配置空间。如下图是MR-IOV的作用，本来每个系统只有一个Host，两个PCIe设备，但是有了MRA Switch之后，系统里面有2个Host，4个PCIe设备。 MR-IOV里有个重要概念：VH，Virtual Hirearchy：每个VH至少包含一个PCIe Switch，这个PCIe Switch是MRA Switch里面的一个虚拟组件。每个VH可以包含各种PCIe设备、MRA PCIe设备、或者PCIe-PCI桥的组合。如下图，在MRA PCIe Switch中，可以有多个根端口Root Port，RP。 这样，有了MR-IOV之后，软件系统变成了下面这种，物理机之间也能相互通信，PCIe设备被多台物理机共享。下图中的SI就是虚拟机OS、VI就是虚拟机监视器VMM。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-18-计算虚拟化之内存虚拟化]]></title>
    <url>%2F2019%2F05%2F18%2F2019-05-18-%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B9%8B%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[内存虚拟化概述所谓的内存虚拟化，即如何在多个虚拟机之间共享物理内存以及如何进行动态分配。在《x86架构基础》一文中已经介绍操作系统对物理服务器内存管理的知识，它的本质就是将物理内存地址映射到一段线性地址空间，也有叫逻辑地址空间，应用程序访问内存物理地址是通过段页查询机制完成。而这个线性地址空间或逻辑地址空间本身就是物理内存的虚拟化呈现。在虚拟化环境中，分配给虚拟机内存非常类似于操作系统中关于线性地址空间的实现。操作系统负责维护虚页号到实页号的映射，并将这一 映射信息保存到页表（Page Table）。在 x86架构的CPU中，内存管理单元MMU与TLB这两个模块就负责实现并优化虚拟内存的性能。详见《DPDK技术在电信云中最佳实践》系列文章。 一个操作系统对其物理内存存在两个主要的基本认识：物理地址从0开始和内存地址连续性（至少在一些大的粒度上连续）。 如上图所示，而VMM与客户机操作系统在对物理内存的认识上存在冲突，这使得真正拥有物理内存的VMM必须对客户机操作系统所访问的内存进行一定程度的虚拟化。换句话说，就是VMM 负责将MMU进行虚拟化，为客户机操作系统提供一段连续的“物理”地址空间，而操作系统本身不会意识到这种变化，仍能够将虚拟机虚拟地址（Guest Virtual Address，GVA）映射到虚拟机物理地址（Guest Physical Address，GPA），但是需要VMM将虚拟机物理地址映射到物理机物理地址（Host Physical Address，HPA）。 所以，内存虚拟化的本质就是把物理机的真实物理内存统一管理，包装成多份虚拟的内存给若干虚拟机使用。内存虚拟化的核心，在于引入一层新的地址空间—客户机物理地址空间，客户机以为自己运行在真实的物理地址空间中，实际上它是通过VMM访问真实的物理地址的。在VMM中保存客户机地址空间和物理机地址空间之间的映射表。 如下图所示：虚拟化系统中包括三层内存地址空间：虚拟机虚拟地址GVA、虚拟机物理地址GPA和物理机物理地址HPA。因此，原先由MMU完成的线性地址到物理地址的映射已经不能满足，必须由VMM接入来完成这三层地址的映射维护和转换。 GVA：指GuestOS提供给其应用程序使用的线性地址空间。 GPA：经VMM抽象的，虚拟机看到的伪物理地址空间。 HPA：真实的机器地址，即地址总线上出现的地址信号。 为了实现上述映射和转换关系，主要有两种解决方案：软件解决方案—影子页表和硬件解决方案—Intel的EPT和AMD的RVI。 内存虚拟化软件解决方案MMU半虚拟化（MMU Paravirtualization）这种方式主要为Xen所用MMU半虚拟化主要原理是： 1）当Guest OS创建新页表时，VMM从维护的空闲内存中为其分配页面并进行注册。后续，Guest OS对该页表的写操作都会陷入VMM进行验证和转换；VMM检查页表中的每一项，确保它们只映射到属于该虚拟机的机器页面，而且不包含对页表页面的可写映射。 2）然后，VMM会根据其维护的映射关系PA-MA，将页表项中的虚拟机逻辑地址VA替换为相应的机器地址MA。 3）最后把修改过的页表载入MMU，MMU就可以根据修改过的页表直接完成虚拟地址VA到机器地址MA的转换。 这种方式的本质是将映射关系VA-MA直接写入Guest OS的页表中，以替换原来的映射VA-PA映射关系。 影子页表相比较MMU半虚，大部分虚拟化厂商在VMM中还使用了一种称为影子页表（Shadow Page Table）的技术实现上述功能。对于每个虚拟机的主页表（Primary Page Table），VMM都维持一个影子页表来记录和维护GVA与HPA的映射关系。 影子页表包括以下两种映射关系，如下图所示： 1）GVA&gt;&gt;&gt;GPA，虚拟机操作系统负责维护从虚拟机逻辑地址到虚拟机物理地址的映射关系，VMM可以从虚拟机主页表中获取这种映射关系。 2）GPA&gt;&gt;&gt;HPA，VMM负责维护从虚拟机物理地址到物理机物理地址的映射关系。 通过这种两级映射的方式，VMM为Guest OS的每个页表维护一个影子页表，并将GVA-HPA的映射关系写入影子页表，Guest OS的页表内容保持不变，然后，VMM将影子页表写入MMU。同时，又对虚拟机可访问的内存边界进行了有效控制。并且，使用TLB缓存影子页表的内容可以大大提高虚拟机问内存的速度。 影子页表的维护将带来时间和空间上的较大开销。时间开销主要体现在Guest OS构造页表时不会主动通知VMM，VMM必须等到Guest OS发生缺页错误时（必须Guest OS要更新主页表），才会分析缺页原因再为其补全影子页表。而空间开销主要体现在VMM需要支持多台虚拟机同时运行，每台虚拟机的 Guest OS通常会为其上运行的每个进程创建一套页表系统，因此影子页表的空间开销会随着进程数量的增多而迅速增大。 为权衡时间开销和空间开销，现在一般采用影子页表缓存（Shadow Page Table Cache）技术，即VMM在内存中维护部分最近使用过的影子页表，只有当影子页表在缓存中找不到时，才构建一个新的影子页表。当前主要的虚拟化技术都采用了影子页表缓存技术。 内存虚拟化的硬件解决方案为了解决影子页表导致的上述开销问题，除了使用影子页表缓存技术外（这项技术虽然能避免时间上的一部分开销，但是空间开销还是实实在在存在的）， Intel与AMD公司都针对MMU虚拟化给出了自 的解决方案：Intel公司在Nehalem微架构CPU中推出扩展页表（Extended Page Table，EPT）技术；AMD公司在四核皓龙CPU中推出快速虚拟化索引（Rapid Virtualization Index，RVI）技术。 RVI与EPT尽管在具体实现细节上有所不同，但是在设计理念上却完全一致：通过在物理MMU中保存两个不同的页表，使得内存地址的两次映射都在硬件中完成，进而达到提高性能的目的。具体来说，MMU中管理管理了两个页表，第一个是GVA &gt;&gt;&gt;GPA，由虚拟机决定；第二个是GPA&gt;&gt;&gt;HPA，对虚拟机透明，由VMM决定。根据这两个映射页表，CPU中的page walker就可以生成最近访问过key-value键值对&lt;GVA，HPA&gt; ，并缓存在TLB中（类似影子页表缓存技术思路）。 另外，原来在影子页表中由VMM维持的GPA&gt;&gt;&gt;HPA映射关系，则由一组新的数据结构扩展页表（Extended Page Table，也称为Nested Page Table）来保存。由于GPA &gt;&gt;&gt;HPA的映射关系非常定，并在虚拟机创建或修改页表时无需更新，因此VMM在虚拟机更新页表的时候无需进行干涉。VMM也无需参与到虚拟机上下文切换，虚拟机可以自己修改GVA &gt;&gt;&gt;GPA的页表。 我们以Intel EPT技术为例说明。Intel EPT是Intel VT-x 提供的内存虚拟化支持技术，其基本原理下图所示。在原有的CR3页表地址映射的基础上，EPT引入EPT页表来实现另一次映射。比如：假设客户机页表和EPT页表都是4级页表，CPU完成一次地址转换的基本过程如下： CPU首先查找客户机CR3寄存器指向的L4页表。客户机CR3寄存器给出的是GPA，所以，CPU通过EPT页表将客户机CR3中的GPA转换为HPA：CPU 首先查找EPT TLB，如果没有相应的记录，就进一步查找EPT页表，如果还没有，CPU则抛出EPT Violation异常交给VMM处理。 CPU获得L4页表地址（指的是HPA）后，CPU根据GVA和L4页表项的内容来获取L3 页表项的GPA。如果L4页表中GVA对应的表项显示为“缺页”，那么CPU 产生Page Fault，直接交由客户机操作系统处理。获得L3 页表项的GPA后，CPU通过查询EPT页表来将L3的GPA转换为HPA。同理，CPU 会依次完成L2、L1页表的查询，获得GVA所对应的GPA，然后进行最后一次查询EPT页表获得HPA。 正如上图所示，CPU需要5次查询EPT页表，每次查询都需要4次内存访问。这样，在最坏的情况下总共需要20次内存访问。EPT硬件通过增大EPT TLB 尽量减少内存访问。 内存虚拟化管理在虚拟化环境中，内存是保证虚拟机工作性能的关键因素。如何尽可能提高虚拟机的性能、提高内存利用率、降低虚拟机上下文切换的内存开销，依然非常复杂，这就引入了内存虚拟化管理的问题。像介绍CPU虚拟化管理一样，我们还是通过实例来说明内存的虚拟化管理。以VMware的ESX解决方案为例，在没有出现硬件支持的内存虚拟化技术之前，ESX/ESXi采用影子页表来实现虚拟机的虚拟地址到物理机物理地址的快速转换。当Intel和AMD公司分别推出了EPT与RIV技术之后，ESX/ESXi很快转向硬件支持来提高内存虚拟化的性能。 在虚拟化内存管理 上，ESX/ESXi实现了主机内存超分配的目标：即多个虚拟机总的内存分配量大于物理机的实际内存容量。如下图所示，一个物理内存只有4GB的Host，可以同时运行三个内存配置为2GB的虚拟机。 主机内存超分配功能意味着VMM必须能够有效地回收虚拟机中不断释放的内存，并在有限的内存容量中尽能 地提高内存利用率。因为，Host Memory与Guest Memory并不是一一对应的关系，通过Host Memory超配技术可以实现某一个Host上某一个虚拟机内存大于主机内存，这一点与CPU虚拟化不一样。但是，在执行超配技术时，需要考虑主机性能问题，不能过大。一般的超配限额是主机内存的50%。要实现主机内存超配，必须通过内存复用技术实现。目前常用的内存复用技术有：零页共享技术、内存气球回收技术和内存置换技术三种。 零页（透明页）共享技术当运行多个虚拟机时，有些内存页面的内容很可能是完全一样的，比如：什么数据都没有的零页。这就为虚拟机之间甚至在虚拟机内部提供了共享内存的可能。例如：当几个虚拟机都运行相同的操作系统、相同的应用程序或者包含相同的用户数据时，那些包含相同数据的内存页面完可以被共享。基于这个原理，VMM完全可通过回收冗余数据的内存页面，仅维持一个被多个虚拟机共享的内存拷贝来实现这个功能。 如下图所示，是华为FusionCompute的零页共享技术示意图： 其基本原理是：用户进程定时扫描虚拟机的的内存数据，如果发现其数据内容全为零，则通过修改PA to MA映射的形式，把其指向一个特定的零页，从而做到在物理内存中只保留一份零页拷贝，虚拟机的所有零页均指向该页，从而达到节省内存资源的目的。当零页数据发生变动时，由Xen动态分配一页内存出来给虚拟机，使修改后的数据有内存页进行存放。因此，对GuestOS来说，整个零页共享技术是完全不感知的。 而在VMware的ESX解决方案中，也有同样的技术。在VMware ESX/ESXi中，检测页面数据是否冗余是通过散列的方法来实现的，如下图所示。 首先VMM会维持一个全局散列表，其中每个表项都记录了一个物理页面数据的散列值与页号。当对某一个虚拟机进行页面共享扫描时，VMM会针对该虚拟机物理页面的数据计算散列值，并在全局散列表 中进行遍历及匹配是否有相同的散列值的表项。当找到了匹配的表项，还要对页面数据内容逐位比较，以避免由于散列冲突而导致的页面内容不一样的可能性。一旦确定页面数据完全一致，则会修改逻辑地址到物理地址的映射关系，即将从逻辑地址对应到包含冗余数据的物理地址的映射关系（上图中虚线所示）改为对应到要被共享物理地址的映射关系，并回收冗余的物理页面。这一过程对于虚拟机操作系统是完全透明的。因此，共享页面中含有敏感数据的部分不会在虚拟机之间泄露。 当虚拟机对共享页面发生写操作时，通过“写时拷贝”（ Copy-on-Write）技术来实现。 如下图所示： 具体来说，任何一个对共享页面的写操作都会引发页面错误（Minor Page Fault）。当VMM捕获到这个错误时，会给发起写操作的虚拟机创建一个该页面的私有拷贝，并将被写的逻辑地址映射到这个私有拷贝页面。这样虚拟机就可以安全地进行写操作，并且不会影响到其他共享该页面的虚拟机。相比于对非共享页面的写操作，尽管这种处理方法的确导致了一些额外的开销，但是却在一定程度上提高了内存页面的利用率。 内存气球回收技术内存气球回收技术也称为内存气泡技术，基于气球回收法的内存管理机制与页面共享完全不同。在虚拟化环境中，VMM会一次性在虚拟机启动后分配给虚拟机内存，由于虚拟机并没有意识到自己运行于 虚拟化平台上，之后它会一直运行在分配好的内存空间，而不主动释放分配的物理内存给其他虚拟机。因此VMM需要一种机制使得虚拟机能够主动释放空闲内存归还给物理机，再由VMM分配给其他有需求的虚拟机。并且，在内存资源需求紧张时还能从物理机中“拿回”自己释放的那部分内存。 如下所示，是华为FusionCompute内存气泡技术示意图： 原理如下：Hypervisor通过利用预装在用户虚拟机中的前端驱动程序，“偷取”Guest OS的内存贡献给VMM，以供其他虚拟机使用，反向时由VMM“偷取”气泡中的内存给特定虚拟机使用。内存气泡本质是将较为空闲的虚拟机内存释放给内存使用率较高的虚拟机，从而提升内存利用率。 在VMware的ESX解决方案中，也有类似的技术。下图给出了内存释放过程的原理图。 在上图（a）中，VMM有四个页面被映射到虚拟机的内存页面空间中，其中左侧两个页面被应用程序占用，而另两个被打上星号的页面则是在内存空闲列表中。当VMM要从虚拟机中回收内存时，比如要回收两个内存页面，VMM就会将Balloon驱动的目标膨胀大小设置为两个页面。Balloon驱动获得了目标膨胀值之后，就会在虚拟机内部申请两个页面空间的内存，并如上图（b）所示，调用虚拟机操系统的接口标示这两个页面被“ 钉住”，即不能再被分配出去。 内存申请完毕后，Balloon驱动会通知VMM这两个页面的页号，这样VMM就可以找到相应的物理页号并进行回收。在上（b）中虚线就标示了这两个页面从虚拟机分配出去的状态。 由于被释放的页面在释放前已经在虚拟机的空闲列表中，因此没有进程会对该页面进行读写操作。如果虚拟机的进程接下来要重新访问这些页面，那么VMM可以像平常分配内存一样，再分配新的物理内存给这台虚拟机。当VMM决定收缩气球膨胀大小时，通过设置更小的目标膨胀值，balloon驱动会将已经被“钉住” 的页面归还给虚拟机。 通过气球回收法，尽管虚拟机的负载略微增加，但VMM却成功地将系统内存压力转移到各个虚拟机上。当balloon驱动发起申请内存的请求时，由虚拟机操作系统决定了是否要将虚拟机物理内存换出来满足balloon驱动的申请内存请求。如果虚拟机有充足的空闲内存，那么balloon驱动申请内存并不会对虚拟机的性能造成影响；如果虚拟机内存已经吃紧，那么就需要由虚拟机的操作系统决定换出哪些内存页面，满足balloon驱动的请求。因此，气球回收法巧妙地利用了各个虚拟机操作系统的内存换页机制来确定哪些页面要被释放给物理机，而不是由VMM来决定。 气球回收法要求虚拟机操作系统必须安装balloon驱动，在VMware的ESX/ESXi产品中，就是VMware Tool。另外，气球回收法回收内存需要一段时间，不能马上满足系统的需求。 内存置换技术页面共享机制与气球回收法都从不同的角度尽可能地提高虚拟机的内存利用率，从虚拟机中收回可以复用或者空闲的内存。然而这两种方法都不能在短时间内满足系统内存回收的要求：页面共享依赖于页面的扫描速度，以及是否有页面可共享；气球回收法则取决于虚拟机操作系统对于balloon驱动申请内存的响应时间。如果这两种温和的方法都不能满足需求，VMM则会采取内存换出机制，即强制性地从虚拟机中夺回内存，这就是内存置换技术。 如下所示，是华为FusionCompute和VMware ESX/ESXI的内存置换技术示意图。 原理如下：通过VMM实现请页功能，这时Guest OS类似进程一样在VMM缺少内存时，能被换出到宿主机磁盘上，也就是将虚拟机长时间未访问的内存内容被置换到存储中，并建立映射，当虚拟机再次访问该内存内容时再置换回来。该方法也对虚拟机透明，即虚拟机不感知。 具体来说，VMM会在每个虚拟机启动时创建一个单独的换页文件（Swap File）。在必要的时候，VMM会主动将虚拟机的物理内存页面换到这个换页文件上，释放给其他虚拟机使用。内存换出机制是VMM需要在短时间内缓解内存压力的一种有效方法，然而这种方法却很可能严重导致VMM的性能下降。由于VMM对于虚拟机的内存使用状态并不解，且该方法对虚拟机透明，强制内存换出可能触发虚拟机操作系统内部的一些换页机制。举例来说，虚拟机操作系统永远都不会将内核的内存页面换出，而VMM并不知道哪些页正在被内核使用，一旦这些页面被换出，会使得虚拟机性能严重受损。 内存的回收接下来以VMware ESX为例，结合上述三种内存复用技术，介绍内存回收机制。一般来说，ESX会对物理机的空闲内存状态按照空闲内存的百分比设置四种状态，分别是：高（6%）、平缓（4%）、繁重（2%）和低（1%）。ESX会按照这四种状态来选择前述三种内存回收机制。 缺省状态下，ESX会启用页面共享机制，因为页面共享机制能以较小的开销提高内存利用率。何时启用气球回收和换页则取决于当前系统的内存状态。当内存状态处于“高”，很显然此时总的虚拟机内存使用量要小于物理机的内存容量，因此不管虚拟机的内存是否已经被过载分配，VMM都不会使用气球 或者换页的方法回收内存。 然而，当物理机空闲内存状态下降到了“平缓” 状态，VMM则开始使用气球回收法。事实上，气球回收法是在空闲内存的百分比高于“平缓” 的阈值4%之前启动的，这是因为该方法总是需要一段时间才能在虚拟机内申请到一些内存。通常气球回收法都能够及时将空闲内存比的阈值控制在“平缓”状态之上。 一旦气球回收法不能够及时回收内存，并且空闲内存下降到“繁重”状态，即空闲内存比低于2%，那么VMM就会再启动内存换出机制强制从虚拟机回收内存。使用这种办法，VMM能够很快回收内存，并将空闲内存比控制回“平缓”状态。 在最坏的情况下，万一空闲内存状态低于“低”状态，即空闲内存比低于1%，那么VMM会继续使用内存换出法，同时将那些消耗内存值超过内存配置值的虚拟机挂起。 在某些情况下，VMM可能不会考虑物理机空闲内存状态，而仍然启动物理机内存回收机制。比如，即使整个系统的物理机空闲内存状态为“高”，如果某个虚拟机的内存使用量超过了其指定的内存上限，那么VMM会启动气球回收法，如有必要，也会启动内存换出机制从虚拟机回收内存，直到该虚拟机的内存低于指定的内存上限。 内存QoS保障在虚拟化系统中，内存虚拟化的QoS保障包括两个基本特征：预留和份额。 内存预留：虚拟机预留的最低内存。预留的内存会被VM独占。即，一旦内存被某个虚拟机预留，即使虚拟机实际内存使用量不超过预留量，其它VM也无法抢占该VM的内存空闲资源。即，上述的三种内存复用技术对该虚拟机不生效。 内存份额：适用上述三种资源复用场景，按比例分配内存资源。以6G内存规格的主机为例，假设其上运行有3台4G内存规格的虚拟机VMA，VMB，VMC。内存份额分别为20480，20480，40960，那么其内存分配比例为1：1：2。当三台VM内部逐步加压，策略会根据三个虚拟机的份额按比例分配调整内存资源，最终三台虚拟机获得的内存量稳定为1.5G/1.5G/3G。 同样需要注意一点：内存份额只在各虚拟机发生资源竞争时生效。如没有竞争，则有需求的虚拟机可独占物理内存。 但是，内存QoS不像CPU QoS一样设置上限？，这是因为分配给虚拟机的内存大小就是其内存上限。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-15-Linux系统命令-第八篇《进程管理命令》]]></title>
    <url>%2F2019%2F05%2F15%2F2019-05-15-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E5%85%AB%E7%AF%87%E3%80%8A%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[ps：查看进程ps命令用于列出执行ps命令的那个时刻的进程快照，就像用手机给进程照了一张照片。如果想要动态地显示进程的信息，就需要使用top命令，该命令类似于把手机切换成录像模式。因为ps命令能够支持多种系统（Linux\UNIX等），所以选项较多。但是学习时只需要掌握常用的参数即可。而且由于ps命令的功能实在是太多了，26个字母已经满足不了，因此，在ps命令的参数中有类似于-a与a这2种写法，这2种写法的功能是不一样的。 语法格式：ps [option] 重要选项参数 【使用示例】 1）PS命令不接任何参数 1234[root@C7-Server01 ~]# ps PID TTY TIME CMD 11097 pts/1 00:00:00 bash 40755 pts/1 00:00:00 ps 默认情况下，ps命令不接任何参数，显示的使用者当前所在终端的进程。 2）常用命令组合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 使用-e选项显示unix格式所有进程[root@C7-Server01 ~]# ps -e PID TTY TIME CMD 1 ? 00:00:01 systemd 2 ? 00:00:00 kthreadd 3 ? 00:00:00 ksoftirqd/0 5 ? 00:00:00 kworker/0:0H 7 ? 00:00:00 migration/0 8 ? 00:00:00 rcu_bh 9 ? 00:00:02 rcu_sched。。。# 同时加上-f选项，额外显示UID、PPID、C和TIME栏# C栏表示进程占用CPU的百分比# TTY栏显示？表示该进程与终端无关，否则显示相应的终端[root@C7-Server01 ~]# ps -ef # 常用组合UID PID PPID C STIME TTY TIME CMDroot 1 0 0 17:12 ? 00:00:01 /usr/lib/systemd/systemd --system --deserialiroot 2 0 0 17:12 ? 00:00:00 [kthreadd]root 3 2 0 17:12 ? 00:00:00 [ksoftirqd/0]root 5 2 0 17:12 ? 00:00:00 [kworker/0:0H]root 7 2 0 17:12 ? 00:00:00 [migration/0]root 8 2 0 17:12 ? 00:00:00 [rcu_bh]root 9 2 0 17:12 ? 00:00:02 [rcu_sched]root 10 2 0 17:12 ? 00:00:00 [lru-add-drain]。。。# 查找特定进程信息，常用ps -ef与grep结合使用# 显示系统SSH进程的相关信息[root@C7-Server01 ~]# ps -ef|grep sshroot 1146 1 0 17:13 ? 00:00:00 /usr/sbin/sshd -Droot 11093 1146 0 17:23 ? 00:00:02 sshd: root@pts/1root 40759 11097 0 22:42 pts/1 00:00:00 grep --color=auto ssh# 使用BSD格式显示每个进程信息# 所谓BSD格式就是选项前面不带“-”符号# VSZ栏表示该进程使用掉的虚拟内存量（单位KB）# RSS栏表示该进程占用的固定内存量（单位KB）# STAT栏表示该进程目前的状态，具体状态解释详见最后[root@C7-Server01 ~]# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 46104 6396 ? Ss 17:12 0:01 /usr/lib/systemd/systemd --syroot 2 0.0 0.0 0 0 ? S 17:12 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? S 17:12 0:00 [ksoftirqd/0]root 5 0.0 0.0 0 0 ? S&lt; 17:12 0:00 [kworker/0:0H]root 7 0.0 0.0 0 0 ? S 17:12 0:00 [migration/0]root 8 0.0 0.0 0 0 ? S 17:12 0:00 [rcu_bh]root 9 0.0 0.0 0 0 ? S 17:12 0:02 [rcu_sched]root 10 0.0 0.0 0 0 ? S&lt; 17:12 0:00 [lru-add-drain]root 11 0.0 0.0 0 0 ? S 17:12 0:00 [watchdog/0]root 12 0.0 0.0 0 0 ? S 17:12 0:00 [watchdog/1]。。。 STAT栏进程状态解释： R：正在运行，或者是可以运行。 S：正在中断睡眠中，可以由某些信号量唤醒。 D：不可中断睡眠 T：正在侦测或停止了。 Z：已经终止，但是其父进程无法终止它，从而编程僵尸进程状态。 +：前台进程。 l：多线程进程 N：低优先级进程 &lt;：高优先级进程 s：进程领导者 L：已将页面锁定到内存中 3）显示指定用户相关的进程 123456789101112# 使用-u选项显示root'用户相关进程，注意-u和u的区别[root@C7-Server01 ~]# ps -u rootPID TTY TIME CMD1 ? 00:00:01 systemd2 ? 00:00:00 kthreadd3 ? 00:00:00 ksoftirqd/05 ? 00:00:00 kworker/0:0H7 ? 00:00:00 migration/08 ? 00:00:00 rcu_bh9 ? 00:00:02 rcu_sched10 ? 00:00:00 lru-add-drain 4）显示进程的详细信息 123456789101112131415161718# F：代表这个进程的标志（flag），4代表使用者为super user# S：代表这个进程的状态（STAT)，见前例说明# PPID：父进程号# PID：本进程号# NI：nice值# ADDR：表示进程在内存中的地址范围# WCHAN：表示这个进程是否在运行，若在运行，取值为“-”[root@C7-Server01 ~]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 11097 11093 0 80 0 - 28859 do_wai pts/1 00:00:00 bash0 R 0 40780 11097 0 80 0 - 38300 - pts/1 00:00:00 ps 5）显示进程树 12345678910111213# 使用-H选项显示进程树，配合-e选项显示所有进程的进程树[root@C7-Server01 ~]# ps -eHPID TTY TIME CMD2 ? 00:00:00 kthreadd3 ? 00:00:00 ksoftirqd/05 ? 00:00:00 kworker/0:0H7 ? 00:00:00 migration/08 ? 00:00:00 rcu_bh9 ? 00:00:02 rcu_sched10 ? 00:00:00 lru-add-drain11 ? 00:00:00 watchdog/012 ? 00:00:00 watchdog/1 123456789101112131415# 或者使用BSD格式axf组合选项也可达到同样效果[root@C7-Server01 ~]# ps axfPID TTY STAT TIME COMMAND2 ? S 0:00 [kthreadd]3 ? S 0:00 \_ [ksoftirqd/0]5 ? S&lt; 0:00 \_ [kworker/0:0H]7 ? S 0:00 \_ [migration/0]8 ? S 0:00 \_ [rcu_bh]9 ? S 0:02 \_ [rcu_sched]10 ? S&lt; 0:00 \_ [lru-add-drain]11 ? S 0:00 \_ [watchdog/0]12 ? S 0:00 \_ [watchdog/1]13 ? S 0:00 \_ [migration/1]。。。 6）查看系统进程，找出CPU占用率最高的进程 123456789101112131415# 使用-o选项自定义显示pcpu字段值，使用--sort选项进行排序，默认从小到大，前面加-号从大到小排序[root@C7-Server01 ~]# ps -eo pid,ppid,pcpu,args,comm --sort -pcpuPID PPID %CPU COMMAND COMMAND30785 1 1.7 /usr/bin/dockerd --insecure dockerd11489 1 1.2 /usr/bin/containerd containerd1 0 0.0 /usr/lib/systemd/systemd -- systemd2 0 0.0 [kthreadd] kthreadd3 2 0.0 [ksoftirqd/0] ksoftirqd/05 2 0.0 [kworker/0:0H] kworker/0:0H7 2 0.0 [migration/0] migration/08 2 0.0 [rcu_bh] rcu_bh9 2 0.0 [rcu_sched] rcu_sched10 2 0.0 [lru-add-drain] lru-add-drain。。。 pstree：显示进程状态树pstree命令以树形结构显示进程和进程之间的关系。如果不指定进程的PID号，或者不指定用户名称，则会以init进程（CentOS 7为systemd进程）为根进程，显示系统的所有进程信息；若指定用户或PID，则将以用户或PID为根进程，显示用户或PID对应的所有进程。 注意：CentOS 7中默认没有pstree命令，因此执行命令后会提示command not found，此时需要通过yum命令安装，执行以下命令即可 1yum install -y psmisc 语法格式：pstree [option] [/] 重要选项参数 【使用示例】 1）显示进程树 12345678910111213141516171819202122232425# 若不指定PID号或者不指定用户，则会以init（CentOS7系统是systemd）进程为根进程，显示系统所有进程[root@C7-Server01 ~]# pstreesystemd─┬─VGAuthService├─agetty├─auditd───&#123;auditd&#125;├─chronyd├─containerd─┬─containerd-shim─┬─registry───7*[&#123;registry&#125;]│ │ └─9*[&#123;containerd-shim&#125;]│ └─16*[&#123;containerd&#125;]├─crond├─dbus-daemon───&#123;dbus-daemon&#125;├─dockerd─┬─docker-proxy───7*[&#123;docker-proxy&#125;]│ └─15*[&#123;dockerd&#125;]├─irqbalance├─master─┬─pickup│ └─qmgr├─polkitd───5*[&#123;polkitd&#125;]├─rsyslogd───2*[&#123;rsyslogd&#125;]├─sshd───sshd───bash───pstree├─systemd-journal├─systemd-logind├─systemd-udevd├─tuned───4*[&#123;tuned&#125;]└─vmtoolsd───&#123;vmtoolsd&#125; 2）显示指定用户的进程 12345678910111213141516171819# 显示apache用户的所有进程，共有5个进程[root@C7-Server01 ~]# pstree apachehttpdhttpdhttpdhttpdhttpd# 使用-c选项显示所有进程，包含父进程和子进程# 使用-p选项显示进程的pid[root@C7-Server01 ~]# pstree -c -p apachehttpd(40920)httpd(40921)httpd(40922)httpd(40923)httpd(40924) 3）显示进程归属的用户 12345678910111213141516171819202122232425262728# 使用-u选项显示进程归属的用户# 进程后[&#123;...&#125;]中的内容就是进程归属的用户信息[root@C7-Server01 ~]# pstree -usystemd─┬─VGAuthService├─agetty├─auditd───&#123;auditd&#125;├─chronyd(chrony)├─containerd─┬─containerd-shim─┬─registry───7*[&#123;registry&#125;]│ │ └─9*[&#123;containerd-shim&#125;]│ └─16*[&#123;containerd&#125;]├─crond├─dbus-daemon(dbus)───&#123;dbus-daemon&#125;├─dockerd─┬─docker-proxy───7*[&#123;docker-proxy&#125;]│ └─15*[&#123;dockerd&#125;]├─httpd───5*[httpd(apache)]├─irqbalance├─master─┬─pickup(postfix)│ └─qmgr(postfix)├─polkitd(polkitd)───5*[&#123;polkitd&#125;]├─rsyslogd───2*[&#123;rsyslogd&#125;]├─sshd───sshd───bash───pstree├─systemd-journal├─systemd-logind├─systemd-udevd├─tuned───4*[&#123;tuned&#125;]└─vmtoolsd───&#123;vmtoolsd&#125; pgrep：查找匹配条件的进程pgrep命令可以查找匹配条件的进程号。 语法格式：pgrep [option] [pattern] 重要选项参数 【使用示例】 1）显示指定进程的pid 123456789# 显示httpd进程进程号[root@C7-Server01 ~]# pgrep httpd409194092040921409224092340924 2）显示指定用户的进程号 12345# 使用-u选项显示指定用户postfix的进程号[root@C7-Server01 ~]# pgrep -u postfix173740931 kill：终止进程kill命令能够终止你希望停止的进程。 语法格式：kill [option] [pid] 重要选项参数 【使用示例】 1）列出所有信号的名称 12345678910111213141516# 使用-l选项，显示所有信号[root@C7-Server01 ~]# kill -l1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 常用信号说明： 2）终止进程说明 kill指令默认使用的信号为15，用于结束进程。如果进程忽略此信号，则可以使用信号9强制终止进程。一般是先通过ps等命令获取到要终止进程的进程号，然后直接使用“kill进程号”就可以了。比如： 123kill 2203 #&lt;==kill命令默认使用的信号为15，这种格式也是最常用的。kill -s 15 2203 #&lt;==这种格式使用-s参数明确指定发送值为15的信号，效果和kill 2203一样。kill -15 2203 #&lt;==上面的-s 15可以简写为-15。 如果用上面的方法还是无法终止进程，那么我们就可以用KILL（9）信号强制终止进程。 kill -9 2203 #&lt;==信号9会强行终止进程，这会带来一些副作用，如数据丢失，或者终端无法恢复到正常状态等，因此应尽量避免使用，除非进程使用其他信号无法终止。 3）特殊信号0的作用 在kill的所有信号中，有一个十分特殊的信号值0，使用格式为kill -0 $pid。其中的-0表示不发送任何信号给$pid对应的进程，但是仍然会对$pid是否存在对应的进程进行检査，如果$pid对应的进程已存在，则返回0，若不存在则返回1。因此，这个特殊信号通常用于系统管理shell脚本中判断某个进程是否运行的条件表达式。 killall：通过进程名终止进程使用kill命令终止进程还需要先获取进程的pid进程号，这个过程有点繁琐，而使用killall命令就可以直接用“killall进程名”这种形式终止进程。 语法格式：killall [option] [name] 重要选项参数 【使用示例】 1）终止定时任务服务进程 1234567891011121314[root@C7-Server01 ~]# killall crond# 查看crond进程状态，确认# 如果什么输出也没有，则表示crond进程被终止[root@C7-Server01 ~]# ps -ef | grep crond | grep -v grep# 同样，也可使用BSD格式aux选项查看进程状态确认# 如果进程的状态是S，则表示被终止[root@C7-Server01 ~]# ps aux | grep corndroot 40964 0.0 0.0 112704 972 pts/1 S+ 00:14 0:00 grep --color=auto cornd 2）终止指定用户的所有进程 123# 终止apache用户的所有httpd进程[root@C7-Server01 ~]# killall -u apache httpd pkill：通过进程名终止进程pkill命令可通过进程名终止指定的进程。使用killall终止进程需要连续执行几次，而pkill可以杀死指定进程及其所有子进程。 语法格式：pkill [option] [name] 重要选项参数 【使用示例】 1）通过进程名终止进程 123456789101112131415161718192021222324252627282930313233343536373839# 查看定时任务crond当前状态[root@C7-Server01 ~]# systemctl status crond● crond.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled) Active: inactive (dead) since Sat 2019-05-04 00:11:34 CST; 11min ago Process: 10272 ExecStart=/usr/sbin/crond -n $CRONDARGS (code=exited, status=0/SUCCESS) Main PID: 10272 (code=exited, status=0/SUCCESS)May 03 17:19:56 C7-Server01 systemd[1]: Started Command Scheduler.May 03 17:19:56 C7-Server01 systemd[1]: Starting Command Scheduler...May 03 17:19:56 C7-Server01 crond[10272]: (CRON) INFO (RANDOM_DELAY will be scaled with f...d.)May 03 17:19:56 C7-Server01 crond[10272]: (CRON) INFO (running with inotify support)May 03 17:19:56 C7-Server01 crond[10272]: (CRON) INFO (@reboot jobs will be run at comput...p.)May 04 00:11:34 C7-Server01 crond[10272]: (CRON) INFO (Shutting down)Hint: Some lines were ellipsized, use -l to show in full.# 启动定时任务crond[root@C7-Server01 ~]# systemctl start crond# 通过进程名终止进程crond[root@C7-Server01 ~]# pkill crond[root@C7-Server01 ~]# systemctl status crond● crond.service - Command Scheduler Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled) Active: inactive (dead) since Sat 2019-05-04 00:24:03 CST; 9s ago Process: 40980 ExecStart=/usr/sbin/crond -n $CRONDARGS (code=exited, status=0/SUCCESS) Main PID: 40980 (code=exited, status=0/SUCCESS)May 04 00:23:29 C7-Server01 systemd[1]: Started Command Scheduler.May 04 00:23:29 C7-Server01 systemd[1]: Starting Command Scheduler...May 04 00:23:29 C7-Server01 crond[40980]: (CRON) INFO (RANDOM_DELAY will be scaled with f...d.)May 04 00:23:29 C7-Server01 crond[40980]: ((null)) No security context but SELinux in per...ab)May 04 00:23:29 C7-Server01 crond[40980]: ((null)) No security context but SELinux in per...ly)May 04 00:23:29 C7-Server01 crond[40980]: (CRON) INFO (running with inotify support)May 04 00:23:29 C7-Server01 crond[40980]: (CRON) INFO (@reboot jobs will be run at comput...p.)Hint: Some lines were ellipsized, use -l to show in full. 2）通过终端名终止进程 123456789101112131415161718192021222324# 查看当前用户运行的终端信息，通过w指令实现# TTY列就是当前用户终端类型[root@C7-Server01 ~]# w 00:29:17 up 7:16, 2 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATkkutysll tty1 00:27 13.00s 0.03s 0.01s vim /etc/ssh/ssh_configroot pts/1 192.168.101.1 17:23 5.00s 0.30s 0.01s w# 上述结果说明：# kkutysllb用户正在服务器本地登录，且正在编辑ssh服务配置文件# root用户正在远程登录，且正在使用w命令查询服务器当前登录用户状态信息# 通过终端名终止kkutysllb的进程[root@C7-Server01 ~]# pkill -t tty1[root@C7-Server01 ~]# w 00:31:48 up 7:18, 2 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATkkutysll tty1 00:27 2:44 0.03s 0.03s -bashroot pts/1 192.168.101.1 17:23 4.00s 0.29s 0.00s w 本地登录用户kkutysllb编辑ssh配置文件的操作被终止了 3）通过用户名终止进程 1234567891011121314[root@C7-Server01 ~]# w 00:34:34 up 7:21, 2 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATkkutysll tty1 00:27 10.00s 0.26s 0.02s vim /etc/sysconfig/network-scrroot pts/1 192.168.101.1 17:23 2.00s 0.29s 0.00s w# 使用-u选项，终止kkutysllb用户编辑网卡配置文件操作[root@C7-Server01 ~]# pkill -u kkutysllb[root@C7-Server01 ~]# w 00:35:23 up 7:22, 2 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATkkutysll tty1 00:27 59.00s 0.24s 0.24s -bashroot pts/1 192.168.101.1 17:23 3.00s 0.29s 0.00s w 用户kkutysllb编辑网卡配置文件的操作被root用户远程终止。 如果kkutysllb不是通过本地登录服务器，而是远程登录，通过-u选项可以将kkutysllb踢下线，效果请大家自行练习。 nice：调整程序运行时的优先级nice命令是一个当程序启动时，修改程序运行优先级的命令。Linux的优先级范围是从-20（最大优先级）到19（最小优先级）。优先级越高的程序占用CPU的次数越多，反之亦然。 语法格式：nice [option] [command] 重要选项参数 【使用示例】 1）单独使用nice命令 1234# 不接任何选项和程序时，显示出当前系统默认的nice程序运行优先级为0[root@C7-Server01 ~]# nice0 2）默认增加优先级10 12345678# 不加-n选项，直接跟程序名[root@C7-Server01 ~]# nice nice10[root@C7-Server01 ~]# nice nice nice19[root@C7-Server01 ~]# nice nice nice nice19 第1个nice命令以默认值10来调整第2个nice命令运行的优先级，即在系统默认的程序运行优先级0的基础之上增加10，得到新的程序运行优先级10，然后以优先级10来运行第2个nice命令，最后第2个nice命令显示当前程序运行的优先级为10。所以，再加一个nice就是19（因为最小优先级为19），再往后加nice一直都是19。 3）查看进程优先级 1234[root@C7-Server01 ~]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 11097 11093 0 80 0 - 28859 do_wai pts/1 00:00:00 bash0 R 0 41043 11097 0 80 0 - 38300 - pts/1 00:00:00 ps 在上面的输出结果中，需要重点关注以下两列。 PRI：代表这个进程的优先级，通俗点说就是进程被CPU执行的先后顺序，此值越小进程的优先级别就越高，就能越早执行。 NI：代表这个进程的nice值，表示进程可被执行的优先级的修正数值，在加入nice值后，将会使得PRI变为：PRI（new）=80（PRI初始默认值）+nice。这样一来，如果nice值为负值，那么该进程的优先级值将变小，即其优先级会变高，也表示其越快被执行。 总结：NI是优先值，是用户层面的概念，PR是进程的实际优先级，是给内核（kernel）用的。进程的nice值不是进程的优先级，它们不是一个概念，但是进程的nice值会影响到进程的优先级变化。 renice：调整运行中的进程的优先级nice命令常用于修改未运行的程序运行时的优先级，但是对于正在运行的进程，若想要修改其优先级，就需要用到renice命令。 在系统运行中，有时会发现某个不是很重要的进程占用了太多的CPU资源，因此会希望限制这个进程或者是希望某个进程优先运行。这些都是renice命令的使用场景。 语法格式：renice [option] 重要选项参数 【使用示例】 1）修改指定进程号的优先级 123456789101112131415161718192021222324252627282930313233343536373839# 在当前进程后台创建一个vim命令进程[root@C7-Server01 ~]# vim /etc/host &amp;[1] 1839# 查看当前系统进程信息[root@C7-Server01 ~]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1770 1767 0 80 0 - 28858 do_wai pts/0 00:00:00 bash0 T 0 1839 1770 0 80 0 - 37284 do_sig pts/0 00:00:00 vim0 R 0 1840 1770 0 80 0 - 38300 - pts/0 00:00:00 ps[1]+ Stopped vim /etc/host# vim当前进程的优先级为80，NI为0# 使用renice的-p选项指定进程1839，将其NI值调整为-5[root@C7-Server01 ~]# renice -n -5 -p 18391839 (process ID) old priority 0, new priority -5# 再次查看当前系统进程信息，发现vim的PRI变为75[root@C7-Server01 ~]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1770 1767 0 80 0 - 28859 do_wai pts/0 00:00:00 bash0 T 0 1839 1770 0 75 -5 - 37284 do_sig pts/0 00:00:00 vim0 R 0 1846 1770 0 80 0 - 38300 - pts/0 00:00:00 ps# 仍然通过renice指令调整[root@C7-Server01 ~]# renice -n -5 -p 18391839 (process ID) old priority -5, new priority -5[root@C7-Server01 ~]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 1770 1767 0 80 0 - 28859 do_wai pts/0 00:00:00 bash0 T 0 1839 1770 0 75 -5 - 37284 do_sig pts/0 00:00:00 vim0 R 0 1848 1770 0 80 0 - 38300 - pts/0 00:00:00 ps 结论：PRI值并不是在上一次的基础上进行变化，而是一直在初始默认值80这个值之上变动。 runlevel：输出当前运行级别runlevel命令用于输出当前Linux系统的运行级别。 语法格式：runlevel [option] 重要参数选项 【使用示例】 查看当前系统的运行级别 12[root@C7-Server01 ~]# runlevel N 3 上面的结果说明当前的运行级别为3。对于系统级别，不同的数字代表的意思不一样，具体如下。 0：停机 1：单用户模式 2：无网络的多用户模式 3：多用户模式 4：未使用 5：图形界面多用户模式 6：重启 init：初始化Linux进程init命令是Linux下的进程初始化工具，init进程是所有Linux进程的父进程，它的进程号为1。init命令的主要任务是依据配置文件“/etc/inittab”创建Linux进程。 语法格式：init [option] 【使用示例】 切换系统运行级别 12345678910111213141516171819# 关机[root@C7-Server01 ~]# init 0# 重启[root@C7-Server01 ~]# init 6# 单用户[root@C7-Server01 ~]# init 1# 多用户[root@C7-Server01 ~]# init 3# 图形模式[root@C7-Server01 ~]# init 5]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-13-计算虚拟化之CPU虚拟化]]></title>
    <url>%2F2019%2F05%2F13%2F2019-05-13-%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B9%8BCPU%E8%99%9A%E6%8B%9F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[CPU虚拟化概述CPU虚拟化的一个很大挑战就是要确保虚拟机发出CPU指令的隔离性。即为了能让多个虚拟机同时在一个主机上安全运行，VMM必须将各个虚拟机隔离，以确保不会相互干扰，同时也不会影响VMM内核的正常运行。尤其要注意的是：由于特权指令会影响到整个物理机，必须要使得虚拟机发出的特权指令仅作用于自身，而不会对整个系统造成影响。例如：当虚拟机发出重启命令时，并不是要重启整个物理机，而仅仅是重启所在的虚拟机。因此，VMM必须能够对来自于虚拟机操作硬件的特权指令 进行翻译并模拟，然后在对应的虚拟设备上执行，而不在整个物理机硬件设备上运行。 软件方式实现的CPU虚拟化—二进制翻译技术二进制翻译（Binary Translation，BT）是一 种软件虚拟化技术，由VMware在Workstations和ESX产品中最早实现。在最初没有硬件虚拟化时代，是全虚拟化的唯一途径。由于BT最开始是用来虚拟化32位平台的，因此，也称为BT32。 二进制翻译，简单来说就是将那些不能直接执行的特权指令进行翻译后才能执行。具体来说，当虚拟机第一次要执行一段指令代码时，VMM会将要执行执行的代码段发给一个称为“Just-In-Time”的BT翻译器，它类似Java中的JVM虚拟机和Python的解释器，实时将代码翻译成机器指令。 翻译器将虚拟机的非特权指令翻译成可在该虚拟机上安全执行的指令子集， 而对于特权指令，则翻译为一组在虚拟机上可执行的特权指令，却不能运行在物理机上。这种机制实现了对虚拟机的隔离与封装，同时又使得x86指令的语义在虚拟机层次上得到保证。 在执行效率上，为了降低翻译指令的开销，VMM会将执行过的二进制指令翻译结果进行缓存。如果虚拟机再次执行同样的指令序列，那么之前被缓存的翻译结果可以被复用。这样就可以均衡整个VM执行指令集的翻译开销。为了进一步降低由翻译指令导致的内存开销，VMM还会将虚拟机的内核态代码 翻译结果和用户态代码直接绑定在一起。由于用户态代码不会执行特权指令，因此这种方法可以保证 安全性。采用BT机制的VMM必须要在虚拟机的地址空间和VMM的地址空间进行严格的边界控制。VMware VMM利用x86CPU中的段检查功能（Segmentation）来确保这一点。但是由于现代操作系统 Windows、Linux以及Solaris等都很少使用段检查功能，因此VMM可以使用段保护机制来限制虚拟机和VMM之间的地址空间边界控制。在极少数情况下，当虚拟机的确使用了段保护机制并且引发了 VMM冲突，VMM可以转而使用软件的段检查机制来解决这一问题。 硬件方式实现的CPU虚拟化—VT-x和AMD-v2003年，当AMD公司将x86从32位扩展到64位时，也将段检查功能从64位芯片上去除。同样的情况也 出现在Intel公司推出的64位芯片上。这一变化意味着基于BT的VMM无法在64位机上使用段保护机制保护VMM。尽管后来AMD公司为了支持虚拟化又恢复了64位芯片的段检查功能，并一直延续到目前所有的AMD 64位芯片，但是Intel公司却并没有简单恢复 ，而是研发了新的硬件虚拟化技术VT-x，AMD公司紧随后也推出了AMD-V技术来提供CPU指令集虚拟化的硬件支持。VT-x与AMD-V尽管在具体实现上有所不同，但其目的都是希望通过硬件的途径来限定某些特权指令操作的权限，而不是原先只能通过二进制动态翻译来解决这个问题。 如前所述，VT-x提供了2个运行环境：根（Root）环境和非根（Non-root）环境。根环境专门为VMM准备，就像没有使用VT-x技术的x86服务器，只是多了对VT-x支持的几条指令。非根环境作为一个受限环境用来运行多个虚拟机。 根操作模式与非根操作模式都有相应的特权级0至特权级3。VMM运行在根模式的特权级0，Guest OS的内核运行在非根模式的特权级0，Guest OS的应用程序运行在非根模式的特权级3。运行环境之间相互转化，从根环境到非根环境叫VM Entry；从非根环境到根环境叫VM Exit。VT-x定义了VM Entry操作，使CPU由根模式切换到非根模式，运行客户机操作系统指令。若在非根模式执行了敏感指令或发生了中断等，会执行VM Exit操作，切换回根模式运行VMM。此外，VT-x还引入了一组新的命令：VMLanch/ VMResume用于调度Guest OS，发起VM Entry；VMRead/ VMWrite则用于配置VMCS。 根模式与非根模式之问的相互转换是通过VMX操作实现的。VMM可以通过VMX ON 和VMX OFF打开或关闭VT-x。如下图所示： VMX操作模式流程： VMM执行VMX ON指令进入VMX操作模式。 VMM可执行VMLAUNCH指令或VMRESUME指令产生VM Entry操作，进入到Guest OS，此时CPU处于非根模式。 Guest OS执行特权指令等情况导致VM Exit的发生，此时将陷入VMM，CPU切换为根模式。VMM根据VM Exit的原因作出相应处理，处理完成后将转到第2步，继续运行GuestOS。 VMM可决定是否退出VMX操作模式，通过执行VMXOFF指令来完成。 这样，就无需二进制翻译和半虚拟化来处理这些指令。同时，VT-x与AMD-V都提供了存放虚拟机状态的模块，这样做的的目的就是将虚拟机上 下文切换状态进行缓存，降低频模式繁切换引入的大量开销。 还是以VT-x解决方案为例，VMX新定义了虚拟机控制结构VMCS(Virtual Machine ControlStructure)。VMCS是保存在内存中的数据结构，其包括虚拟CPU的相关寄存器的内容及相关的控制信息。CPU在发生VM Entry或VM Exit时，都会查询和更新VMCS。VMM也可通过指令来配置VMCS，达到对虚拟处理器的管理。VMCS架构图如下图所示： 每个vCPU都需将VMCS与内存中的一块区域关联起来，此区域称为VMCS区域。对VMCS区域的操纵是通过VMCS指针来实现的，这个指针是一个指向VMCS的64位的地址值。VMCS区域是一个最大不超过4KB的内存块，且需4KB对齐。 VMCS区域分为三个部分：偏移地址0起始存放VMCS版本标识，通过不同的版本号，CPU可维护不同的VMCS数据格式；偏移地址4起始存放VMX终止指示器，在VMX终止发生时，CPU会在此处存入终止的原因；偏移地址8起始存放VMCS数据区，这一部分控制VMX非根操作及VMX切换。 VMCS 的数据区包含了VMX配置信息：VMM在启动虚拟机前配置其哪些操作会触发VM Exit。VMExit 产生后，处理器把执行权交给VMM 以完成控制，然后VMM 通过指令触发VM Entry 返回原来的虚拟机或调度到另一个虚拟机。。 VMCS 的数据结构中，每个虚拟机一个，加上虚拟机的各种状态信息，共由3个部分组成： Gueststate：该区域保存了虚拟机运行时的状态，在VM Entry 时由处理器装载；在VM Exit时由处理器保存。它又由两部分组成： Guest OS寄存器状态：包括控制寄存器、调试寄存器、段寄存器等各类寄存器的值。 Guest OS非寄存器状态：记录当前处理器所处状态，是活跃、停机（HLT）、关机（Shutdown）还是等待启动处理器间中断（Startup-IPI）。 Hoststate：该区域保存了VMM 运行时的状态，主要是一些寄存器值，在VM Exit时由处理器装载。 Control data：该区域包含虚拟机执行控制域、VM Exit控制域、VM Entry控制域、VM Exit信息域和VM Entry信息域。 有了VMCS结构后，对虚拟机的控制就是读写VMCS结构。比如：对vCPU设置中断，检查状态实际上都是在读写VMCS数据结构。 CPU虚拟化管理为了保证电信云/NFV中关键业务虚机运行的性能，就要求同一台物理机上的多个业务虚机实例所获取的资源既能满足其运行的需要，同时不互相产生干扰，这就需要对CPU资源进行精细化的调优和管理，也就是CPU QoS保障技术。在介绍CPU QoS保障技术之前，我们首先来看下CPU虚拟化的本质和超配技术。 CPU虚拟化的本质和超配 vCPU数量和物理CPU对应关系如上图所示，以华为RH2288H V3服务器使用2.6GHz主频CPU为例，单台服务器有2个物理CPU，每颗CPU有8核，又因为超线程技术，每个物理内核可以提供两个处理线程，因此每颗CPU有16线程，总vCPU数量为282=32个vCPU。总资源为32*2.6GHz=83.2GHz。 虚拟机vCPU数量不能超过单台物理服务器节点可用vCPU数量。但是，由于多个虚拟机间可以复用同一个物理CPU，因此单物理服务器节点上运行的虚拟机vCPU数量总和可以超过实际vCPU数量，这就叫做CPU超配技术。 例如，以华为的FusionCompute为例，查询显示的服务器可用CPU的物理个数为2个，每个主频2.4GHz。 这是特权虚机中占用CPU资源，占用了4个vCPU 在资源池性能页，可以查看每个物理CPU有6个核，并且开启了超线程，也就是每个物理CPU有12个核的资源，一台服务器总共vCPU数量为：122=24个。由于在华为的FUSinCompute中CPU的资源统计单位不是逻辑核数，而是频率HZ。因此，可用资源（122-4）*2.4=48GHz（为什么减4个？因为系统DM0占用了4个vCPU，因此单个客户机最多只能使用20个vCPU）。如下所示，当前已经使用了19.15GHZ，占用率为39.39%。 CPU的QoS保障了解了CPU虚拟化的本质和超配，前文提到的CPU的QoS保障技术主要指的是CPU上下限配额及优先级调度技术。 CPU的上下限配额主要指的是vCPU资源管理层面的解决方案。在CPU虚拟化后，根据虚拟化的资源—频率HZ，来对虚拟机进行分配时，为了保证虚拟机的正常运行，特地定义了三种vCPU资源的划分方式，这就是CPU资源的QoS管理。可以按照限额、份额和预留三种方式进行vCPU资源划分，三者之间是有一定依赖和互斥关系的，其定义具体如下： CPU资源限额：控制虚拟机占用物理资源的上限。 CPU资源份额：CPU份额定义多个虚拟机在竞争物理CPU资源的时候按比例分配计算资源。 CPU资源预留：CPU预留定义了多个虚拟机竞争物理CPU资源的时候分配的最低计算资源。 为了描述上述三种vCPU资源划分方式的关系，我们还是举例来说明。比如：单核CPU主频为3GHz，该资源供两个虚拟机VM1和VM2使用。 场景一：当VM1资源限额为2GHz，VM1可用的CPU资源最多为2GHz，也就意味着如果VM2没有设定CPU QoS，那么VM2最多只有1GHZ的vCPU可以使用。 场景二：当VM1和VM2的资源份额分别是1000和2000，在资源紧张场景下发生竞争时，VM1最多可获得1GHz的vCPU，VM2最多可获得2GHz的vCPU。 场景三：当给虚拟机VM1资源预留2GHz的vCPU资源，VM2资源预留为0，在资源紧张发生竞争时，VM1最少可获得2GHz的vCPU，最多可获得3GHZ的vCPU（在没有超配时），而VM2最多可获得1GHz（3-2=1）的vCPU资源，最少为0。 上例只是简单描述了各种vCPU划分方式单独生效时的场景，也是在实际中较常用的场景。同时，在实际中还有一些场景是多种vCPU划分方式共同作用的，虽然不常用，但是却是实实在在有意义的。比如：以一个主频为2.8GHz的单核物理机为例，如果运行有三台单CPU的虚拟机A、B、C，份额分别为1000、2000、4000，预留值分别为700MHz、0MHz、0MHz。当三个虚拟机满CPU负载运行时，每台虚拟机应分配到资源计算如下： 虚拟机A按照份额分配本应得400MHz，由于其预留值大于400MHz，最终计算能力按照预留值700MHz算，剩余的2100MHz资源按照2000:4000也就是1:2的比例在B和C之间进行划分，因此虚拟机B得到700MHz计算资源，虚拟机C得到1400MHz计算资源。 这里有一点需要注意：CPU的份额和预留只在多个虚拟机竞争物理CPU资源时发生，如果没有竞争发生，有需求的虚拟机可以独占物理CPU的资源。 而CPU的优先级调度技术主要指的的服务器虚拟化后资源的重分配机制。从上述虚拟化的结构可以看出，虚拟机和VMM共同构成虚拟机系统vCPU资源的两极调度框架。如下图所示，是一个多核环境下的虚拟机vCPU资源的两级调度框架。 虚拟机操作系统负责第2级调度，即线程或进程在vCPU上的调度（将核心线程映射到相应的vCPU 上）。VMM负责第1级调度，即vCPU在物理处理单元上的调度。两级调度的策略和机制不存在依赖关系。 vCPU调度器负责物理处理器资源在各个虚拟机之间的分配与调度，本质上把各个虚拟机中的vCPU按照一定的策略和机制调度在物理处理单元上，可以采用任意的策略（如上面资源管理方案）来分配物理资源，满足虚拟机的不同需求。vCPU可以调度在一个或多个物理处理单元执行（分时复用或空间复用物理处理单元），也可以与物理处理单元建立一对一绑定关系（限制访问指定的物理理单元）。 NUMA架构感知的调度技术除了基础的两级调度技术外，还有基于NUMA架构的精细化调度技术。在《DPDK技术栈在电信云中的最佳实践（一）》一文中，我们介绍过服务器的NUMA架构，其产生的主要原因就是解决服务器SMP架构扩展性能的问题。同样，在计算虚拟化中的CPU资源调度方面，也有基于服务器NUMA架构的调度技术，这就是Host NUMA和Guest NUMA技术，它们都是虚拟化软件技术。 Host NUMA主要提供CPU负载均衡机制，解决CPU资源分配不平衡引起的VM性能瓶颈问题，当启动VM时，Host NUMA根据当时主机内存和CPU负载，选择一个负载较轻的node放置该VM，使VM的CPU和内存资源分配在同一个node上。 如上图左边所示，Host NUMA把VM的物理内存放置在一个node上，对VM的vCPU调度范围限制在同一个node的物理CPU上，并将VM的vCPU亲和性绑定在该node的物理CPU上。考虑到VM的CPU负载是动态变化，在初始放置的node上，node的CPU资源负载也会随之变化，这会导致某个node的CPU资源不足，而另一个node的CPU资源充足，在此情况下，Host NUMA会从CPU资源不足的node上选择VM，把VM的CPU资源分配在CPU资源充足的node上，从而动态实现node间的CPU负载均衡。 Host NUMA保证VM访问本地物理内存，减少了内存访问延迟，可以提升VM性能，性能提升的幅度与VM虚拟机访问内存大小和频率相关。对于VM的vCPU个数超过node中CPU的核数时，如上图右边所示，Host NUMA把该VM的vCPU和内存均匀地放置在每个node 上，vCPU的调度范围为所有node的CPU。 如果用户绑定了VM的vCPU亲和性，Host NUMA特性根据用户的vCPU亲和性设置决定VM的放置，若绑定在一个node的CPU上，Host NUMA把VM的内存和CPU放置在一个node上，若绑定在多个node的CPU上，Host NUMA把VM的内存均匀分布在多个node 上，VM的vCPU在多个node的CPU上均衡调度。 Host NUMA技术的本质保证了VM访问本地物理内存，减少了内存访问延迟，可以提升VM性能，性能提升的幅度与VM访问内存大小和频率相关。Host NUMA主要应用于针对大规格、高性能虚拟机场景，适用Oracle、 SQL Server等关键应用。 Guest NUMA如上图所示，能够使得虚拟机内部程序运行时针对NUMA结构进行优化，CPU会优先使用同一个Node上的内存，从而减小内存访问延时、提高访问效率，以此达到提升应用性能的目的。目前OS和应用都会有针对NUMA的优化，VMM通过向虚拟机呈现NUMA结构，使Guest OS及其内部应用识别Numa结构， CPU会优先使用同一个Node上的内存，减小内存访问延时、提高访问效率。 Guest NUMA的本质就是VMM保证NUMNA结构的透传，使虚拟机中的关键应用在NUMA方面的优化生效，减少了内存访问延迟，可以提升VM性能。Guest NUMA主要应用于虚拟机中应用程序减小内存访问延时、提高访问效率，以此达到提升应用性能的目的。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-11-计算虚拟化概述]]></title>
    <url>%2F2019%2F05%2F11%2F2019-05-11-%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[所谓计算虚拟化，从狭义角度可理解为对单个物理服务器的虚拟化，主要包括对服务器上的CPU、内存、I/O设备进行虚拟化，目的就是实现多个虚拟机能各自独立、相互隔离地运行于一个服务器之上。从广义角度还可延伸到云资源池下，各类资源池组网场景下的CPU、内存、I/O设备等资源进行整合、抽象和虚拟化。 服务器虚拟化平台概念回顾在上一篇文章《虚拟化基础》中，我们介绍虚拟化基础的一些基本概念，这里我们按照服务器平台虚拟化后的一个分层结构来简单回顾下。如下： 一个完整的服务器虚拟化平台从下到上包括以下几个部分： 底层物理资源：包括网卡、CPU、内存、存储设备等硬件资源，一般将包含物理资源的物理机称为 宿主机（Host）。 虚拟机监控器（Virtual Machine Monitor，VMM）：VMM是位于虚拟机与底层硬件设备之间的虚拟层，直接运行于硬件设备之上，负责对硬件资源进行抽象，为上层虚拟机提供运行环境所需资源，并使每个虚拟机都能够互不干扰、相互独立地运行于同一个系统中。 抽象化的虚拟机硬件：即虚拟层呈现的虚拟化的硬件设备。虚拟机能够发现哪种硬件设施，完全由VMM决定。虚拟设备可以是模拟的真实设备，也可以是现实中并不存在的虚拟设备，如VMware的vmxnet网卡。 虚拟机：相对于底层提物理机，也称为客户机（Guest）。运行在其上的操作系统则称为客户机操作系统（Guest OS）。每个虚拟机操作系统都拥有自己的虚拟硬件，并在一个独立的虚拟环境中执行。通过VMM的隔离机制，每个虚拟机都认为自己作为一个独立的系统在运行。 同时，在上一篇文章《虚拟化基础》中，我们提到过Hypervisor就是VMM。其实，这个说法并不准确，至少在VMware的虚拟化解决方案中不准确，在VMware的ESX产品架构中，VMM和Hypervisor还是有一定区别的，如下图所示。 Hypervisor是位于虚拟机和底层物理硬件之间的虚拟 层，包括boot loader、x86 平台硬件的抽象层，以及内存与CPU调度器，负责对运行在其上的多个虚拟机进行资源调度。而VMM则是与上层的虚机 一一对应 的进程，负责对指令集、内存、中断与基本的I/O设备进行虚拟化。当运行一个虚拟机时，Hypervisor中的vmkernel会装载VMM，虚拟机直接运行于VMM之上，并通过VMM的接口与Hypervisor进行通信。而在KVM和Xen架构中，虚拟层都称为Hypervisor，也就是**VMM=Hypervisor**。 判断一个VMM能否有效确保服务器系统实现虚拟化功能，必须具备以下三个基本特征： 等价性（Equivalence Property）：一个 运行于VMM控制 之下的程序（虚拟机），除了时序和资源可用性可能不一致外， 其行为应该与相同条件下运行在物理服务器上的行为一致。 资源可控 性（Resource Control Property）：VMM必须能够完全控制虚拟化的资源。 效率性（Efficiency Property）：除了特权指令，绝大部分机器指令都可以直接由硬件执行，而无需VMM干涉控制。 上述三个基本特征也是服务器虚拟化实现方案的指导思想。 x86平台虚拟化面临的问题与挑战基于x86的操作系统在一开始就被设计为能够直接运行在裸机硬件环境之上，所以自然拥有整个机器硬件的控制权限。为确保操作系统能够安全地操作底层硬件，x86平台使用了特权模式和用户模式的概念对内核程序与用户应用程序进行隔离。 在这个模型下，CPU提供了4个特权级别，分别是Ring0、1、2和3。如下图所示： Ring 0是最高特权级别，拥有对内存和硬件的直接访问控制权。Ring 1、2和3权限依次降低， 无法执行操作讷河系统级别的指令集合。相应的，运行于Ring 0的指令称为“特权指令”；运行于其他级别的称为“非特权指令”。常见的操作系统如Linux与Windows都运行于Ring 0，而用户级应用程序运行于Ring 3。如果低特权级别的程序执行了特权指令，会引起“ 陷入”（Trap）内核态，并抛出一个异常。 当这种分层隔离机制应用于虚拟化平台 ，为了满足 VMM的“资源可控” 特征，VMM必须处于Ring 0级别控制所有的硬件资源，并且执行最高特权系统调用。而虚拟机操作系统Guest OS则要被降级运行在Ring 1级别，故Guest OS在执行特权指令时都会引起”陷入“。如果VMM能够正常捕获异常，模拟Guest OS发出的指令并执行，就达到了目的。这就是IBM的Power系列所采用的特权解除和陷入模拟的机制，支持这种特性的指令集合通常被认为是“ 可虚拟化的”。 但是。。。但是。。。但是。。。x86平台的指令集是不虚拟化的。为什么这么说？首先我们来看下x86平台指令集分类，x86平台的指令集大致分为以下4类： 访问或修改机器状态的指令。 访问或修改敏感寄存器或存储单元的指令， 比如访问时钟寄存器和中断寄存器。 访问存储保护系统或内存、地址分配系统的指令（段页之类）。 所有I/O指令。 其中，1~4在x86平台都属于敏感指令，第1、4类指令属于敏感指令中的特权指令，由操作系统内核执行，Guest OS在执行两类指令时，因为不处于Ring 0级别，所以会陷入，并抛出异常，这个异常会被VMM捕获，然后模拟Gust OS去执行，并将执行结果返回给Guest OS。到此为止，一切都OK。但是，第2、3类指令属于非特权指令，可以由应用程序调用，也就是可以在Ring 3级别执行，并调用Guest OS内核进程来完成。当应用程序调用这些指令时，由于要修改内存和内部寄存器，这些状态修改需要由Guest OS完成，而Guse OS此时运行在Ring 1级别，虽然也会发生陷入，但是不会抛出异常，这样VMM就捕获不到，也就无法模拟完成。因此，当Guest OS执行这些指令就会导致虚拟机状态异常，甚至影响服务器的状态。在x86平台下，这类指令共有19个，我自己称之为x86平台敏感指令中的边界指令。 就是因为x86平台指令集有上述缺陷，所以为了计算虚拟化技术在x86平台应用，各大虚拟化厂商推出了五花八门的虚拟化技术，其目的都是围绕“如何捕获模拟这19条边界指令”这一命题来设计。在很长一段时间，都是通过软件的方式来解决这个问题，其中包括无需修改内核的全虚拟化与需要修改内核的半虚拟化。尽管半虚拟化要求修改Guest OS内核的方式在一定程度上并不满足“ 等价性”要求，但是在性能上却明显优于全虚拟化。直到2005年Intel与AMD公司分别推出了VT-d与AMD-V，能够在芯片级别支持全虚拟化时，虚拟化技术才得到彻底完善，这就是现在称之为的硬件辅助虚拟化技术。 x86平台计算虚拟化解决方案全虚拟化全虚拟化（Full Virtualization）与半虚拟化（Para- Virtualization）的划分，是相对于是否修改Guest OS而言的。如下图所示，全虚拟化通过一层能够完整模拟物理硬件环境的虚拟软件，使得Guest OS与底层物理硬件彻底解耦。因此，Guest OS无需任何修改，虚拟化的环境对其完全透明，也就是说在全虚方案中，虚拟机感知不到自己处于虚拟化环境中，认为自己一直运行在物理硬件上。如下图所示： 在实现上，通常是结合特权指令的二进制翻译机制与一般指令的直接执行的方式。具体来说， 对于Guest OS发出的特权指令和边界指令，VMM会进行实时翻译，并缓存结果（目的是提高虚拟化性能），而对于一般级别的指令，则无需VMM干涉，可以直接在硬件上执行。异常-捕获-模拟的过程如下图所示： 由于虚拟化环境对Guest OS是完全透明的，全虚拟化模式对于虚拟机的迁移以及可移植性是最佳解决方案，虚拟机可以无缝地从虚拟环境迁移到物理环境中。但是，软件模拟实现的全虚拟化无疑会增加VMM的上下文切换，因为这种方案实现的虚拟机性能不如半虚拟化方案。 VMware的ESX系列产品 和Workstations系列产品是全虚拟化技术的典型产品。 半虚拟化如前所述，x86平台上一直存在一些Ring 3级别可以执行的边界指令，尽管全虚拟化模式通过实时译 这些特殊指令解决了这一问题，但是实现开销较大，性能并不如在实际物理机上运行。为了改善能，半虚拟化技术应运而生， “Para-Virtualization” 可理解为通过某种辅助的方式实现虚拟化。半虚拟化的解决方案如下图所示。 半虚拟化在Guest OS和虚拟层之间增加了一个特殊指令的过渡模块，通过修改Guest OS内核，将执行特权指令和边界指令替换为对虚拟层进行hypercall的调用方式来达到目的。同时，虚拟层也对内存管理、中断处理、时间同步提供了hypercall的调用接口。Hypercall调用过程如下图所示： 通过这种方式，虚拟机运行的性能得以显著提升。但是，对于某些无法修改内核的操作系统，比如：Windows，则不能使其运行于半虚拟化环境中。而且，由于需要修改Guest OS内核，无法保证虚拟机在物理环境与虚拟环境之间的透明切换。开源项目Xen和华为6.3版本之前的虚拟化解决方案Fusion Compute就是通过修改Linux内核以及提供I/O虚拟化操作的Domain 0的特殊虚拟机，使得运行于虚拟化环境上的虚拟机性能可以接近运行于物理环境的性能，属于半虚拟化技术方案的典型产品。但是，随着业务规模的增大，特殊虚机Domain 0是这种解决方案扩展性和性能方面的瓶颈。 硬件辅助虚拟化所谓“解铃还须系铃人”，针对敏感指令引发的一系列虚拟化问题，处理器硬件厂商最终给出了自己的解决方案。2005年Intel与AMD公司都效法IBM大型机虚拟化技术分别推出VT-x和AMD-V技术。如下图所示： 第一代VT-x与AMD-V都试图通过定义新的运行模式，使Guest OS恢复到Ring 0，而让VMM运行在比 Ring 0低的级别（可以理解为Ring -1）。比如： Intel公司的VT-x解决方案中，运行于非根模式下的Guest OS可以像在非虚拟化平台下一样运行于Ring 0级别，无论是Ring 0发出的特权指令还是Ring 3发出的敏感指令都会被陷入到根模式的虚拟层。VT-x解决方案具体如下图所示： VT-x与AMD-V推出之后，完美解决解决x86平台虚拟化的缺陷，且提升了性能，所以各个虚拟化厂商均快速开发出对应的产品版本，用于支持这种技术。比如：KVM-x86、Xen 3.0与VMware ESX 3.0之后的虚拟化产品。随后Intel和AMD在第二代硬件辅助虚拟化技术中均推出了针对I/O的硬件辅助虚拟化技术VT-d和IOMMU。 总结：x86平台下的三种虚拟化技术，都是围绕x86在虚拟化上的一些缺陷产生的。下图对三种虚拟 化技术进行了比较。 从图中可以看出，全虚拟化与半虚拟化的Guest OS的特权级别都被压缩在Ring 1中，而硬件虚拟化则将Guest OS恢复到了Ring 0级别。 在半虚拟化中，Guest OS的内核经过修改，所有敏感指令和特权指令都以Hypercall的方式进行调用，而在全虚拟化与硬件虚拟化中，则无需对Guest OS 进行修改。全虚拟化中对于特权指令和敏感指令采用了动态二进制翻译的方式，而硬件虚拟化由于在芯片中增加了根模式的支持，并修改 了敏感指令的语义，所有特权指令与敏感指令都能够自动陷入到根模式的VMM中。 利用二进制翻译的全虚拟化 硬件辅助虚拟化 操作系统协助/半虚拟化 实现技术 BT和直接执行 遇到特权指令转到root模式执行 Hypercall 客户操作系统修改/兼容性 无需修改客户操作系统，最佳兼容性 无需修改客户操作系统，最佳兼容性 客户操作系统需要修改来支持hypercall，因此它不能运行在物理硬件本身或其他的hypervisor上，兼容性差，不支持Windows 性能 差 全虚拟化下，CPU需要在两种模式之间切换，带来性能开销；但是，其性能在逐渐逼近半虚拟化。 好。半虚拟化下CPU性能开销几乎为0，虚机的性能接近于物理机。 应用厂商 VMware Workstation/QEMU/Virtual PC VMware ESXi/Microsoft Hyper-V/Xen 3.0/KVM Xen]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-03-虚拟化技术基础]]></title>
    <url>%2F2019%2F05%2F03%2F2019-05-03-%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[DPDK技术奠定了NFV领域数据包转发性能提升的基础，那么软硬件解耦后，在通用服务器上实现各功能网元，资源层面的隔离和共生问题就需要虚拟化技术来解决。虚拟化使用软件的方法重新定义划分IT资源，可以实现IT资源的动态分配、灵活调度、跨域共享，提高IT资源利用率，使IT资源能够真正成为社会基础设施，服务于各行各业中灵活多变的应用需求。 什么是虚拟化坦白地说，虚拟化就是欺骗。随着个人计算机的普及，“虚拟化”这个广泛使用的术语已经脱离了其技术本身，成为一种共同语言、流行文化和理念。自20世纪90年代互联网热潮的早期，任何与Web相关的活动均被称为“虚拟”。通过菲利浦•狄克的科幻小说、让•鲍德里亚的后现代主义研究，以及电影（如《黑客帝国》和《盗梦空间》）的影响，模拟现实的概念已经深入人心。 在技术领域，虚拟化是指利用“欺骗”技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。 传统构架是在每台物理机器上仅能拥有一个操作系统，而且多数情况下仅有一个负载。很难在服务器上运行多个主应用程序，否则可能会产生冲突和性能问题（上世纪90年代前，服务器一直面临的问题）。要解决上述冲突，最佳做法是每个服务器仅运行一个应用程序以避免这些问题。但是，这么做的结果是大部分时间资源利用率很低，且成本很高。因此，必须在加大投资和降低风险间寻找平衡。但是，随着业务的增长，随之而来的成本压力也变化，相关管理效率也会变低，需消耗的资源也会变大。 企业实施虚拟化战略的核心目的就是提高IT部门作为业务支撑部门的工作效率，达到节约成本与提高效率并重的目的。虚拟化的重要使命之一就是提高管理效率，从而降低成本、提高硬件使用率，把管理变得更加轻松。虚拟化的主攻方向集中在减少实体服务器的部署数量，并将实体机器上的操作系统及应用程序，无缝转移至虚拟机器上，以便集中管理这些不同平台的虚拟环境。 传统构架下，APP:OS:Phy = 1:1:1。这样子就造成资源利用率低，为不造成资源浪费，会增加APP部署。进而产生的影响就是不同应用之间的资源抢占，隔离性差。而OS主要提供应用运行的环境，在资源调度方面相对薄弱，不能完全有效解决以上问题。 为在不造成冲突的前提下提高资源利用率，最好是在一个OS上部署一个APP，于是就出现了虚拟化的技术的雏形。在一台主机上部署多个虚拟客户机并安装OS，每个OS安装一个APP，这样就解决了问题，达到的效果是APP:OS:Phy = n:n:1。 虚拟化之后实现了上层操作系统与下层硬件的解耦，就是说操作系统不再依赖物理的硬件，而是在VMM层上建立OS，由VMM层来实现OS对硬件的需求。 虚拟化的几个重要概念 宿主机（Host Machine）：指物理机资源，被Hypervisor用来执行一个或多个虚拟机器的电脑称为主机。 客户机（Guest Machine）：指虚拟机资源，在Hypervisor之上运行多个虚拟机器则称为客户机。 Guest OS和Host OS：如果将一个物理机虚拟成多个虚拟机，则称物理机为Host Machine，运行在其上的OS为Host OS；多个虚拟机称为Guest Machine，运行在其上的OS为Guest OS。 Hypervisor：通过虚拟化层的模拟，虚拟机在上层软件看来就是一个真实的机器，这个虚拟化层一般称为虚拟机监控机（Virtual Machine Monitor，VMM）。需要注意一点：在VMware的ESX虚拟化架构中VMM只是Hypervisor中一个进程，因此在这种场景下VMM不等于Hypervisor。 什么是Hypervisor（VMM） 维基百科的定义如下：Hypervisor，又称虚拟机器监视器（英语：virtual machine monitor，缩写为 VMM），是用来建立与执行虚拟机器的软件、固件或硬件。 通俗的讲：hypervisor是一种运行在物理服务器和操作系统之间的中间层软件，可以允许多个操作系统和应用共享一套基础物理硬件。可以将hypervisor看做是虚拟环境中的“元”操作系统，可以协调访问服务器上的所有物理设备和虚拟机，所以又称为虚拟机监视器（virtual machine monitor）。 hypervisor是所有虚拟化技术的核心，非中断的支持多工作负载迁移是hypervisor的基本功能。当服务器启动并执行hypervisor时，会给每一台虚拟机分配适量的内存，cpu，网络和磁盘资源，并且加载所有虚拟机的客户操作系统。当前主流的Hypervisor有微软的Hyper-V，VMware、Xen和KVM，但在电信云NFV领域主要用到的就是KVM，在后续虚拟化技术分类中，会专门讲解KVM的相关部署和优化，同时为加深大家对KVM的理解，也会讲 一点Xen的知识，毕竟在KVM广泛应用之前，云架构底层的虚拟化技术都是Xen。 虚拟化和hypervisor到底什么关系？ 虚拟化就是通过某种方式隐藏底层物理硬件的过程，从而让多个操作系统可以透明地使用和共享它。这种架构的另一个更常见的名称是平台虚拟化。在典型的分层架构中，提供平台虚拟化的层称为 hypervisor （有时称为虚拟机管理程序 或 VMM）。来宾操作系统称为虚拟机（VM），因为对这些 VM 而言，硬件是专门针对它们虚拟化的。如下图示： 在上图中可以看到，hypervisor 是提供底层机器虚拟化的软件层（在某些情况下需要处理器支持）。并不是所有虚拟化解决方案都是一样的（详见Hypervisor分类部分）。客户机操作系统（GuestOS）对机器的底层资源的访问通过VMM来实现。hypervisor 面对的对象不是客户机中的进程，而是整个客户操作系统（GuestOS）。 hypervisor主要可以划分为两大类：类型1和类型2，以及在此基础上衍生出来的混合类型和操作系统类型。 类型1这种hypervisor是可以直接运行在物理硬件之上的。如下图所示，也就是说它不需要宿主机操作系统（HostOS）的支持，本身就可以管理底层硬件的资源，其本质是在Hypervisor中嵌入了一个精简的Linux操作系统内核。Xen 和 VMWare 的 ESXi 都属于这个类型，这种虚拟化类型的模型如下图所示，其特点就是需要硬件的支持，转发性能强（因为少了HostOS这一层转发），VMM就是HostOS。 类型2这种hypervisor运行在另一个操作系统（运行在物理硬件之上）中。如下图所示，也就是说这种类型的Hypervisor是部署在HostOS之上的，从HostOS角度来看，其上层的所有VM都对应Hypervisor这一个进程。从VM的角度来看，其访问底层硬件资源需要Hypervisor和HostOS共同配合完成。KVM、VirtualBox 和 VMWare Workstation 都属于这个类型。这种虚拟化类型的模型如下图所示，其特点就是比较灵活，比如支持虚拟机嵌套（嵌套意味着可以在虚拟机中再运行hypervisor），但是转发性能明显不如类型1。 目前，随着转发性能提升需求和应用的微服务化架构需求，在类型2的基础上又演进出混合类型虚拟化和基于HostOS的操作系统虚拟化。 混合虚拟化：通过在主机的操作系统中增加虚拟硬件管理模块，通过虚拟硬件管理模块来生成各个虚拟机。属于类型2虚拟化的一种增强模型。特点是相对于类型2虚拟化，没有冗余，性能高，可支持多种操作系统。但是，需要底层硬件支持虚拟化扩展功能。现阶段的KVM和Hyper-V都属于这种增强型类型2虚拟化技术。 操作系统虚拟化：没有独立的hypervisor层。相反，主机操作系统本身就负责在多个虚拟服务器之间分配硬件资源，并且让这些服务器彼此独立。最重要的前提是：如果使用操作系统层虚拟化，所有虚拟服务器必须运行同一操作系统(不过每个实例有各自的应用程序和用户账户)，其本质就是操作系统上面应用程序的一个进程，主要在应用的微服务化架构场景中使用。特点是：简单、易于实现，管理成本非常低。但是，隔离性差，多个虚拟化实例共享同一个操作系统。最典型就是目前炙手可热的容器技术Docker和Virtuozzo。 虚拟化的特征和优势从前面描述可知，虚拟化技术就是一个“大块的资源”逻辑分割成“具有独立功能的小块资源”，这个功能通过Hypervisor来实现。对于服务器领域而言，通过Hypervisor将一个物理服务器虚拟化成若干个小的逻辑服务器，每个逻辑服务器具有与物理服务器相同的功能，所有逻辑服务器的资源总和等于物理服务器的全部资源。 因此，运行在Hypervisor上的逻辑服务器，其本质就是由物理服务器上一个个文件组成。相对物理服务器，天生具备分区、隔离、封装和相对硬件独立四大特征。 1）分区：在单一物理服务器同时运行多个虚拟机。分区意味着虚拟化层为多个虚拟机划分服务器资源的能力；每个虚拟机可以同时运行一个单独的操作系统（相同或不同的操作系统），使您能够在一台服务器上运行多个应用程序。每个操作系统只能看到虚拟化层为其提供的“虚拟硬件”（虚拟网卡、CPU、内存等），以使它认为运行在自己的专用服务器上。 2）隔离：在同一服务器虚拟机之间相互隔离。虚拟机是互相隔离的。例如：一个虚拟机的崩溃或故障（例如，操作系统故障、应用程序崩溃、驱动程序故障，等等）不会影响同一服务器上的其它虚拟机；一个虚拟机中的病毒、蠕虫等与其它虚拟机相隔离，就像每个虚拟机都位于单独的物理机器上一样。可以通过资源控制以提供性能隔离，比如：可以为每个虚拟机指定最小和最大资源使用量，以确保某个虚拟机不会占用所有的资源而使得同一系统中的其它虚拟机无资源可用；可以在单一机器上同时运行多个负载/应用程序/操作系统，而不会出现因为传统 x86 服务器体系结构的局限性发生DLL冲突等问题。 3）封装：整个虚拟机都保存在文件中，可以通过移动文件的方式来迁移虚拟机。封装意味着将整个虚拟机（硬件配置、BIOS 配置、内存状态、磁盘状态、CPU 状态）储存在独立于物理硬件的一小组文件中。这样，只需复制几个文件就可以随时随地根据需要复制、保存和移动虚拟机。 4）相对硬件独立：无需修改即可在任意服务器上运行（主要基于全虚技术的虚拟机，半虚技术的虚拟机只支持开源操作系统，如Linux）。因为虚拟机运行于虚拟化层之上，所以只能看到虚拟化层提供的虚拟硬件，无需关注物理服务器的情况。这样，虚拟机就可以在任何 x86 服务器（IBM、Dell、HP等）上运行而无需进行任何修改。这打破了操作系统和硬件以及应用程序和操作系统/硬件之间的约束。 物理资源在经过Hypervisor虚拟化后，在资源利用率、独立性、运行效率和安全性等方面与传统物理服务器相比均有不同的优势。 1）资源利用率：虚拟化前每台主机一个操作系统，系统的资源利用率低。虚拟化后，主机与操作系统不一一对应，按需分配使用，系统的资源利用率高。 2）独立性：虚拟化前软硬件紧密结合，硬件成本高昂且不够灵活。虚拟化后，操作系统和硬件不相互依赖，虚拟机独立于硬件，能在任何硬件上运行。 3）程序运行效率：虚拟化前同一台主机上同时运行多个程序容易产生冲突，运行效率较低。虚拟化后，操作系统和应用程序被封装成单一个体，不同个体间不冲突。同一台机器上运行多个程序，效率高。 4）安全性：虚拟化前，故障影响范围大，安全性较差。虚拟化后，通过资源的池化，有强大的安全和故障隔离机制。 虚拟化技术的发展最近几年，随着云计算技术广泛应用，虚拟化技术也被大家所关注。其实，虚拟化技术的出现要早于云计算技术约半个世纪。在上世纪60年代，虚拟化技术就已经在大型机上有所应用，在1999年小型机上出现逻辑分区的概念，这就是存储虚拟化的雏形。而到了2000年，在x86平台上VMware首先提出了平台虚拟化技术的概念，以及后续随着CPU速度越来越快，Intel和AMD分别在CPU指令架构中引入虚拟化指令，在服务器领域和数据中心范围内虚拟化技术得到的极大发展，从而催生了云计算技术的出现。可以说，虚拟化技术是云计算技术得以实现并推广落地的重要基石，同时，随着云计算技术的演进，虚拟化技术也同样在不断演进，从最早的计算虚拟化发展到目前的应用虚拟化，两者是一种相辅相成，螺旋式推进的关系。 云计算技术从诞生到当前，共经历了3个阶段，分别称为云计算1.0、云计算2.0和云计算3.0，在不同的阶段，虚拟化技术的表现形式和关注点也不相同，两者关系如下图所示： 在云计算1.0时代，主要是将传统IT硬件基础设施转换为虚拟化基础设施，来提升资源利用率。该阶段的关键特征体现为：通过计算虚拟化技术的引入，将企业IT应用与底层的基础设施彻底分离解耦，将多个企业IT应用实例及运行环境(客户机操作系统，GuestOS)复用在相同的物理服务器上，并通过虚拟化集群调度软件，将更多的IT应用复用在更少的服务器节点上，从而实现资源利用效率的提升。 在云计算2.0时代，主要是向云租户提供池化资源服务和精细化自动管理，推动企业业务的云化演进。该阶段的关键特征体现为：不仅通过计算虚拟化完成CPU、内存、裸金属服务器等池化资源的集中管理和自动调度，同时引入存储虚拟化和网络虚拟化技术，实现数据中心内部存储资源和网络资源的池化集中管理和统一调度。面向内部和外部的租户，将原本需要通过数据中心管理员人工干预的基础设施资源复杂低效的申请、释放与配置过程，转变为一键式全自动化资源发放服务过程。这个阶段大幅提升了企业基础设施资源的快速敏捷发放能力，缩短了基础设施资源准备周期，实现资源的按需弹性供给。为企业核心业务走向敏捷，更好地应对瞬息万变的竞争与发展奠定了基础。云计算2.0阶段面向云租户的基础设施资源服务供给，可以是虚拟机形式，可以是容器(轻量化虚拟机)，也可以是物理机形式。该阶段的企业云化演进，暂时还不涉及基础设施层之上的IT应用与中间件、数据库软件架构的变化。 在云计算3.0时代，面向应用开发者及管理维护者提供分布式微服务化应用架构和大数据智能化服务。 该阶段的关键特征体现为：企业IT应用架构逐步开始去IOE化，依托开源增强、跨不同业务应用领域高度共享的数据库、中间件平台服务层以及功能更加轻量化解耦、数据与应用逻辑彻底分离的分布式无状态化架构，从而实现支撑企业业务敏捷化、智能化以及资源利用效率提升。 数据中心内部虚拟化技术分类目前，在数据中心内虚拟服务器、虚拟网络、虚拟存储、虚拟设备和其他“虚拟技术”等已对传统基础设施产生了逆袭。在上述云计算的三个阶段，使得虚拟化技术和云计算技术得到极大发展的关键就是2.0时代，主要的特征就是从计算虚拟化走向存储虚拟化和网络虚拟化。 从支撑云计算按需、弹性分配资源，与硬件解耦的虚拟化技术的角度来看，云计算早期阶段主要聚焦在计算虚拟化领域。事实上，计算虚拟化技术早在IBM 370时代就已经在其大型机操作系统上诞生。技术原理是通过在OS与裸机硬件之间插入虚拟化层，来在裸机硬件指令系统之上仿真模拟出多个370大型机的“运行环境”，使得上层“误认为”自己运行在一个独占系统之上，实际上是由计算虚拟化引擎在多个虚拟机之间进行CPU分时调度，同时对内存、I/O、网络等访问也进行访问屏蔽。 后来，当x86平台演进成为在IT领域硬件平台的主流之后，VMware ESX、XEN、KVM等依托于单机OS的计算虚拟化技术才将IBM 370的虚拟化机制在x86服务器的硬件体系架构下实现，并且在单机/单服务器虚拟化的基础上引入了具备虚拟机动态迁移和HA调度能力的中小集群管理软件，比如：VMware的vCenter/vSphere、Citrix的XEN Center和华为的FusionSphere等，从而形成当前的计算虚拟化主体。 与此同时，作为数据信息持久化载体的存储已经逐步从服务器计算中剥离出来，与必不可少的CPU计算能力一样，在数据中心发挥着至关重要的作用。现在数据中心内部不再封闭，内部服务器互访和对外部互联网访问需求，使得存储和网络也同计算一样，成为数据中心IT基础设施不可或缺的“三大要素”。就数据中心端到端基础设施解决方案而言，不仅需要计算资源的按需分配、弹性伸缩、与硬件解耦的需求，对存储资源和网络资源需求同样如此，因此，存储虚拟化和网络虚拟化技术应运而生。 对于普通x86服务器来说，CPU和内存资源虚拟化后再将其以vCPU/vMemory的方式，按需供给用户/租户使用。计算虚拟化中仅存在资源池的“大分小”的问题。然而对于存储来说，由于硬盘的容量有限，客户/租户对数据容量的需求越来越大，因此必须对数据中心内多个分布式服务器存储资源，比如：服务器内的存储资源、外置SAN/NAS等进行“小聚大”的整合，组成存储资源池。 这个存储资源池，可能是单一厂家提供的同构资源池，也可以是被存储虚拟化层整合成为跨多厂家异构的统一资源池。各种存储资源池均能以统一的块存储、对象存储或者文件存储格式进行访问。数据存储虚拟化示意图如下所示： 对于数据中心网络来说，网络对于业务应用，作为连接服务器节点的计算和存储资源是一种实实在在的资源需求。传统数据中心内部，网络交换功能都是在物理交换机和路由器设备上完成的，网络功能对上层业务应用而言仅仅体现为一个一个被通信链路连接起来的孤立的“盒子”，无法动态感知来自上层业务的网络功能需求，完全需要人工配置的方式来实现对业务层网络组网与安全隔离策略的需要。 在云时代多租户虚拟化的环境下，不同租户对于边缘的路由及网关设备的配置管理需求也存在极大的差异化，即使物理路由器和防火墙自身的多实例能力也无法满足云环境下租户数量的要求，如果采用与租户数量等量的路由器与防火墙设备，成本上无法接受。于是，伯克利大学的Nick Mckeown教授提出将网络自身的功能从专用封闭平台迁移到服务器通用x86平台上来，SDN概念从此诞生。 网络资源虚拟化后，服务器节点的应用VM连接差异化，就可由云操作系统来自动化地创建和销毁，并通过一次性建立起来的物理网络连接矩阵，进行任意两个网络端节点之间的虚拟通讯链路建立，以及必要的安全隔离保障，从而实现业务驱动的网络自动化管理配置，大幅度降低数据中心网络管理的复杂度。从资源利用率来看，任意两个虚拟网络节点之间的流量带宽，都需要通过物理网络来交换和承载，只要不超过物理网络的资源配额上限（一般建议物理网络按照无阻塞的CLOS模式来设计实施)，一旦虚拟节点被释放，其所对应的网络带宽占用也将被同步释放，因此也就相当于实现对物理网络资源的最大限度的“网络资源动态共享”。通俗点讲，网络虚拟化让多个盒子式的网络实体第一次以一个统一整合的“网络资源池”的形态，出现在业务应用层面前，同时与计算和存储资源之间，也有了统一协同机制。网络虚拟化示意图如下图所示： 上面基础设施虚拟化技术的“三要素”是电信云领域需要重点关注的三个分类，属于云计算中IaaS服务部分的内容。 除此之外，还有基于PaaS和SaaS的桌面虚拟化技术，这部分内容因电信云领域目前不涉及，因此在本站的云计算分类中会有相关介绍，这里不再赘述。后续，会在NFV关键技术分类中按照计算虚拟化、存储虚拟化和网络虚拟化三大部分逐一介绍，并会重点KVM的部署和性能调优。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-02-Linux系统命令-第七篇《磁盘和文件系统管理命令》]]></title>
    <url>%2F2019%2F05%2F02%2F2019-05-02-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%B8%83%E7%AF%87%E3%80%8A%E7%A3%81%E7%9B%98%E5%92%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[fdisk：磁盘分区工具fdisk是Linux下常用的磁盘分区工具。受mbr分区表的限制，fdisk工具只能给小于2TB的磁盘划分分区。如果使用fdisk对大于2TB的磁盘进行分区，虽然可以分区，但其仅识别2TB的空间，所以磁盘容量若超过2TB，就要使用parted分区工具（后面会讲）进行分区。 语法格式：fdisk [option] [device] 重要参数选项 【使用示例】 1）显示系统磁盘分区列表 1234567891011[root@C7-Server01 ~]# fdisk -lDisk /dev/sda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d2190Device Boot Start End Blocks Id System/dev/sda1 * 2048 821247 409600 83 Linux/dev/sda2 821248 17598463 8388608 82 Linux swap / Solaris/dev/sda3 17598464 104857599 43629568 83 Linux 上述信息每列功能说明具体如下： Device：分区，这里有三个分区； Boot：启动分区，用*表示的是启动分区； Start：表示开始的柱面； End：表示结束的柱面； Blocks：block块数量； Id：分区类型Id； System：分区类型 2）模拟添加第二块磁盘 #给C7 Server01再挂载一块20G的磁盘 12345678910111213141516171819202122232425262728293031# 重启系统检查磁盘分区状态[root@C7-Server01 ~]# fdisk -l# 新添加的磁盘为/devsdb，表示sata接口的第二块磁盘Disk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/sda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d2190 Device Boot Start End Blocks Id System/dev/sda1 * 2048 821247 409600 83 Linux/dev/sda2 821248 17598463 8388608 82 Linux swap / Solaris/dev/sda3 17598464 104857599 43629568 83 Linux# 如果不想显示其他分区，还可以指定分区查看[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 3）交互式分区实战 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 查看当前系统的分区设备信息[root@C7-Server01 ~]# ls -l /dev/sd*brw-rw---- 1 root disk 8, 0 May 2 09:41 /dev/sdabrw-rw---- 1 root disk 8, 1 May 2 09:41 /dev/sda1brw-rw---- 1 root disk 8, 2 May 2 09:41 /dev/sda2brw-rw---- 1 root disk 8, 3 May 2 09:41 /dev/sda3brw-rw---- 1 root disk 8, 16 May 2 09:41 /dev/sdb# 对/dev/sdb进行交互式分区[root@C7-Server01 ~]# fdisk /dev/sdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0xc0a89c6e.Command (m for help): m # 输入m，打印帮助信息Command action a toggle a bootable flag # 设置引导扇区 b edit bsd disklabel # 编辑bsd卷标 c toggle the dos compatibility flag # 设置dos兼容分区 d delete a partition # 删除一个分区 g create a new empty GPT partition table # 创建一个新的且为空的GPT分区表 G create an IRIX (SGI) partition table # 创建一些IRIX分区表 l list known partition types # 查看分区类型对应的编号列表 m print this menu # 打印帮助菜单 n add a new partition # 新建一个分区 o create a new empty DOS partition table # 创建一个新的空的DOS分区表 p print the partition table # 打印分区表 q quit without saving changes # 退出且不保存更改 s create a new empty Sun disklabel # 创建一个新的空的sun卷标 t change a partition's system id # 更改分区系统的id u change display/entry units # 改变/显示条目的单位 v verify the partition table # 验证分区表 w write table to disk and exit # 将操作写入分区表并退出程序 x extra functionality (experts only) # 额外的功能Command (m for help): n # 输入n，创建一个分区Partition type: p primary (0 primary, 0 extended, 4 free) # 创建主分区，编号1-4 e extended # 创建扩展分区Select (default p): p # 输入p，创建主分区Partition number (1-4, default 1): 1 # 输入1，设置第一个主分区编号为1First sector (2048-41943039, default 2048): # 直接回车，默认采用2048作为起始柱面Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-41943039, default 41943039): +5G # 设置结束柱面，一般情况下如果整个磁盘采用一个分区，这里就直接回车就行，否则，采用+size的方式进行分区大小设置，我们这里给第一个分区设置5G的空间 Partition 1 of type Linux and of size 5 GiB is setCommand (m for help): p # 输入p，打印刚创建的分区信息Disk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xc0a89c6e Device Boot Start End Blocks Id System/dev/sdb1 2048 10487807 5242880 83 LinuxCommand (m for help): # 重复上述步骤再次创建三个主分区，大家自己练习，注意全部完成后要按w保存，否则分区信息丢失。。。# 打印最终的分区信息[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xc0a89c6e Device Boot Start End Blocks Id System/dev/sdb1 2048 10487807 5242880 83 Linux/dev/sdb2 10487808 20973567 5242880 83 Linux/dev/sdb3 20973568 31459327 5242880 83 Linux/dev/sdb4 31459328 41943039 5241856 83 Linux# 不重启情况下通知内核新的分区表已生效[root@C7-Server01 ~]# partprobe /dev/sdb 需要注意一点：用交互指令d删除分区时要小心，要注意分区的序号，如果删除了扩展分区，那么扩展分区之下的逻辑分区都会删除，所以操作时一定要小心。如果不小心操作错了，直接使用交互指令q不保存退出，这样先前的操作就会无效。如果输入w（保存指令）则会保存所有修改。 4）非交互式分区 上面的示例是交互式分区，有时需要在脚本中自动执行分区，这时需要非交互式分区。如果使用fdisk分区工具来完成，可以使用如下两种办法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 首先备份/dev/sdb分区表信息[root@C7-Server01 ~]# dd if=/dev/sdb of=sdb-partb.info bs=1 count=512512+0 records in512+0 records out512 bytes (512 B) copied, 0.00159899 s, 320 kB/s# 然后清除分区数据[root@C7-Server01 ~]# dd if=/dev/zero of=/dev/sdb bs=1 count=512512+0 records in512+0 records out512 bytes (512 B) copied, 0.0017926 s, 286 kB/s# 查看/dev/sdb分区信息，确认是否被清除[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes# 方法一：使用echo指令模拟交互式分区输入过程，自动执行分区# 此方法需要注意最后一次分区时不要输入+size大小，直接回车即可（虚拟机虚拟磁盘原因，在物理机上无此限制）# 脚本中p就是指主分区，如果要分扩展分区，将p改为m[root@C7-Server01 ~]# echo -e "n\np\n1\n\n+5G\nw\n" | fdisk /dev/sdb...[root@C7-Server01 ~]# echo -e "n\np\n2\n\n+5G\nw\n" | fdisk /dev/sdb...[root@C7-Server01 ~]# echo -e "n\np\n3\n\n+5G\nw\n" | fdisk /dev/sdb...[root@C7-Server01 ~]# echo -e "n\np\n4\n\n\nw\n" | fdisk /dev/sdb...# 查看分区信息[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xd0b6c715 Device Boot Start End Blocks Id System/dev/sdb1 2048 10487807 5242880 83 Linux/dev/sdb2 10487808 20973567 5242880 83 Linux/dev/sdb3 20973568 31459327 5242880 83 Linux/dev/sdb4 31459328 41943039 5241856 83 Linux 方法二：也是模拟交互式分区的过程，将输入的交互式指令写入一个文本文件，然后通过标准输入的方式传递给fdisk /dev/sdb指令，其中交互式指令中回车在文本中用换行替代。与上面的实现方式类似，请大家自行练习。 partprobe：更新内核的硬盘分区表信息partprobe命令用于在硬盘分区发生改变时，更新Linux内核中的硬盘分区表数据。有时在使用fdisk、part命令对硬盘进行分区后，会发现找不到新分区，此时需要重启系统才能使修改生效，但使用partprobe可以不重启系统就让修改的分区表生效。 语法格式：partprobe [option] 重要参数选项 【使用示例】 12345# 更新分区表信息# 最好加上具体的磁盘，否则可能会报错，那就只能重启系统[root@C7-Server01 ~]# partprobe /dev/sdb parted：磁盘分区工具对于小于2TB的磁盘可以用fdisk和parted命令进行分区，这种情况一般采用fdisk命令，但对于大于2TB的磁盘则只能用parted分区，且需要将磁盘转换为GPT格式。 语法格式：parted [option] [device] 重要参数选项 【分区命令】 通过parted -h或直接parted进入交互模式后，输入h查看帮助信息 123456789101112131415161718192021222324252627282930313233343536373839[root@C7-Server01 ~]# parted -hUsage: parted [OPTION]... [DEVICE [COMMAND [PARAMETERS]...]...]Apply COMMANDs with PARAMETERS to DEVICE. If no COMMAND(s) are given, run ininteractive mode.OPTIONs:-h, --help displays this help message-l, --list lists partition layout on all block devices-m, --machine displays machine parseable output-s, --script never prompts for user intervention-v, --version displays the version-a, --align=[none|cyl|min|opt] alignment for new partitionsCOMMANDs:align-check TYPE N check partition N for TYPE(min|opt)alignmenthelp [COMMAND] print general help, or help onCOMMANDmklabel,mktable LABEL-TYPE create a new disklabel (partitiontable)mkpart PART-TYPE [FS-TYPE] START END make a partitionname NUMBER NAME name partition NUMBER as NAMEprint [devices|free|list,all|NUMBER] display the partition table,available devices, free space, all found partitions, or a particularpartitionquit exit programrescue START END rescue a lost partition near STARTand ENDresizepart NUMBER END resize partition NUMBERrm NUMBER delete partition NUMBERselect DEVICE choose the device to editdisk_set FLAG STATE change the FLAG on selected devicedisk_toggle [FLAG] toggle the state of FLAG on selecteddeviceset NUMBER FLAG STATE change the FLAG on partition NUMBERtoggle [NUMBER [FLAG]] toggle the state of FLAG on partitionNUMBERunit UNIT set the default unit to UNITversion display the version number andcopyright information of GNU PartedReport bugs to bug-parted@gnu.org 【使用示例】 1）显示分区的情况 1234567891011121314151617181920[root@C7-Server01 ~]# parted -lModel: ATA VMware Virtual S (scsi)Disk /dev/sda: 53.7GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 1049kB 420MB 419MB primary xfs boot2 420MB 9010MB 8590MB primary linux-swap(v1)3 9010MB 53.7GB 44.7GB primary xfsModel: ATA VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 1049kB 5370MB 5369MB primary2 5370MB 10.7GB 5369MB primary3 10.7GB 16.1GB 5369MB primary4 16.1GB 21.5GB 5368MB primary 上述信息显示系统两块磁盘的分区信息，包括大小，起始，终止柱面，类型，文件系统类型等。磁盘/dev/sda为系统盘，有3个主分区，其中2个位xfs文件系统，1个位swap分区。磁盘/dev/sdb为刚添加的数据盘，有4个主分区，每个大小5G，因为还没有格式化，所以没有文件系统格式。 2）通过给虚拟机再挂载一块100G的磁盘/dev/sdc，来模拟2TB磁盘用parted分区工具进行分区 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 通过交互式方式完成分区[root@C7-Server01 ~]# parted /dev/sdcGNU Parted 3.1Using /dev/sdcWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) mklabel gpt # 为/dev/sdc磁盘创建GPT分区，大于2T的磁盘必须进行这一步(parted) mkpart primary 0 40G # 创建主分区，大小为40GWarning: The resulting partition is not properly aligned for best performance.Ignore/Cancel? Ignore # 输入Ignore，忽略告警信息(parted) p # 输入p，显示分区表信息 Model: ATA VMware Virtual S (scsi)Disk /dev/sdc: 107GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 17.4kB 40.0GB 40.0GB primary # 第一个主分区已经创建完毕(parted) mkpart logical 40G 50G # 创建第一个逻辑分区，大小为10G(parted) p # 打印分区表信息 Model: ATA VMware Virtual S (scsi)Disk /dev/sdc: 107GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 17.4kB 40.0GB 40.0GB primary 2 40.0GB 50.0GB 10.0GB logical # 第一个逻辑分区创建完毕(parted) mkpart logical 50G 70G # 创建第二个逻辑分区，大小为20G(parted) mkpart logical 70G 100G # 创建第三个逻辑分区，大小为30G (parted) p # 打印分区表信息 Model: ATA VMware Virtual S (scsi)Disk /dev/sdc: 107GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 17.4kB 40.0GB 40.0GB primary 2 40.0GB 50.0GB 10.0GB logical 3 50.0GB 70.0GB 20.0GB logical 4 70.0GB 100GB 30.0GB logical(parted) quit # 退出 Information: You may need to update /etc/fstab.# 查看系统设备信息[root@C7-Server01 ~]# ll -h /dev/sd*brw-rw---- 1 root disk 8, 0 May 2 11:51 /dev/sdabrw-rw---- 1 root disk 8, 1 May 2 11:51 /dev/sda1brw-rw---- 1 root disk 8, 2 May 2 11:51 /dev/sda2brw-rw---- 1 root disk 8, 3 May 2 11:51 /dev/sda3brw-rw---- 1 root disk 8, 16 May 2 11:51 /dev/sdbbrw-rw---- 1 root disk 8, 17 May 2 11:51 /dev/sdb1brw-rw---- 1 root disk 8, 18 May 2 11:51 /dev/sdb2brw-rw---- 1 root disk 8, 19 May 2 11:51 /dev/sdb3brw-rw---- 1 root disk 8, 20 May 2 11:51 /dev/sdb4brw-rw---- 1 root disk 8, 32 May 2 12:01 /dev/sdcbrw-rw---- 1 root disk 8, 33 May 2 12:01 /dev/sdc1brw-rw---- 1 root disk 8, 34 May 2 12:01 /dev/sdc2brw-rw---- 1 root disk 8, 35 May 2 12:01 /dev/sdc3brw-rw---- 1 root disk 8, 36 May 2 12:01 /dev/sdc4 用parted磁盘分区工具非交互式创建分区的方法类似fdisk，唯一区别就是将交互式下输入的命令作为参数传递parted工具，比如：将交互执行的命令直接放在parted /dev/sdb后面就实现非交互分区了。整体上实现其实比fdisk工具简单，请大家自行练习。 mkfs：创建Linux文件系统 （格式化）mkfs命令用于在指定的设备（或硬盘分区等）上格式化并创建文件系统，fdisk和parted等分区工具相当于建房的人，把房子（硬盘），分成几居室（分区），mkfs就相当于对不同的居室装修（创建文件系统）了，只有装修好的房子（有文件系统）才能入住，分区也是一样，只有格式化创建文件系统（存取数据的机制）后，才能用来存取数据。 mkfs只是一个前端命令，它通过-t参数指定文件系统类型后会调用相应的命令mkfs.fstype。因此，也可以直接使用mkfs.ext4、mkfs.xfs这类命令创建相应的文件系统。 语法格式：mkfs [option] [filesys] 重要参数选项 【使用示例】 通过-t选项创建xfs文件系统和ext4文件系统 123456789101112# 创建/dev/sdb1分区的文件系统为xfs[root@C7-Server01 ~]# mkfs -t xfs /dev/sdb1meta-data=/dev/sdb1 isize=512 agcount=4, agsize=327680 blks= sectsz=512 attr=2, projid32bit=1= crc=1 finobt=0, sparse=0data = bsize=4096 blocks=1310720, imaxpct=25= sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=2560, version=2= sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 确认是否创建成功 123456789101112# 使用mkfs.xfs创建/dev/sdb2的文件系统为xfs[root@C7-Server01 ~]# mkfs.xfs /dev/sdb2meta-data=/dev/sdb2 isize=512 agcount=4, agsize=327680 blks= sectsz=512 attr=2, projid32bit=1= crc=1 finobt=0, sparse=0data = bsize=4096 blocks=1310720, imaxpct=25= sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=2560, version=2= sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 再次确认 创建ext4文件系统的方法类似，请大家自行练习。 dumpe2fs：导出ext2/ext3/ext4文件系统信息dumpe2fs命令用于导出ext2/ext3/ext4文件系统内部的相关信息，例如：文件系统的组成包含超级快、块组、inode、block等信息。如果要导出xfs文件系统的信息，需要使用xfs_info指令。 语法格式：dumpe2fs [option] [device] 重要参数选项 【使用示例】 1）查看分区文件系统的inode信息 123456789101112131415161718192021222324252627282930# 创建/dev/sdb3分区为ext4文件系统格式[root@C7-Server01 ~]# mkfs.ext4 /dev/sdb3mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks327680 inodes, 1310720 blocks65536 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=134217728040 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done # 导出/dev/sdb3分区的中inode相关信息[root@C7-Server01 ~]# dumpe2fs /dev/sdb3 | egrep -i "inode size|inode count"dumpe2fs 1.42.9 (28-Dec-2013)Inode count: 327680Inode size: 256 2）在xfs文件系统下，使用xfs_info指令查看/dev/sdb1的inode信息和block信息 12345678910111213# 首先需要个/dev/sdb1分区设置一个挂载点，然后使用xfs_info + 挂载点的方式进行查看[root@C7-Server01 ~]# mount /dev/sdb1 ~/mydata/[root@C7-Server01 ~]# xfs_info ~/mydata/meta-data=/dev/sdb1 isize=512 agcount=4, agsize=327680 blks= sectsz=512 attr=2, projid32bit=1= crc=1 finobt=0 spinodes=0data = bsize=4096 blocks=1310720, imaxpct=25= sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal bsize=4096 blocks=2560, version=2= sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 fsck：检查并修复Linux文件系统fsck命令用于检查并修复文件系统中的错误，即针对有问题的系统或磁盘进行修复，类似的命令还有e2fsck命令。有关fsck的使用需要特别注意的是：1）文件系统必须是卸载状态，否则可能会出现故障。2）不要对正常的分区使用fsck，在不加参数的情况下，fsck会根据/etc/fstab进行文件系统检查，这相当于fsck-As参数的功能。 注意：必须卸载文件系统后才能对其进行检查，否则可能会出现错误。平时没有必要使用这个命令检查磁盘，只有当系统开机显示磁盘错误时，才需要执行。 语法格式：fsck [option] [filesys] 重要选项参数 【使用示例】 1）系统开机通过fsck自检 Linux在开机过程中系统会自动调用fsck命令对需要自检的磁盘进行自检，如下图： 这是因为系统开机过程中会优先读取/etc/fstab文件，当最后一列设置为1或2时，这个磁盘在开机时就会调用fsck进行自检，fstab的文件（man fstab看帮助）信息如下： 123456789101112131415161718[root@C7-Server01 ~]# cat /etc/fstab ## /etc/fstab# Created by anaconda on Sun Apr 7 20:32:53 2019## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=0887567f-1df6-425f-ba3d-ce58584279e0 / xfs defaults 0 0UUID=26954b3f-dd29-4a25-85ba-471cdbbf82df /boot xfs defaults 0 0UUID=1f9ad06d-860c-4166-b142-6b633ee82851 swap swap defaults 0 0 在CentOS6系统中，系统分区的根分区最后一列一般是1，boot分区最后一列是2，其余是0。但是在CentOS7系统中，为了不影响系统启动，把系统分区最后1列均设为0，即开机不自检。 需要提醒一下：有时我们自己增加硬盘规划分区，一般最后一列都设置为0，即开机过程中不对磁盘检查，否则，一旦自定义挂载的磁盘有问题，会影响系统启动。 如果真有问题，可以在启动系统后人为进行检查。 2）Linux断电后重启故障修复 当Linux系统遭遇突然断电等非正常关机操作时，很容易导致文件系统数据损坏，造成系统不能重新启动，此时，屏幕出现的提示可能是如下内容： 12345* AN error occurred during the file system check*** xxx*** xxxGive root password for maintenance(or type Control-D to continue): 此时根据系统提示输入root用户的密码，注意而不是直接按Control-D继续，会再重启。 当输入正确的密码之后，正常会出现下面的提示： 1(Repair filesystem) 1 # 此时就可以输入fsck或者fsck-A对磁盘进行修复检查，执行后可能出现一堆询问，按yes即可。 12(Repair filesystem) 1 # fsck -A #&lt;==可能会等待一段时间或fsck。(Repair filesystem) 2 # #&lt;==修复完毕会返回到这个提示符，此时就可以试着重启系统看故障是否修复了。 除了按照开机的提示进行修复外，也可以利用系统盘进入救援模式或单用户模式对系统故障进行修复。千万不要在开机正常工作的情况下执行fsck来检查磁盘，因为这样有可能会导致正常的磁盘发生故障。 mount：挂载文件系统mount命令可以将指定的文件系统挂载到指定目录（挂载点），在Linux系统下必须先挂载所有的设备，然后才能被访问，挂载其实就是为要访问的设置开个门（开门才能访问）。挂载的目录必须事先存在且最好为空，如果目录不为空，那么挂载设备后会掩盖以前的目录内容，但原目录下的内容不会受损，所以，如果卸载了相应的设备，那么此前的目录内容又可以访问了。 语法格式：mount [option] [device] [dir] 重要选项参数 其中，-o选项后接的挂载参数如下： 【使用示例】 1）显示系统已挂载的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 不加参数或加-l选项[root@C7-Server01 ~]# mountsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)devtmpfs on /dev type devtmpfs (rw,nosuid,size=3984384k,nr_inodes=996096,mode=755)securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)tmpfs on /run type tmpfs (rw,nosuid,nodev,mode=755)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)configfs on /sys/kernel/config type configfs (rw,relatime)/dev/sda3 on / type xfs (rw,relatime,attr2,inode64,noquota)debugfs on /sys/kernel/debug type debugfs (rw,relatime)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)systemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=32,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=24337)mqueue on /dev/mqueue type mqueue (rw,relatime)/dev/sda1 on /boot type xfs (rw,relatime,attr2,inode64,noquota)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=799032k,mode=700)/dev/sdb1 on /root/mydata type xfs (rw,relatime,attr2,inode64,noquota)[root@C7-Server01 ~]# mount -lsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)devtmpfs on /dev type devtmpfs (rw,nosuid,size=3984384k,nr_inodes=996096,mode=755)securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)tmpfs on /run type tmpfs (rw,nosuid,nodev,mode=755)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)configfs on /sys/kernel/config type configfs (rw,relatime)/dev/sda3 on / type xfs (rw,relatime,attr2,inode64,noquota)debugfs on /sys/kernel/debug type debugfs (rw,relatime)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)systemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=32,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=24337)mqueue on /dev/mqueue type mqueue (rw,relatime)/dev/sda1 on /boot type xfs (rw,relatime,attr2,inode64,noquota)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=799032k,mode=700)/dev/sdb1 on /root/mydata type xfs (rw,relatime,attr2,inode64,noquota) 2）对系统的光驱进行挂载 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 不使用-t选项指定类型为 iso9660，但mount命令可以自动识别[root@C7-Server01 ~]# mount /dev/cdrom /mnt# 提示为只读挂载mount: /dev/sr0 is write-protected, mounting read-only# 查看/dev/cdrom文件，发现设备cdrom是sr0的一个软链接[root@C7-Server01 ~]# ll -h /dev/cdromlrwxrwxrwx 1 root root 3 May 2 18:10 /dev/cdrom -&gt; sr0# 查看是否挂载[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata/dev/sr0 4.3G 4.3G 0 100% /mnt# 查看挂载点内容[root@C7-Server01 ~]# ll -h /mnttotal 686K-rw-rw-r-- 1 root root 14 Nov 26 00:01 CentOS_BuildTagdrwxr-xr-x 3 root root 2.0K Nov 26 00:20 EFI-rw-rw-r-- 1 root root 227 Aug 30 2017 EULA-rw-rw-r-- 1 root root 18K Dec 10 2015 GPLdrwxr-xr-x 3 root root 2.0K Nov 26 00:21 imagesdrwxr-xr-x 2 root root 2.0K Nov 26 00:20 isolinuxdrwxr-xr-x 2 root root 2.0K Nov 26 00:20 LiveOSdrwxrwxr-x 2 root root 648K Nov 26 07:52 Packagesdrwxrwxr-x 2 root root 4.0K Nov 26 07:53 repodata-rw-rw-r-- 1 root root 1.7K Dec 10 2015 RPM-GPG-KEY-CentOS-7-rw-rw-r-- 1 root root 1.7K Dec 10 2015 RPM-GPG-KEY-CentOS-Testing-7-r--r--r-- 1 root root 2.9K Nov 26 07:54 TRANS.TBL# 卸载挂载点[root@C7-Server01 ~]# umount /mnt/ 在实际中，我们经常会挂载NFS（网络文件系统），这就要用到mount命令的-o选项，来保证性能和安全性。具体的说明，请参见本站Linux常用工具分类中的NFS文件系统一文。 umount：卸载文件系统umount命令可以卸载已经挂载的文件系统，如上文中示例2的卸载挂载点。umount卸载可以接挂载点目录，也可以接设备文件。 语法格式：umount [option] [dir|device] 重要选项参数 【使用示例】 1）卸载光驱挂载 12345678910111213141516171819202122# 先挂载光驱[root@C7-Server01 ~]# mount -t iso9660 /dev/cdrom /mntmount: /dev/sr0 is write-protected, mounting read-only# 查看系统挂载信息[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata/dev/sr0 4.3G 4.3G 0 100% /mnt# 使用设备文件卸载[root@C7-Server01 ~]# umount /dev/cdrom 2）强制卸载 1234567891011121314151617181920212223242526# 挂载光驱，并进入挂载点[root@C7-Server01 ~]# mount /dev/cdrom /mnt &amp;&amp; cd /mntmount: /dev/sr0 is write-protected, mounting read-only[root@C7-Server01 mnt]# # 此时尝试卸载光驱，会提示设备忙，无法卸载[root@C7-Server01 mnt]# umount /mntumount: /mnt: target is busy. (In some cases useful info about processes that use the device is found by lsof(8) or fuser(1))# 使用-lf选项强制卸载[root@C7-Server01 mnt]# umount -lf /mnt[root@C7-Server01 mnt]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata sync：刷新文件系统缓冲区sync命令会将内存缓冲区内的数据强制刷新到磁盘。Linux内核为了达到最佳的磁盘操作效率，默认会先在内存中将需要写入到磁盘的数据缓存起来，然后等待合适的时机将它们真正写入到磁盘中，这在绝大多数情况下都是没有任何问题的，而且还提高了系统的效率，但是如果系统出现宕机、掉电等情况，就可能会导致有些文件内容没能保存下来。当然，在Linux系统正常关机或者重启时，会将缓冲区中的内容自动同步到磁盘中。我们也可以手工执行sync命令，将内存中的文件缓冲内容强制写到磁盘中。 但是通常情况下没有必要执行这个命令，一是Linux内核会尽快让内存中的数据自动同步到磁盘上去，二是我们也无法预计什么时候会宕机、掉电。 语法格式：sync [option] 【使用示例】 手动将数据从缓冲区刷到磁盘中并重启系统 ** 123# 写一个测试循环脚本来完成三次同步，每次间隔1秒，然后重启系统**[root@C7-Server01 mnt]# for i in `seq 3`;do sync;sleep 1; done &amp;&amp; reboot; dd：转换或复制文件dd命令具有复制文件、转换文件和格式化文本的功能。 语法格式：dd [option] 重要选项参数 【使用示例】 1）将/dev/sda1分区复制（备份）到文件中 1234567891011121314151617181920212223# 查看磁盘使用情况[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0# 备份分区表信息[root@C7-Server01 ~]# dd if=/dev/sda1 of=sda1-partb.info819200+0 records in819200+0 records out419430400 bytes (419 MB) copied, 5.68928 s, 73.7 MB/s# 查看输出文件的信息[root@C7-Server01 ~]# ll -h sda1-partb.info -rw-r--r-- 1 root root 400M May 2 19:48 sda1-partb.info 2）删除/dev/sdb1分区数据 12345678910111213141516171819202122232425262728# 挂载/dev/sdb1分区到root用户家目录下的mydata/目录[root@C7-Server01 ~]# mount /dev/sdb1 mydata/# 在mydata/目录下创建1000个文件[root@C7-Server01 ~]# touch mydata/file&#123;01..1000&#125;# 查看系统挂载信息，发现/dev/sdb1已使用33M，占比1%[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 3.0G 39G 8% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata# /dev/zero设备读取数据，写入到/dev/sdb1中，就会清空/dev/sdb1分区的数据[root@C7-Server01 ~]# dd if=/dev/zero of=/dev/sdb1dd: writing to ‘/dev/sdb1’: No space left on device # 提示磁盘被写满10485761+0 records in10485760+0 records out5368709120 bytes (5.4 GB) copied, 73.561 s, 73.0 MB/s /dev/zero是0字符设备，可产生连续不断的特殊数据流，生成的文件为特殊格式的数据文件（二进制）。 3）生成任意大小的文件 12345678# 生成一个大小为10M的测试文件test01[root@C7-Server01 ~]# dd if=/dev/zero of=test01 bs=1M count=1010+0 records in10+0 records out10485760 bytes (10 MB) copied, 0.136475 s, 76.8 MB/s[root@C7-Server01 ~]# ll -h test01 -rw-r--r-- 1 root root 10M May 2 20:11 test01 生成文件test01的大小为bs*count=1M*10=10M。 4）生成CentOS7的镜像文件 在Windows系统里制作光盘的ISO镜像，还需要安装其他软件。但在Linux系统中只需要dd命令就足够了，可以使用dd命令，将从光驱读取的镜像复制到系统中，相当于光驱与磁盘对拷。使用此类防范可以不用ftp工具或lrzsz工具对镜像文件进行上传。 123456[root@C7-Server01 ~]# dd if=/dev/cdrom of=CentOS74.img8962048+0 records in8962048+0 records out4588568576 bytes (4.6 GB) copied, 30.8377 s, 149 MB/s[root@C7-Server01 ~]# ll -h CentOS74.img -rw-r--r-- 1 root root 4.3G May 2 20:16 CentOS74.img 这样我们就创建了一个用于KVM或OpenStack的母版镜像文件CentOS74.img。 5）使用dd复制文件，并转换大小写 123456789101112131415161718192021222324# 在当前目录下创建测试文件，内容随便编辑[root@C7-Server01 ~]# cat &gt; file01 &lt;&lt; EOFWWW.sn.Chinamobile.com我爱北京天安门！！！1234www.sina.com.CN###!www.openstack.orgEOF# 使用dd复制文件，并将原文件中所有大写字母转换为小写字母&gt; EOF&gt; [root@C7-Server01 ~]# dd if=file01 of=file01_new conv=lcase&gt; 0+1 records in&gt; 0+1 records out&gt; 96 bytes (96 B) copied, 0.000108961 s, 881 kB/s# 产看file01_new文件内容[root@C7-Server01 ~]# cat file01_newwww.sn.chinamobile.com我爱北京天安门！！！1234www.sina.com.cn###!www.openstack.org Linux磁盘与文件系统管理命令掌握上述命令即可，还有三个用于交换分区管理的命令mkswap（创建交换分区）、swapon（激活交换分区）和swapoff（关闭交换分区）很少会被使用到，大家知道即可。如果在实际运维中需要，到时再通过man查询帮助即可。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-01-Linux系统命令-第六篇《用户和权限管理命令》]]></title>
    <url>%2F2019%2F05%2F02%2F2019-05-01-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E5%85%AD%E7%AF%87%E3%80%8A%E7%94%A8%E6%88%B7%E5%92%8C%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[useradd：创建用户useradd命令可用于创建新的用户或者更改用户的信息。在使用useradd命令时，若不加任何参数选项，后面直接跟所添加的用户名，那么系统首先会读取/etc/login.defs（用户定义文件）和/etc/default/useradd（用户默认配置文件）文件中所定义的参数和规则，然后根据所设置的规则添加用户，同时还会向/etc/passwd（用户文件）和/etc/group（组文件）文件内添加新用户和新用户组记录，向/etc/shadow（用户密码文件）和/etc/gshadow（组密码文件）文件里添加新用户和组对应的密码信息的相关记录。最后，系统还会根据/etc/default/useradd文件所配置的信息建立用户的家目录，并将/etc/skel中的所有文件（包括隐藏的环境配置文件）都复制到新用户的家目录中。 当执行useradd带-D参数时，可以更改新建用户的默认配置值（/etc/default/useradd）或者由命令行编辑文件更改预设值。可简单理解该参数（-D）就是用于修改/etc/default/useradduseradd配置文件的内容的，若这个文件的内容被修改，则添加新用户不加参数时默认值就会从该/etc/default/useradd中读取。 语法格式：useradd [options] [login] 或 useradd -D [options] 重要选项参数 useradd不加选项-D的参数选项及说明 useradd加-D选项参数说明：改变新建用户的预设值 【使用示例】 1）不加任何参数添加用户的例子 123456789101112131415161718192021222324252627282930# 不带任何参数创建一个名为ett的用户[root@C7-Server01 ~]# useradd ett# 查看/home目录下，发现多个一个ett的目录[root@C7-Server01 ~]# ls -l /home/total 4drwx------ 2 ett ett 62 May 1 16:35 ettdrwx------ 7 kkutysllb kkutysllb 4096 Apr 28 00:00 kkutysllb# 查看/etc/passwd文件关于新建用户ett的记录[root@C7-Server01 ~]# cat /etc/passwd | grep -w ettett:x:1001:1001::/home/ett:/bin/bash # 这里设置根据文件/etc/login.defs里面预设内容设置# 查看/etc/shadow文件关于新建用户ett的记录[root@C7-Server01 ~]# cat /etc/shadow | grep -w ettett:!!:18017:0:99999:7::: # 虽然没有设置密码，但是还有相关一行记录# 查看/etc/group文件中关于新建ett的记录[root@C7-Server01 ~]# cat /etc/group | grep -w ettett:x:1001:# 查看/etc/gshadow文件中关于新建用户ett的记录[root@C7-Server01 ~]# cat /etc/gshadow | grep -w ettett:!:: 根据上面的结果，我们将会发现/etc/shadow、/etc/group和/etc/gshadow几个文件都存在与ett用户相关的记录。 2）useradd的-g、-u参数，执行useradd[参数]username添加用户 123456789# 首先创建一个用户组sa，并设置组id为801（创建用户组的命令groupadd后面会将）[root@C7-Server01 ~]# groupadd -g 801 sa# 创建用户user01，使其属于sa用户组，uid为901[root@C7-Server01 ~]# useradd -g sa -u 901 user01[root@C7-Server01 ~]# id user01uid=901(user01) gid=801(sa) groups=801(sa) 3）useradd的-M、-s参数的例子 123456789# 创建一个虚拟化vuser01，不自动创建其家目录，并禁止其登录# /sbin/nologin参数可以设置某个用户没有登录权限[root@C7-Server01 ~]# useradd -M -s /sbin/nologin vuser01[root@C7-Server01 ~]# ls -ld /home/vuser01ls: cannot access /home/vuser01: No such file or directory # 家目录不存在，-M选项作用[root@C7-Server01 ~]# cat /etc/passwd | grep -w vuser01vuser01:x:1002:1002::/home/vuser01:/sbin/nologin # 该用户没有登录权限，-s选项作用 4）useradd的-c、-u、-G、-s、-d、-m、-e、-f等多个参数组合的综合例子 123456789101112131415161718192021# 创建用户user02，并设置用户注释信息为“SysUser”# UID指定为806，同时归属root和sa用户组，登录shell使用/bin/sh# 设置其家目录为/tmp/user02，用户过期时限为2019-10-31，过期后2天停权[root@C7-Server01 ~]# useradd -u 801 -s /bin/sh -c SysUser -d /tmp/user02 -G root,sa -e "2019-10-31" -f 2 user02[root@C7-Server01 ~]# tail -1 /etc/passwd # 查看user02的家目录，注释信息，登录shell等信息user02:x:801:1003:SysUser:/tmp/user02:/bin/sh[root@C7-Server01 ~]# id user02 # 查看user02的uid和gid，所属用户组信息uid=801(user02) gid=1003(user02) groups=1003(user02),0(root),801(sa)[root@C7-Server01 ~]# chage -l user02 # 查看user02的账号期限信息Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Oct 31, 2019Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7[root@C7-Server01 ~]# tail -1 /etc/shadow # 查询用户超期停权时限user02:!!:18017:0:99999:7:2:18200: 5）useradd-D参数的使用说明及案例实践 使用useradd-D参数的结果实际上就是修改用户的初始配置文件/etc/default/useradd，我们首先看下用户初始配置文件内容，如下： 12345678910111213141516171819202122232425262728293031[root@C7-Server01 ~]# cat /etc/default/useradd # useradd defaults file# 依赖于/etc/login.defs的USERGROUP_ENAB参数，如果参数值为no，则组设置由此处控制GROUP=100 # 默认把用户家目录创建在/home/目录下HOME=/home # 是否启用用户过期停权，-1表示不使用INACTIVE=-1 # 用户终止日期设置，默认不设置EXPIRE= # 新建用户默认使用shell类型SHELL=/bin/bash # 配置新建用户家目录的默认文件存放路径。/etc/skell就是在这里配置生效的，即当我们使用useradd创建用户时，用户家目录下的文件（主要是隐藏文件）都是从这里配置的目录下复制过去的SKEL=/etc/skel # 是否创建mail文件CREATE_MAIL_SPOOL=yes 修改实践： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 良好习惯，一般修改系统配置文件，首先创建好备份[root@C7-Server01 ~]# cp /etc/default/useradd&#123;,.bak&#125;# 修改默认登录的shell为/bin/sh[root@C7-Server01 ~]# useradd -D -s /bin/sh# 比对修改前后的两个文件[root@C7-Server01 ~]# diff /etc/default/useradd&#123;,.bak&#125;# 表示前后两个文件第6行不同，新文件已经修改成为/bin/sh6c6## &lt; SHELL=/bin/sh&gt; SHELL=/bin/bash# 增加新建用户失效时限为2039-12-31[root@C7-Server01 ~]# useradd -D -e "2039-12-31"# 比对前后两个文件区别[root@C7-Server01 ~]# diff /etc/default/useradd&#123;,.bak&#125;5,6c5,6&lt; EXPIRE=2039-12-31## &lt; SHELL=/bin/sh&gt; EXPIRE=&gt; SHELL=/bin/bash&gt;&gt; # 新建一个用户user03&gt;&gt; [root@C7-Server01 ~]# useradd user03# 查看新建用户user03的配置信息，发现登陆后默认使用/bin/sh作为shell解释器[root@C7-Server01 ~]# tail -1 /etc/passwduser03:x:1003:1004::/home/user03:/bin/sh# 查看新建用户user03的账号期限，发现为2039年12月31日[root@C7-Server01 ~]# chage -l user03Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Dec 31, 2039Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7# 完成上述验证后，需要恢复现场[root@C7-Server01 ~]# cp /etc/default/useradd&#123;.bak,&#125;cp: overwrite ‘/etc/default/useradd’? y # 输入y确认 usermod：修改用户信息usermod命令用于修改系统已经存在的用户的账号信息。usermod的作用是修改用户，而useradd的作用是添加用户，本质上都是对用户进行操作，因此，参数作用大部分都是类似的，只不过命令不同，就是添加和修改的区别。 语法格式：usermod [options] [login] 重要参数选项 【使用示例】 usermod的-c、-u、-G、-s、-d、-m、-e、-f等多个参数组合的例子 12345678910111213141516171819202122232425262728293031323334353637# 修改上面例子创建的user02用户，注释信息修改为TmpUser# UID修改为1999，归属用户组修改为只归属sa用户组# shell类型修改为/bin/sh# 家目录修改为/home/user02# 用户过期时限修改为2020-01-01，过期后30天停权[root@C7-Server01 ~]# usermod -u 1999 -s /bin/sh -d /home/user02 -G sa -c TmpUser -e "2020-01-01" -f 30 user02# 查询用户登录shell和注释信息[root@C7-Server01 ~]# cat /etc/passwd | grep -w user02user02:x:1999:1003:TmpUser:/home/user02:/bin/sh# 查询用户uid ,gid和归属用户组信息[root@C7-Server01 ~]# id user02uid=1999(user02) gid=1003(user02) groups=1003(user02),801(sa)# 查询用户user02的过期时限[root@C7-Server01 ~]# chage -l user02Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Jan 01, 2020Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7# 查询用户停权的时限[root@C7-Server01 ~]# cat /etc/shadow | grep -e user02user02:!!:18017:0:99999:7:30:18262: userdel：删除用户userdel命令用于删除指定的用户及与该用户相关的文件。 语法格式：userdel [options] [login] 重要选项参数 【使用示例】 1）不加参数删除用户 123456789101112131415# 查询刚才添加的虚拟用户vuser01的信息[root@C7-Server01 ~]# grep -e vuser01 /etc/passwdvuser01:x:1002:1002::/home/vuser01:/sbin/nologin# 不带任何选项参数删除虚拟用户vuser01[root@C7-Server01 ~]# userdel vuser01# 再次查看vuser01在系统配置文件/etc/passwd中的信息，结果什么也没有输出# 为显示效果，在后面追加wc -l命令用于统计屏幕输出行数信息，结果为0[root@C7-Server01 ~]# grep -e vuser01 /etc/passwd | wc -l0 2） 加-r参数删除用户及家目录 12345678910# 在不加任何选项删除用户时，用户信息虽然从/etc/passwd文件删除，但是创建的家目录仍然存在# 加-r选项删除用户user01，可以同步删除用户家目录信息[root@C7-Server01 ~]# userdel -r user01[root@C7-Server01 ~]# grep -w user01 /etc/passwd | wc -l0[root@C7-Server01 ~]# grep -w user01 /home/user01 | wc -lgrep: /home/user01: No such file or directory0 3）强制删除用户示例（此命令请谨慎使用） 我们复制一个终端，在宿主机使用kkutysllb用户登录，如下： 12345678910111213141516171819# 我们在老的终端下，使用root用户删除kkutysllb用户# 使用who命令查询当前登录的用户[root@C7-Server01 ~]# whoroot pts/0 2019-05-01 16:35 (192.168.101.1)kkutysllb pts/1 2019-05-01 18:17 (192.168.101.1)# 因为本机/home/kkutysllb/目录下有私有配置文件，所以本次演示不使用-r选项# 使用-f选项强制删除已登录用户kkutysllb[root@C7-Server01 ~]# userdel -f kkutysllbuserdel: user kkutysllb is currently used by process 10633# 虽然上面提示用户正在登录，但其实用户信息已被删除，当退出后查询/etc/passwd文件已经没有kkutysllb用户的配置信息[root@C7-Server01 ~]# grep -w kkutysllb /etc/passwd | wc -l0 groupadd：创建新的用户组groupadd命令用于创建新的用户组。但groupadd命令的用途一般不大，因为useradd命令在创建用户的同时还会创建与用户同名的用户组。 语法格式：groupadd [options] [group] 重要参数选项 groupadd命令用于创建新的用户组。但groupadd命令的用途一般不大，因为useradd命令在创建用户的同时还会创建与用户同名的用户组。 语法格式：groupadd [options] [group] 重要参数选项 【使用示例】 指定gid添加用户组的例子 12345678910111213# 使用-g选项指定新增用户组的gid为1024[root@C7-Server01 ~]# groupadd -g 1024 kkutysllb# 查看新增用户组在/etc/group配置文件的信息[root@C7-Server01 ~]# tail -1 /etc/groupkkutysllb:x:1024:# 虽然没有指定组密码，但是在系统配置/etc/gshadow文件中仍有默认配置[root@C7-Server01 ~]# tail -1 /etc/gshadowkkutysllb:!:: groupadd的命令在工作场景中的应用绝大多数情况下仅限于此，一般掌握上述用法即可。 groupdel：删除用户组 groupdel命令用于删除指定的用户组，此命令的使用频率极低，了解基本用法即可。需要注意一点：groupdel不能删除还有用户归属的主用户组。 语法格式：groupdel [group] 【使用示例】 删除用户组的例子 12345678910111213# 尝试删除root用户组，会提示失败，因为root用户还存在[root@C7-Server01 ~]# groupdel rootgroupdel: cannot remove the primary group of user 'root'# 删除sa用户组[root@C7-Server01 ~]# groupdel sa# 查询系统配置文件sa用户组的信息[root@C7-Server01 ~]# grep -w sa /etc/group | wc -l0 passwd：修改用户密码passwd命令可以修改用户密码及密码过期时间等内容，是实际中很常用的命令。普通用户和超级用户都可以运行passwd命令，但普通用户只能更改自身的用户密码，超级用户root则可以设置或修改所有用户的密码。root用户修改密码时，如果不符合系统密码规则，则给出警告信息，但密码设置仍然生效。普通用户修改密码时，如果使用弱密码，则给出告警信息，且修改无效。 语法格式：passwd [option] [username] 重要选项参数 【使用示例】 1）新增/修改用户的密码 123456789101112131415161718192021222324252627282930313233# 创建kkutysllb用户（刚才已删除）[root@C7-Server01 ~]# useradd -u 1024 -d /home/kkutysllb -g kkutysllb -c AdminUser -e "2199-12-31" -f 30 kkutysllb[root@C7-Server01 ~]# tail -1 /etc/passwdkkutysllb:x:1024:1024:AdminUser:/home/kkutysllb:/bin/bash# root用户使用passwd命令新增kkutysllb用户的密码[root@C7-Server01 ~]# passwd kkutysllbChanging password for user kkutysllb.New password: # 输入新密码Retype new password: # 确认新密码passwd: all authentication tokens updated successfully.# 使用su命令切换到kkutysllb用户下（从root用户切换到非root用户不用输入密码）[root@C7-Server01 ~]# su - kkutysllbLast login: Wed May 1 19:32:00 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ # 在kkutysllb用户下修改自身密码为123456，会提示三次密码过于简单，然后退出，设置失败[kkutysllb@C7-Server01 ~]$ passwdChanging password for user kkutysllb.Changing password for kkutysllb.(current) UNIX password: # 首先输入用户当前密码New password: # 然后输入新密码BAD PASSWORD: The password is shorter than 8 charactersNew password: BAD PASSWORD: The password is shorter than 8 charactersNew password: BAD PASSWORD: The password is shorter than 8 characterspasswd: Have exhausted maximum number of retries for service 2）显示账号密码 12345678910# 只能在root用户下使用，所以先切换回root用户[kkutysllb@C7-Server01 ~]$ su -Password: # 从非root用户切换回root用户要求输入密码Last login: Wed May 1 19:35:06 CST 2019 on pts/0# 使用-S选项查看kkutysllb用户密码简单信息[root@C7-Server01 ~]# passwd -S kkutysllbkkutysllb PS 2019-05-01 0 99999 7 30 (Password set, SHA512 crypt.) 3）使用一条命令设置密码 12345678910# 使用--stdin选项从标准输入中读取密码，并设置给用户user03[root@C7-Server01 ~]# echo "Adsds#@123"| passwd --stdin user03Changing password for user user03.passwd: all authentication tokens updated successfully.# 查看user03用户密码简单信息[root@C7-Server01 ~]# passwd -S user03user03 PS 2019-05-01 0 99999 7 -1 (Password set, SHA512 crypt.) 4）要求kkutysllb用户7天内不能更改密码，60天以后必须修改密码，过期前10天通知用户，过期后30天后禁止用户登录。 12345678[root@C7-Server01 ~]# passwd -n 7 -x 60 -w 10 -i 30 kkutysllbAdjusting aging data for user kkutysllb.passwd: Success# 查看kkutysllb密码简单信息[root@C7-Server01 ~]# passwd -S kkutysllbkkutysllb PS 2019-05-01 7 60 10 30 (Password set, SHA512 crypt.) chage：修改用户密码有效期 chage命令用于查看或修改用户密码的有效期，有些参数和passwd的功能相同。 语法格式：chage [option] [login] 重要选项参数 【使用示例】 1）要求kkutysllb用户7天内不能更改密码，60天以后必须修改密码，过期前10天通知用户，过期后30天后禁止用户登录。 1234567891011121314# 使用chage命令实现[root@C7-Server01 ~]# chage -m 7 -M 60 -W 10 -I 30 kkutysllb# 使用-l选项，查看kkutysllb账号密码期限信息[root@C7-Server01 ~]# chage -l kkutysllbLast password change : May 01, 2019Password expires : Jun 30, 2019Password inactive : Jul 30, 2019Account expires : Dec 31, 2199Minimum number of days between password change : 7Maximum number of days between password change : 60Number of days of warning before password expires : 10 2） 测试-E选项 1234567891011# 使用-E选项，将user03用户的账号有限期设置为2019-12-31[root@C7-Server01 ~]# chage -E "2019-12-31" user03[root@C7-Server01 ~]# chage -l user03Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Dec 31, 2019Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7 chpasswd：批量更新用户密码chpasswd命令用于从标准输入中读取一定格式的用户名、密码来批量更新用户的密码，其格式为“用户名：密码”。 语法格式：chpasswd [option] 重要参数选项 【使用示例】 1）命令行批量修改密码 1234567# 输入chapasswd后，按照格式 用户名：密码逐行修改密码，每行一个# 修改密码的用户必须存在，修改完成后按ctrl+d退出[root@C7-Server01 ~]# chpasswd user02:123456 # 修改user02的密码为123456user03:123456 # 修改user03的密码为123456 2）在不使用shell循环下，批量创建10个用户stu01-stu10，并且设置8位随机密码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 批量创建10个用户[root@C7-Server01 ~]# echo test&#123;01..10&#125;|xargs -n 1 useradd# 查看刚创建的10个用户[root@C7-Server01 ~]# tail /etc/passwdtest01:x:2000:2000::/home/test01:/bin/bashtest02:x:2001:2001::/home/test02:/bin/bashtest03:x:2002:2002::/home/test03:/bin/bashtest04:x:2003:2003::/home/test04:/bin/bashtest05:x:2004:2004::/home/test05:/bin/bashtest06:x:2005:2005::/home/test06:/bin/bashtest07:x:2006:2006::/home/test07:/bin/bashtest08:x:2007:2007::/home/test08:/bin/bashtest09:x:2008:2008::/home/test09:/bin/bashtest10:x:2009:2009::/home/test10:/bin/bash# 建立账号:密码格式文件[root@C7-Server01 ~]# echo test&#123;01..10&#125;:$((RANDOM+10000000))|tr " " "\n" &gt; userpass.txt# 检验刚创建用户名:密码格式文件[root@C7-Server01 ~]# cat userpass.txt test01:10013080test02:10008930test03:10025107test04:10009300test05:10012914test06:10029118test07:10018810test08:10024358test09:10003684test10:10008929# 使用chpasswd命令从格式文件中读取内容，进行批量设置[root@C7-Server01 ~]# chpasswd &lt; userpass.txt# 从root用户先切换到test01用户下（无需密码），再切换到test10用户下（需要输入密码）[root@C7-Server01 ~]# su - test01[test01@C7-Server01 ~]$ su - test10Password: # 输入testt10用户的密码[test10@C7-Server01 ~]$# 使用whoami指令检验[test10@C7-Server01 ~]$ whoami test10 su：切换用户su命令用于将当前用户切换到指定用户或者以指定用户的身份执行命令或程序。若省略了命令后面的用户名，则默认切换为root用户。）从root用户切换到普通用户时，不需要任何密码；从普通用户切换到root用户时，需要输入root密码。 语法格式：su [option] [user] 重要选项参数 【使用示例】 1）切换用户忘记-选项，导致环境变量未同步切换 1234567891011121314151617181920212223# 首先查询当前root用户下环境变量设置[root@C7-Server01 ~]# env | egrep "USER|MAIL|PWD|LOGNAME"USER=rootMAIL=/var/spool/mail/rootPWD=/rootLOGNAME=root# 先从root用户切换到kkutysllb用户，再切回，不带-选项[root@C7-Server01 ~]# su - kkutysllbLast login: Wed May 1 19:36:49 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ suPassword: [root@C7-Server01 kkutysllb]# # 再次查询当前root用户的环境变量，发现是kkutysllb用户的环境变量设置[root@C7-Server01 kkutysllb]# env | egrep "USER|MAIL|PWD|LOGNAME"USER=kkutysllbMAIL=/var/spool/mail/kkutysllbPWD=/home/kkutysllbLOGNAME=kkutysllb 上述示例告诉我们，在切换用户时要保持好的习惯，带-和用户名切换，确保环境变量与当前用户设置一致。 2）向shell传递单个命令示例 在后续我们不熟openstack的各项服务时，经常会进行数据库同步操作，完成此操作的语句如下： 1su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron 上述语句的意思就是：切换到neutron用户下，以/bin/sh作为shell解释器，执行neutron-db-manage –config-file /etc/neutron/neutron.conf –config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head命令完成neutron数据的库表创建工作。 因此，如果仅希望在某用户下执行命令，而不用直接切换到该用户下来操作，可以使用su - 用户名 -c”命令”的方式。 visudo：编辑sudoers文件visudo命令是专门用来编辑/etc/sudoers这个文件的，同时提供语法检查等功能。/etc/sudoers文件是sudo命令的配置文件。 语法格式：visudo [option] 重要选项参数 【使用示例】 1）执行visudo对普通用户kkutysllb授权 执行visudo命令，按照vim编辑器的使用方法在101行添加如下内容，使得kkutysllb用户拥有root用户的权限。 上述授权内容对应的说明如下： visudo 相当于直接执行vim /etc/sudoers编辑，但用命令方式更安全，推荐使用该命令。同时，通过sudo进行系统授权管理的目的：即能让运维人员干活，又不会威胁系统安全，还可以审计用户使用sudo的提权操作命令，默认的用户是无法获得root权限的。 2）检查sudoer文件语法 有的时候用户不是使用visudo（保存时会自动检查语法）编辑的sudoer文件，而是使用vim或者echo等命令编辑的sudoer文件，此时就需要执行如下命令来检查编辑文件的语法是否正确，如果语法不正确，则可能会导致授权无法生效的问题。 12345678910# 通过echo命令将kkutysllb用户组也授权为root权限[root@C7-Server01 kkutysllb]# echo "%kkutysllb ALL=(ALL) ALL" &gt;&gt; /etc/sudoers[root@C7-Server01 kkutysllb]# tail -1 /etc/sudoers%kkutysllb ALL=(ALL) ALL# 使用-c选项检查sudoer文件语法合规性[root@C7-Server01 kkutysllb]# visudo -c/etc/sudoers: parsed OK users：显示已登录用户users命令可以显示已经登录系统的用户。如果是同一用户登录多次，则登录几次就会显示几次用户名。 语法格式：users 【使用示例】 显示已登录用户，如果是一个用户登录多次，就显示几个同名用户 1234567[root@C7-Server01 kkutysllb]# usersroot # 再通过root用户登录系统2次[root@C7-Server01 kkutysllb]# usersroot root root whoami：显示当前登录的用户名 whoami命令用于显示当前登录的用户名，这个命令可以看作英文短句who am i的简写。 语法格式：whoami 【使用示例】 显示当前登录的用户名 123456[root@C7-Server01 kkutysllb]# whoami root[root@C7-Server01 kkutysllb]# su - kkutysllbLast login: Wed May 1 22:17:04 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ whoamikkutysllb last：显示用户登录列表 last命令能够从日志文件/var/log/wtmp读取信息并显示用户最近的登录列表。 语法格式：last [option] 重要选项参数 【使用示例】 1）显示用户最近登录的列表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495[root@C7-Server01 ~]# lastroot pts/2 192.168.101.1 Wed May 1 22:51 still logged in root pts/1 192.168.101.1 Wed May 1 22:51 still logged in kkutysll pts/1 192.168.101.1 Wed May 1 18:17 - 22:51 (04:33) root pts/1 192.168.101.1 Wed May 1 18:13 - 18:17 (00:04) root pts/0 192.168.101.1 Wed May 1 16:35 still logged in reboot system boot 3.10.0-957.10.1. Wed May 1 16:35 - 22:57 (06:22) root pts/0 192.168.101.1 Mon Apr 29 16:42 - 18:05 (01:23) reboot system boot 3.10.0-957.10.1. Mon Apr 29 16:41 - 22:57 (2+06:16) root pts/1 192.168.101.1 Sun Apr 28 19:14 - crash (21:27) root pts/0 192.168.101.1 Sun Apr 28 16:53 - crash (23:48) reboot system boot 3.10.0-957.10.1. Sun Apr 28 16:53 - 22:57 (3+06:04) root pts/0 192.168.101.1 Sat Apr 27 21:43 - down (04:27) reboot system boot 3.10.0-957.10.1. Sat Apr 27 21:43 - 02:11 (04:27) root pts/0 192.168.101.1 Sat Apr 27 21:42 - down (00:00) reboot system boot 3.10.0-957.10.1. Sat Apr 27 21:42 - 21:43 (00:01) root pts/0 192.168.101.1 Sat Apr 27 12:00 - down (00:56) reboot system boot 3.10.0-957.10.1. Sat Apr 27 12:00 - 12:57 (00:57) root pts/0 192.168.101.1 Sat Apr 27 11:13 - crash (00:46) reboot system boot 3.10.0-957.10.1. Sat Apr 27 10:51 - 12:57 (02:05) root pts/0 192.168.101.1 Fri Apr 26 17:24 - down (01:04) reboot system boot 3.10.0-957.10.1. Fri Apr 26 17:24 - 18:29 (01:04) root pts/1 192.168.101.1 Tue Apr 23 19:56 - down (01:39) root pts/0 192.168.101.1 Tue Apr 23 18:25 - down (03:10) reboot system boot 3.10.0-957.10.1. Tue Apr 23 18:24 - 21:36 (03:11) root pts/0 192.168.101.1 Tue Apr 23 12:51 - 13:14 (00:23) reboot system boot 3.10.0-957.10.1. Tue Apr 23 12:50 - 13:14 (00:23) root pts/0 192.168.101.1 Tue Apr 23 12:42 - down (00:06) reboot system boot 3.10.0-957.10.1. Tue Apr 23 12:41 - 12:48 (00:06) root pts/0 192.168.101.1 Tue Apr 23 10:29 - crash (02:12) reboot system boot 3.10.0-957.10.1. Tue Apr 23 10:28 - 12:48 (02:19) kkutysll tty1 Tue Apr 23 00:17 - 00:17 (00:00) root pts/0 192.168.101.1 Mon Apr 22 22:53 - crash (11:35) reboot system boot 3.10.0-957.10.1. Mon Apr 22 22:53 - 12:48 (13:55) root pts/0 192.168.101.1 Mon Apr 22 22:52 - down (00:00) reboot system boot 3.10.0-957.10.1. Mon Apr 22 22:52 - 22:53 (00:01) root pts/0 192.168.101.1 Mon Apr 22 22:38 - down (00:13) reboot system boot 3.10.0-957.10.1. Mon Apr 22 22:23 - 22:52 (00:28) reboot system boot 3.10.0-957.10.1. Sun Apr 21 16:27 - 22:52 (1+06:24) root pts/1 192.168.101.1 Sat Apr 20 18:22 - crash (22:05) root pts/0 192.168.101.1 Sat Apr 20 15:57 - crash (1+00:29) reboot system boot 3.10.0-862.el7.x Sat Apr 20 15:57 - 22:52 (2+06:54) root pts/1 192.168.101.1 Sat Apr 20 12:42 - down (03:14) root pts/1 192.168.101.1 Sat Apr 20 12:40 - 12:40 (00:00) root pts/1 192.168.101.1 Sat Apr 20 12:37 - 12:37 (00:00) root pts/0 192.168.101.1 Sat Apr 20 11:06 - down (04:50) reboot system boot 3.10.0-862.el7.x Sat Apr 20 11:06 - 15:57 (04:50) root pts/0 192.168.101.1 Fri Apr 19 17:13 - down (01:16) reboot system boot 3.10.0-862.el7.x Fri Apr 19 16:37 - 18:30 (01:52) root pts/1 192.168.101.1 Wed Apr 17 13:12 - crash (2+03:25) root pts/0 192.168.101.1 Wed Apr 17 10:41 - crash (2+05:55) reboot system boot 3.10.0-862.el7.x Wed Apr 17 10:38 - 18:30 (2+07:51) root pts/0 192.168.101.1 Tue Apr 16 23:04 - down (01:13) reboot system boot 3.10.0-862.el7.x Tue Apr 16 22:30 - 00:18 (01:48) root pts/1 192.168.101.1 Tue Apr 16 14:29 - down (04:09) root pts/0 192.168.101.1 Tue Apr 16 10:42 - 16:39 (05:57) reboot system boot 3.10.0-862.el7.x Tue Apr 16 10:41 - 18:38 (07:57) reboot system boot 3.10.0-862.el7.x Tue Apr 16 02:05 - 18:38 (16:32) root pts/0 192.168.101.1 Tue Apr 16 02:05 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 02:05 - 02:05 (00:00) root pts/0 192.168.101.1 Tue Apr 16 02:02 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 02:01 - 02:02 (00:00) root pts/0 192.168.101.1 Tue Apr 16 02:00 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 01:59 - 02:00 (00:00) root pts/0 192.168.101.1 Tue Apr 16 01:59 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 01:59 - 01:59 (00:00) root pts/0 192.168.101.1 Tue Apr 16 01:49 - down (00:08) root pts/0 192.168.101.1 Tue Apr 16 01:45 - 01:49 (00:03) root pts/0 192.168.101.1 Tue Apr 16 01:34 - 01:45 (00:10) reboot system boot 3.10.0-862.el7.x Tue Apr 16 01:34 - 01:57 (00:23) root pts/0 192.168.101.1 Mon Apr 15 23:45 - down (00:41) reboot system boot 3.10.0-862.el7.x Mon Apr 15 23:44 - 00:27 (00:42) reboot system boot 3.10.0-862.el7.x Mon Apr 15 18:00 - 00:27 (06:26) root pts/0 192.168.101.1 Mon Apr 15 17:59 - crash (00:00) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:59 - 00:27 (06:27) root pts/0 192.168.101.1 Mon Apr 15 17:58 - crash (00:01) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:57 - 00:27 (06:29) root pts/0 192.168.101.1 Mon Apr 15 17:45 - down (00:09) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:45 - 17:55 (00:10) root pts/0 192.168.101.1 Mon Apr 15 17:44 - down (00:00) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:43 - 17:44 (00:01) root pts/0 192.168.101.1 Mon Apr 15 17:06 - crash (00:36) root pts/0 192.168.101.1 Mon Apr 15 16:45 - 17:06 (00:21) reboot system boot 3.10.0-862.el7.x Mon Apr 15 16:43 - 17:44 (01:00) root pts/0 192.168.101.1 Sat Apr 13 15:44 - 04:36 (12:51) reboot system boot 3.10.0-862.el7.x Sat Apr 13 15:44 - 11:54 (20:10) reboot system boot 3.10.0-862.el7.x Sat Apr 13 15:43 - 11:54 (20:11) reboot system boot 3.10.0-862.el7.x Sat Apr 13 15:42 - 15:42 (00:00) root pts/0 192.168.101.1 Tue Apr 9 17:58 - down (01:04) reboot system boot 3.10.0-862.el7.x Tue Apr 9 17:58 - 19:03 (01:04) root pts/0 192.168.101.1 Mon Apr 8 01:05 - down (00:27) root pts/0 192.168.101.1 Sun Apr 7 23:35 - 01:05 (01:29) reboot system boot 3.10.0-862.el7.x Sun Apr 7 23:34 - 01:32 (01:57) root pts/0 192.168.101.1 Sun Apr 7 20:43 - down (00:03) reboot system boot 3.10.0-862.el7.x Sun Apr 7 20:43 - 20:47 (00:04) 2）只显示前10行信息 1234567891011[root@C7-Server01 ~]# last -10root pts/2 192.168.101.1 Wed May 1 22:51 still logged in root pts/1 192.168.101.1 Wed May 1 22:51 still logged in kkutysll pts/1 192.168.101.1 Wed May 1 18:17 - 22:51 (04:33) root pts/1 192.168.101.1 Wed May 1 18:13 - 18:17 (00:04) root pts/0 192.168.101.1 Wed May 1 16:35 still logged in reboot system boot 3.10.0-957.10.1. Wed May 1 16:35 - 22:59 (06:24) root pts/0 192.168.101.1 Mon Apr 29 16:42 - 18:05 (01:23) reboot system boot 3.10.0-957.10.1. Mon Apr 29 16:41 - 22:59 (2+06:17) root pts/1 192.168.101.1 Sun Apr 28 19:14 - crash (21:27) root pts/0 192.168.101.1 Sun Apr 28 16:53 - crash (23:48) 3）显示指定用户登录情况 1234[root@C7-Server01 ~]# last kkutysllbkkutysll pts/1 192.168.101.1 Wed May 1 18:17 - 22:51 (04:33) kkutysll tty1 Tue Apr 23 00:17 - 00:17 (00:00) wtmp begins Sun Apr 7 20:43:09 2019 lastb：显示用户登录失败的记录lastb命令可以从日志文件/var/log/btmp中读取信息，并显示用户登录失败的记录，用于发现系统登录异常。 语法格式：lastb [option] 重要选项参数： 【使用示例】 显示用户登录失败的列表 12345[root@C7-Server01 ~]# lastbkkutysll ssh:notty 192.168.101.1 Wed May 1 18:21 - 18:21 (00:00) kkutysll ssh:notty 192.168.101.1 Wed May 1 18:20 - 18:20 (00:00) kkutysll ssh:notty 192.168.101.1 Wed May 1 18:20 - 18:20 (00:00) btmp begins Wed May 1 18:20:46 2019 需要多加注意这个命令执行的结果，如果发现未知的登录失败信息，那就要考虑系统是否遭到暴力破解登录。 lastlog：显示所有用户的最近登录记录lastlog命令从日志文件/var/log/lastlog中读取信息，并显示所有用户的最近登录记录，用于查看系统是否有异常登录。 语法格式：lastlog 【使用示例】 显示所有用户的最近登录记录 12345678910111213141516171819202122232425262728293031323334353637[root@C7-Server01 ~]# lastlog Username Port From Latestroot pts/0 Wed May 1 22:57:53 +0800 2019bin **Never logged in**daemon **Never logged in**adm **Never logged in**lp **Never logged in**sync **Never logged in**shutdown **Never logged in**halt **Never logged in**mail **Never logged in**operator **Never logged in**games **Never logged in**ftp **Never logged in**nobody **Never logged in**systemd-network **Never logged in**dbus **Never logged in**polkitd **Never logged in**tss **Never logged in**sshd **Never logged in**postfix **Never logged in**chrony **Never logged in**ntp **Never logged in**ett **Never logged in**user02 **Never logged in**user03 **Never logged in**kkutysllb pts/0 Wed May 1 22:54:01 +0800 2019test01 pts/0 Wed May 1 22:02:48 +0800 2019test02 **Never logged in**test03 **Never logged in**test04 **Never logged in**test05 **Never logged in**test06 **Never logged in**test07 **Never logged in**test08 **Never logged in**test09 **Never logged in**test10 pts/0 Wed May 1 22:03:22 +0800 2019 上面显示Never logged in的表示从未登录过系统的用户，一般都是系统服务的虚拟用户。 chown：改变文件或目录的用户和用户组chown命令用于改变文件或目录的用户和用户组。要授权的用户和组名，必须是Linux系统实际存在的。 常用格式： chown 用户 文件或目录 #&lt;==仅仅授权用户。 chown :组 文件或目录 #&lt;==仅仅授权组。 语法格式：chown [option] [OWNER][:[GROUP]] [file]，其中的“：”可以用“.”来代替 【重要选项参数】 【使用示例】 1）更改文件所属的用户属性 12345678910# 查询userpass.txt文件的用户和用户组[root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 root root 160 May 1 21:59 userpass.txt# 更改userpass.txt文件用户为test01[root@C7-Server01 ~]# chown test01 userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 test01 root 160 May 1 21:59 userpass.txt 2）更改文件所属的用户组 12345# 更改userpass.txt文件的所属用户组为kkutysllb[root@C7-Server01 ~]# chown .kkutysllb userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 test01 kkutysllb 160 May 1 21:59 userpass.txt 3 ）同时更改文件的用户和用户组 12345# 更改userpass.txt文件的用户为kkutysllb，用户组为root[root@C7-Server01 ~]# chown kkutysllb:root userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 kkutysllb root 160 May 1 21:59 userpass.txt 4）递归更改目录及目录下的所有子目录及文件的用户和用户组的属性 1234567891011# 查询/home/test目录的所属用户和用户组信息[root@C7-Server01 ~]# ls -ld /home/testdrwxr-xr-x 5 root root 45 May 1 23:16 /home/test# 使用-R选项，递归修改/home/test目录及其子目录的用户和用户组为kkutysllb:kkutysllb[root@C7-Server01 ~]# chown -R kkutysllb:kkutysllb /home/test[root@C7-Server01 ~]# ls -ld /home/test /home/test/stu01drwxr-xr-x 5 kkutysllb kkutysllb 45 May 1 23:16 /home/testdrwxr-xr-x 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu01 chmod：改变文件或目录权限chmod命令是用来改变文件或目录权限的命令，但是只有文件的属主和超级用户root才能够执行这个命令。模式有两种格式：一种是采用权限字母和操作符表达式；另一种是采用数字。权限示意图如下： 语法格式：chmod [option] [mode] [file] 重要选项参数 【使用示例】 1）权限字母和操作符表达式 1234567891011121314151617181920212223242526272829# 设置所有权限为空[root@C7-Server01 ~]# chmod a= userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---------- 1 kkutysllb root 160 May 1 21:59 userpass.txt# 设置文件属主有执行权限[root@C7-Server01 ~]# chmod u+x userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---x------ 1 kkutysllb root 160 May 1 21:59 userpass.txt# 设置文件归属用户组有可写权限[root@C7-Server01 ~]# chmod g+w userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---x-w---- 1 kkutysllb root 160 May 1 21:59 userpass.txt# 设置其他用户对文件有可读权限[root@C7-Server01 ~]# chmod o+r userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---x-w-r-- 1 kkutysllb root 160 May 1 21:59 userpass.txt# 恢复刚才的设置，即恢复用户的默认权限[root@C7-Server01 ~]# chmod u=rwx,g=rx,o=x userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rwxr-x--x 1 kkutysllb root 160 May 1 21:59 userpass.txt 2）文件的数字权限授权 12345# 设置文件userpass.txt为归属用户组可读写执行，其他用户可读可执行[root@C7-Server01 ~]# chmod 075 userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ----rwxr-x 1 kkutysllb root 160 May 1 21:59 userpass.txt 3） 使用-R选项递归授权目录 12345678# 授权/home/test及其子目录所有用户可读，可写，可执行[root@C7-Server01 ~]# chmod -R 777 /home/test[root@C7-Server01 ~]# ls -ld /home/test /home/test/stu&#123;01..03&#125;drwxrwxrwx 5 kkutysllb kkutysllb 45 May 1 23:16 /home/testdrwxrwxrwx 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu01drwxrwxrwx 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu02drwxrwxrwx 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu03 Linux普通文件的读、写、执行权限说明 Linux目录的读、写、执行权限说明 chgrp：更改文件用户组chgrp命令只用于更改文件的用户组，功能被chown取代了，了解一下即可。 语法格式：chgrp [option] [group] [file] 重要选项参数 【使用示例】 1）修改文件的用户组属性信息 12345678910# 查看文件userpass.txt文件的用户组信息[root@C7-Server01 ~]# ll userpass.txt ----rwxr-x 1 kkutysllb root 160 May 1 21:59 userpass.txt# 修改文件用户组为kkutysllb[root@C7-Server01 ~]# chgrp kkutysllb userpass.txt [root@C7-Server01 ~]# ll userpass.txt ----rwxr-x 1 kkutysllb kkutysllb 160 May 1 21:59 userpass.txt 2）递归修改目录的用户组信息 1234567# 修改/home/test目录及子目录的用户组为root[root@C7-Server01 ~]# chgrp -R root /home/test[root@C7-Server01 ~]# ll -d /home/test/stu&#123;01..03&#125;drwxrwxrwx 3 kkutysllb root 25 May 1 23:16 /home/test/stu01drwxrwxrwx 3 kkutysllb root 25 May 1 23:16 /home/test/stu02drwxrwxrwx 3 kkutysllb root 25 May 1 23:16 /home/test/stu03 umask：显示或设置权限掩码umask是通过八进制的数值来定义用户创建文件或目录的默认权限。 语法格式：umask [option] [mode] 重要选项参数 通过umask计算文件目录权限 文件权限计算：创建文件默认最大的权限为666（-rw-rw-rw-），默认创建的文件没有可执行权限x位。对于文件来说，umask的设置是在假定文件拥有八进制666的权限上进行的，文件的权限就是666减umask（umask的各个位数字也不能大于6，比如077就不符合条件）的掩码数值，如果得到的3位数字其每一位都是偶数，那么这就是最终结果；如果有若干位的数字是奇数，那么这个奇数需要加1变成偶数，最后得到全是偶数的结果。 目录权限计算：创建目录默认最大权限777（-rwx-rwx-rwx），默认创建的目录属主是有x权限的，允许用户进入。对于目录来说，umask的设置是在假定文件拥有八进制777权限上进行，目录八进制权限777减去umask的掩码数值。 【使用示例】 1）查看root用户和非root用户的umask值 123456[root@C7-Server01 ~]# umask 0022 # 超级用户root的umask值是0022[root@C7-Server01 ~]# su - kkutysllbLast login: Wed May 1 22:54:01 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ umask 0002 # 普通用户的umask值是0002 上述结果意味着，通过root用户创建文件默认权限是644，目录默认权限是755；通过普通用户（普通用户名和用户组名相同的情况下）创建的文件默认权限是664，目录默认权限是775。详细情况参见示例2。 2）查询umask在系统配置文件的设置规则 123456[kkutysllb@C7-Server01 ~]$ sed -n '59,63p' /etc/profileif [ $UID -gt 199 ] &amp;&amp; [ "`/usr/bin/id -gn`" = "`/usr/bin/id -un`" ]; then umask 002else umask 022fi 上述结果是shell条件判断语句，有两个判断条件，且为与关系： 条件1是判断用户的uid是否大于199，条件2是判断用户名是否和用户组名相同，当两个条件都满足时，则为普通用户，umask取值为002，否则为root用户，umask取值为022。 注意一点：普通用户的umask未必是002，比如满足以下条件，kkutysllb用户属于root组的时候，由于id-gn的执行结果不等于id-un的执行结果，所以umask值为022。 3）使用-p和-S选项 123456789# 使用-p选项，输出的权限掩码可直接作为命令来执行，也就是可以使用umask+数字更改umask的默认值[kkutysllb@C7-Server01 ~]$ umask -pumask 0002# 使用-S选项，[kkutysllb@C7-Server01 ~]$ umask -Su=rwx,g=rwx,o=rx]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-01-DPDK技术栈在电信云中的最佳实践（三）]]></title>
    <url>%2F2019%2F05%2F01%2F2019-05-01-DPDK%E6%8A%80%E6%9C%AF%E6%A0%88%E5%9C%A8%E7%94%B5%E4%BF%A1%E4%BA%91%E4%B8%AD%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[DPDK技术基础（3）网络报文转发模式我们来看看发展了十几年的DPDK，从Intel主导开发，到华为、思科、AWS等大厂商的加入，核心玩家都在该圈子里，拥有完善的社区，生态形成闭环。早期，主要是传统电信领域3层以下的应用，如华为、中国电信、中国移动都是其早期使用者，交换机、路由器、网关是主要应用场景。但是，随着上层业务的需求以及DPDK的完善，在更高的未来网络转发性能提升方面的应用也在逐步出现。 在x86服务器中，基于网络包的处理架构如下： Packet input：报文输入。 Pre-processing：对报文进行比较粗粒度的处理。 Input classification：对报文进行较细粒度的分流。 Ingress queuing：提供基于描述符的队列FIFO。 Delivery/Scheduling：根据队列优先级和CPU状态进行调度。 Accelerator：提供加解密和压缩/解压缩等硬件功能。 Egress queueing：在出口上根据QOS等级进行调度。 Post processing：后期报文处理释放缓存。 Packet output：从硬件上发送出去。 可以看到在浅色和阴影对应的模块都是和硬件相关的，因此要提升这部分性能的最佳选择就是去选择网卡上或网络设备芯片上具有网络功能硬件卸载特性的设备，而在深色软件部分可以通过提高算法的效率和结合CPU相关的并行指令来提升网络性能。 传统的Network Processor（专用网络处理器）转发的模型可以分为run to completion（运行至终结，简称RTC）模型和pipeline（流水线）模型。 1）pipeline模型：pipeline模型借鉴于工业上的流水线模型，将一个功能（大于模块级的功能）分解成多个独立的阶段，不同阶段间通过队列传递产品。这样，对于一些CPU密集和I/O密集的应用，通过pipeline模型，可以把CPU密集的操作放在一个微处理引擎上执行，将I/O密集的操作放在另外一个微处理引擎上执行。通过过滤器可以为不同的操作分配不同的线程，通过连接两者的队列匹配两者的处理速度，从而达到最好的并发效率。 如上图所示，在NPA中最主要的就是TOP（Task Optimized Processor）单元，每个TOP单元都是对特定的事务进行过优化的特殊微处理单元，在处理特定的事务时会在性能上有较大的提升，一个报文从Input进入后会经历五个不同的TOP单元，每个TOP的输出又会是下个TOP的输入，这种硬件模型决定了在报文的不同处理中必须按照TOP的顺序来进行，不可能先进行报文修改再进行报文查找。如果需要这种顺序修改处理，必须从TOP修改这个单元将报文再回传到TOP解析上，但这样包的处理速度会大幅下降。 2）run to completion模型：主要将一个程序分为几个不同的逻辑功能，但是这几个逻辑功能会在一个CPU的核上运行，可以进行水平扩展使得在SMP的系统中多个核上执行一样的逻辑，从而提高单位时间内事务处理的量。但是，由于每个核上的处理能力其实都是一样的，并没有针对某个逻辑功能进行优化，因此在这个层面上与pipeline模型比较，run to completion模型是不高效的。 如上图所示，在AMCC 345x的模型中，对于报文的处理没有特殊的运算单元，只有两个NP核，两个NP核利用已烧录的微码进行报文的处理，如果把运行的微码看成处理报文的逻辑，两个NP核上总共可以跑48个任务，每个任务的逻辑都共享一份微码，则同一时刻可以处理48份网络报文。 基于通用IA平台的DPDK中是怎么利用专用网络处理器中的这两种模型来进行高速包处理？如下图所示： 从RTC模型中（上图左边），我们可以清楚地看出，每个IA的物理核都负责处理整个报文的生命周期从RX到TX，这点非常类似前面所提到的AMCC的nP核的作用。 从pipeline模型中（上图右边），我们可以看出，报文的处理被划分成不同的逻辑功能单元A、B、C，一个报文需分别经历A、B、C三个阶段，这三个阶段的功能单元可以不止一个并且可以分布在不同的物理核上，不同的功能单元可以分布在相同的核上（也可以分布在不同的核上）。因此，其对于模块的分类和调用比EZchip的硬件方案更加灵活。 DPDK RTC模型 普通的Linux网络驱动中的扩展方法如下：把不同的收发包队列对应的中断转发到指定核的local APIC（本地中断控制器）上，并且使得每个核响应一个中断，从而处理此中断对应的队列集合中的相关报文。 而在DPDK的轮询模式中主要通过一些DPDK中eal中的参数-c、-l、-lcores来设置哪些核可以被DPDK使用，最后再把处理对应收发队列的线程绑定到对应的核上。每个报文的整个生命周期都只可能在其中一个线程中出现。 和普通网络处理器的RTC模式相比，基于IA平台的通用CPU也有不少的计算资源，比如一个socket上面可以有独立运行的16运算单元（核），每个核上面可以有两个逻辑运算单元（thread）共享物理的运算单元。而多个socket可以通过QPI总线连接在一起，这样使得每一个运算单元都可以独立地处理一个报文，并且通用处理器上的编程更加简单高效，在快速开发网络功能的同时，利用硬件AES-NI、SHA-NI等特殊指令可以加速网络相关加解密和认证功能。RTC模型虽然有许多优势，但是针对单个报文的处理始终集中在一个逻辑单元上，无法利用其他运算单元，并且逻辑的耦合性太强，而流水线模型正好解决了以上的问题。下面我们来看DPDK的流水线模型，DPDK中称为Packet Framework。 DPDK pipeline模型 pipeline的主要思想就是不同的工作交给不同的模块，而每一个模块都是一个处理引擎，每个处理引擎都只单独处理特定的事务，每个处理引擎都有输入和输出，通过这些输入和输出将不同的处理引擎连接起来，完成复杂的网络功能。 DPDK pipeline的多处理引擎实例和每个处理引擎中的组成框图可见如下两个实例的图片：zoom out（多核应用框架）和zoom in（单个流水线模块）。 Zoom out的实例中包含了五个DPDK pipeline处理模块，每个pipeline作为一个特定功能的包处理模块。一个报文从进入到发送，会有两个不同的路径，上面的路径有三个模块（解析、分类、发送），下面的路径有四个模块（解析、查表、修改、发送）。Zoom in的图示中代表在查表的pipeline中有两张查找表，报文根据不同的条件可以通过一级表或两级表的查询从不同的端口发送出去。 DPDK的pipeline是由三大部分组成的，逻辑端口（port）、查找表（table）和处理逻辑（action）。DPDK的pipeline模型中把网络端口作为每个处理模块的输入，所有的报文输入都通过这个端口来进行报文的输入。查找表是每个处理模块中重要的处理逻辑核心，不同的查找表就提供了不同的处理方法。而转发逻辑指明了报文的流向和处理。 用户可以根据以上三大类构建数据自己的pipeline，然后把每个pipeline都绑定在指定的核上从而使得我们能快速搭建属于我们自己的packet framework。DPDK实现报文转发上述两种方案的优缺点比较如下： PCIe与包处理I/OPCI Express（Peripheral Component Interconnect Express）又称PCIe，它是一种高速串行通信互联标准。PCIe规范遵循开放系统互联参考模型（OSI），自上而下分为事务传输层、数据链路层、物理层，如下图所示： 如果在PCIe的线路上抓取一个TLP（Transaction Layer Packet，事务传输层数据包），其格式是一种分组形式，层层嵌套，事务传输层也拥有头部、数据和校验部分。应用层的数据内容就承载在数据部分，而头部定义了一组事务类型。 应用层数据作为有效载荷被承载在事务传输层之上，网卡从线路上接收的以太网包整个作为有效载荷在PCIe的事务传输层上进行内部传输。对于PCIe事务传输层操作而言，应用层数据内容是透明的。一般网卡采用DMA控制器通过PCIe Bus访问内存，除了对以太网数据内容的读写外，还有DMA描述符操作相关的读写。 既然应用层数据只是作为有效载荷，那么PCIe协议的三层栈有多少额外开销呢？下图列出了每个部分的长度。物理层开始和结束各有1B的标记，整个数据链路层占用6B。TLP头部64位寻址占用16B（32位寻址占用12B），TLP中的ECRC为可选位。所以，对于一个完整的TLP包来说，除去有效载荷，额外还有24B的开销（TLP头部以16B计算）。 PCIe逐代的理论峰值带宽都有显著提升，Gen1到Gen2单路传输率翻倍，Gen2到Gen3虽然传输率没有翻倍，但随着编码效率的提升，实际单路有效传输仍然有接近一倍的提升，Gen1和Gen2采用8b/10b编码，Gen3采用128b/130b编码，如下所示： 以8b/10b编码为例，每10个比特传输8个比特（1个字节）的有效数据。以GEN2为例，传输1个字节数据需要500M*10b/s=500MB/s单向每路，对于8路同时传输8个字节数据就有4GB/s的理论带宽。 要查看特定PCIe设备的链路能力和当前速率，可以用Linux工具lspci读取PCIe的配置寄存器（Configuration Space），以下是在虚拟机中读出的信息展示，与实际物理服务器并不一致 除了TLP的协议开销以外，有时还会有实际实现的开销。比如有些网卡可能会要求每个TLP都要从Lane0开始，甚至要求从偶数的时钟周期开始。由于存在这样的实现因素影响，有效带宽还会进一步降低。同时，真实的网卡收发包由DMA驱动，除了写包内容之外，还有一系列的控制操作，这些操作也会进一步影响PCIe带宽的利用。 DMA（Direct Memory Access，直接存储器访问）是一种高速的数据传输方式，允许在外部设备和存储器之间直接读写数据。数据既不通过CPU，也不需要CPU干预。整个数据传输操作在DMA控制器的控制下进行。除了在数据传输开始和结束时做一点处理外，在传输过程中CPU可以进行其他的工作。 网卡DMA控制器通过环形队列与CPU交互。环形队列由一组控制寄存器和一块物理上连续的缓存构成。主要的控制寄存器有Base、Size、Head和Tail。通过设置Base寄存器，可以将分配的一段物理连续的内存地址作为环形队列的起始地址，通告给DMA控制器。同样通过Size寄存器，可以通告该内存块的大小。Head寄存器往往对软件只读，它表示硬件当前访问的描述符单元。而Tail寄存器则由软件来填写更新，通知DMA控制器当前已准备好被硬件访问的描述符单元。 为Intel 82599网卡的收发描述符环形队列为例，硬件控制所有Head和Tail之间的描述符。如下所示： Head等于Tail时表示队列为空，Head等于Next（Tail）时表示队列已满。 环形队列中每一条记录就是描述符。 描述符的格式和大小根据不同网卡各不相同，Intel 82599网卡的一个描述符大小为16B，整个环形队列缓冲区的大小必须是网卡支持的最大Cache line（128B））的整数倍，所以描述符的总数是8的倍数。 环形队列的起始地址也需要对齐到最大Cache line的大小。 无论网卡是工作在中断方式还是轮询方式下，判断包是否接收成功，或者包是否发送成功，都会需要检查描述符中的完成状态位（Descriptor Done，DD）。该状态位由DMA控制器在完成操作后进行回写。 对网络帧的封装及处理有两种方式：将网络帧元数据（metadata）和帧本身存放在固定大小的同一段缓存中；或将元数据和网络帧分开存放在两段缓存里。前者的好处是高效：对缓存的申请及释放均只需要一个指令，缺点是因为缓存长度固定而网络帧大小不一，大部分帧只能使用填0（padding）的方式填满整个缓存，较为耗费内存空间。后者的优点则是相对自由：帧数据的大小可以任意，同时对元数据和网络帧的缓存可以分开申请及释放；缺点是低效，因为无法保证数据存在于一个Cache Line中，可能造成Hit Miss。 为保持包处理的效率，DPDK采用了前者。网络帧元数据的一部分内容由DPDK的网卡驱动写入，包括：VLAN标签、RSS哈希值、网络帧入口端口号以及巨型帧所占的Mbuf个数等。对于巨型帧，网络帧元数据仅出现在第一个帧的Mbuf结构中，其他的帧该信息为空。 Mbuf主要用来封装网络帧缓存，也可用来封装通用控制信息缓存（缓存类型需使用CTRL_MBUF_FLAG来指定）。Mbuf结构报头经过精心设计，原先仅占1个Cache Line。随着Mbuf头部携带的信息越来越多，现在Mbuf头部已经调整成两个Cache Line，原则上将基础性、频繁访问的数据放在第一个Cache Line字节，而将功能性扩展的数据放在第二个Cache Line字节。Mbuf报头包含包处理所需的所有数据，对于单个Mbuf存放不下的巨型帧（Jumbo Frame），Mbuf还有指向下一个Mbuf结构的指针来形成帧链表结构。有兴趣的参见DPDK开发者手册。 当一个网络帧被网卡接收时，DPDK的网卡驱动将其存储在一个高效的环形缓存区中，同时在Mbuf的环形缓存区中创建一个Mbuf对象。这两个行为都不涉及向系统申请内存，因为这些内存已经在内存池被创建时就申请好了。Mbuf对象被创建好后，网卡驱动根据分析出的帧信息将其初始化，并将其和实际帧对象逻辑相连。对网络帧的分析处理都集中于Mbuf，仅在必要的时候访问实际网络帧。这就是内存池的双环形缓存区结构。 IO包处理中轮询和中断的抉择DPDK采用了轮询或者轮询混杂中断的模式来进行收包和发包，而以前网卡驱动程序基本都是基于异步中断处理模式。 异步中断模式：当有包进入网卡收包队列后，网卡会产生硬件（MSIX/MSI/INTX）中断，进而触发CPU中断，进入中断服务程序，在中断服务程序来完成收包的处理。为了改善包处理性能，也可以在中断处理过程中加入轮询，来避免过多的中断响应次数。总之，基于异步中断信号模式的收包，是不断地在做中断处理，上下文切换，每次处理这种开销是固定的，累加带来的负荷显而易见。当有包需要发送出去的时候，基于异步中断信号的驱动程序会准备好要发送的包，配置好发送队列的各个描述符。在包被真正发送完成时，网卡同样会产生硬件中断信号，进而触发CPU中断，进入中断服务程序，来完成发包后的处理，例如释放缓存等。 轮询模式：是指收发包完全不使用中断处理的高吞吐率的方式。DPDK所有的收发包有关的中断在物理端口初始化的时候都会关闭，也就是说，CPU这边在任何时候都不会收到收包或者发包成功的中断信号，也不需要任何收发包有关的中断处理。DPDK的轮询驱动程序负责初始化每一个收包描述符，包括把包缓冲内存块的物理地址填充到收包描述符对应的位置，以及把相应的收包成功标志复位。然后驱动程序修改相应的队列管理寄存器来通知网卡硬件，网卡硬件会把收到的包填充到对应的收包描述符表示的缓冲内存块里，同时标记好收包成功标志。当一个收包描述符所代表的缓冲内存块大小不够存放一个完整的包时，这时候候就需要两个甚至多个收包描述符来处理一个包。 1）每一个收包队列，DPDK都会有一个相应的线程负责轮询里面的收包描述符的收包成功的标志。一旦发现某一个收包描述符的收包成功标志被硬件置位了，就意味着有一个包已经进入到网卡，并且网卡已经存储到描述符对应的缓冲内存块里面，这时候驱动程序会解析相应的收包描述符，把收包缓冲内存块放到收包函数提供的数组里面，同时分配好一个新的缓冲内存块给这个描述符，以便下一次收包。 2）每一个发包队列，DPDK都会有一个相应的线程负责设置需要发送出去的包，DPDK的驱动程序负责提取发包缓冲内存块的有效信息，例如包长、地址、校验和信息、VLAN配置信息等。DPDK的轮询驱动程序根据内存缓存块中的包的内容来负责初始化每一个发包描述符。其中最关键的有两个，一个就是标识完整的包结束的标志EOP（End Of Packet），另外一个就是请求报告发送状态RS（Report Status）。 由于一个包可能存放在一个或者多个内存缓冲块里面，需要一个或者多个发包描述符来表示一个等待发送的包，EOP就是驱动程序用来通知网卡硬件一个完整的包结束的标志。每当驱动程序设置好相应的发包描述符，硬件就可以开始根据发包描述符的内容来发包，那么驱动程序可能会需要知道什么时候发包完成，然后回收占用的发包描述符和内存缓冲块。基于效率和性能上的考虑，驱动程序可能不需要每一个发包描述符都报告发送结果，RS就是用来由驱动程序来告诉网卡硬件什么时候需要报告发送结果的一个标志。不同的硬件会有不同的机制，有的网卡硬件要求每一个包都要报告发送结果，有的网卡硬件要求相隔几个包或者发包描述符再报告发送结果，而且可以由驱动程序来设置具体的位置。 3）发包的轮询就是轮询发包结束的硬件标志位。DPDK驱动程序根据需要发送的包的信息和内容，设置好相应的发包描述符，包含设置对应的RS标志，然后会在发包线程里不断查询发包是否结束。只有设置了RS标志的发包描述符，网卡硬件才会在发包完成时以写回的形式告诉发包结束。当驱动程序发现写回标志，意味着包已经发送完成，就释放对应的发包描述符和对应的内存缓冲块，这时候就全部完成了包的发送过程。 混和中断轮询模式：由于实际网络中可能存在的潮汐效应，在某些时间段网络数据流量可能很低，甚至完全没有需要处理的包，这样完全轮询的方式会让处理器一直全速运行，明显浪费处理能力。因此在DPDK R2.1和R2.2陆续添加了收包中断与轮询的混合模式的支持，类似NAPI的思路，用户可以根据实际应用场景来选择完全轮询模式，或者混合中断轮询模式。而且，完全由用户来制定中断和轮询的切换策略，比如什么时候开始进入中断休眠等待收包，中断唤醒后轮询多长时间等等。所谓混合中断轮询模式，就是应用程序开始就是轮询收包，这时候收包中断是关闭的。但是当连续多次收到的包的个数为0，应用程序可以定义一个简单的策略来决定是否切换到中断以及什么时候让使能收包中断。轮询线程进入休眠之后，对应的核的运算能力就被释放出来，完全可以用于其他任何运算，或者干脆进入省电模式。当后续有任何包收到的时候，会产生一个收包中断，并且最终唤醒对应的应用程序收包线程。线程被唤醒后，就会关闭收包中断，再次轮询收包。应用程序完全可以根据不同的需要来定义不同的策略来让收包线程休眠或者唤醒收包线程。 1）DPDK的混合中断轮询机制是基于UIO或VFIO来实现其收包中断通知与处理流程的。如果是基于VFIO的实现，该中断机制是可以支持队列级别的，即一个接收队列对应一个中断号，这是因为VFIO支持多MSI-X中断号。如果是基于UIO的实现，该中断机制就只支持一个中断号，所有的队列共享一个中断号。 2）混合中断轮询模式相比完全轮询模式，会在包处理性能和时延方面有一定的牺牲，比如：当需要把DPDK工作线程从睡眠状态唤醒并运行，这样会引起中断触发后的第一个接收报文的时延增加。由于时延的增加，需要适当调整Mbuf队列的大小，以避免当大量报文同时到达时可能发生的丢包现象。 综上，在应用场景下如何更高效地利用处理器的计算能力，用户需要根据实际应用场景来做出最合适的选择。 零拷贝技术在每一次网络io过程，数据都要经过几个缓存，再发送出去。如下图： 上图右侧为client，左侧为Server为例： 当Server收到Client发送的index.html文件的请求时，负责处理请求的httpd子进程/线程总是会先发起系统调用，让内核将index.html从存储设备中载入。但是加载到的位置是内核空间的缓冲区kernel buffer，而不是直接给进程/线程的内存区。由于是内存设备和存储设备之间的数据传输，没有CPU的参与，所以这是一次DMA操作。 当数据准备好后，内核唤醒httpd子进程/线程，让它使用read()函数把数据复制到它自己的缓冲区，也就是图中的app buffer。到了app buffer中的数据，已经独属于进程/线程，也就可以对它做读取、修改等等操作。由于这次是使用CPU来复制的，所以会消耗CPU资源。由于这个阶段从内核空间切换到用户空间，所以进行了上下文切换。 当数据修改完成(也可能没做任何操作)后，Server需要发送响应消息给Client，也就是说要通过TCP连接传输出去。但TCP协议栈有自己的缓冲区，要通过它发送数据，必须将数据写到它的buffer中，对于发送者就是send buffer，对于接受者就是recv buffer。于是，通过write()函数将数据再次从app buffer复制到send buffer。这次也是CPU参与进行的复制，所以会消耗CPU。同样也会进行上下文切换。 非本机数据最终还是会通过网卡传输出去的，所以再使用send()函数就可以将send buffer中的数据交给网卡并通过网卡传输出去。由于这次是内存和设备之间的数据传输，没有CPU的参与，所以这次也是一次DMA操作。 当Client的网卡收到响应数据后，将它传输到TCP的recv buffer。这是一次DMA操作。 数据源源不断地填充到recv buffer中，但是Client的应用程序却不一定会去读取，而是需要通知应用程序的进程使用recv()函数将数据从read buffer中取走。这次是CPU操作。 在Server端，每次进程/线程需要一段数据时，总是先拷贝到kernel buffer，再拷贝到app buffer，再拷贝到socket buffer，最后再拷贝到网卡上。也就是说，总是会经过4段拷贝经历。通过分析上述4段拷贝，数据从存储设备到kernel buffer是必须的，从socket buffer到NIC也是必须的，但是从kernel buffer到app buffer却不一定。对于web服务来说，如果不修改http响应报文，数据完全可以不用经过用户空间。也就是不用再从kernel buffer拷贝到app buffer，这就是零复制的概念。 零复制的概念是避免将数据在内核空间和用户空间进行拷贝。主要目的是减少不必要的拷贝，避免让CPU做大量上下文切换。 mmap()函数将文件直接映射到用户程序的内存中，映射成功时返回指向目标区域的指针。这段内存空间可以用作进程间的共享内存空间，内核也可以直接操作这段空间。在映射文件之后，暂时不会拷贝任何数据到内存中，只有当访问这段内存时，发现没有数据，于是产生缺页访问，使用DMA操作将数据拷贝到这段空间中。可以直接将这段空间的数据拷贝到socket buffer中。所以也算是零复制技术。如图： mmap()的代码如下： 123#include void *mmap(void *addr, size_t length, int prot, int flags,int fd, off_t offset); 另外，sendfile()函数也是一种零复制函数。sendfile()函数借助文件描述符来实现数据拷贝：直接将文件描述in_fd的数据拷贝给文件描述符out_fd，其中in_fd是数据提供方，out_fd是数据接收方。文件描述符的操作都是在内核进行的，不会经过用户空间，所以数据不用拷贝到app buffer，实现了零复制。如下图所示： sendfile()的代码如下： 12#include ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 还有一种常用的零拷贝技术就是COW（copy-on-write，写时复制）。当父进程fork生成子进程时，会复制它的所有内存页。这至少会导致两个问题：消耗大量内存；复制操作消耗时间。特别是fork后使用exec加载新程序时，由于会初始化内存空间，所以复制操作几乎是多余的。使用copy-on-write技术，使得在fork子进程时不复制内存页，而是共享内存页(也就是说，子进程也指向父进程的物理空间)，只有在该子进程需要修改某一块数据，才会将这一块数据拷贝到自己的app buffer中并进行修改，那么这一块数据就属于该子进程的私有数据，可随意访问、修改、复制。这在一定程度上实现了零复制，即使复制了一些数据块，也是在逐渐需要的过程进行复制的。 网卡多队列技术多队列与流分类是当今网卡通用的技术。利用多队列及流分类技术可以使得网卡更好地与多核处理器、多任务系统配合，从而达到更高效IO处理的目的。说起网卡多队列，顾名思义，也就是传统网卡的DMA队列有多个，网卡有基于多个DMA队列的分配机制。 网卡多队列技术应该是与处理器的多核技术密不可分的。早期的多数计算机，处理器可能只有一个核，从网卡上收到的以太网报文都需要这个处理器处理。随着多核处理技术的发展，2007年在Intel的82575、82598网卡上引入多队列技术，可以将各个队列通过绑定到不同的核上来满足高速流量的需求。 Linux内核对多队列的支持：众所周知，Linux的网卡由结构体net_device表示，一个该结构体可对应多个可以调度的数据包发送队列，数据包的实体在内核中以结构体sk_buff（skb）表示。网卡驱动程序为每个接收队列设定相应的中断号，通过中断的均衡处理，或者设置中断的亲和性（SMP IRQ Affinity），从而实现队列绑定到不同的核。Linux提供了较为灵活的队列选择机制。dev_pick_tx用于选取发送队列，可以由driver定制其策略，也可以根据队列优先级选取，按照hash来做均衡。也就是利用XPS（Transmit Packet Steering，内核2.6.38后引入）机制，智能地选择多队列设备的队列来发送数据包。为此，需要对CPU核到硬件队列做一个表来记录映射关系，每一个映射记录着专门分配的队列到一个CPU核列表，这个映射的CPU核负责完成队列中的数据传输。这样做的目的一是减少设备队列上的锁竞争，二是增加传输时的缓存命中概率。下面的代码简单说明了在发送时队列的选取是考虑在其中的： 1234567891011121314151617int dev_queue_xmit(struct sk_buff *skb) &#123; struct net_device *dev = skb-&gt;dev; txq = dev_pick_tx(dev, skb); // 选出一个队列 spin_lock_prefetch(&amp;txq-&gt;lock); dev_put(dev); &#125;struct netdev_queue *netdev_pick_tx(struct net_device *dev, struct sk_buff *skb) &#123; int queue_index = 0; if (dev-&gt;real_num_tx_queues！= 1) &#123; const struct net_device_ops *ops = dev-&gt;netdev_ops; if (ops-&gt;ndo_select_queue) queue_index = ops-&gt;ndo_select_queue(dev, skb); // 按照driver提供的策略来选择一个队列的索引 else queue_index = __netdev_pick_tx(dev, skb); queue_index = dev_cap_txqueue(dev, queue_index); &#125; skb_set_queue_mapping(skb, queue_index); return netdev_get_tx_queue(dev, queue_index); &#125; 除此之外，收发队列一般会被绑在同一个中断上，其目的也是为了增加cache命中率。 除了硬件支持的多队列技术外，还有软件支持的流量均衡技术，主要用于单队列网卡，将其上流量均衡的分摊到多个核上，该项技术就是RPS（Receive Packet Steering）。在接收侧，RPS主要是把软中断的负载均衡到CPU的各个core上，网卡驱动对每个流生成一个hash标识，这个hash值可以通过四元组（源IP地址SIP，源四层端口SPORT，目的IP地址DIP，目的四层端口DPORT）来计算，然后由中断处理的地方根据这个hash标识分配到相应的core上去，这样就可以比较充分地发挥多核的能力了。在发送侧，无论来自哪个CPU的数据包只能往这唯一的队列上发送。通俗点来说，就是在软件层面模拟实现硬件的多队列网卡功能，其实现机制如下图所示： DPDK对多队列的支持：观察DPDK提供的一系列以太网设备的API，可以发现其Packet I/O机制具有与生俱来的多队列支持功能，可以根据不同的平台或者需求，选择需要使用的队列数目，并可以很方便地使用队列，指定队列发送或接收报文。根据这样的特性，可以很容易实现CPU核、缓存与网卡队列之间的亲和性，从而达到很好的性能。除此之外，DPDK的队列管理机制还可以避免多核处理器中的多个收发进程采用自旋锁产生的不必要等待。以RTC模型为例，可以从核、内存与网卡队列之间的关系来理解DPDK是如何利用网卡多队列技术带来性能的提升： 1）将网卡的某个接收队列分配给某个核，从该队列中收到的所有报文都应当在该指定的核上处理。 2）从核对应的本地存储中分配内存池，接收报文和对应的报文描述符都位于该内存池。 3）为每个核分配一个单独的发送队列，发送报文和对应的报文描述符都位于该核和发送队列对应的本地内存池中。 可以看出不同的核，操作的是不同的队列，从而避免了多个线程同时访问一个队列带来的锁的开销。那么，网卡是如何将网络中的报文分发到不同的队列呢？常用的方法有微软提出的RSS与英特尔提出的Flow Director技术，前者是根据哈希值希望均匀地将包分发到多个队列中。后者是基于查找的精确匹配，将包分发到指定的队列中。此外，网卡还可以根据优先级分配队列提供对QoS的支持。除此之外，网卡多队列机制还可以应用于虚拟化，详见后文《I/O虚拟化详解》。 高级的网卡设备可以分析出包的类型，包的类型会携带在接收描述符中，应用程序可以根据描述符快速地确定包是哪种类型的包，避免了大量的解析包的软件开销。DPDK的Mbuf结构中含有相应的字段来表示网卡分析出的包的类型，从下面的代码可见Packet_type由二层、三层、四层及tunnel的信息来组成，应用程序可以很方便地定位到它需要处理的报文头部或是内容。 12345678910111213141516struct rte_mbuf &#123; …… union &#123; uint32_t packet_type; /**&lt; L2/L3/L4 and tunnel information．*/ struct &#123; uint32_t l2_type：4; /**&lt; (Outer) L2 type．*/ uint32_t l3_type：4; /**&lt; (Outer) L3 type．*/ uint32_t l4_type：4; /**&lt; (Outer) L4 type．*/ uint32_t tun_type：4; /**&lt; Tunnel type．*/ uint32_t inner_l2_type：4; /**&lt; Inner L2 type．*/ uint32_t inner_l3_type：4; /**&lt; Inner L3 type．*/ uint32_t inner_l4_type：4; /**&lt; Inner L4 type．*/ &#125;; &#125;; ……&#125;; 网卡硬件卸载功能网卡的硬件卸载功能可能是基于端口设置，也有可能是基于每个包设置使能，需要仔细区分。在包粒度而言，每个包都对应一个或者多个Mbuf，DPDK软件利用rte_mbuf数据结构里的64位的标识（ol_flags）来表征卸载与状态。 如果需要使用硬件卸载功能，网卡驱动需要提供相应的API给上层应用，通过调用API驱动硬件完成相应的工作。而驱动硬件的工作实际上是由网卡驱动程序完成的，网卡驱动程序也是通过硬件提供的接口来驱动硬件。硬件提供的接口一般包括寄存器（Register）和描述符（Descriptor）。寄存器是全局的设置，一般用于开启某项功能或者为某项功能设置全局性的参数配置，一般情况下是基于以太网端口为基本单位。描述符可以看做是每个数据包的属性，和数据包一起发送给硬件，一般用于携带单个数据包的参数或设置。 对于各种各样的硬件卸载功能，按照功能的相似性大致可分成三类，分别是计算及更新功能、分片功能、组包功能。 VLAN硬件卸载：VLAN在以太网报文中增加了了一个4字节的802.1q Tag（也称为VLAN Tag），如果由软件完成VLAN Tag的插入将会给CPU带来额外的负荷，涉及一次额外的内存拷贝（报文内容复制），最坏场景下，这可能是上百周期的开销。大多数网卡硬件提供了VLAN卸载的功能，VLAN Tag的插入和剥离由网卡硬件完成，可以减轻服务器CPU的负荷。 1）网卡最典型的卸载功能之一就是在接收侧针对VLAN进行包过滤。在网卡硬件端口设计了VLAN过滤表，无法在过滤表中匹配的VLAN包会被丢弃，没有VLAN信息的以太网则会通过网卡的过滤机制，在DPDK中app/testpmd提供了测试命令与实现代码。 2）网卡硬件能够对接收到的包的VLAN Tag进行剥离。首先硬件能够对VLAN包进行识别，原理上是判断以太帧的以太网类型来确定是否是VLAN包。启动这项硬件特性，需要在网卡端口，或者是属于这个网卡端口的队列上设置使能标志，将VLAN剥离特性打开，对应到软件，是通过驱动将配置写入相应的寄存器。DPDK的app/testpmd提供了如何基于端口使能与去使能的测试命令： 12testpmd&gt; vlan set strip (on|off) (port_id)testpmd&gt; vlan set stripq (on|off) (port_id,queue_id) 3）网卡硬件会将4字节的VLAN tag从数据包中剥离，VLAN Tag中包含的信息对上层应用是有意义的，不能丢弃，此时，网卡硬件会在硬件描述符中设置两个域，将需要的信息通知驱动软件，包含此包是否曾被剥离了VLAN Tag以及被剥离的Tag。软件省去了剥离VLAN Tag的工作负荷，还获取了需要的信息。在DPDK中，驱动会根据硬件描述符信息对每个接收的数据包进行检测，如果剥离动作发生，需要将rte_mbuf数据结构中的PKT_RX_VLAN_PKT置位，表示已经接收到VLAN的报文，并且将被剥离VLAN Tag写入到下列字段，供上层应用处理。 123Struct rte_mbuf&#123; uint16_t vlan_tci; /**&lt; VLAN Tag Control Identifier(CPU order) */ &#125; 4）在发送端口需要在数据包中插入VLAN标识。VLAN Tag由两部分组成：TPID（Tag Protocol Identifier），也就是VLAN的Ether type，和TCI（Tag Control Information）。TPID是一个固定的值，作为一个全局范围内起作用的值，可通过寄存器进行设置。而TCI是每个包相关的，需要逐包设置，在DPDK中，在调用发送函数前，必须提前设置mbuf数据结构中PKT_TX_VLAN_PKT位，同时将具体的Tag信息写入vlan_tci字段。 123struct rte_mbuf&#123; uint16_t vlan_tci; /**&lt; VLAN Tag Control Identifier(CPU order) */ &#125; 5）为了解决VLAN个数局限，业界发展出了采用双层乃至多层VLAN堆叠模式，随着这种模式（也被称为QinQ技术）在网络应用中变得普遍，现代网卡硬件大多提供对两层VLAN Tag进行卸载，如VLAN Tag的剥离、插入。DPDK的app/testapp应用中提供了测试命令，网卡数据手册有时也称VLAN Extend模式。在DPDK相应测试代码如下： 1testpmd&gt; vlan set qinq (on|off) (port_id) checksum硬件卸载功能：checksum计算是网络协议的容错性设计的一部分，基于网络传输不可靠的假设，因此在Ethernet、IPv4、UDP、TCP、SCTP各个协议层设计中都有checksum字段，用于校验包的正确性，checksum不涉及复杂的逻辑，是简单机械的计算，算法稳定，适合固化到硬件中。checksum虽然可以硬件卸载，但依然需要软件的协同配合实现。checksum在收发两个方向上都需要支持，操作并不一致，在接收方向上，主要是检测，通过设置端口配置，强制对所有达到的数据报文进行检测，即判断哪些包的checksum是错误的，对于这些出错的包，可以选择将其丢弃，并在统计数据中体现出来。在DPDK中，和每个数据包都有直接关联的是rte_mbuf，网卡自动检测进来的数据包，如果发现checksum错误，就会设置错误标志。软件驱动会查询硬件标志状态，通过mbuf中的ol_flags字段来通知上层应用。但在发送侧就会复杂一些，硬件需要计算协议的checksum，将且写入合适的位置。。在原理上，网卡在设计之初时就依赖软件做额外设置，软件需要逐包提供发送侧上下文状态描述符，这段描述符需要通过PCIe总线写入到网卡设备内，帮助网卡进行checksum计算。设置上下文状态描述符，在DPDK驱动里面已经实现，对于使用DPDK的程序员，真正需要做的工作是设置rte_mbuf和改写报文头部，保证网卡驱动得到足够的mbuf信息，完成整个运算。 1）从设置mbuf的角度，需要关注如下字段：IPv6头部没有checksum字段，无需计算。 12345678/* fields to support TX offloads */ uint64_t tx_offload; /**&lt; combined for easy fetch */ struct &#123; uint64_t l2_len：7; /**&lt; L2 (MAC) Header Length．*/ uint64_t l3_len：9; /**&lt; L3 (IP) Header Length．*/ uint64_t l4_len：8; /**&lt; L4 (TCP/UDP) Header Length．*/ uint64_t tso_segsz：16; /**&lt; TCP TSO segment size */&#125; 2）对于IPv4的checksum，在发送侧如果需要硬件完成自动运算与插入，准备工作如下： 12ipv4_hdr-&gt;hdr_checksum = 0; // 将头部的checksum字段清零 ol_flags |= PKT_TX_IP_CKSUM; // IP层checksum请求标识置位 3）对于UDP或者TCP，checksum计算方法一样，准备工作如下： 123udp_hdr-&gt;dgram_cksum = 0; // 将头部的checksum字段清零 ol_flags |= PKT_TX_UDP_CKSUM; // UDP层checksum请求标识置位 udp_hdr-&gt;dgram_cksum = get_psd_sum(l3_hdr, info-&gt;ethertype, ol_flags); /* 填入IP层伪头部计算码，具体实现参阅DPDK代码*/ 4）对于SCTP checksum计算方法一样，准备工作如下： 1sctp_hdr-&gt;hdr_checksum = 0; ol_flags |= PKT_TX_SCTP_CKSUM; TSO硬件卸载：TSO（TCP Segment Offload）是TCP分片功能的硬件卸载，显然这是发送方向的功能。TCP会协商决定发送的TCP分片的大小，对于从应用层获取的较大的数据，TCP需要根据下层网络的报文大小限制，将其切分成较小的分片发送。硬件提供的TCP分片硬件卸载功能可以大幅减轻软件对TCP分片的负担。而且这项功能本身也是非常适合由硬件来完成的，因为它是比较简单机械的实现。如下所示，就是采用TSO和不采用TSO的示意图： 在下图中，我们可以看到，TCP分片需要将现有的较大的TCP分片拆分成较小的TCP分片，在这个过程中，不需要提供特殊的信息，仅仅需要复制TCP的包头，更新头里面的长度相关的信息，重新计算校验和，显然这些功能非常适合硬件来实现。 在dpdk/testpmd中提供了两条TSO相关的命令行： 12tso set 14000：用于设置tso分片大小。tso show 0：用于查看tso分片的大小。 和csum硬件卸载功能类似，tso分片硬件卸载功能也需要对mbuf进行设置，同样从设置mbuf的角度，如下字段需要关注： 12345678/* fields to support TX offloads */ uint64_t tx_offload; /**&lt; combined for easy fetch */ struct &#123; uint64_t l2_len：7; /**&lt; L2 (MAC) Header Length．*/ uint64_t l3_len：9; /**&lt; L3 (IP) Header Length．*/ uint64_t l4_len：8; /**&lt; L4 (TCP/UDP） Header Length．*/ uint64_t tso_segsz：16; /**&lt; TCP TSO segment size */&#125; 同时tso使用了ol_flag中的PKT_TX_TCP_SEG来指示收发包处理流程中当前的包需要开启tso的硬件卸载功能。 RSC组包功能卸载：RSC（Receive Side Coalescing，接收方聚合）是TCP组包功能的硬件卸载。硬件组包功能针对TCP实现，是接收方向的功能，可以将拆分的TCP分片聚合成一个大的分片，从而减轻软件的处理。如下图所示，LRO是RCS的另一种表述，下图左边是通过硬件层面完成组包功能，下图右边是通过驱动层面完成组包。 当硬件接收到TCP分片后，硬件可以将多个TCP分片缓存起来，并且将其排序，这样多个TCP分片最终传递给软件时将会呈现为一个分片，软件将不再需要分析处理多个数据包的头，同时对TCP包的排序的负担也有所减轻。如下图所示： RSC是一种硬件能力，使用此功能时需要先明确硬件支持此能力。我们通过配置来开启RSC功能，需要关注下面的数据结构： 12345678910111213141516/** * A structure used to configure the RX features of an Ethernet port. */ struct rte_eth_rxmode &#123; /** The multi-queue packet distribution mode to be used, e.g．RSS．*/ enum rte_eth_rx_mq_mode mq_mode; uint32_t max_rx_pkt_len; /**&lt; Only used if jumbo_frame enabled．*/ uint16_t split_hdr_size; /**&lt; hdr buf size (header_split enabled).*/ uint16_t header_split：1, /**&lt; Header Split enable．*/ hw_ip_checksum ：1, /**&lt; IP/UDP/TCP checksum offload enable．*/ hw_vlan_filter ：1, /**&lt; VLAN filter enable．*/ hw_vlan_strip ：1, /**&lt; VLAN strip enable．*/ hw_vlan_extend ：1, /**&lt; Extended VLAN enable．*/ jumbo_frame ：1, /**&lt; Jumbo Frame Receipt enable．*/ hw_strip_crc ：1, /**&lt; Enable CRC stripping by hardware．*/ enable_scatter ：1, /**&lt; Enable scatter packets rx handler */ enable_lro ：1; /**&lt; Enable LRO */ &#125;; 当对接收处理进行初始化ixgbe_dev_rx_init时，会调用ixgbe_set_rsc，此函数中对enable_lro进行判断，如果其为真，则会对RSC进行相关设置，从而使用此功能。 DPDK在NFV中应用案例回顾ETSI NFV参考架构，NFV技术通过运行在通用x86架构硬件上的虚拟化网络功能，通过软硬件解耦及功能抽象来实现各类网络功能在x86标准服务器上的灵活部署和业务的快速迭代。不同于典型数据中心业务和企业网业务，电信广域网业务要求网元（如DPI、FW等）具有高吞吐、低时延、海量流表支持、用户级QoS控制的特点。考虑到现实环境中的NFV解决方案一般由NFV基础设施和VNF两类系统服务商提供。因此，相应的NFV端到端性能优化，也应划分为底层的NFV基础设施性能与上层的VNF性能两类，以方便明确各自的性能瓶颈，避免不同层次的性能调优工作带来相互干扰。 在NFV基础设施性能优化技术方案中，DPDK软件加速方案已成为一种普遍采用的基本方法，它以用户数据I/O通道优化为基础，结合了Intel VT技术、操作系统、虚拟化层与vSwitch等多种优化方案，已经形成了完善的性能加速整体架构，并提供了用户态API供高速转发类应用访问。 场景一：VNF在物理机上应用运营商现有网络大部分都是专用网络设备，物理设备与应用软件紧耦合，设备升级成本高、功能扩展困难。将这些专用的网络功能设备，以软件化的VNF形式直接运行在物理服务器上，可以实现网络设备形态的通用化，方便设备功能灵活扩展。 该方案将一部分原来由硬件实现的网络功能，以VNF软件的形式直接运行在x86服务器OS上，同时在物理服务器上加载DPDK组件。此时，DPDK接管了物理网卡的I/O驱动， VNF也不再使用传统Linux内核网络协议栈，而是通过调用DPDK的用户态API进行快速转发。同时，DPDK进程将使用少量的处理器核（如2个核）与内存以满足高速转发处理， VNF 可以直接使用剩余的全部硬件资源，适用于数据转发频繁等资源利用率长期较高的网络业务，比如C/U分离后的U面网元采用VNF部署。该方案下整台服务器仅支持单一VNF应用，但因设备形态统一，软件功能部署灵活，仍具有较高应用价值。具体实现架构如下图所示： 场景二：VNF + OVS应用当多个VNF分别运行在一台服务器的多个VM中时，为满足VNF之间可能的流量交换或者共享物理网卡的需要，可以在Host OS上安装OVS类虚拟交换机，用于连接各个VNF（VM）和服务器的物理网卡端口。此时，这多个VNF一般是不同类型的VNF应用，VNF之间可能产生交互流量或者业务链处理流量（东西+南北流量模型），而同类型多个VNF或者纯粹共享网卡的多个VNF，一般采用场景三的“VNF + SR-IOV”方案。 在“VNF + OVS”方案中，因主要的性能瓶颈存在于VNF的虚拟I/O通道和OVS交换机，DPDK需要分别安装在运行VNF的VM镜像内部和运行OVS的物理服务器OS上：前者用于优化VM内部的VNF数据平面转发性能，包括提供虚拟化网卡驱动、提供用户态转发API等，DPDK的各种配置方法与VNF运行在物理机中的机制类似，VNF并不能感知是运行在VM环境；后者用于优化OVS交换机性能，连接VM与各NUMA节点上的DPDK端口。该场景可以细分为两种： ​ A：VM到OVS仅使用单一连接。 ​ B：VM到OVS采用一进一出的双向连接，如各类业务链中的VNF应用，见下图所示。 该方案可以实现VNF的灵活扩容/缩容，以及在资源池中按需迁移，方案中的第三方VNF厂商可以屏蔽物理设备差异，提供各自的高性能业务产品。同时，使用经过DPDK优化后的OVS 交换机，可以灵活实现VNF间流量的灵活转发与互联互通，节省硬件交换机。 场景三：VNF + SR-IOV当多个VNF运行在VM中时，各VM的虚拟网卡可以直接连至HOST上支持SR-IOV功能的物理网卡（VF）进行数据收发，如同独占物理网卡一样。该方案适用于VNF间无需流量交互的场景，或者是基于硬件交换机进行VNF互连和流量控制的场景。由于旁路了HOST的虚拟化层实现直接转发，可以达到近似物理转发的性能，被业界普遍用于消除Hypervisor带来的数据转发性能影响。如下图所示： 尽管“VNF+SR-IOV”方案消除了从物理网卡到VM虚拟网卡的性能瓶颈，但VM内部仍然需要通过加载DPDK以进一步优化各VNF（VM内部）的转发性能。此时，DPDK可以采用与前两种场景中类似的方法进行加载，同时占用VM内部一定CPU核和内存资源。 综上，就是DPDK技术在电信云中最佳实践的全部内容，后续我们将开始讲述电信云中另一个重要领域内容–虚拟化。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-30-DPDK技术栈在电信云中的最佳实践（二）]]></title>
    <url>%2F2019%2F05%2F01%2F2019-04-30-DPDK%E6%8A%80%E6%9C%AF%E6%A0%88%E5%9C%A8%E7%94%B5%E4%BF%A1%E4%BA%91%E4%B8%AD%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[DPDK技术基础（2）原子操作虽然，DPDK提出理念之一就是“遵循资源局部化的原则，解耦数据的跨核共享，使得性能可以有很好的水平扩展”。这种跨核解耦数据并同步的的本质就是原子操作，所谓原子操作简单来说就是：多个线程执行一个操作时，其中任何一个线程要么可以完全执行完此操作，要么根本不执行。比如：在单处理器系统（UniProcessor）中，能够在单条指令中完成的操作都可以认为是“原子操作”，因为中断只能发生于指令之间。 在多核CPU的时代，架构中运行着多个独立的CPU，即使在单个指令中可以完成的操作也可能会被干扰。典型的例子就是decl指令（递减指令），它细分为三个过程：“读-&gt;改-&gt;写”，涉及两次内存操作。如果多个CPU运行的多个进程或线程在同时对同一块内存执行这个指令，那情况是无法预测的。 这里需要特别介绍一下CMPXCHG这条指令，它的语义是比较并交换操作数（CAS，Compare And Set）。而用XCHG类的指令做内存操作，CPU自动遵循LOCK语义，可见该指令是一条原子的CAS单指令操作，它是实现很多无锁数据结构的基础。CAS操作需要输入两个数值，一个旧值（期望操作前的值）和一个新值，在操作期间先比较下旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化，则不交换。比如：CMPXCHG r/m，r将累加器AL/AX/EAX/RAX中的值与首操作数（目的操作数）比较，如果相等，第2操作数（源操作数）的值装载到首操作数，zf置1。如果不等， 首操作数的值装载到AL/AX/EAX/RAX并将zf清0。该指令只能用于486及其后继机型。第2操作数（源操作数）只能用8位、16位或32位寄存器。第1操作数（目地操作数）则可用寄存器或任一种存储器寻址方式。我们通过下列代码可以对CMPXCHG指令（CAS)进行测试理解： 1）不相等条件测试 12345678910111213141516171819202122#includeusing namespace std;int main()&#123; int a=0,b=0,c=0; __asm&#123; mov eax,100; mov a,eax &#125; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; b = 99; c = 11; __asm&#123; mov ebx,b cmpxchg c,ebx mov a,eax &#125; cout &lt;&lt; "b := " &lt;&lt; b &lt;&lt; endl; cout &lt;&lt; "c := " &lt;&lt; c &lt;&lt; endl; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; return 0;&#125; 输出:(如果不等， “首操作数”(c)的值装载到AL/AX/EAX/RAX并将zf清0) 1234a := 100b := 99c := 11a := 11 2）相等条件测试 1234567891011121314151617181920212223#includeusing namespace std;int main()&#123; int a=0,b=0,c=0; __asm&#123; mov eax,100; mov a,eax &#125; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; b = 99; c = 99; __asm&#123; mov eax,99 mov ebx,777 cmpxchg c,ebx mov a,eax &#125; cout &lt;&lt; "b := " &lt;&lt; b &lt;&lt; endl; cout &lt;&lt; "c := " &lt;&lt; c &lt;&lt; endl; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; return 0;&#125; 输出:(如果相等，第2操作数（源操作数）的值装载到首操作数，zf置1) 1234a := 100b := 99c := 777a := 99 在x86平台上，CPU提供的硬件原子操作分为三种独立的原子锁机制：原子保证操作、加LOCK指令前缀和缓存一致性协议。我们这里只讨论软件原子锁机制，硬件原子锁有兴趣的详见英特尔的软件开发者手册《Volume 38.1LOCKED ATOMIC OPERATIO》章节。 软件级的原子操作实现依赖于硬件原子操作的支持。对于Linux而言，内核提供了两组原子操作接口：一组是针对整数进行操作；另一组是针对单独的位进行操作。针对整数的原子操作只能处理atomic_t类型的数据，这是一种抽象出来的整型数据类型结构，而没有使用C语言的int类型是因为：1）将原子操作涉及的数据与普通int类型数据进行区分；2）可以屏蔽不同硬件架构的差异性。而针对位的原子操作，在Linux内核中，原子位操作分别定义于include\linux\types.h和arch\x86\include\asm\bitops.h。 尽管Linux支持的所有机器上的整型数据都是32位，但是使用atomic_t的代码只能将该类型的数据当作24位来使用。这个限制完全是因为在SPARC体系结构上，原子操作的实现不同于其他体系结构：32位int类型的低8位嵌入了一个锁，因为SPARC体系结构对原子操作缺乏指令级的支持，所以只能利用该锁来避免对原子类型数据的并发访问。 原子操作在DPDK代码中的定义都在rte_atomic.h文件中，主要包含两部分：内存屏蔽和原16、32和64位的原子操作API。 rte_mb（）：内存屏障读写API rte_wmb（）：内存屏障写API rte_rmb（）：内存屏障读API 这三个API的实现在DPDK代码中没有什么区别，都是直接调用__sync_synchronize()。比如：在virtio_dev_rx()函数中，在读取avail-&gt;flags之前，加入内存屏障API以防止乱序的执行： 1234567*(volatile uint16_t *)&amp;vq-&gt;used-&gt;idx += count; vq-&gt;last_used_idx = res_end_idx; /* flush used-&gt;idx update before we read avail-&gt;flags．*/ rte_mb(); /* Kick the guest if necessary．*/ if (！(vq-&gt;avail-&gt;flags &amp; VRING_AVAIL_F_NO_INTERRUPT)) eventfd_write(vq-&gt;callfd, (eventfd_t)1); DPDK代码中提供了16、32和64位原子操作的API，以rte_atomic64_add()API源代码为例，讲解一下DPDK中原子操作的实现，其代码如下： 12345678910static inline void rte_atomic64_add(rte_atomic64_t *v, int64_t inc) &#123; int success = 0; uint64_t tmp; while (success == 0) &#123; tmp = v-&gt;cnt; success = rte_atomic64_cmpset((volatile uint64_t *)&amp;v-&gt;cnt, tmp, tmp + inc); &#125;&#125; 可以看到这个API中主要是使用了比较和交换的原子操作API函数rte_atomic64_cmpset()，里面通过嵌入汇编来实现，代码如下： 123456rte_atomic64_cmpset(volatile uint64_t *dst, uint64_t exp, uint64_t src) &#123; uint8_t res; asm volatile( MPLOCKED "cmpxchgq ％[src], ％[dst];" "sete ％[res];" ：[res] "=a" (res), [dst] "=m" (*dst) ：[src] "r" (src), "a" (exp), "m" (*dst) ："memory"); return res; &#125; 在现网VXLAN的数据包校验和错包统计中，就是通过加上原子操作的数据包统计才能实现多核场景下的准确性。 12345678910111213141516171819int vxlan_rx_pkts(struct virtio_net *dev, struct rte_mbuf **pkts_burst, uint32_t rx_count) &#123; uint32_t i = 0; uint32_t count = 0; int ret; struct rte_mbuf *pkts_valid[rx_count]; for (i = 0; i &lt; rx_count; i++) &#123; if (enable_stats) &#123; rte_atomic64_add( &amp;dev_statistics[dev- &gt;device_fh].rx_bad_ip_csum, (pkts_burst[i]-&gt;ol_flags &amp; PKT_RX_IP_CKSUM_BAD) ！= 0); rte_atomic64_add( &amp;dev_statistics[dev-&gt;device_fh].rx_bad_ip_csum, (pkts_burst[i]-&gt;ol_flags &amp; PKT_RX_L4_CKSUM_BAD) ！= 0); &#125; ret = vxlan_rx_process(pkts_burst[i]); if (unlikely(ret &lt; 0)) continue; pkts_valid[count] = pkts_burst[i]; count++; &#125; ret = rte_vhost_enqueue_burst(dev, VIRTIO_RXQ, pkts_valid, count); return ret; &#125; 自旋锁何谓自旋锁（spin lock）？它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，“自旋”一词就是因此而得名。 自旋锁必须基于CPU的数据总线锁定，它通过读取一个内存单元（spinlock_t）来判断这个自旋锁是否已经被别的CPU锁住。如果否，它写进一个特定值，表示锁定了总线，然后返回。如果是，它会重复以上操作直到成功，或者spin次数超过一个设定值。 锁定数据总线的指令只能保证一个指令操作期间CPU独占数据总线。（自旋锁在锁定的时侯，不会睡眠而是会持续地尝试）。其作用是为了解决某项资源的互斥使用。因为自旋锁不会引起调用者睡眠，所以自旋锁的效率远高于互斥锁，但是它也有些不足之处： 自旋锁一直占用CPU，它在未获得锁的情况下，一直运行——自旋，所以占用着CPU，如果不能在很短的时间内获得锁，这无疑会使CPU效率降低。 在用自旋锁时有可能造成死锁，当递归调用时有可能造成死锁，调用有些其他函数（如copy_to_user()、copy_from_user()、kmalloc()等）也可能造成死锁。 因此，要慎重使用自旋锁，自旋锁只有在内核可抢占式或SMP的情况下才真正需要，在单CPU且不可抢占式的内核下，自旋锁的操作为空操作。自旋锁适用于锁使用者保持锁时间比较短的情况。 Linux内核中的自旋锁API 在Linux kernel实现代码中，自旋锁的实现与体系结构有关，所以相应的头文件&lt;asm/spinlock.h&gt;位于相关体系结构的代码中。 在Linux内核中，自旋锁的基本使用方式如下： 先声明一个spinlock_t类型的自旋锁变量，并初始化为“未加锁”状态。在进入临界区之前，调用加锁函数获得锁，在退出临界区之前，调用解锁函数释放锁。比如： 1234spinlock_t lock = SPIN_LOCK_UNLOCKED; spin_lock(&amp;lock);/* 临界区 */ spin_unlock(&amp;lock); spin_lock函数用于获得自旋锁，如果能够立即获得锁，它就马上返回，否则，它将自旋在那里，直到该自旋锁的保持者释放。spin_unlock函数则用于释放自旋锁。此外，还有一个spin_trylock函数用于尽力获得自旋锁，如果能立即获得锁，它获得锁并返回真；若不能立即获得锁，立即返回假。它不会自旋等待自旋锁被释放。 自旋锁使用时有两点需要注意： 1）自旋锁是不可递归的，递归地请求同一个自旋锁会造成死锁。 2）线程获取自旋锁之前，要禁止当前处理器上的中断。 比如：当前线程获取自旋锁后，在临界区中被中断处理程序打断，中断处理程序正好也要获取这个锁，于是中断处理程序会等待当前线程释放锁，而当前线程也在等待中断执行完后再执行临界区和释放锁的代码。 Linux中自旋锁方法汇总如下： DPDK自旋锁实现和应用 DPDK中自旋锁API的定义在rte_spinlock.h文件中，其中下面三个API被广泛的应用在告警、日志、中断机制、内存共享和link bonding的代码中，用于临界资源的保护。 123rte_spinlock_init(rte_spinlock_t *sl)； // 初始化自旋锁rte_spinlock_lock(rte_spinlock_t *sl); // 获取自旋锁rte_spinlock_unlock (rte_spinlock_t *sl); // 释放自旋锁 rte_spinlock_t定义如下，简洁并且简单: 123456/** * The rte_spinlock_t type. */ typedef struct &#123; volatile int locked; /**&lt; lock status 0 = unlocked, 1 = locked */ &#125; rte_spinlock_t; 读写锁读写锁实际是一种特殊的自旋锁，它把对共享资源的访问操作划分成读操作和写操作。这种锁相对于自旋锁而言，能提高并发性，因为在多处理器系统中，它允许同时有多个读操作来访问共享资源，最大可能的读操作数为实际的逻辑CPU数。写操作是排他性的，一个读写锁同时只能有一个写操作或多个读操作（与CPU数相关），但不能同时既有读操作又有写操作。 读写锁除了和普通自旋锁一样有自旋特性以外，还有以下特点： 读锁之间资源是共享的：即一个线程持有了读锁之后，其他线程也可以以读的方式持有这个锁。 写锁之间是互斥的：即一个线程持有了写锁之后，其他线程不能以读或者写的方式持有这个锁。 读写锁之间是互斥的：即一个线程持有了读锁之后，其他线程不能以写的方式持有这个锁。 每个共享资源关联一个唯一的读写锁，线程只允许以下方式访问共享资源： 申请锁。 获得锁后，读写共享资源。 释放锁。 读写锁主要用于比较短小的代码片段，线程等待期间不能进入睡眠状态，因为睡眠/唤醒操作相当耗时，大大延长了获得锁的等待时间，所以读写锁适用的场景是要求忙等待。申请锁的线程必须不断地查询是否发生退出等待的事件，不能进入睡眠状态。 Linux中读写锁主要API函数 上述API函数的定义在各个Linux内核文件的&lt;asm/rwlock.h&gt;中。 DPDK读写锁实现和应用 DPDK读写锁的定义在rte_rwlock.h文件中， rte_rwlock_init（rte_rwlock_t*rwl）：初始化读写锁到unlocked状态。 rte_rwlock_read_lock（rte_rwlock_t*rwl）：尝试获取读锁直到锁被占用。 rte_rwlock_read_unlock（rte_rwlock_t*rwl）：释放读锁。 rte_rwlock_write_lock（rte_rwlock_t*rwl）：获取写锁。 rte_rwlock_write_unlock（rte_rwlock_t*rwl）：释放写锁。 读写锁在DPDK中主要应用在下面几个地方，主要用于对操作的对象进行保护。 在查找空闲的memory segment的时候，使用读写锁来保护memseg结构。LPM表创建、查找和释放。 Memory ring的创建、查找和释放。 ACL表的创建、查找和释放。 Memzone的创建、查找和释放等。 比如：查找空闲的memory segment的时候，使用读写锁来保护memseg结构的代码实例如下: 123456789101112/* Lookup for the memzone identified by the given name */ const struct rte_memzone * rte_memzone_lookup(const char *name) &#123; struct rte_mem_config *mcfg; const struct rte_memzone *memzone = NULL; mcfg = rte_eal_get_configuration()-&gt;mem_config; rte_rwlock_read_lock(&amp;mcfg-&gt;mlock); memzone = memzone_lookup_thread_unsafe(name); rte_rwlock_read_unlock(&amp;mcfg-&gt;mlock); return memzone; &#125; 无锁环形缓冲区高性能的服务器软件（例如，HTTP加速转发器）在大部分情况下是运行在多核服务器上的，当前的硬件可以提供32、64或者更多的CPU，在这种高并发的环境下，锁竞争机制有时会比数据拷贝、上下文切换等更伤害系统的性能。因此，在多核环境下，需要把重要的数据结构从锁的保护下移到无锁环境，以提高软件性能。现在无锁机制变得越来越流行，在特定的场合使用不同的无锁队列，可以节省锁开销，提高程序效率。Linux内核中有无锁队列的实现，可谓简洁而不简单。 Linux内核无锁环形缓冲 环形缓冲区通常有一个读指针和一个写指针。读指针指向环形缓冲区中可读的数据，写指针指向环形缓冲区中可写的数据。通过移动读指针和写指针就可以实现缓冲区的数据读取和写入。在通常情况下，环形缓冲区的读用户仅仅会影响读指针，而写用户仅仅会影响写指针。如果仅仅有一个读用户和一个写用户，那么不需要添加互斥保护机制就可以保证数据的正确性。但是，如果有多个读写用户访问环形缓冲区，那么必须添加互斥保护机制来确保多个用户互斥访问环形缓冲区。具体来讲，如果有多个写用户和一个读用户，那么只是需要给写用户加锁进行保护；反之，如果有一个写用户和多个读用户，那么只是需要对读用户进行加锁保护。 在Linux内核代码中，kfifo就是采用无锁环形缓冲的示例，kfifo是一种“First In First Out”数据结构，它采用了前面提到的环形缓冲区来实现，提供一个无边界的字节流服务。采用环形缓冲区的好处是，当一个数据元素被用掉后，其余数据元素不需要移动其存储位置，从而减少拷贝，提高效率。更重要的是，kfifo采用了并行无锁技术，kfifo实现的单生产/单消费模式的共享队列是不需要加锁同步的。详情可以参考Linux内核代码中的kififo的头文件（include/linux/kfifo.h）和源文件（kernel/kfifo.c）。 DPDK无锁环形缓冲 基于无锁环形缓冲的的原理，Intel DPDK提供了一套无锁环形缓冲区队列管理代码。支持单生产者或者多生产者入队列，单消费者或多消费者出队列。下面会记录dpdk中是如何管理所有使用的无锁环形缓冲区以及无锁环形缓冲区中支持的一些操作。 DPDK中的rte_ring的数据结构定义，可以清楚地理解rte_ring的设计基础。 123456789101112131415161718192021222324252627282930313233343536373839404142/* * An RTE ring structure. * * The producer and the consumer have a head and a tail index．The particularity * of these index is that they are not between 0 and size(ring)．These indexes * are between 0 and 2^32, and we mask their value when we access the ring[] * field．Thanks to this assumption, we can dosubtractions between 2 index * values in a modulo-32bit base：that's why the overflow of the indexes is not * a problem. */ struct rte_ring &#123; char name[RTE_RING_NAMESIZE]; /**&lt; Name of the ring．*/ int flags; /**&lt; Flags supplied at creation．*/ /** Ring producer status．*/ struct prod &#123; uint32_t watermark; /**&lt; Maximum items before EDQUOT．*/ uint32_t sp_enqueue; /**&lt; True, if single producer．*/ uint32_t size; /**&lt; Size of ring．*/ uint32_t mask; /**&lt; Mask (size-1) of ring．*/ volatile uint32_t head; /**&lt; Producer head．*/ volatile uint32_t tail; /**&lt; Producer tail．*/ &#125; prod __rte_cache_aligned; /** Ring consumer status．*/ struct cons &#123; uint32_t sc_dequeue; /**&lt; True, if single consumer．*/ uint32_t size; /**&lt; Size of the ring．*/ uint32_t mask; /**&lt; Mask (size-1) of ring．*/ volatile uint32_t head; /**&lt; Consumer head．*/ volatile uint32_t tail; /**&lt; Consumer tail．*/ #ifdef RTE_RING_SPLIT_PROD_CONS &#125; cons __rte_cache_aligned; #else &#125; cons;#endif#ifdef RTE_LIBRTE_RING_DEBUG struct rte_ring_debug_stats stats[RTE_MAX_LCORE];#endif void * ring[0] __rte_cache_aligned; /**&lt; Memory space of ring starts here. not volatile so need to be careful dering */&#125;; 从上面的定义可以看出，无锁环形缓冲区对象中定义了一个生产者对象和一个消费者对象，对应的也就是缓冲区的写对象和读对象。另外，也可以看出无锁环形缓冲区在内存中的组织形式是前面是无锁环形缓冲区对象本身，然后紧接着就是实际用于存储内容的环形队列，在某一时刻其内存布局如下图所示： 无锁环形缓冲区是一种通用的数据结构，所以可能会在多个地方使用，在dpdk中就会有多种情况会使用，比如内存池。所以，随之而来的一个问题就是dpdk如何管理其所使用的所有的无锁环形缓冲区？从它的源码实现(rte_ring.c/rte_ring.h)中我们可以找到答案，与此相关的部分源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647TAILQ_HEAD(rte_ring_list, rte_tailq_entry);static struct rte_tailq_elem rte_ring_tailq = &#123; .name = RTE_TAILQ_RING_NAME,&#125;;EAL_REGISTER_TAILQ(rte_ring_tailq)struct rte_ring *rte_ring_create(const char *name, unsigned count, int socket_id,unsigned flags)&#123; char mz_name[RTE_MEMZONE_NAMESIZE]; struct rte_ring *r; struct rte_tailq_entry *te; const struct rte_memzone *mz; ssize_t ring_size; int mz_flags = 0; struct rte_ring_list* ring_list = NULL; int ret; ring_list = RTE_TAILQ_CAST(rte_ring_tailq.head, rte_ring_list); /* get the size of memory occupied by ring */ ring_size = rte_ring_get_memsize(count); …… te = rte_zmalloc("RING_TAILQ_ENTRY", sizeof(*te), 0); if (te == NULL) &#123; RTE_LOG(ERR, RING, "Cannot reserve memory for tailq\n"); rte_errno = ENOMEM; return NULL; &#125; rte_rwlock_write_lock(RTE_EAL_TAILQ_RWLOCK); mz = rte_memzone_reserve(mz_name, ring_size, socket_id, mz_flags); if (mz != NULL) &#123; r = mz-&gt;addr; /* no need to check return value here, we already checked the arguments above */ rte_ring_init(r, name, count, flags); /* 低维尾队列的entry存放着rte_ring的管理对象地址 */ te-&gt;data = (void *) r; r-&gt;memzone = mz; /* 将存放着环形缓冲区对象的尾队列entry插入到低维尾队列的末端 */ TAILQ_INSERT_TAIL(ring_list, te, next); &#125; else &#123; r = NULL; RTE_LOG(ERR, RING, "Cannot reserve memory\n"); rte_free(te); &#125; rte_rwlock_write_unlock(RTE_EAL_TAILQ_RWLOCK); return r;&#125; 从上面的源码我们可以知道，dpdk是用尾队列来管理其所使用的所有无锁环形缓冲区的，也就是说一个尾队列中的元素就是一个无锁环形缓冲区对象。那用来管理所有无锁环形缓冲区的尾队列，dpdk又如何管理呢？在函数rte_ring_create()中可以看到管理着无锁环形缓冲区的尾队列头部是存放在一个类型为struct rte_tailq_elem的全局变量rte_ring_tailq的head成员中，其中struct rte_tailq_elem定义如下： 12345struct rte_tailq_elem &#123; struct rte_tailq_head *head; TAILQ_ENTRY(rte_tailq_elem) next; const char name[RTE_TAILQ_NAMESIZE];&#125;; 从struct rte_tailq_elem的定义可以看到，管理着无锁环形缓冲区尾队列的头部是另外一个尾队列的一个元素，类似的管理方式还用在了dpdk的内存池等数据结构中，所以在dpdk中采用了两级尾队列来管理所使用的数据结构。其实这样说也不完整，因为除了用二级尾队列来管理所使用的数据结构之外，dpdk还用了一个全局共享内存中的列表数组rte_config.mem_config-&gt;tailq_head[RTE_MAX_TAILQ]来分别存储这个二级尾队列中低维尾队列存放的元素，即管理某一种特定数据结构的尾队列头部。 接下来对dpdk中实现的无锁环形缓冲区所支持的操作做一个说明。从一开始说过无锁环形缓冲区支持单生产者或者多生产者入队列，单消费者或多消费者出队列等操作。其实从另外一个角度还可以说dpdk中的无锁环形缓冲区中支持两种出入队列的模式，即出入队列元素数目固定模式和尽力而为模式，出入队列元素数目固定模式就是说只有进入队列的元素数目达到指定数目才算操作成功，否则失败；而出入队列元素数目尽力而为模式就是说对于指定的数目，如果当时队列状态并不能满足，则以当时队列状态为准，尽可能满足指定的数目。比如：如果参数指定需要入队列3个元素，但队列中只剩下2个空闲空间，那么就将其中2个元素入队列，出队列情况同理。 下面以两个CPU核同时往队列各写入一个元素来介绍无锁环形缓冲区支持的多生产者入队列功能，其他的操作方式都可以从这里推演出来。在代码中多生产者入队列相关的函数为__rte_ring_mp_do_enqueue()，下面的流程也是根据这个函数整理出来的。 初始状态下生产者的头和尾指向了同一个位置。如下图所示： 1）在两个核上，将r-&gt;prod.head和r-&gt;prod.tail分别拷贝到本地的临时变量prod_head和prod_tail中，然后将本地临时变量prod_next指向队列的下一个空闲位置。检查队列中是否有足够的空间，如果没有，则返回失败（如果是写入多个元素，没有足够剩余空间的话，则需要看指定的模式，如果是尽力而为模式，则尽可能往队列中写入元素；否则返回失败）。如下图所示： 2）使用CAS操作指令将本地变量prod_next的值赋值给r-&gt;prod.head，即两者指向同一个位置。CAS操作指令有如下特性：如果r-&gt;prod.head不等于本地变量prod_head，则CAS操作失败，代码重新从第一步开始执行；如果r-&gt;prod.head等于本地变量prod_head，则CAS操作成功，代码继续往后执行。在这里我们假定在核1上操作成功，那么对于核2该操作就会失败，核2会从第一步重新执行。如下图所示： 3）核2上的CAS操作成功，核1上成功往环形缓冲区中写入了一个元素（obj4），接着核2也成功往环形缓冲区中写入了一个元素（obj5）。如下图所示： 4）在上一步中两个核都已经成功往环形缓冲区写入了一个元素，现在两个核都需要更新r-&gt;prod.tail。这里又有一个条件，就是只有r-&gt;prod.tail等于本地变量prod_head的核才能去更新r-&gt;prod.tail的值。从图中可以看到，目前只有在核1上才能满足这个条件，核2不满足，因此更新r-&gt;prod.tail的操作只在核1上进行，核2需要等待核1完成。如下图所示： 5）在上一步中，一旦核1完成了更新r-&gt;prod.tail的操作，那么核2也能满足更新r-&gt;prod.tail的条件，核2此时也会去更新r-&gt;prod.tail。如下图所示： DPDK中的无锁环形缓冲区还有另一个特性，那就是充分利用了unsigned类型的回绕特点，这样对于缓冲区中已用空间和剩余空间的计算就得到了极大的简化，也使得生产者头和尾、消费者头和尾的下标值不局限在0和size(ring) – 1之间，只要在0和2^32 - 1范围之内即可。这一点可以参考dpdk的开发者文档，其具体实现也包含在了上节介绍的操作流程当中，感兴趣的可以去看下。参考文献http://dpdk.org/doc/guides/prog_guide/ 以上部分是DPDK技术栈在电信云领域CPU密集性操作的优化技术介绍，后面第三篇部分将开始DPDK技术栈在电信云领域IO密集性操作的优化技术介绍和NFV的应用场景。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-28-未来网络如何重构]]></title>
    <url>%2F2019%2F04%2F29%2F2019-04-28-%E6%9C%AA%E6%9D%A5%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[写在前面的话虽然本人因为出差关系没有参加公司组织的现场培训，但是通过自学相关材料还是想谈一谈自己的一点浅见。 与其说是对5G认识，不如更广泛地将其定义为“未来网络”这个概念，来谈谈对未来网络的一点儿浅见；与其说是网络转型，不如更彻底说是网络重构；与其说是学习心得，不如说是我自己的思考，尤其是对现有网络如何重构的一点儿思考。 下面我将按照自己学习新东西的“3W”原则（即，What？Why？How？）做一具体阐述: 首先放张本文主体思路的架构图，让大家先有一点儿感性认识。接下来，开始进入正题。 What？—网络重构的目标和趋势是什么？什么是未来网络？广义上来说，未来网络应该是网络质量足够好、响应速度足够快、组网足够灵活、覆盖足够广、能能够连接人和万物、性价比足够好的庞大基础设。要满足上述复杂多变的要求，运营商必须先打破现在刚性网络架构的制约，构筑一个简洁、开放、敏捷和集约的新型网路架构。 狭义上来说，未来网络是以云计算、SDN和NFV为三大技术支柱的网络架构。云计算打破了网络与IT资源分离的局面，构筑了统一云化的虚拟资源池；SDN打破了控制与转发一体的封闭网路架构，实现网络软件可编程；NFV打破了软硬件一体的封闭网元架构，实现了网络资源虚拟化。 网络重构的背景是什么？现有的网络架构是在电话网时代建立的，是以本地电话网为基础、按照行政区域规划构建的省-市-县的分层分域管理体系。 随着云计算、大数据、“互联网+”、物理网、虚拟现实、人工智能等新技术与新业务的出现和加速发展，网络应用由消费型向生产型扩展，网络的连接也由人人互联向万物互联延伸，信息通信业面临新的机遇和挑战，开放、创新、融合成为重要趋势。而我们现有网络一直以来针对各类业务问题总是以出台具体解决方案的方式来解决，不仅各业务解决方案间不兼容，而且长期积累后形成一套十分复杂且僵化的业务流程，同时其烟囱式的网络架构在面对全新业务时变得越来越力不从心，这使得现有网络架构必须重新审视，重新定义，重新设计，重新架构。 运营商网络重构的终极目标是什么？国内三大运营商—中国移动、中国电信和中国联通分别提出了网络重构的战略目标—NovoNet2020、CTNet2025和CUBE-Net2.0，核心是构建能够主动适应新业务发展需求的智能化柔性网络，实现网络基础设施巨大价值的重新释放。 而个人认为网络重构的战略目标应该是致力于实现根本性的转变：一是从互联网+被动地适应网络转变为网络主动、快速、灵活地适应网络应用；二是从传统的“烟囱式”分省、分专业的网络转变为“水平整合”的扁平化网络；三是从分层次、分专业基于中心端局（CO）的组网模式转变为以数据中心（DC）为核心的组网模式。 未来网络的演进趋势是什么？网络作为我们的核心资源，也是我们实现战略转型的主要抓手。随着IT与CT融合，未来网络的演进趋势将有以下几个特征。 1) 网络以DC（即Data Center，数据中心）为中心 随着云计算、大数据等新技术的快速应用，网络的流量、流向已经发生了巨大变化。未来80%甚至更多的应用将部署在云上，DC正逐渐成为网络流量的中心。而现有的网络一般是以省-市-县的划分来组织，在这种逐级收敛的树状架构中，DC仅仅作为一种普通的接入点。因此，必须调整为以DC为中心的新型架构，以适应网络流量及流向变化的新趋势。 2) 网络和云深度融合 云计算的不断发展对网络带宽提出了越来越高的要求，用户需求的不断升级使得云网融合成为必然趋势。业务、IT和网络都可以基于云化技术实现和部署，但是当前云与网之间缺乏灵活互动的机制。比如：在现有的厂家给出的云网融合解决方案中，网络资源是随计算资源同步分配的，即NaaS服务打包在IaaS服务中一起提供给用户，相对于客户或业务的不同需求，两种资源调配存在一定的浪费。个人认为随着容器技术兴起，云化的网络资源池在提供计算、存储等虚拟化资源的同时，网络资源也可以随云资源池的需求而按需变动，即将NaaS从IaaS中拆分独立出来，通过SDN和NFV的跨域协同，真正实现计算、存储和网络资源的统一动态分配和调度，实现云与网的深度协同。 3) 网络功能的软件化 现有的网络以专有硬件设备为主，网络调整以及功能升级的周期很长，且成本较高，完全无法适应新兴业务的快速、灵活的特点。 随着SDN技术逐步成熟、北向接口标准化的制定，将进一步实现三层解耦，甚至通过定义东/西向接口的标准化来进一步解耦VNFM与VNF之间的对接，利用通用的硬件平台为功能网元提供统一的虚拟化运行环境，上层的网元功能按照应用层的模式统一通过软件来实现，软件的升级和更新不再与硬件绑定。一方面实现了网络容量的按需动态伸缩；另一方面利于业务的快速部署和升级，面对不同客户的不同业务需求也可灵活定制，也就是常说的切片网络概念。 4) 集中管控、灵活智能 由于现有网络以行政区域和地理位置划分，受此影响现有网络的管理的也是分段、分级进行。一个跨省链路的开通往往需在在十几个系统上制作几百条甚至上千条数据，同时还要历经多级流程的审批、确认，导致网络服务存在响应慢、资源利用率低下、端到端体验差等弊端。近期最典型的的例子就是江苏的全国虚假主叫拦截平台的升级，从2017年6月各省反馈误拦截VoLTE用户呼叫问题开始，一直到现在都因为跨省升级测试流程未完成而没法部署，各省目前只能采取临时规避措施进行规避。 网络功能软件化后，网络的控制与调度均可通过软件来实现，通过控制与转发分离等方式，使得网络的集中控制成为可能，从而可以更好地实现网络的敏捷部署及灵活调整，提供端到端的业务保障，满足客户全网一致性的体验要求。 Why？—我们为什么要进行网络重构？我们现有网络面临什么样的挑战？近年来，随着网络承载业务的富丰，尤其互联网IT企业推出的业务体验逐渐与运营商推出的业务体验同一化，迫使运营商在当前网络运营中面临一系列挑战。 1) 网络连接数和流量增长推动网络规模快速膨胀 近期随着不限量套餐的普及，物联网行业兴起和发展，集团大连接战略布局，未来将有海量的设备和用户接入网络，连接将变得无处不在。宽带从连接十几亿将增长到几百亿，同时宽带流量将有10倍以上的增长。家庭千兆以及个人百兆服务将成为普遍服务，而一些新业务（如4K/8K视频、虚拟现实游戏、无人驾驶等）对网络丢包率、时延等QoS要求更苛刻。 2) 业务云化和终端虚拟化将颠覆网络全局的流量模型 随着云计算的发展，私有云、公有云的逐步普及，必将推动大规模的移动网络建设，用户对宽带的需求必将从基于覆盖的连接，转向基于内容和社交体验的连接。现有网络业务流量主要服务于网络终端节点之间的通信，符合泊松分布模型。但是，现在随着内容和社交体验业务对流量和流向进行牵引，导致业务流量难以预测。因此必须以数据中心（DC）为主要的流量生产和分发中心，且要呈现无尺度分布的特征，而我们现有的网络部署架构是与之并不匹配的。 3) 专用网络和专用设备极大增加网络经营压力 随着固移融合业务发展，固定和移动网络覆盖范围的扩大，网络规模日趋庞大。比如中国移动现有2/3/4G/固网的融合是全球最庞大、最复杂的一张网络。这就导致网络服务需要由具有不同功能的多个专业网络组合提供，各专业网络彼此之间条块化分割，能力参差不齐，业务的端到端部署和优化困难，导致新业务的创新乏力以及响应滞后，无法满足互联网+时代应对业务的动态请求。比如最近为了解决苹果手表呼叫时无法回落的问题，全国所有省份核心设备、接入设备和网关设备均在大范围改造升级，已经历时3个多月还未全部完成。 4) 互联网+业务创新加快驱动网络智能化转型 以亚马逊、阿里和腾讯为首的互联网IT巨头借助运营商管道资源，加快部署自己的云化业务服务，不仅可以及时洞察用户需求，实时响应客户需求，而且可以提供更加智能、弹性的网络服务。比如我个人前期浏览过阿里的EC云服务器资源，只过了一天就有阿里的专有客户经理来电咨询我的业务需求，拿到我的答复后1小时内就给出让我满意的解决方案。这种灵活性、高效性和专业性以我们目前的网络能力，是难以胜任的。 IT行业发展经验给了我们哪些启示？既然提到IT行业，必然离不开摩尔定律。摩尔定律其实揭示了两个发展方向：高性能和低成本。作为运营商的我们更重视前者，代表性就是要求电信级的质量；而IT企业更重视后者，通过采用适度性能要求、宽松可靠性要求的通用IT设备来进行低成本解决方案部署。 其实，IT行业在20世纪70年代以前也是一个封闭的“烟囱式”架构群，但从80年代开始就打破了这种封闭的垂直架构（标志性事件就是小型机向X86转型），转向开放的水平架构。其基本思路就是我们今天谈到的“软硬件解耦”（标志性事件就是操作系统从设备中分离），双方分工明确，各自发展，从而也就诞生了今天的云计算（云计算其本质上就是一个分布式的、借助虚拟化技术实现、与硬件无关的linux操作系统）。 而对性能追求更高的我们，通常采用电信级网络设备，因此一直保持着专业化和高成本的特点。设备仍然是软硬件一体化的垂直架构，整个生态环境较为封闭（有限的电信设备厂商控制着从硬件、软件一直到业务实现的生产过程），不仅设计复杂、成本高、升级难、而且不同厂家之间兼容性不好。比如：爱立信EPC设备、卡特IMS设备、诺西HSS设备经现网验证均存在不同程度的兼容性问题。这也就直接造成我们的网络改造难、升级慢、维护成本高、开放性差、灵活性不足，同时也限制了我们在网络和业务上的创新。 随着SDN、NFV技术的引入，我们的网络设备封闭性有望打破（注意，我这里只用了有望一词，而不是必须，至于原因是因为VNFM与VNF之间的东西向接口仍是私有接口还未标准化，因此VNFM和VNF必然是同厂家组网，这其实也是一种设备封闭性问题），软硬件解耦，产业链生态走向开放，不仅有利于降低CAPEX和OPEX，更有利于实现网络开放和弹性，促进新型网络和业务的创新。 现有网络为何无法满足新兴业务的需求？在中国甚至全球，5G从提出就是被提到国家战略的地位，这将进一步推动云计算、大数据、移动互联、物联网等与现代制造业和服务业相结合，对运营商的基础设施提出了更高的要求。为了顺应新环境下业务的发展需求，我们的基础网络改造势在必行，特别是作为网络灵魂的网络架构必须重新定义、重新设计，构建新型的泛在、敏捷、按需的智能型网络，提升公共服务水平。 从顺应业务需求的角度来看，我们主要面临以下几个方面的挑战。 1) 全面“流量经营”时代，我们面临超大流量对网络挑战 流量经营已经获得广泛认可，随着不限量业务的普及以及物联网的兴起，直到现在流量对网络的挑战才刚刚开始。最近热门的4K、8K等富媒体技术将进一步激发流量的爆发增长，而AR/VR的发展普及将会把流量增长推向新的高潮。很多行业、部门都在预测流量趋势，包括中国移动自己的大数据平台也在做相关工作。从预测数据和历史经验看，流量的规模将更加刺激我们对宽带化的发展要求，但其背后对网络容量的压力和挑战也是巨大的。如果还是像现在这种“搭积木”方式进行简单扩容来解决，那以后势必难以为继，自动化、弹性化的重构势在必行。 2) 万物互联到万物智联既是新机遇也是新挑战 万物互联到万物智联是实现“工业互联网”的重要基础。一方面带来巨大的连接规模，为我们运营商提供广泛的业务增长；另一方面也带来了更大的连接广度和深度，需要我们提供面向无线、有线全连接的接入广度，更对我们的网络提出高密度、低时延、广覆盖的个性化高要求。所以，我们的网络能不能适应万物智联，能不能承担“工业互联网”的发展重任，将是决定我们转型成败的关键，而现有网络模式势必进行智能化重构。 How？—我们如何拥抱网络重构？我们期望重构后的网络特征和架构是什么样？1) 重构后的网络应该具备以下关键特征 结构简化：网络层级、种类、类型等尽量减少，降低运营和维护的复杂性和成本，也有助于业务和应用的保障能力提升。比如：通过层级简化，业务路由优化，在全国90%的地方实现不大于30ms的业务传输网时延，这也是5G时代“三朵云”中接入云设计的初衷。 灵活高效：网络通过软件定义的方式具备弹性可伸缩的能力，实现业务的快速部署和扩/缩n 容。比如：面向客户的VIP网络可以提供分钟级的配套开通和调整能力，使得客户按照需求来随时调整网络连接，这也是切片网络概念引入的初衷。 集中控制：通过软件定义的方式实现网络的集中控制，打破目前分级、分层、分段的管理模式，实现面向全局的最优化网络管理，为客户提供全网一致性体验。比如：一点受理全国性的跨域VPN业务，并能实现即时开通。 泛在安全：一方面满足客户无论何时何地的无缝接入，另一方面通过安全防护的配套建设对承载客户信息实现安全保障。 2) 重构后网络应该具备以下参考架构 要实现重构后网络的上述特征，我们希望未来网络的架构简化为三层组网架构，如下图所示。 基础设施层：分为3类资源。第一类是可虚拟化的通用基础设施，一般由云资源池提供，之上承载各类虚拟化的网元；第二类是可以将控制和转发进行分离的专用基础设施，其控制层可以抽象出来由上层SDN控制器直接进行管理；第三类是高性能专用设施，一般指无法升级改造的传统设备，依靠现有的传统网管进行管理。 网络功能层：主要面向软件化的网络功能实现，结合虚拟资源和物理资源的管理系统/平台，实现逻辑功能和网元实体的分离，便于资源的集约化调度管控。其中，云管理平台主要负责对虚拟化资源的的管理协同，包括计算、存储和网络的统一管控；VNFM主要负责对基于NFV实现的虚拟网络功能的管控；SDN控制器实现基础设施的管控。 协同编排层：主要提供对网络功能协同和对业务能力的编排，以及对上层应用的接口和能力开放。其中，网络协同和业务编排主要负责向上对业务需求的网络进行语言翻译和能力封装，向下对网络功能层的不同系统和网元进行协同，保障网络端到端打通。IT系统和业务平台则主要服务于网络资源的标准化封装，支持各类标准化API的调用。 重构后的网络与我们现有网络相比，主要改变在以下几点： 硬件通用化：绝大多数功能网元都是通过标准化的云资源池进行承载，除了少数采用专用设施的设备。 功能软件化：网元功能与底层硬件完全解耦，主要以软件的形式存在，充分发挥弹性、灵活和敏捷的特征。 管控集中化：各网元的控制部分进行剥离，由上层云管理平台、VNFM、SDN控制器、传统网管进行管理，并由上层协同编排器进行集中协调与控制，更加体现了全程全网以及端到端的概 能力标准化：标准与开源并存。标准是为了满足服务的规模化和普适化，开源是为了实现服务的创新性和开放性。因此，对网络能力封装不仅要制定完善相关技术的标准框架，更要借助开源社区新的技术能力来完善新的标准化推动。制定与之对应的、满足生产需求，提升生产效率的标准化接口和协议（这一点也是中国移动集团从开始推出NFV试点以来一直致力要做的事情），与上层业务及应用通过API进行互动，使得网络不再仅仅是哑管道，而是能够及时感知业务需求并能随之进行灵活调整的开放式能力平台。 网络重构过程中我们需要具备哪些关键技术？SDN：软件定义网络 SDN主要将网络的控制平面与数据转发平面进行分离，采用集中控制替代现有的分布式控制，并通过开放的可编程的接口实现“软件定义”的网络架构。SDN是IT化的网络，是“软件主导一切”的趋势从IT产业向网络领域延伸的标志性技术，其核心就是网络的“软化”。 SDN的标准架构就是俗称的“三层两接口”，其实这种架构并不是SDN所独有，在现有网络中如VoLTE网络的核心网IMS也是一个标准的“三层两接口”架构，其目的就是实现转控分离。别忘了，IMS可是转发、控制和业务三层完全分离的一个架构体系。：） SDN的核心特点是将实体设备作为基础资源，抽象出NOS（网络操作系统），隐藏底层物理细节并向上层提供统一的管理和编程接口。以NOS为平台，开发的应用程序可以实现通过软件定义的网络拓扑、资源分配和处理流程及机制等。 SDN的技术重点在南/北向接口标准化，南向接口已经实现了标准化定义，统一采用openflow协议，而北向接口虽然业内共识采用REST ful协议，大家都号称支持REST ful协议，但是对接起来仍然问题百出。因此，更需要一个组织或机构来明确这层接口实现的各种细节，同时形成标准化规范，大家共同遵循开发。通过下面的简图，大家可以对SDN有个初步感性认识。 NFV：网络功能虚拟化* NFV其本质就是实现硬件资源和软件功能的解耦，其最终目标是通过标准X86服务器、存储和交换设备来取代现有网络中的私有专用网元。主要包括：NFVI、VNF、MANO和OSS/BSS四个逻辑层面。关键特征包括上层的业务云化、底层硬件标准化、分层运营和加快业务的上线与创新。 其中，NFVI就是典型云计算平台，主要实现对底层物理设备资源进行计算、存储、网络虚拟化呈现。VNF是软件实现的虚拟网元功能，其利用NFVI创建的虚拟化资源，在其上通过软件编程的方式实现各类网元的功能。MANO是业务编排层，主要用于整体的编排和控制管理，将网络服务从上而下进行业务层到资源层逐步分解和调度（主要通过NFVO、VNFM和VIM三个子模块互相配合、调度NFVI层资源实现）。OSS/BSS与传统网络功能类似，主要实现业务的发放，计费、网络管理和营帐等功能。（其实现在随着K8s技术的兴起，这一层也完全可通过云调度的方式呈现。） 通过对比SDN技术我们可以发现，NFV技术其本质也是一个“三层两接口”的架构模型，可以说它是SDN技术一个衍生品（个人观点：））。但是，它与SDN技术主要区别还是技术呈现侧重点的区别，NFV更偏重网络功能的软件化，在控制转发层面也可以不依赖于SDN技术通过专用物理设备也能实现，这也是现阶段NFV网络建设的基本模式。而SDN技术更偏重控制和转发层面，不看重网元功能是否软件化实现。如一台具备openflow协议物理交换机也可在SDN网络中承担相应的转发功能，同样它也能抽象出对应网络抽象层由SDN控制器来实现调度。比如山东省内济南和青岛两地市的DC大二层互通就是通过大量这样的物理交换机来实现转发控制。个人判断随着SDN技术的逐步演进，网络重构过程初期这类大量物理交换机会逐渐被openflow软件交换机替换。国际惯例，同样下面放张图让大家对NFV的架构有个初步感性认识。 云计算：OpenStack开源云—王者归来* 在现阶段云计算技术在网络重构过程中主要作用是各种硬件资源的虚拟化呈现。对于未来网络的维护人员或者DevOps人员掌握和熟悉云计算是必不可少的一项基础技能。NFV技术中NFVI层使用的OpenStack就是一个典型的云计算架构，同样SDN中控制器OpenDayLight也是一种云计算的架构，现在OpenStack社区在Pike版本和Queens版本已经将这两种云计算架构进行融合，也就是说在OpenStack服务中中包含了OpenDayLight功能。（相比于OpenDayLight，OpenStack功能更强大，最重要是完全符合生产条件，华为云化设备底层也是采用OpenStack的H版本搭建，中兴同样）。而在我自己的实验环境也分别通过手工搭建和容器部署的方式实现这两种架构的融合。 OpenStack的原理主要通过各种服务的交互来实现业务的调用和流程分发。其核心思想就是对虚拟机VM进行各类操作控制来实现物理设备的功能。其基本服务包括Keystone（认证服务）、Nova（计算服务）、Glance（镜像服务）、Neutron（网络服务）、Cinder（虚拟块存储服务）、Swift（虚拟对象存储服务）、Heat（云编排服务）和Horizon（人机交互Web服务），对于我们运营商可能还会用到Ceilometer（流量监控、计费服务）。除了这些基础服务外，还有很多负载均衡、密钥共享存储以及开源NFV等高级服务，这里不再一一列举。OpenStack的各类服务之间通过一种叫消息队列（Queus）公共服务来进行异步通信，因此各类服务不仅可以部署同一台物理服务器上，也可采用高可用集群的方式在多个物理服务器上分布式部署。 我在外面学习时，很多外省的学员甚至华为的老师都在询问云计算怎么学习，我的答案是学习云计算的前提条件需要具备一定操作系统原理知识、虚拟化技术知识和linux操作系统基础知识。最好的学习方式就是自己搭建一套实验环境一遍学习理论、流程，一边动手实践。由于个人时间实在有限，特意在我个人微信公众号上写了一篇实验环境的搭建流程供各位初学者参考学习。下面还是放上一张OpenStack的原理图，让大家对云计算有个初步感性认识。 我们如何紧跟网络重构的步伐进行运营转型升级？网络重构对我们现有的网络架构将进行颠覆性的变革，将促进我们的网络布局由传统的电信机房（CO）向数据中心架构（DC）转变，网元部署形态由软硬一体化的专用设备向基础设施的通用化和虚拟化以及网元功能的软件化方向转变，实现对网络的集中、跨层、跨域控制，这与我们现有网络采用分专业、分层、分域的规划、建设和运营管理模式截然相反。未来网络的这种变化将对我们现有业务模式、运营模式、管理模式、人才培养模式都提出了巨大的挑战，将迫使我们改变现有观念，积极拥抱变革。 1) 业务模式的转型升级 业务需求是技术创新的原动力，同时技术创新也将激发新业务的增长。按照集团“大连接”战略，我们基础设施的建设布局、基础技术的演进，网络系统的革新都是以支撑数字化新服务的智能应用为核心，秉承“连接无限可能”的目标来逐步实施。 未来，我们面向的对象包括人、物、企业和信息，将提供他们自身及之间的沟通、连接服务。个人理解，可能主要包括十大类数字化业务模式： 人-人：未来通信业务。—通过VoLTE/VoNR技术，以通信服务提供商的角色将提供随时、随地、即时、高效、高感知、无障碍的人与人之间的通信服务。 人-信息：内容信息业务。—通过内容运营平台，以内容提供商的角色，将提供引人入胜、引领潮流、多方共赢、私人定制的数字娱乐服务。 企业-企业：企业信息化业务。—通过企业网布局，以移动ICT生态服务商的角色，将提供安全可靠、高效、低成本、可灵活定制的企业级移动ICT服务。（这一点华为已经走在了我们前面，未来我们和华为将是竞争对手关系） 企业-人：行业应用业务。—通过跨领域合作，主要针对教育、医疗、金融行业以行业产品服务提供商的角色，提供方便、实惠、开放的跨界民生服务。 企业-信息：能力开放业务。—通过大数据平台运营和信息处理的模式，以大数据信息处理专家的角色，提供丰富、精选、定制化、实时的数据能力开放服务。（这一点阿里等互联网IT厂商已经走在了我们前面） 物-物：智能物联网业务。—通过物联网基础服务平台，以物理网运营专家的角色，提供无处不在、无所不能、自动化、低成本的万物互联服务。 物-人：智能家居业务。—通过智能CPE、机顶盒、行车记录仪等智能终端产品，以解决方案提供商的角色，将提供亲切、便捷、个性化、智能化的人物交互服务。 物-信息：社交化物联网业务。—通过社交化物联网平台，以社交化服务提供商的角色，提供主动关注、动态建圈、自动沟通的社交化物联网服务。 物-企业：产业信息化业务。—通过与传统产业融合，助力产业升级，实现产业再造，以服务和解决方案提供商的角色，将提供安全、低成本、大规模、可灵活定制的产业信息化服务。 信息-信息：数据资源提供业务。—通过大数据、人工智能平台，以数据服务商的角色提供无所不知、实施精确的知识服务及数据资源运营服务。 2) 运营模式的转型升级 未来网络的架构需要能够灵活地适配各类应用，主动对网络资源进行弹性伸缩。对我们目前的网络运营以及市场、网络和IT的协同能力提出了更高的要求，需要我们构建快速响应、高效率、灵活服务的运营能力。 虽然，我们目前正在逐步强化大数据应用，聚焦产品运营、渠道销售、客户服务、网络的开放合作等领域，但是还远远不够。对于未来网络运营，更需要我们实现集约化、智能化。需要我们以流程优化/变革为基础，以智能IT系统为载体，构建面向客户和业务一体化智慧运营服务。主要思路如下： 将数据中心作为我们智慧运营的核心资源，通过数据汇聚、数据清洗、数据挖掘来实时驱动我们的营销、服务、运营等生产和管理流程。 加强服务产品的统筹和规划，重点布局战略级服务和市场化服务，同时要掌控服务产品的核心能力，推进DevOps模式，提升专业化自主开发和迭代优化能力。 突出市场需求化导向，加强大数据应用，强化渠道O2O协同和跨界合作，提升全业务布局和服务能力。 充分利用大数据手段，加强用户需求和行为洞察，做精服务品质，持续提升价值经营能力。 持续推进网络简化和现有网络的智能化升级，积极布局SDN/NFV/云计算能网络能力，结合大数据、人工智能等技术手段，变革创新生产流程和机制，加强对新兴业务的支撑，实现目前的网络运维向端到端集约、云网协同的智能网络运营转变。 3) 管理模式的转型升级 网络重构对我们目前的组织、生产、运营、人力等管理体制将产生重大影响，主要体现在组织架构调整、规划模式转变、运营组织整合、创新体系建设、团队人才培养等方面。 未来网络架构将采用水平分层、纵向解耦的技术路线。基于SDN/NFV的网络架构将打破专业界限，管控和调度也将突破传统省-市-县三级的限制。由于网络以DC为核心的组网新格局，现有分域分层的组织架构已成为端到端自动化运营的最大障碍，必须建立纵向的面向云化网络的运维管理团队新模式，按照我们预期重构后网络参考模型，从上而下主要包括：编排器维护管理团队、虚拟网元维护管理团队、云计算维护管理团队、SDN控制器维护管理团队、虚拟化资源维护管理团队和基础设施维护管理团队。必须提前完成各团队的带头人，积极做好迎接转型的准备。这里可以拿我们网管中心内部做一简单举例。（只是举例，不包含任何其他意思） 按照目前网管中心的科室划分主要分为：监控室、质量室、安全室、传输室、互联网室和核心业务室。那么，我们对比云化网络纵向组织架构做一简单规划。从下而上： 基础设施维护管理团队。该团队的主要职责是维护各IDC机房的物理服务器、交换机等设备，需要一定IT维护从业经验的人才能胜任，最主要的是现场维护模式。我们的自有人员可以通过学习培训来上岗，但从IDC机房的分布和维护成本来看，我的建议采用外包代维的方式来解决。 虚拟化资源维护管理团队。该团队的主要职责是维护各类虚拟化资源池，主要涉及资源和虚机维护管理，需要具备虚拟化技术基础的人才能胜任，属于远程维护模式。而我们目前监控NOC大厅各类终端其实就是一种虚拟化部署模式，所以我们的监控室完全可以胜任。 SDN控制器维护管理团队。该团队主要职责主要完成各类软件定义的网络拓扑、路由寻址、资源分布调用等进行维护管理，不仅需要SDN控制器的知识，更需要大量数通的基础知识人员才能胜任。因此我们的传输室通过培训SDN方面技能就可以完全胜任。 云计算维护管理团队。主要负责云计算中各类服务维护和管理，需要具备linux系统知识、操作系统知识、虚拟化技术知识和云计算知识以及相关维护经验的人员才能胜任，属于纯IT范畴。而我们的安全室目前工作也基本属于IT范畴，因此通过培训和学习应该可以胜任。 虚拟网元维护管理团队。主要负责各类虚拟网元的维护管理，现阶段主要包括vIMS、vEPC、vBRAS、虚拟化短信中心、虚拟化智能网和虚拟化彩铃平台等。这部分的维护人员主要涉及CT类的知识和技能储备，在此基础上再进行相关IT类培训（开发语言类和协议类）。因此目前互联网室与核心业务室相关专业的维护人员通过培训完全可以胜任。 编排器维护管理团队。主要负责向上各类业务设计、流程规划和API调用，向下进行各类网元功能封装和策略下发。因此该部分实际上属于网业协同层，既要懂业务，还要懂网络，同时具备协议解读、功能封装、流程制定、策略制定等技能的人员才能胜任，属于端到端专业的范畴。因此目前质量室的人员通过培训上岗再加上外聘其他业务部门人员应该可以满足要求。 4) 人才培养模式转型升级 在现有的网络运维模式下，厂家和我们之间是一种高度依赖的关系，更确切的说我们更依赖厂家，主要表现在网络维护、保障、演练和优化等方面的实操环节。 CT行业除了设备厂家自有人员外，运营商内部人员更看重人才在信令分析、流程分析、数据分析和优化措施制定方面的能力，而实际措施部署和网络调整完全是厂家人员执行。这一点也正是我入职中国移动以来最疑惑的一点。当然，这其中也存在厂家对运营商有所保留的原因，但更重要是长期以来厂家这种保姆式协维让我们对其有了更大的依赖（全国一样）。 而IT行业恰恰相反，更看重实操能力，IT的运维效率除了依靠自动化手段外，更多的依赖个人维护经验来提升。比如：阿里前段时间一个高级运维人员离职，曾开出双倍年薪也没挖到同级别的人才。 因此，随着网络重构逐步进行，除了集团要求的三级解耦要求外（这一点其实也是集团层面和厂家之间的博弈），我们的自有人员转型或者人才培养必须提升DevOps能力，构建自有研发能力体系。一方面，是因为未来ICT的网络需要集成和运维大量不同来源的新老软件、硬件、网管，我们的自有人员必须要能够在代码级层次上深度介入开发、测试、集成、维护直至业务提供的全过程。另一方面，我们现有的IT和CT是一种割裂分离的状态，随着IT和CT的深度融合，需要既懂网络又懂软件、适应开发运营一体化的人才，包括软件开发人才、IT系统人才和网络技术专家。 结束语以上就是我个人对未来网络的一点儿浅见，有不妥或错误之处还请各位领导和专家批评指正。 目前，传统网络已经越来越力不从心，这一点通过现在互联网企业不断推出各类业务模式已经让我深有感触，重新构建一个基于SDN/NFV的云化网络架构成为必然选择。目前全球的运营商均已推出了自己的网络重构计划，可以预见未来基于SDN/NFV的云化网络架构将愈发充满活力。 既然网络重构势必进行，重构之路已经开启，一个更加开放、灵活的新网络时代即将到来，我们与其消极抵触，不如以更加积极的姿态去迎接挑战，拥抱未来。 未来？已来！ 你，准备好了吗？！]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-27-Linux系统命令-第五篇《文件备份与压缩命令》]]></title>
    <url>%2F2019%2F04%2F28%2F2019-04-27-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%BA%94%E7%AF%87%E3%80%8A%E6%96%87%E4%BB%B6%E5%A4%87%E4%BB%BD%E4%B8%8E%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[tar：打包备份在Linux系统里，tar是将多个文件打包在一起，并且可以实现解压打包的文件的命令。是系统管理员最常用的命令之一，tar命令不但可以实现对多个文件进行打包，还可以对多个文件打包后进行压缩。 打包是指将一大堆文件或目录变成一个总的文件，压缩则是将一个大的文件通过一些压缩算法变成一个小文件。tar命令选项的使用有点特殊，对于CentOS、Linux来说，“tar-z”和“tar z”的效果相同，加或不加“-”这个符号都是可以的。而在FreeBSD系统下，必须加“-”符号。 语法格式：tar [option] [file] 重要参数选项 【使用示例】 1）备份站点目录html 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 创建实验环境# 创建测试目录/var/www/html/kkutysllb/test[root@C7-Server01 kkutysllb]# mkdir -p /var/www/html/kkutysllb/test# 在测试目录下创建测试文件[root@C7-Server01 kkutysllb]# touch /var/www/html/kkutysllb/test/html&#123;01..10&#125;# 查看刚创建的文件信息[root@C7-Server01 kkutysllb]# ls -l /var/www/html/kkutysllb/test/total 0-rw-r--r-- 1 root root 0 Apr 27 21:45 html01-rw-r--r-- 1 root root 0 Apr 27 21:45 html02-rw-r--r-- 1 root root 0 Apr 27 21:45 html03-rw-r--r-- 1 root root 0 Apr 27 21:45 html04-rw-r--r-- 1 root root 0 Apr 27 21:45 html05-rw-r--r-- 1 root root 0 Apr 27 21:45 html06-rw-r--r-- 1 root root 0 Apr 27 21:45 html07-rw-r--r-- 1 root root 0 Apr 27 21:45 html08-rw-r--r-- 1 root root 0 Apr 27 21:45 html09-rw-r--r-- 1 root root 0 Apr 27 21:45 html10# 进入到目标目录的上一级目录，并对目标目录打包[root@C7-Server01 kkutysllb]# cd /var/www/html/kkutysllb/[root@C7-Server01 kkutysllb]# tar zcvf www.kkutysllb.gz ./test./test/./test/html01./test/html02./test/html03./test/html04./test/html05./test/html06./test/html07./test/html08./test/html09./test/html10# 查看打包文件信息[root@C7-Server01 kkutysllb]# ls -lhi *.gz101098015 -rw-r--r-- 1 root root 233 Apr 27 21:49 www.kkutysllb.gz 2）查看压缩包内的内容 1234567891011121314# 通过t选项可以不解压就能查看压缩包内的内容，加选项v可以显示文件的属性[root@C7-Server01 kkutysllb]# tar ztvf www.kkutysllb.gz drwxr-xr-x root/root 0 2019-04-27 21:45 ./test/-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html01-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html02-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html03-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html04-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html05-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html06-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html07-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html08-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html09-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html10 3）对压缩包进行解压 1234567891011121314151617181920212223# 将www.kkutysllb.gz压缩包的内容解压到/home/kkutysllb/html目录中# 如果目标目录不存在，不会自动创建，需要手动先建立目标目录[root@C7-Server01 kkutysllb]# mkdir -p /home/kkutysllb/html[root@C7-Server01 kkutysllb]# tar zxvf www.kkutysllb.gz -C /home/kkutysllb/html/./test/./test/html01./test/html02./test/html03./test/html04./test/html05./test/html06./test/html07./test/html08./test/html09./test/html10# 查看目标目录的信息[root@C7-Server01 kkutysllb]# ll -h /home/kkutysllb/html/total 0drwxr-xr-x 2 root root 146 Apr 27 21:45 test 如果不想看到太多的输出，则可以去掉v选项，功能不受影响。同时z选项也可以省略，只要涉及解压的操作，tar命令都能自动识别压缩包的压缩类型，但是压缩时必须要加上z选项。 4）排除打包 12345678910111213# 在我们刚才打包的文件中包含了目录test，如果不想打包test目录，可以使用exclude选项排除[root@C7-Server01 kkutysllb]# cd ..[root@C7-Server01 html]# tar zcvf www.kkutysllb.new.gz ./kkutysllb/ --exclude=kkutysllb/test./kkutysllb/./kkutysllb/www.kkutysllb.gz# 查看刚才打包的文件内容[root@C7-Server01 kkutysllb]# cd ..[root@C7-Server01 html]# tar ztvf www.kkutysllb.new.gz drwxr-xr-x root/root 0 2019-04-27 22:21 ./kkutysllb/-rw-r--r-- root/root 233 2019-04-27 21:49 ./kkutysllb/www.kkutysllb.gz 如果要排除多个目录，可以在后面接多个–exclude选项。 使用exclude选项排除某个子目录时，需要注意以下几点，否则不会排除成功： 若需要打包的目录为相对路径，则–exclude后只能接相对路径。 若需要打包的目录为绝对路径，则–exclude后既能接绝对路径也能接相对路径。 为方便，统一起见，建议–exclude的后接路径和打包路径应保持形式一致，要么都是相对路径，要么都是绝对路径。 5）排除多个文件打包参数-X 123456789101112131415161718192021222324252627282930313233343536# 将要排除的文件名写入一个文件[root@C7-Server01 kkutysllb]# cat &gt;&gt; filtername &lt;&lt; EOF&gt; html03&gt; html06&gt; html02&gt; html08&gt; html10&gt; EOF&gt; [root@C7-Server01 kkutysllb]# cat -n filtername &gt; 1 html03&gt; 2 html06&gt; 3 html02&gt; 4 html08&gt; 5 html10# 使用参数X排除上述文件进行打包[root@C7-Server01 kkutysllb]# tar zcvfX afterfilter.gz ./filtername ./test/./test/./test/html01./test/html04./test/html05./test/html07./test/html09# 查看新打包的文件内容[root@C7-Server01 kkutysllb]# tar ztvf afterfilter.gz drwxr-xr-x root/root 0 2019-04-27 21:45 ./test/-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html01-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html04-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html05-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html07-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html09 6）打包链接文件 1234567891011121314151617181920# 首先使用常规选项zc完成打包[root@C7-Server01 kkutysllb]# cd /etc/[root@C7-Server01 etc]# tar zcf local.tar.gz ./rc.local # 查看刚打包的文件内容，发现这是一个符号链接（软链接文件）[root@C7-Server01 etc]# tar ztvf local.tar.gz lrwxrwxrwx root/root 0 2019-04-20 16:07 ./rc.local -&gt; rc.d/rc.local# 这里是个坑，如果不加特殊参数，那么打包之后的文件是个软链接文件，不是rc.local的实体内容# 采用-h参数打包链接文件[root@C7-Server01 etc]# tar zcfh local.new.tar.gz ./rc.local # 再次查看打包的文件内容，发现内容是软链接文件指向的真实文件[root@C7-Server01 etc]# tar ztvf local.new.tar.gz -rw-r--r-- root/root 473 2019-02-20 01:35 ./rc.local 用tar的通用选项zcf打包文件时，如果这个文件是链接文件如/etc/rc.local，那么tar只会对链接文件本身打包，而不是对链接文件指向的真实文件打包，因此需要额外使用-h选项将软链接文件对应的实体文件打包。 对文件打包时，除了上述命令选项参数要掌握外，还需要遵循一些好的操作习惯，避免实际运维中不可预知的错误： 1）在打包一个目录之前，先进入到这个目录的上一级目录，然后执行打包命令，这是大部分情况下打包文件的规范操作流程。 2）少数情况下打包需要完整的目录结构时，也可以使用绝对路径打包，但是需要注意的是解压tar包时压缩包内的文件是否会覆盖本地文件。 小练习：请对/etc/目录下所有普通文件进行打包。 gzip：压缩或解压文件gzip命令用于将一个大的文件通过压缩算法（Lempel-Ziv coding（LZ77））变成一个小的文件。gzip命令不能直接压缩目录，因此目录需要先用tar打包成一个文件，然后tar再调用gzip进行压缩。 语法格式：gzip [option] [file] 重要选项参数 【使用示例】 1）把目录下每个文件都压缩单独的.gz文件 123456789101112131415161718192021222324252627282930313233# 查看test目录下所有文件的信息[root@C7-Server01 kkutysllb]# ls -l test/total 0-rw-r--r-- 1 root root 0 Apr 27 21:45 html01-rw-r--r-- 1 root root 0 Apr 27 21:45 html02-rw-r--r-- 1 root root 0 Apr 27 21:45 html03-rw-r--r-- 1 root root 0 Apr 27 21:45 html04-rw-r--r-- 1 root root 0 Apr 27 21:45 html05-rw-r--r-- 1 root root 0 Apr 27 21:45 html06-rw-r--r-- 1 root root 0 Apr 27 21:45 html07-rw-r--r-- 1 root root 0 Apr 27 21:45 html08-rw-r--r-- 1 root root 0 Apr 27 21:45 html09-rw-r--r-- 1 root root 0 Apr 27 21:45 html10# 使用gzip对每个单独压缩[root@C7-Server01 kkutysllb]# gzip test/html*# 再次查看test目录下的所有文件信息[root@C7-Server01 kkutysllb]# ls -l test/total 40-rw-r--r-- 1 root root 27 Apr 27 21:45 html01.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html02.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html03.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html04.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html05.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html06.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html07.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html08.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html09.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html10.gz gzip命令的缺点是压缩后源文件不见了，它的特性是压缩、解压都会自动删除源文件。 2）不解压显示上一个例子中每个压缩文件的信息 1234567891011121314# 使用-l选项显示压缩文件信息[root@C7-Server01 kkutysllb]# gzip -l test/*.gzcompressed uncompressed ratio uncompressed_name27 0 0.0% test/html0127 0 0.0% test/html0227 0 0.0% test/html0327 0 0.0% test/html0427 0 0.0% test/html0527 0 0.0% test/html0627 0 0.0% test/html0727 0 0.0% test/html0827 0 0.0% test/html0927 0 0.0% test/html10 因为源文件都是空文件，所以压缩率都为0.0％。 3）解压文件，并显示解压过程 12345678910111213141516171819202122232425262728# 使用-d选项解压，-v显示解压过程[root@C7-Server01 kkutysllb]# gzip -dv test/*.gztest/html01.gz: 0.0% -- replaced with test/html01test/html02.gz: 0.0% -- replaced with test/html02test/html03.gz: 0.0% -- replaced with test/html03test/html04.gz: 0.0% -- replaced with test/html04test/html05.gz: 0.0% -- replaced with test/html05test/html06.gz: 0.0% -- replaced with test/html06test/html07.gz: 0.0% -- replaced with test/html07test/html08.gz: 0.0% -- replaced with test/html08test/html09.gz: 0.0% -- replaced with test/html09test/html10.gz: 0.0% -- replaced with test/html10# 查看test目录下文件信息，发现解压后gzip也会将原来压缩包自动删除[root@C7-Server01 kkutysllb]# ls -l test/total 0-rw-r--r-- 1 root root 0 Apr 27 21:45 html01-rw-r--r-- 1 root root 0 Apr 27 21:45 html02-rw-r--r-- 1 root root 0 Apr 27 21:45 html03-rw-r--r-- 1 root root 0 Apr 27 21:45 html04-rw-r--r-- 1 root root 0 Apr 27 21:45 html05-rw-r--r-- 1 root root 0 Apr 27 21:45 html06-rw-r--r-- 1 root root 0 Apr 27 21:45 html07-rw-r--r-- 1 root root 0 Apr 27 21:45 html08-rw-r--r-- 1 root root 0 Apr 27 21:45 html09-rw-r--r-- 1 root root 0 Apr 27 21:45 html10 4）压缩解压保留源文件 1234567891011121314151617181920212223# 使用-c选项与输出重定向结合完成操作[root@C7-Server01 kkutysllb]# gzip -c test/html* &gt; test.gz# 查看当前目录下生成的test.gz压缩文件信息[root@C7-Server01 kkutysllb]# gzip -lv test.gzmethod crc date time compressed uncompressed ratio uncompressed_namedefla 00000000 Apr 27 23:07 270 0 0.0% test# 查看源文件是否还存在[root@C7-Server01 kkutysllb]# ls -l test/html*-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html01-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html02-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html03-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html04-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html05-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html06-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html07-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html08-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html09-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html10 解压操作类似，请大家自行练习。 虽然上面使用重定向符号解决了保留源文件的问题，但是使用起来还是不太方便。其实，gzip套件包含了许多可以“在原地”处理压缩文件的实用程序。zcat、zgrep、zless、zdiff等实用程序的作用分别与cat、grep、less和diff相同，但是它们操作的是压缩的文件。比如： 12345678910111213141516171819202122# zcat命令可以直接读取压缩文件内容[root@C7-Server01 etc]# zcat local.new.tar.gz | head./rc.local0000644000000000000000000000073113433037107011333 0ustar rootroot#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure 请大家思考下，zcat命令是否可以对gz文件完成解压缩？如果可以，该怎么操作？ zip：打包和压缩文件zip压缩格式是Windows与Linux等多平台通用的压缩格式。和gzip命令相比，zip命令压缩文件不仅不会删除源文件，而且还可以压缩目录。 语法格式：zip [option] [file] 重要选项参数 【使用示例】 1）压缩文件 123456789# 因为测试文件都是空文件，所以压缩率为0%[root@C7-Server01 kkutysllb]# zip test.zip ./test adding: test/ (stored 0%)# 查看新生成的zip压缩文件信息[root@C7-Server01 kkutysllb]# ll -h *.zip-rw-r--r-- 1 root root 160 Apr 27 23:21 test.zip 2）压缩目录 12345678910111213141516171819202122232425262728293031323334# 进入根目录/下，压缩tmp目录[root@C7-Server01 /]# zip tmp.zip ./tmp/ adding: tmp/ (stored 0%)# 上面操作只是压缩tmp这个目录，目录下的文件并没有压缩# 要想同时压缩目录下的文件，可以使用-r选项递归压缩[root@C7-Server01 /]# zip -r tmp.zip ./tmp/updating: tmp/ (stored 0%) adding: tmp/.XIM-unix/ (stored 0%) adding: tmp/.font-unix/ (stored 0%) adding: tmp/.X11-unix/ (stored 0%) adding: tmp/.Test-unix/ (stored 0%) adding: tmp/.ICE-unix/ (stored 0%) adding: tmp/vmware-root/ (stored 0%) adding: tmp/vmware-root_16212-827959067/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/ (stored 0%) adding: tmp/vmware-root_9443-3886520225/ (stored 0%) adding: tmp/vmware-root_9458-2857896559/ (stored 0%) adding: tmp/vmware-root_9420-2857896526/ (stored 0%) adding: tmp/vmware-root_9400-3101375875/ (stored 0%) adding: tmp/vmware-root_9456-2866351085/ (stored 0%) adding: tmp/vmware-root_9609-4121731445/ (stored 0%) adding: tmp/vmware-root_9573-4146439782/ (stored 0%) adding: tmp/vmware-root_9660-3101179142/ (stored 0%) adding: tmp/vmware-root_9604-3101310240/ (stored 0%) adding: tmp/vmware-root_9409-3878589979/ (stored 0%) adding: tmp/vmware-root_9328-3134798637/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/ (stored 0%) adding: tmp/vmware-root_9392-3134798733/ (stored 0%) 3）排除压缩 1234567891011121314151617181920212223242526272829303132# 排除tmp目录下vmware-root/目录后，进行压缩[root@C7-Server01 /]# zip -r tmp_new.zip /tmp/ -x /tmp/vmware-root/ adding: tmp/ (stored 0%) adding: tmp/.XIM-unix/ (stored 0%) adding: tmp/.font-unix/ (stored 0%) adding: tmp/.X11-unix/ (stored 0%) adding: tmp/.Test-unix/ (stored 0%) adding: tmp/.ICE-unix/ (stored 0%) adding: tmp/vmware-root_16212-827959067/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/ (stored 0%) adding: tmp/vmware-root_9443-3886520225/ (stored 0%) adding: tmp/vmware-root_9458-2857896559/ (stored 0%) adding: tmp/vmware-root_9420-2857896526/ (stored 0%) adding: tmp/vmware-root_9400-3101375875/ (stored 0%) adding: tmp/vmware-root_9456-2866351085/ (stored 0%) adding: tmp/vmware-root_9609-4121731445/ (stored 0%) adding: tmp/vmware-root_9573-4146439782/ (stored 0%) adding: tmp/vmware-root_9660-3101179142/ (stored 0%) adding: tmp/vmware-root_9604-3101310240/ (stored 0%) adding: tmp/vmware-root_9409-3878589979/ (stored 0%) adding: tmp/vmware-root_9328-3134798637/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/ (stored 0%) adding: tmp/vmware-root_9392-3134798733/ (stored 0%)# 对比tmp.zip和tmp_new.zip文件内容差别# 压缩文件时二进制文件，所以只能使用vimdiff命令进行对比[root@C7-Server01 /]# vimdiff tmp.zip zip_new.zip unzip：解压zip文件unzip命令可以解压zip命令或其他压缩软件压缩的zip格式的文件。 语法格式：unzip [option] [file] 重要选项参数 【使用示例】 1）不解压直接查看压缩文件内容 123456789101112131415161718192021222324252627282930313233343536# 使用-l选项[root@C7-Server01 /]# unzip -l tmp.zip Archive: tmp.zipLength Date Time Name------0 04-27-2019 22:29 tmp/0 04-07-2019 20:34 tmp/.XIM-unix/0 04-07-2019 20:34 tmp/.font-unix/0 04-07-2019 20:34 tmp/.X11-unix/0 04-07-2019 20:34 tmp/.Test-unix/0 04-07-2019 20:34 tmp/.ICE-unix/0 04-20-2019 11:06 tmp/vmware-root/0 04-20-2019 16:08 tmp/vmware-root_16212-827959067/0 04-21-2019 16:27 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/0 04-21-2019 16:27 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/0 04-21-2019 16:27 tmp/vmware-root_9443-3886520225/0 04-22-2019 22:23 tmp/vmware-root_9458-2857896559/0 04-22-2019 22:52 tmp/vmware-root_9420-2857896526/0 04-22-2019 22:53 tmp/vmware-root_9400-3101375875/0 04-23-2019 10:29 tmp/vmware-root_9456-2866351085/0 04-23-2019 12:41 tmp/vmware-root_9609-4121731445/0 04-23-2019 12:50 tmp/vmware-root_9573-4146439782/0 04-23-2019 18:24 tmp/vmware-root_9660-3101179142/0 04-26-2019 17:24 tmp/vmware-root_9604-3101310240/0 04-27-2019 12:00 tmp/vmware-root_9409-3878589979/0 04-27-2019 21:42 tmp/vmware-root_9328-3134798637/0 04-27-2019 21:43 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/0 04-27-2019 21:43 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/0 04-27-2019 21:43 tmp/vmware-root_9392-3134798733/------0 24 files 2）常规解压文件的例子 123456789101112131415161718192021222324252627282930313233343536# 使用-v选项显示解压过程[root@C7-Server01 /]# unzip -v tmp.zipArchive: tmp.zipLength Method Size Cmpr Date Time CRC-32 Name------0 Stored 0 0% 04-27-2019 22:29 00000000 tmp/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.XIM-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.font-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.X11-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.Test-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.ICE-unix/0 Stored 0 0% 04-20-2019 11:06 00000000 tmp/vmware-root/0 Stored 0 0% 04-20-2019 16:08 00000000 tmp/vmware-root_16212-827959067/0 Stored 0 0% 04-21-2019 16:27 00000000 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/0 Stored 0 0% 04-21-2019 16:27 00000000 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/0 Stored 0 0% 04-21-2019 16:27 00000000 tmp/vmware-root_9443-3886520225/0 Stored 0 0% 04-22-2019 22:23 00000000 tmp/vmware-root_9458-2857896559/0 Stored 0 0% 04-22-2019 22:52 00000000 tmp/vmware-root_9420-2857896526/0 Stored 0 0% 04-22-2019 22:53 00000000 tmp/vmware-root_9400-3101375875/0 Stored 0 0% 04-23-2019 10:29 00000000 tmp/vmware-root_9456-2866351085/0 Stored 0 0% 04-23-2019 12:41 00000000 tmp/vmware-root_9609-4121731445/0 Stored 0 0% 04-23-2019 12:50 00000000 tmp/vmware-root_9573-4146439782/0 Stored 0 0% 04-23-2019 18:24 00000000 tmp/vmware-root_9660-3101179142/0 Stored 0 0% 04-26-2019 17:24 00000000 tmp/vmware-root_9604-3101310240/0 Stored 0 0% 04-27-2019 12:00 00000000 tmp/vmware-root_9409-3878589979/0 Stored 0 0% 04-27-2019 21:42 00000000 tmp/vmware-root_9328-3134798637/0 Stored 0 0% 04-27-2019 21:43 00000000 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/0 Stored 0 0% 04-27-2019 21:43 00000000 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/0 Stored 0 0% 04-27-2019 21:43 00000000 tmp/vmware-root_9392-3134798733/------0 0 0% 24 files 3） 指定解压目录解压文件 123456789101112131415161718192021222324252627# 将tmp_new.zip文件加压到/home/kkutysllb/目录下[root@C7-Server01 /]# unzip -d /home/kkutysllb/ tmp_new.zip Archive: tmp_new.zipcreating: /home/kkutysllb/tmp/creating: /home/kkutysllb/tmp/.XIM-unix/creating: /home/kkutysllb/tmp/.font-unix/creating: /home/kkutysllb/tmp/.X11-unix/creating: /home/kkutysllb/tmp/.Test-unix/creating: /home/kkutysllb/tmp/.ICE-unix/creating: /home/kkutysllb/tmp/vmware-root_16212-827959067/creating: /home/kkutysllb/tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/creating: /home/kkutysllb/tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/creating: /home/kkutysllb/tmp/vmware-root_9443-3886520225/creating: /home/kkutysllb/tmp/vmware-root_9458-2857896559/creating: /home/kkutysllb/tmp/vmware-root_9420-2857896526/creating: /home/kkutysllb/tmp/vmware-root_9400-3101375875/creating: /home/kkutysllb/tmp/vmware-root_9456-2866351085/creating: /home/kkutysllb/tmp/vmware-root_9609-4121731445/creating: /home/kkutysllb/tmp/vmware-root_9573-4146439782/creating: /home/kkutysllb/tmp/vmware-root_9660-3101179142/creating: /home/kkutysllb/tmp/vmware-root_9604-3101310240/creating: /home/kkutysllb/tmp/vmware-root_9409-3878589979/creating: /home/kkutysllb/tmp/vmware-root_9328-3134798637/creating: /home/kkutysllb/tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/creating: /home/kkutysllb/tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/creating: /home/kkutysllb/tmp/vmware-root_9392-3134798733/ scp：远程文件复制scp命令用于在不同的主机之间复制文件，它采用SSH协议来保证复制的安全性。scp命令每次都是全量完整复制，因此效率不高，适合第一次复制时使用，增量复制建议使用rsync命令替代。 语法格式：scp [option] [[user@]host1:]file1 [[user@]host2:]file2 重要参数选项 【使用示例】 1）将文件或目录从服务器01复制推懂到服务器02 123456789101112131415# 将C7 Server01（192.168.101.81）根目录下tmp_new.zip文件推动C7 Server02（192.168.101.81）的/tmp目录下[root@C7-Server01 /]# scp /tmp_new.zip 192.168.101.82:/tmpThe authenticity of host '192.168.101.82 (192.168.101.82)' can't be established.ECDSA key fingerprint is SHA256:xX2VNfiU72qXldv5e8O0B4jJ6+AR4UdVH1AsOvRKeOw.ECDSA key fingerprint is MD5:37:bc:16:82:b1:55:0f:55:2e:ae:08:9c:55:db:4e:f5.Are you sure you want to continue connecting (yes/no)? yes # 这里输入连接确认信息Warning: Permanently added '192.168.101.82' (ECDSA) to the list of known hosts.root@192.168.101.82's password: # 这里输入C7 Server02服务器登录密码tmp_new.zip 100% 4574 4.2MB/s 00:00 # 查看C7 Server02的/tmp目录下文件信息[root@C7-Server02 ~]# ll -h /tmp/tmp*.zip-rw-r--r-- 1 root root 4.5K Apr 28 00:09 /tmp/tmp_new.zip 2）保持文件属性进行远程文件/目录推送 123456789101112131415# 查看当前服务器/home/kkutysllb/test.sh文件属性信息[root@C7-Server01 /]# ls -lhi /home/kkutysllb/test.sh1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 /home/kkutysllb/test.sh# 使用-p选项，保持文件属性推送[root@C7-Server01 /]# scp -p /home/kkutysllb/test.sh 192.168.101.82:/tmproot@192.168.101.82's password: test.sh 100% 114 1.4KB/s 00:00# 查看C7 Server02服务器上/tmp目录下test.sh文件属性[root@C7-Server02 ~]# ls -lhi /tmp/test.sh 34631592 -rw-r--r-- 1 root root 114 Apr 19 18:03 /tmp/test.sh 3）递归推送目录到远程服务器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 使用-r选项递归复制目录到远程服务器# 复制目录，最好加上-p选项保持源目录下所有文件和子目录的属性# 查看C7 Server02下/home目录下信息[root@C7-Server02 ~]# ls /home[root@C7-Server02 ~]# # 将Sever01下/home/kkutysllb/目录整体推送到Server02下[root@C7-Server01 /]# scp -rp /home/kkutysllb/ 192.168.101.82:/homeroot@192.168.101.82's password: .bash_logout 100% 18 17.3KB/s 00:00 .bash_profile 100% 193 322.5KB/s 00:00 .bashrc 100% 231 3.1KB/s 00:00 .bash_history 100% 41 21.9KB/s 00:00 image008 100% 0 0.0KB/s 00:00 image009 100% 0 0.0KB/s 00:00 202012312234.55 100% 0 0.0KB/s 00:00 image010 100% 0 0.0KB/s 00:00 hard_link 100% 158 114.9KB/s 00:00 soft_link 100% 187 259.1KB/s 00:00 data001 100% 54 106.1KB/s 00:00 data002 100% 73 137.8KB/s 00:00 test.sh 100% 114 150.9KB/s 00:00 data003 100% 121 176.4KB/s 00:00 data004 100% 66 131.4KB/s 00:00 test01.sh 100% 82 38.6KB/s 00:00 test01 100% 0 0.0KB/s 00:00 data005 100% 102 86.8KB/s 00:00 html01 100% 0 0.0KB/s 00:00 html02 100% 0 0.0KB/s 00:00 html03 100% 0 0.0KB/s 00:00 html04 100% 0 0.0KB/s 00:00 html05 100% 0 0.0KB/s 00:00 html06 100% 0 0.0KB/s 00:00 html07 100% 0 0.0KB/s 00:00 html08 100% 0 0.0KB/s 00:00 html09 100% 0 0.0KB/s 00:00 html10 100% 0 0.0KB/s 00:00 # 再次查看Server02下/home目录信息[root@C7-Server02 ~]# ls -l /home/total 4drwx------ 7 root root 4096 Apr 28 00:00 kkutysllb 4）从远程服务器上拉取文件到本地 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 与推送时选项参数位置对调[root@C7-Server01 /]# scp -rp 192.168.101.82:/home/ /tmp/root@192.168.101.82's password: .bash_logout 100% 18 14.3KB/s 00:00 .bash_profile 100% 193 270.0KB/s 00:00 .bashrc 100% 231 439.6KB/s 00:00 .bash_history 100% 41 83.3KB/s 00:00 image008 100% 0 0.0KB/s 00:00 image009 100% 0 0.0KB/s 00:00 202012312234.55 100% 0 0.0KB/s 00:00 image010 100% 0 0.0KB/s 00:00 hard_link 100% 158 266.8KB/s 00:00 soft_link 100% 187 142.2KB/s 00:00 data001 100% 54 64.2KB/s 00:00 data002 100% 73 111.3KB/s 00:00 test.sh 100% 114 179.0KB/s 00:00 data003 100% 121 49.8KB/s 00:00 data004 100% 66 92.3KB/s 00:00 test01.sh 100% 82 156.1KB/s 00:00 test01 100% 0 0.0KB/s 00:00 data005 100% 102 170.8KB/s 00:00 html01 100% 0 0.0KB/s 00:00 html02 100% 0 0.0KB/s 00:00 html03 100% 0 0.0KB/s 00:00 html04 100% 0 0.0KB/s 00:00 html05 100% 0 0.0KB/s 00:00 html06 100% 0 0.0KB/s 00:00 html07 100% 0 0.0KB/s 00:00 html08 100% 0 0.0KB/s 00:00 html09 100% 0 0.0KB/s 00:00 html10 100% 0 0.0KB/s 00:00 # 查看本地服务器/tmp目录下文件信息[root@C7-Server01 /]# ls -lhi /tmp/total 0101098026 drwxr-xr-x 3 root root 23 Apr 28 00:22 home # 刚拉取过来的目录 35134 drwx------ 3 root root 17 Apr 27 21:43 systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806 1237 drwx------ 3 root root 17 Apr 21 16:27 systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm 1231048 drwx------ 2 root root 6 Apr 20 11:06 vmware-root101013160 drwx------ 2 root root 6 Apr 20 16:08 vmware-root_16212-827959067 689143 drwx------ 2 root root 6 Apr 27 21:42 vmware-root_9328-3134798637 689145 drwx------ 2 root root 6 Apr 27 21:43 vmware-root_9392-3134798733 266371 drwx------ 2 root root 6 Apr 22 22:53 vmware-root_9400-3101375875 470132 drwx------ 2 root root 6 Apr 27 12:00 vmware-root_9409-3878589979 266297 drwx------ 2 root root 6 Apr 22 22:52 vmware-root_9420-2857896526 1875 drwx------ 2 root root 6 Apr 21 16:27 vmware-root_9443-3886520225 266369 drwx------ 2 root root 6 Apr 23 10:29 vmware-root_9456-2866351085 266299 drwx------ 2 root root 6 Apr 22 22:23 vmware-root_9458-2857896559 689124 drwx------ 2 root root 6 Apr 23 12:50 vmware-root_9573-4146439782 689142 drwx------ 2 root root 6 Apr 26 17:24 vmware-root_9604-3101310240 266294 drwx------ 2 root root 6 Apr 23 12:41 vmware-root_9609-4121731445 689141 drwx------ 2 root root 6 Apr 23 18:24 vmware-root_9660-3101179142]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-27-DPDK技术栈在电信云中的最佳实践（一）]]></title>
    <url>%2F2019%2F04%2F27%2F2019-04-27-DPDK%E6%8A%80%E6%9C%AF%E6%A0%88%E5%9C%A8%E7%94%B5%E4%BF%A1%E4%BA%91%E4%B8%AD%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文篇幅有限，很难用短短几语就勾勒出DPDK的完整轮廓，概括来说，DPDK是一个技术栈，主要用于Intel架构的服务器领域，其主要目的就是提升x86标准服务器的转发性能。因此，本文只重点介绍DPDK平台部分技术在电信云中的最佳实践。 为什么需要DPDK？在IA上，网络数据包处理远早于DPDK而存在。从商业版的Windows到开源的Linux操作系统，所有跨主机通信几乎都会涉及网络协议栈以及底层网卡驱动对于数据包的处理。然而，低速网络数据转发与高速网络数据转发的处理对系统的要求完全不一样。以Linux为例，传统网络设备驱动包处理的动作可以概括如下： 数据包到达网卡设备。 网卡设备依据配置进行DMA操作。 网卡发送中断，唤醒处理器。 驱动软件填充读写缓冲区数据结构。 数据报文达到内核协议栈，进行高层处理。 如果最终应用在用户态，数据从内核搬移到用户态。 如果最终应用在内核态，在内核继续进行。 随着网络接口带宽从千兆向万兆迈进，原先每个报文就会触发一个中断，中断带来的开销变得突出，大量数据到来会触发频繁的中断开销，导致系统无法承受。 在网络包高性能转发技术领域，有两个著名的技术框架NAPI和Netmap。NAPI策略用于高吞吐的场景，其策略是系统被中断唤醒后，尽量使用轮询的方式一次处理多个数据包，直到网络再次空闲重新转入中断等待，其目的就是解决数据包在转发过程过程中频繁中断引入的大量系统开销。Netmap就是采用共享数据包池的方式，减少内核到用户空间的包复制，从而解决大多数场景下需要把包从内核的缓冲区复制到用户缓冲区引入大量系统开销问题。 NAPI与Netmap两方面的努力其实已经明显改善了传统Linux系统上的包处理能力，但是，Linux作为分时操作系统，要将CPU的执行时间合理地调度给需要运行的任务。相对于公平分时，不可避免的就是适时调度。早些年CPU核数比较少，为了每个任务都得到响应处理，进行充分分时，用效率换响应，是一个理想的策略。现今CPU核数越来越多，性能越来越强，为了追求极端的高性能高效率，分时就不一定总是上佳的策略。以Netmap为例，即便其减少了内核到用户空间的内存复制，但内核驱动的收发包处理和用户态线程依旧由操作系统调度执行，除去任务切换本身的开销，由切换导致的后续cache替换（不同任务内存热点不同），对性能也会产生负面的影响。为此，Intel针对IA架构的这些问题，就提出了DPDK技术栈的架构，其根本目的就是尽量采用用户态驱动能力来替代内核态驱动，从而减少内核态的开销，提升转发性能。 鸟瞰DPDK什么是DPDK？在《DPDK深入浅出》一书中，有以下一段描述： 针对不同的对象，其定义并不相同。对于普通用户来说，它可能是一个性能出色的包数据处理加速软件库；对于开发者来说，它可能是一个实践包处理新想法的创新工场；对于性能调优者来说，它可能又是一个绝佳的成果分享平台。当下火热的网络功能虚拟化，则将DPDK放在一个重要的基石位置。 DPDK最初的动机很简单，就是为了证明IA多核处理器能够支撑高性能数据包处理。随着早期目标的达成和更多通用处理器体系的加入，DPDK逐渐成为通用多核处理器高性能数据包处理的业界标杆。 目前，DPDK技术主要应用于计算领域的硬件加速器、通信领域的网络处理器和IT领域的多核处理器。随着软件（例如，DPDK）在I/O性能提升上的不断创新，将多核处理器的竞争力提升到一个前所未有的高度。在SDN/NFV领域，DPDK技术得到了空前应用，产生了不少最佳实践案例。 DPDK提出的目的就是为IA上的高速包处理。下图所示的DPDK主要模块分解展示了以基础软件库的形式，为上层应用的开发提供一个高性能的基础I/O开发包。主要利用了有助于包处理的软硬件特性，如大页、缓存行对齐、线程绑定、预取、NUMA、IA最新指令的利用、Intel DDIO、内存交叉访问等。 核心库Core Libs，提供系统抽象、大页内存、缓存池、定时器及无锁环等基础组件。 PMD库，提供全用户态的驱动，以便通过轮询和线程绑定得到极高的网络吞吐，支持各种本地和虚拟的网卡。 Classify库，支持精确匹配（Exact Match）、最长匹配（LPM）和通配符匹配（ACL），提供常用包处理的查表操作。 QoS库，提供网络服务质量相关组件，如限速（Meter）和调度（Sched）。 除了这些组件，DPDK还提供了几个平台特性，比如节能考虑的运行时频率调整（POWER），与Linux kernel stack建立快速通道的KNI（Kernel Network Interface）。而Packet Framework和DISTRIB为搭建更复杂的多核流水线处理模型提供了基础的组件。 DPDK软件包内有一个最基本的三层转发实例（l3fwd），可用于测试双路服务器整系统的吞吐能力，通过现场实验，可以达到220Gbit/s的数据报文吞吐能力。除了通过硬件或者软件提升性能之外，如今DPDK整系统报文吞吐能力上限已经不再受限于CPU的核数，当前瓶颈在于PCIe（IO总线）的LANE数。换句话说，系统性能的整体I/O天花板不再是CPU，而是系统所提供的所有PCIe LANE的带宽，也就是能插入多少个高速以太网接口卡。 在这样的性能基础上，网络节点的软化（NFV）就成为可能。对于网络节点上运转的不同形态的网络功能，通过软化并适配到一个通用的硬件平台，就是软硬件解耦。解耦正是NFV的一个核心思想，而硬件解耦的多个网络功能在单一通用节点上的隔离共生问题，就是另一个核心思想—虚拟化。 DPDK技术基础（1）cache的作用在当今服务器领域，一个处理器通常包含多个核心（Core），集成Cache子系统，内存子系统通过内部或外部总线与其通信。在经典计算机系统中一般都有两个标准化的部分：北桥（North Bridge）和南桥（SouthBridge）。它们是处理器和内存以及其他外设沟通的渠道。在这类系统中，北桥就是真个架构的瓶颈，一旦北桥处理不过来或故障，整个系统的处理效率就会变低或瘫痪。因此，后来计算机系统中只存在南桥芯片，而北桥部分就被全部移植到CPU的SoC中，其中最重要的部分就是内存控制器，并在此基础上进一步衍生出NUMA和MPP架构，这个放在后面会讲。 我们在本科学习计算机基础课程时，都知道计算机的内存分为SRAM、DRAM、SDRAM和DDR(1/2/3/4)等不同类型。在早期的PC系统中，主要使用DRAM和SDRAM来作为内存，相比SRAM在成本、功耗方面有不小的优势，而且速度也还可以。后来在现今的PC系统中，利用SDRAM在一个时钟周期的上下边沿进行数据读写，整体数据吞吐率带宽翻倍，也就是DDR RAM，DDR根据不同的主频，又分为DDR1/DDR2/DDR3/DDR4。而SRAM，由于其功耗高、成本高，速度很快，一般都作为CPU的cache使用，目前都被封装的CPU的SoC中。 一般来说，Cache由三级组成，之所以对Cache进行分级，也是从成本和生产工艺的角度考虑的。一级（L1）最快，但是容量最小；三级（LLC，Last Level Cache）最慢，但是容量最大。 一级Cache：一般分为数据Cache和指令Cache，数据Cache用来存储数据，而指令Cache用于存放指令。这种Cache速度最快，一般处理器只需要3～5个指令周期就能访问到数据，因此成本高，容量小，一般都只有几十KB。 二级Cache：和一级Cache分为数据Cache和指令Cache不同，数据和指令都无差别地存放在一起。速度相比一级Cache慢一些，处理器大约需要十几个处理器周期才能访问到数据，容量也相对来说大一些，一般有几百KB到几MB不等。 三级Cache：速度更慢，处理器需要几十个处理器周期才能访问到数据，容量更大，一般都有几MB到几十个MB。在多核处理器内部，三级Cache由所有的核心所共有。这样的共享方式，其实也带来一个问题，有的处理器可能会极大地占用三级Cache，导致其他处理器只能占用极小的容量，从而导致Cache不命中，性能下降。因此，Intel公司推出了Intel® CAT技术，确保有一个公平，或者说软件可配置的算法来控制每个核心可以用到的Cache大小，有兴趣的可参考https://software.intel.com/zh-cn/articles/introduction-to-cache-allocation-technology?_ga=2.54835683.913561365.1556263296-1903721401.1556263296。 为了将cache与内存进行关联，需要对cache和内存进行分块，并采用一定的映射算法进行关联。分块就是将Cache和内存以块为单位进行数据交换，块的大小通常以在内存的一个存储周期中能够访问到的数据长度为限。当今主流块的大小都是64字节，因此一个Cache line就是指64个字节大小的数据块。而映射算法是指把内存地址空间映射到Cache地址空间。具体来说，就是把存放在内存中的内容按照一定规则装入到Cache中，并建立内存地址与Cache地址之间的对应关系。当CPU需要访问这个数据块内容时，只需要把内存地址转换成Cache地址，从而在Cache中找到该数据块，最终返回给CPU。 根据Cache和内存之间的映射关系的不同，Cache可以分为三类：第一类是全关联型Cache（full associative cache），第二类是直接关联型Cache（direct mapped cache），第三类是组关联型Cache（N-ways associative cache）。 全关联型cache：需要在cache中建立一个目录表，目录表的每一项由内存地址、cache块号和一个有效位组成。当CPU需要访问某个内存地址时，首先查询该目录表判断该内容是否缓存在Cache中，如果在，就直接从cache中读取内容；如果不在，就去通过内存地址转换去内存冲读取。具体原理如下： 首先，用内存的块地址A在Cache的目录表中进行查询，如果找到等值的内存块地址，检查有效位是否有效，只有有效的情况下，才能通过Cache块号在Cache中找到缓存的内存，并且加上块内地址B，找到相应数据，这时则称为Cache命中，处理器拿到数据返回；否则称为不命中，CPU则需要在内存中读取相应的数据。使用全关联型Cache，块的冲突最小（没有冲突），Cache的利用率也高，但是需要一个访问速度很快的相联存储器。随着Cache容量的增加，其电路设计变得十分复杂，因此一般只有TLB cache才会设计成全关联型。 直接关联型Cache：是指将某一块内存映射到Cache的一个特定的块，即Cache line中。假设一个Cache中总共存在N个Cache line，那么内存就被分成N等分，其中每一等分对应一个Cache line。比如：Cache的大小是2K，而一个Cache line的大小是64B，那么就一共有2K/64B=32个Cache line，那么对应我们的内存，第1块（地址0～63），第33块（地址6432～6433-1），以及第（N32+1）块都被映射到Cache第一块中；同理，第2块，第34块，以及第（N32+2）块都被映射到Cache第二块中；可以依次类推其他内存块。直接关联型Cache的目录表只有两部分组成：区号和有效位。具体原理如下： 首先，内存地址被分成三部分：区号A、块号B和块内地址C。根据区号A在目录表中找到完全相等的区号，并且在有效位有效的情况下，说明该数据在Cache中，然后通过内存地址的块号B获得在Cache中的块地址，加上块内地址C，最终找到数据。如果在目录表中找不到相等的区号，或者有效位无效的情况下，则说明该内容不在Cache中，需要到内存中读取。可以看出，直接关联是一种很“死”的映射方法，当映射到同一个Cache块的多个内存块同时需要缓存在Cache中时，只有一个内存块能够缓存，其他块需要被“淘汰”掉。因此，直接关联型命中率是最低的，但是其实现方式最为简单，匹配速度也最快。 组关联型Cache：是目前Cache中用的比较广泛的一种方式，是前两种Cache的折中形式。在这种方式下，内存被分为很多组，一个组的大小为多个Cache line的大小，一个组映射到对应的多个连续的Cache line，也就是一个Cache组，并且该组内的任意一块可以映射到对应Cache组的任意一个。可以看出，在组外，其采用直接关联型Cache的映射方式，而在组内，则采用全关联型Cache的映射方式。比如：有一个4路组关联型Cache，其大小为1M，一个Cache line的大小为64B，那么总共有16K个Cache line，但是在4路组关联的情况下，就拥有了4K个组，每个组有4个Cache line。一个内存单元可以缓存到它所对应的组中的任意一个Cache line中去。具体原理如下： 目录表由三部分组成：“区号+块号”、Cache块号和有效位。一个内存地址被分成四部分：区号A、组号B、块号C和块内地址D。首先，根据组号B按地址查找到一组目录表项；然后，根据区号A和块号C在该组中进行关联查找（即并行查找，为了提高效率），如果匹配且有效位有效，则表明该数据块缓存在Cache中，得到Cache块号，加上块内地址D，可以得到该内存地址在Cache中映射的地址，得到数据；如果没有找到匹配项或者有效位无效，则表示该内存块不在Cache中，需要处理器到内存中读取。 Cache之所以能够提高系统性能，主要是因为程序执行存在局部性现象，即时间局部性（程序中指令和数据在时间上的关联性，比如：循环体中的变量和指令）和空间局部性（程序中指令和数据在空间上的关联性，比如：列表数据结构中的元素）。cache就可以根据程序的局部性特点，以及当前执行状态、历史执行过程、软件提示等信息，然后以一定的合理方法，在数据/指令被使用前取入Cache，也就是cache预取。 内存的数据被加载进cache后，最终还是需要写回到内存中，这个写回的过程存在两种策略： 直写（write-through）：在CPU对Cache写入的同时，将数据写入到内存中。这种策略保证了在任何时刻，内存的数据和Cache中的数据都是同步的，这种方式简单、可靠。但由于CPU每次对Cache更新时都要对内存进行写操作，总线工作繁忙，内存的带宽被大大占用，因此运行速度会受到影响。 回写（write-back）：回写相对于直写而言是一种高效的方法。回写系统通过将Cache line的标志位字段添加一个Dirty标志位，当处理器在改写了某个Cache line后，并不是马上把其写回内存，而是将该Cache line的Dirty标志设置为1。当处理器再次修改该Cache line并且写回到Cache中，查表发现该Dirty位已经为1，则先将Cache line内容写回到内存中相应的位置，再将新数据写到Cache中。 除了上述这两种写策略，还有WC（write-combining）和UC（uncacheable）。这两种策略都是针对特殊的地址空间来使用的，这里不做详细讨论，有兴趣的可以参考Intel官方社区。 在采用回写策略的架构中，如果多个CPU同时对一个cache line进行修改后的写回操作，就存在“脏”数据区域的问题，这就是cache一致性问题。其本质原因是存在多个处理器独占的Cache，而不是多个处理器。解决Cache一致性问题的机制有两种：基于目录的协议（Directory-based protocol）和总线窥探协议（Bus snooping protocol）。这里因为篇幅问题，不再展开讨论，有兴趣的可参见《深入浅出DPDK》一书相关内容。 事实上，Cache对于绝大多数程序员来说都是透明不可见的，cache完成数据缓存的所有操作都是硬件自动完成的。但是，硬件也不是完全智能的。因此，Intel体系架构引入了能够对Cache进行预取的指令，使一些对程序执行效率有很高要求的程序员能够一定程度上控制Cache，加快程序的执行。DPDK对cache进行预取操作如下： 1234567891011121314151617181920212223242526272829303132333435while (nb_rx &lt; nb_pkts) &#123; rxdp = &amp;rx_ring[rx_id]; //读取接收描述符 staterr = rxdp-&gt;wb.upper.status_error; //检查是否有报文收到 if (！(staterr &amp; rte_cpu_to_le_32(IXGBE_RXDADV_STAT_DD))) break; rxd = *rxdp; //分配数据缓冲区 nmb = rte_rxmbuf_alloc(rxq-&gt;mb_pool); nb_hold++; //读取控制结构体 rxe = &amp;sw_ring[rx_id]; …… rx_id++; if (rx_id == rxq-&gt;nb_rx_desc) &#123; rx_id = 0; //预取下一个控制结构体mbuf rte_ixgbe_prefetch(sw_ring[rx_id].mbuf); //预取接收描述符和控制结构体指针 &#125; /* 预取报文 */ if ((rx_id &amp; 0x3) == 0) &#123; rte_ixgbe_prefetch(&amp;rx_ring[rx_id]); rte_ixgbe_prefetch(&amp;sw_ring[rx_id]); &#125; /* 把接收描述符读取的信息存储在控制结构体mbuf中 */ rte_packet_prefetch((char *)rxm-&gt;buf_addr + rxm-&gt;data_off); rxm-&gt;nb_segs = 1; rxm-&gt;next = NULL; rxm-&gt;pkt_len = pkt_len; rxm-&gt;data_len = pkt_len; rxm-&gt;port = rxq-&gt;port_id; …… rx_pkts[nb_rx++] = rxm; &#125; 同时，DPDK在定义数据结构或者数据缓冲区时就申明cache line对齐，代码如下： 123456789101112131415#define RTE_CACHE_LINE_SIZE 64 #define __rte_cache_aligned __attribute__((__aligned__(RTE_CACHE_LINE_SIZE)))struct rte_ring_debug_stats &#123; uint64_t enq_success_bulk; uint64_t enq_success_objs; uint64_t enq_quota_bulk; uint64_t enq_quota_objs; uint64_t enq_fail_bulk; uint64_t enq_fail_objs; uint64_t deq_success_bulk; uint64_t deq_success_objs; uint64_t deq_fail_bulk; uint64_t deq_fail_objs; &#125; __rte_cache_aligned; 大页内存在前文《x86架构基础》一文中提到了TLB的概念，其主要用来缓存内存地址转换中的页表项，其本质上也是一个cache，称之为TLB cache。TLB和cache的区别是：TLB缓存内存地址转换用的页表项，而cache缓存程序用到的数据和指令。 TLB中保存着程序线性地址前20位[31：12]和页框号的对应关系，如果匹配到线性地址就可以迅速找到页框号，通过页框号与线性地址后12位的偏移组合得到最终的物理地址。TLB使用虚拟地址进行搜索，直接返回对应的物理地址，相对于内存中的多级页表需要多次访问才能得到最终的物理地址，TLB查找大大减少了CPU的开销。如果需要的地址在TLB Cache中，就会迅速返回结果，然后CPU用该物理地址访问内存，这样的查找操作也称为TLB命中；如果需要的地址不在TLB Cache中，也就是不命中，CPU就需要到内存中访问多级页表，才能最终得到物理地址。但是，TLB的大小是有限的，因此TLB不命中的概率很大，为了提高内存地址转换效率，减少CPU的开销，就提出了大页内存的概念。 在x86架构中，一般都分成以下四组TLB： 第一组：缓存一般页表（4KB页面）的指令页表缓存（Instruction-TLB）。 第二组：缓存一般页表（4KB页面）的数据页表缓存（Data-TLB）。 第三组：缓存大尺寸页表（2MB/4MB页面）的指令页表缓存（Instruction-TLB）。 第四组：缓存大尺寸页表（2MB/4MB页面）的数据页表缓存（Data-TLB） 如果采用常规页（4KB）并且使TLB总能命中，需要寻址的内容都在该内容页内，那么至少需要在TLB表中存放两个表项。如果一个程序使用了512个内容页也就是2MB大小，那么需要512个页表表项才能保证不会出现TLB不命中的情况。但是，如果采用2MB作为分页的基本单位，那么只需要一个表项就可以保证不出现TLB不命中的情况；对于消耗内存以GB为单位的大型程序，可以采用1GB为单位作为分页的基本单位，减少TLB不命中的情况。需要注意的是：系统能否支持大页，支持大页的大小为多少是由其使用的处理器决定的。 在Linux启动之后，如果想预留大页，则可以使用以下的方法来预留内存。在非NUMA系统中，可以使用以下方法预留2MB大小的大页。 123# 预留1024个大小为2MB的大页，也就是预留了2GB内存。echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages 系统未开启大页内存的状态 系统开启大页内存后的状态 如果是在NUMA系统中，假设有两个NODE的系统中，则可以用以下的命令： 1234# 在NODE0和NODE1上各预留1024个大小为2MB的大页，总共预留了4GB大小。echo 1024 &gt; /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages echo 1024 &gt; /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages 而对于大小为1GB的大页，则必须在Linux的GRUB配置文件中进行修改，并重启系统生效，不能动态预留。 DPDK中也是使用HUGETLBFS来使用大页。首先，它需要把大页mount到某个路径，比如/mnt/huge，以下是命令： 12mkdir /mnt/huge mount -t hugetlbfs nodev /mnt/huge 需要注意的是：在mount之前，要确保之前已经成功预留内存，否则会失败。该命令只是临时的mount了文件系统，如果想每次开机时省略该步骤，可以修改/etc/fstab文件，加上如下一行： 1nodev /mnt/huge hugetlbfs defaults 0 0 对于1GB大小的大页，则必须用如下的命令： 1nodev /mnt/huge_1GB hugetlbfs pagesize=1GB 0 0 然后，在DPDK运行的时候，会使用mmap（）系统调用把大页映射到用户态的虚拟地址空间，然后就可以正常使用了。 DDIO（Data Direct I/O）数据直连技术 如今，随着大数据和云计算的爆炸式增长，宽带的普及以及个人终端网络数据的日益提高，对运营商服务节点和数据中心的数据交换能力和网络带宽提出了更高的要求。并且，数据中心本身对虚拟化功能的需求也增加了更多的网络带宽需求。为此，英特尔公司提出了Intel® DDIO（Data Direct I/O）的技术。该技术的主要目的就是让服务器能更快处理网络接口的数据，提高系统整体的吞吐率，降低延迟，同时减少能源的消耗。 当一个网络报文送到服务器的网卡时，网卡通过外部总线（比如PCI总线）把数据和报文描述符送到内存。接着，CPU从内存读取数据到Cache进而到寄存器。进行处理之后，再写回到Cache，并最终送到内存中。最后，网卡读取内存数据，经过外部总线送到网卡内部，最终通过网络接口发送出去。可以看出，对于一个数据报文，CPU和网卡需要多次访问内存。而内存相对CPU来讲是一个非常慢速的部件。CPU需要等待数百个周期才能拿到数据，在这过程中，CPU什么也做不了。 DDIO技术思想就是使外部网卡和CPU通过LLC Cache直接交换数据，绕过了内存这个相对慢速的部件。这样，就增加了CPU处理网络报文的速度（减少了CPU和网卡等待内存的时间），减小了网络报文在服务器端的处理延迟。这样做也带来了一个问题，就是网络报文直接存储在LLC Cache中，对这一级cache的容量有很大需求。因此，在英特尔的E5处理器系列产品中，把LLC Cache的容量提高到了20MB。DDIO处理网络报文流程示意图如下： 为了发送一个数据报文到网络上去，首先是运行在CPU上的软件分配了一段内存，然后把这段内存读取到CPU内部，更新数据，并且填充相应的报文描述符（网卡会通过读取描述符了解报文的相应信息），然后写回到内存中，通知网卡，最终网卡把数据读回到内部，并且发送到网络上去。但是，没有DDIO技术和有DDIO技术条件的处理方式是不同的。 a) 没有DDIO时，如下图所示： 1）CPU更新报文和控制结构体。由于分配的缓冲区在内存中，因此会触发一次Cache不命中，CPU把内存读取到Cache中，然后更新控制结构体和报文信息。之后通知NIC来读取报文。 2）NIC收到有报文需要传递到网络上的通知后，读取控制结构体进而知道去内存中读取报文信息。 3）由于之前CPU刚把该缓冲区从内存读到Cache中并且做了更新，很有可能Cache还没有来得及把更新的内容写回到内存中(回写机制)。因此，当NIC发起一个对内存的读请求时，很有可能这个请求会发送到Cache系统中，Cache系统会把数据写回到内存中。 4）最后，内存控制器再把数据写到PCI总线上去，NIC从PCI总线上读取数据。 b) 有DDIO时，如下图所示： 1）CPU更新报文和控制结构体。这个步骤和没有DDIO的技术类似，但是由于DDIO的引入，处理器会开始就把内存中的缓冲区和控制结构体预取到Cache，因此减少了内存读的时间。 2）NIC收到有报文需要传递到网络上的通知后，通过PCI总线去读取控制结构体和报文。利用DDIO技术，I/O访问可以直接将Cache的内容送到PCI总线上。这样，就减少了Cache写回时等待的时间。 由此可以看出，由于DDIO技术的引入，网卡的读操作减少了访问内存的次数，因而提高了访问效率，减少了报文转发的延迟。在理想状况下，NIC和CPU无需访问内存，直接通过访问Cache就可以完成更新数据，把数据送到NIC内部，进而送到网络上的所有操作。 有网络报文需要送到系统内部进行处理，其过程一般是NIC从网络上收到报文后，通过PCI总线把报文和相应的控制结构体送到预先分配的内存，然后通知相应的驱动程序或者软件来处理。和之前网卡的读数据操作类似，有DDIO技术和没有DDIO技术的处理也是不一样的。 a) 没有DDIO时，如下图所示： 1）报文和控制结构体通过PCI总线送到指定的内存中。如果该内存恰好缓存在Cache中（有可能之前CPU有对该内存进行过读写操作），则需要等待Cache把内容先写回到内存中，然后才能把报文和控制结构体写到内存中。 2）运行在CPU上的驱动程序或者软件得到通知收到新报文，去内存中读取控制结构体和相应的报文，Cache不命中。之所以Cache一定不会命中，是因为即使该内存地址在Cache中，在步骤1中也被强制写回到内存中。因此，只能从内存中读取控制结构体和报文。 b) 有DDIO时，如下图所示： 1）这时，报文和控制结构体通过PCI总线直接送到Cache中。这时有两种情形：场景一就是如果该内存恰好缓存在Cache中（有可能之前处理器有对该内存进行过读写操作），则直接在Cache中更新内容，覆盖原有内容。场景二就是如果该内存没有缓存在Cache中，则在最后一级Cache中分配一块区域，并相应更新Cache表，表明该内容是对应于内存中的某个地址的。 2）运行在CPU上的驱动或者软件被通知到有报文到达，其产生一个内存读操作，由于该内容已经在Cache中，因此直接从Cache中读。 由此可以看出，DDIO技术在CPU和外设之间交换数据时，减少了CPU和外设访问内存的次数，也减少了Cache写回的等待，提高了系统的吞吐率和数据的交换延迟。 NUMA系统从系统架构来看，目前的商用服务器大体可以分为三类，即对称多处理器结构 (SMP ： Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP ： Massive Parallel Processing) 。它们的特征如下： SMP (Symmetric Multi Processing),对称多处理系统内有许多紧耦合多处理器，在这样的系统中，所有的CPU共享全部资源，如总线，内存和I/O系统等，操作系统或管理数据库的复本只有一个，这种系统有一个最大的特点就是共享所有资源。多个CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。SMP 服务器的主要特征是共享，系统中所有资源 (CPU 、内存、 I/O 等 ) 都是共享的。也正是由于这种特征，导致了 SMP 服务器的主要问题，那就是它的扩展能力非常有限。对于 SMP 服务器而言，每一个共享的环节都可能造成 SMP 服务器扩展时的瓶颈，而最受限制的则是内存。由于每个 CPU 必须通过相同的内存总线访问相同的内存资源，因此随着 CPU 数量的增加，内存访问冲突将迅速增加，最终会造成 CPU 资源的浪费，使 CPU 性能的有效性大大降低。实验证明， SMP 服务器 CPU 利用率最好的情况是 2 至 4 个 CPU 。 NUMA 服务器的基本特征是具有多个 CPU 模块，每个 CPU 模块由多个 CPU( 如 4 个 ) 组成，并且具有独立的本地内存、 I/O 槽口等。由于其节点之间可以通过互联模块 ( 如称为 Crossbar Switch) 进行连接和信息交互，因此每个 CPU 可以访问整个系统的内存 ( 这是 NUMA 系统与 MPP 系统的重要差别 ) 。显然，访问本地内存的速度将远远高于访问远地内存 ( 系统内其它节点的内存 ) 的速度，这也是非一致存储访问 NUMA 的由来。由于这个特点，为了更好地发挥系统性能，开发应用程序时需要尽量减少不同 CPU 模块之间的信息交互。利用 NUMA 技术，可以较好地解决原来 SMP 系统的扩展问题，在一个物理服务器内可以支持上百个 CPU 。NUMA 技术同样有一定缺陷，由于访问远地内存的延时远远超过本地内存，因此当 CPU 数量增加时，系统性能无法线性增加。 和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现 512 个节点互联，数千个 CPU 。MPP不是处理器内部节点互联，而是多个服务器通过外部互联。在 MPP 系统中，每个 SMP 节点也可以运行自己的操作系统、数据库等。但和 NUMA 不同的是，它不存在异地内存访问的问题。换言之，每个节点内的 CPU 不能访问另一个节点的内存。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为数据重分配 (Data Redistribution) 。MPP 服务器需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程。 NUMA系统是一种多处理器环境下设计的内存结构。在NUMA架构出现前，CPU欢快的朝着频率越来越高的方向发展。受到物理极限的挑战，又转为核数越来越多的方向发展。如果每个core的工作性质都是share-nothing（类似于map-reduce的node节点的作业属性），那么也许就不会有NUMA。由于所有CPU Core都是通过共享一个北桥来读取内存，无论核数如何的发展，北桥在响应时间上的性能瓶颈越来越明显。于是，聪明的硬件设计师们，想到了把内存控制器（原本北桥中读取内存的部分）也做个拆分，平分到了每个die上。于是NUMA就出现了！ NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die上。只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称Local Access）。而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称Remote Access）。所以NUMA（Non-Uniform Memory Access）就此得名。 NUMA的几个概念（Node，socket，core，thread） socket：就是主板上的CPU插槽; Core：就是socket里独立的一组程序执行的硬件单元，比如寄存器，计算单元等; Thread：就是超线程hyperthread的概念，逻辑的执行单元，独立的执行上下文，但是共享core内的寄存器和计算单元。 Node：这个概念其实是用来解决core的分组的问题，具体参见下图来理解（图中的OS CPU可以理解thread，那么core就没有在图中画出），从图中可以看出共有4个socket，每个socket 2个node，每个node中有8个thread，总共4（Socket）× 2（Node）× 8 （4core × 2 Thread） = 64个thread。另外每个node有自己的内部CPU，总线和内存，同时还可以访问其他node内的内存，NUMA的最大的优势就是可以方便的增加CPU的数量，因为Node内有自己内部总线，所以增加CPU数量可以通过增加Node的数目来实现，如果单纯的增加CPU的数量，会对总线造成很大的压力，所以UMA结构不可能支持很多的核。下图出自：《NUMA Best Practices for Dell PowerEdge 12th Generation Servers》 由于每个node内部有自己的CPU总线和内存，所以如果一个虚拟机的vCPU跨不同的Node的话，就会导致一个node中的CPU去访问另外一个node中的内存的情况，这就导致内存访问延迟的增加。在NFV环境中，对性能有比较高的要求，就非常需要同一个虚拟机的vCPU尽量被分配到同一个Node中的pCPU上，所以在OpenStack的Kilo版本及后续版本均增加了基于NUMA感知的虚拟机调度的特性。详见www.openstack.org官方社区管理员手册文档。 查看服务器中NUMA拓扑架构常用以下命令： 1）比较常用的是lscpu 1234567891011121314151617181920212223242526[root@C7-Server01 ~]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 4On-line CPU(s) list: 0-3Thread(s) per core: 1Core(s) per socket: 2Socket(s): 2NUMA node(s): 2Vendor ID: GenuineIntelCPU family: 6Model: 158Model name: Intel(R) Core(TM) i9-8950HK CPU @ 2.90GHzStepping: 10CPU MHz: 2903.998BogoMIPS: 5807.99Virtualization: VT-xHypervisor vendor: VMwareVirtualization type: fullL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 12288KNUMA node0 CPU(s): 0-3Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec arat spec_ctrl intel_stibp flush_l1d arch_capabilities 从上面报文输出可以看出，当前机器有2个sockets，每个sockets包含1个numa node，每个numa node中有2个cores，每个cores包含1个thread，所以总的threads数量=2（sockets）×1（node）×2（cores）×1（threads）=4. 2）通过shell脚本打印出当前机器的socket，core和thread的数量 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/bin/bash# 简单打印系统CPU拓扑# Author: kkutysllbfunction get_nr_processor()&#123; grep '^processor' /proc/cpuinfo | wc -l&#125;function get_nr_socket()&#123; grep 'physical id' /proc/cpuinfo | awk -F: '&#123; print $2 | "sort -un"&#125;' | wc -l&#125;function get_nr_siblings()&#123; grep 'siblings' /proc/cpuinfo | awk -F: '&#123; print $2 | "sort -un"&#125;'&#125;function get_nr_cores_of_socket()&#123; grep 'cpu cores' /proc/cpuinfo | awk -F: '&#123; print $2 | "sort -un"&#125;'&#125;echo '===== CPU Topology Table ====='echoecho '+--------------+---------+-----------+'echo '| Processor ID | Core ID | Socket ID |'echo '+--------------+---------+-----------+'while read line; do if [ -z "$line" ]; then printf '| %-12s | %-7s | %-9s |\n' $p_id $c_id $s_id echo '+--------------+---------+-----------+' continue fi if echo "$line" | grep -q "^processor"; then p_id=`echo "$line" | awk -F: '&#123;print $2&#125;' | tr -d ' '` fi if echo "$line" | grep -q "^core id"; then c_id=`echo "$line" | awk -F: '&#123;print $2&#125;' | tr -d ' '` fi if echo "$line" | grep -q "^physical id"; then s_id=`echo "$line" | awk -F: '&#123;print $2&#125;' | tr -d ' '` fidone &lt; /proc/cpuinfoechoawk -F: '&#123; if ($1 ~ /processor/) &#123; gsub(/ /,"",$2); p_id=$2; &#125; else if ($1 ~ /physical id/)&#123; gsub(/ /,"",$2); s_id=$2; arr[s_id]=arr[s_id] " " p_id &#125;&#125; END&#123; for (i in arr) printf "Socket %s:%s\n", i, arr[i];&#125;' /proc/cpuinfoechoecho '===== CPU Info Summary ====='echonr_processor=`get_nr_processor`echo "Logical processors: $nr_processor"nr_socket=`get_nr_socket`echo "Physical socket: $nr_socket"nr_siblings=`get_nr_siblings`echo "Siblings in one socket: $nr_siblings"nr_cores=`get_nr_cores_of_socket`echo "Cores in one socket: $nr_cores"let nr_cores*=nr_socketecho "Cores in total: $nr_cores"if [ "$nr_cores" = "$nr_processor" ]; then echo "Hyper-Threading: off"else echo "Hyper-Threading: on"fiechoecho '===== END =====' 运行后输出结果如下： DPDK中有以下策略来适应NUMA系统： 1）Per-core memory：一个CPU上有多个核（core），per-core memory是指每个核都有属于自己的内存，即对于经常访问的数据结构，每个核都有自己的备份。这样做一方面是为了本地内存的需要，另外一方面也是前面提到的Cache一致性的需要，避免多个核访问同一个Cache Line。 2）本地设备本地处理：即用本地的处理器、本地的内存来处理本地的设备上产生的数据。如果有一个PCI设备在node0上，就用node0上的核来处理该设备，处理该设备用到的数据结构和数据缓冲区都从node0上分配。以下是一个分配本地内存的例子： 123/* allocate memory for the queue structure */ /* 该例分配一个结构体，通过传递socket_id，即node id获得本地内存，并且以Cache Line对齐 */q = rte_zmalloc_socket("fm10k", sizeof(*q), RTE_CACHE_LINE_SIZE, socket_id); CPU的亲和性调度当前，属于多核处理器时代，这类多核处理器自然会面对一个问题，按照什么策略将任务线程分配到各个处理器上执行。众所周知，这个分配工作一般由操作系统完成。负载均衡当然是比较理想的策略，按需指定的方式也是很自然的诉求，因为其具有确定性。简单地说，CPU亲和性（Core affinity）就是一个特定的任务要在某个给定的CPU上尽量长时间地运行而不被迁移到其他处理器上的倾向性。这意味着线程可以不在处理器之间频繁迁移，从而减少不必要的开销。 Linux内核包含了一种机制，它让开发人员可以编程实现CPU亲和性。也就是说可以将应用程序显式地指定线程在哪个（或哪些）CPU上运行。 在Linux内核中，所有的线程都有一个相关的数据结构，称为task_struct。这个结构非常重要，这里不展开讨论，只讨论其中与亲和性相关度最高的是cpus_allowed位掩码。这个位掩码由n位组成，与系统中的n个逻辑处理器一一对应。具有4个物理CPU的系统可以有4位。如果这些CPU都启用了超线程，那么这个系统就有一个8位的位掩码。 如果针对某个线程设置了指定的位，那么这个线程就可以在相关的CPU上运行。因此，如果一个线程可以在任何CPU上运行，并且能够根据需要在处理器之间进行迁移，那么位掩码就全是1。**实际上，在Linux中，这就是线程的默认状态。** Linux内核API提供了一些方法，让用户可以修改位掩码或查看当前的位掩码： sched_set_affinity（）（用来修改位掩码） sched_get_affinity（）（用来查看当前的位掩码） 注意，cpu_affinity会被传递给子线程，因此应该适当地调用sched_set_affinity。 将线程与CPU绑定，最直观的好处就是提高了CPU Cache的命中率，从而减少内存访问损耗，提高程序的速度。在多核体系CPU上，提高外设以及程序工作效率最直观的办法就是让各个物理核各自负责专门的事情。尤其在在NUMA架构下，这个操作对系统运行速度的提升有更大的意义，跨NUMA节点的任务切换，将导致大量三级Cache的丢失。从这个角度来看，NUMA使用CPU绑定时，每个核心可以更专注地处理一件事情，资源体系被充分使用，减少了同步的损耗。 通常Linux内核都可以很好地对线程进行调度，在应该运行的地方运行线程，也就是说在可用的处理器上运行并获得很好的整体性能。内核包含了一些用来检测CPU之间任务负载迁移的算法，可以启用线程迁移来降低繁忙的处理器的压力。只有在以下三个特殊场景会用到CPU亲和性绑定机制： 大量计算：在科学计算和理论计算中，如果不进行CPU亲和性绑定，会发现自己的应用程序要在多处理器的机器上花费大量时间进行迁移从而完成计算。 复杂程序测试：比如在线性可伸缩测试中，我们期望的理论模型是如果应用程序随着CPU的增加可以线性地伸缩，那么每秒事务数和CPU个数之间应该会是线性的关系。这样建模可以测试应用程序是否可以有效地使用底层硬件。如果一个给定的线程迁移到其他地方去了，那么它就失去了利用CPU缓存的优势。实际上，如果正在使用的CPU需要为自己缓存一些特殊的数据，那么其他所有CPU都会使这些数据在自己的缓存中失效。因此，如果有多个线程都需要相同的数据，那么将这些线程绑定到一个特定的CPU上，就可以确保它们访问相同的缓存数据或者至少可以提高缓存的命中率。 实时性线程：对于实时性线程经常会希望使用亲和性来指定一个8路主机上的某个CPU来处理，而同时允许其他7个CPU处理所有普通的系统调度。这种做法对长时间运行、对时间敏感的应用程序可以确保正常运行，同时可以允许其他应用程序独占其余的计算资源。 Linux内核提供了启动参数isolcpus。对于有4个CPU的服务器，在启动的时候加入启动参数isolcpus=2，3。那么系统启动后将不使用CPU3和CPU4。注意，这里说的不使用不是绝对地不使用，系统启动后仍然可以通过taskset命令指定哪些程序在这些核心中运行。 1）修改/etc/default/grub文件中内容，在CMDLINE中添加如下图所示设置 2）编译内核启动文件 123456789[root@C7-Server01 myshell]# grub2-mkconfig -o /boot/grub2/grub.cfgGenerating grub configuration file ...Found linux image: /boot/vmlinuz-3.10.0-957.10.1.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-957.10.1.el7.x86_64.imgFound linux image: /boot/vmlinuz-3.10.0-862.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-862.el7.x86_64.imgFound linux image: /boot/vmlinuz-0-rescue-e344b139f44946638783478bcb51f820Found initrd image: /boot/initramfs-0-rescue-e344b139f44946638783478bcb51f820.imgdone 3）重启系统后查看/proc/cmdline配置文件是否设置生效 12[root@C7-Server01 ~]# cat /proc/cmdline BOOT_IMAGE=/vmlinuz-3.10.0-957.10.1.el7.x86_64 root=UUID=0887567f-1df6-425f-ba3d-ce58584279e0 ro crashkernel=auto biosdevname=0 net.ifnames=0 rhgb quiet isolcpu=2,3 DPDK的线程基于pthread接口创建，属于抢占式线程模型，受内核调度支配。DPDK通过在多核设备上创建多个线程，每个线程绑定到单独的核上，减少线程调度的开销，以提高性能。DPDK的线程可以作为控制线程，也可以作为数据线程。控制线程一般绑定到MASTER核上，接受用户配置，并传递配置参数给数据线程等；数据线程分布在不同核上处理数据包。 DPDK的lcore指的是EAL线程，本质是基于pthread（Linux/FreeBSD）封装实现。Lcore（EAL pthread）由remote_launch函数指定的任务创建并管理。在每个EAL pthread中，有一个TLS（Thread Local Storage）称为_lcore_id。当使用DPDK的EAL‘-c’参数指定coremask时，EAL pthread生成相应个数lcore，并默认是1：1亲和到coremask对应的CPU逻辑核，_lcore_id和CPU ID是一致的。 123456789101112131415161718/* rte_eal_cpu_init（）函数中，通过读取/sys/devices/system/cpu/cpuX/下的相关信息，确定当前系统有哪些CPU核，以及每个核属于哪个CPU Socket。eal_parse_args（）函数，解析-c参数，确认哪些CPU核是可以使用的，以及设置第一个核为MASTER。为每一个SLAVE核创建线程，并调用eal_thread_set_affinity（）绑定CPU。线程的执行体是eal_thread_loop（）,函数内部的主体是一个while死循环，调用不同模块注册到lcore_config[lcore_id].f的回调函数 */RTE_LCORE_FOREACH_SLAVE(i) &#123; /* * create communication pipes between master thread * and children */ if (pipe(lcore_config[i].pipe_master2slave) &lt; 0) rte_panic("Cannot create pipe\n"); if (pipe(lcore_config[i].pipe_slave2master) &lt; 0) rte_panic("Cannot create pipe\n"); lcore_config[i].state = WAIT; /* create a thread for each lcore */ ret = pthread_create(&amp;lcore_config[i].thread_id, NULL, eal_thread_loop, NULL); if (ret！= 0) rte_panic("Cannot create thread\n"); &#125;/* 不同的模块需要调用rte_eal_mp_remote_launch（），将自己的回调处理函数注册到lcore_config[].f中。以l2fwd为例，注册的回调处理函数是l2fwd_launch_on_lcore（）*/rte_eal_mp_remote_launch(l2fwd_launch_one_lcore, NULL, CALL_MASTER); DPDK每个核上的线程最终会调用eal_thread_loop（）&gt;&gt;&gt; l2fwd_launch_on_lcore（），调用到自己实现的处理函数。默认情况下，lcore是与逻辑核一一亲和绑定的。带来性能提升的同时，也牺牲了一定的灵活性和能效。在现网中，往往有流量潮汐现象的发生，在网络流量空闲时，没有必要使用与流量繁忙时相同的核数。于是，EAL pthread和逻辑核之间进而允许打破1：1的绑定关系，使得_lcore_id本身和CPU ID可以不严格一致。EAL定义了长选项“——lcores”来指定lcore的CPU亲和性。对一个特定的lcore ID或者lcore ID组，这个长选项允许为EAL pthread设置CPU集。这个选项以及对应的一组API（rte_thread_set/get_affinity（））为lcore提供了亲和的灵活性。lcore可以亲和到一个CPU或者一个CPU集合，使得在运行时调整具体某个CPU承载lcore成为可能。同时，多个lcore也可能亲和到同一个核，但是这种情况下如果调度占用的内核库是非抢占式，就存在锁机制，DPDK技术栈在电信云中的最佳实践（2）中会专门针对不同锁进制进行讨论。 除了使用DPDK提供的逻辑核之外，用户也可以将DPDK的执行上下文运行在任何用户自己创建的pthread中。在普通用户自定义的pthread中，lcore id的值总是LCORE_ID_ANY，以此确定这个thread是一个有效的普通用户所创建的pthread。用户创建的pthread可以支持绝大多数DPDK库，没有任何影响。但少数DPDK库可能无法完全支持用户自创建的pthread，如timer和Mempool。详细请参见《DPDK开发者手册多线程章节》。 DPDK不仅可以通过绑核完成大量计算任务资源亲和性调度，同时在计算任务较小，一个核的资源绰绰有余的情况下，还可以通过Linux的cgroup对资源进行释放。因为，DPDK的线程其实就是普通的pthread，其本质就是使用cgroup能把CPU的配额灵活地配置在不同的线程上。因此，DPDK可以借助cgroup实现计算资源配额对于线程的灵活配置，可以有效改善I/O核的闲置利用率。 最后，用一张图来总结lcore的启动过程和执行任务分发的流程。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-22-Linux系统命令-第四篇《系统信息显示命令》]]></title>
    <url>%2F2019%2F04%2F23%2F2019-04-22-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E5%9B%9B%E7%AF%87%E3%80%8A%E7%B3%BB%E7%BB%9F%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[uname：显示系统信息uname命令用于显示系统相关信息，比如内核版本号、硬件架构等。 语法格式：uname [option] 重要选项参数 【使用示例】 1）显示当前系统所有信息 12[root@C7-Server01 kkutysllb]# uname -aLinux C7-Server01 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux 2）显示当前服务器硬件架构 12[root@C7-Server01 kkutysllb]# uname -mx86_64 3）显示当前服务器主机名 12[root@C7-Server01 kkutysllb]# uname -nC7-Server01 4）显示内核发行版本 12[root@C7-Server01 kkutysllb]# uname -r3.10.0-957.10.1.el7.x86_64 5）显示当前服务器处理器类型 12[root@C7-Server01 kkutysllb]# uname -p x86_64 6）显示当前服务器硬件平台 12[root@C7-Server01 kkutysllb]# uname -ix86_64 hostname：显示或设置系统的主机名hostname命令用于显示或设置系统的主机名称。许多网络程序均用主机名来标识主机，若没有设置好主机名，则可能会导致网络服务不正常。 语法格式：hostname [option] 重要参数选项 【使用示例】 1）显示主机名 12[root@C7-Server01 kkutysllb]# hostnameC7-Server01 2）临时修改主机名 123[root@C7-Server01 kkutysllb]# hostname kkutysllb # 将主机名临时修改为kkutysllb[root@C7-Server01 kkutysllb]# hostnamekkutysllb 3）永久修改主机名 123[root@C7-Server01 kkutysllb]# hostnamectl set-hostname C7-Server01[root@C7-Server01 kkutysllb]# hostnamec7-server01 还有修改的方式就是修改/etc/hostname文件的内容： 4）配置主机名的DNS解析 1234567&gt; # 修改/etc/hosts文件，在其后追加如下配置&gt; &gt; [root@C7-Server01 kkutysllb]# echo "&gt; &gt; &gt; 192.168.101.81 C7-Server01&gt; &gt; " &gt;&gt; /etc/hosts&gt; stat：显示文件或文件系统状态stat命令用于详细显示文件或文件系统的状态信息。 语法格式：stat [option] [file] 重要选项参数 【使用示例】 1）查看文件的属性信息 123456789[root@C7-Server01 kkutysllb]# stat /etc/hostsFile: ‘/etc/hosts’Size: 187 Blocks: 8 IO Block: 4096 regular fileDevice: 803h/2051d Inode: 33589107 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2019-04-22 22:56:50.071492174 +0800Modify: 2019-04-22 22:56:43.218501936 +0800Change: 2019-04-22 22:56:43.218501936 +0800Birth: - 输出各项解释如下： size：文件大小 Blocks：占用block的数量 IO Blocks：Block总大小为4096（8*512） regular file：文件类型为普通文件 Device：设备编号的16进制和10进制 Inode：文件的inode值 Links：文件的硬链接数 Access：（0644/-rw-r–r–）：文件权限 Uid：文件归属用户 Gid：文件归属用户组 Access：文件的访问时间 Modify：文件的修改时间 Change：文件状态改变时间 2） 查看文件系统属性 12345678# 查看/etc/hosts文件所在分区的文件系统属性[root@C7-Server01 kkutysllb]# stat -f /etc/hostsFile: "/etc/hosts"ID: 80300000000 Namelen: 255 Type: xfsBlock size: 4096 Fundamental block size: 4096Blocks: Total: 10902067 Free: 10223852 Available: 10223852Inodes: Total: 21814784 Free: 21699625 显示/etc/hosts文件所在分区的文件系统类型为xfs，Blocks和Inodes的占用情况 3）使用指定格式输出文件内容 1234567891011121314[root@C7-Server01 kkutysllb]# stat -c %a /etc/hosts # %a显示文件的10进制格式权限644[root@C7-Server01 kkutysllb]# stat -c %A /etc/hosts # %A显示文件的可读格式权限-rw-r--r--[root@C7-Server01 kkutysllb]# stat -c %o /etc/hosts # %o显示文件IO块数量4096[root@C7-Server01 kkutysllb]# stat -c %n /etc/hosts # %n显示文件名/etc/hosts[root@C7-Server01 kkutysllb]# stat -c %i /etc/hosts # %i显示文件的inode值33589107[root@C7-Server01 kkutysllb]# stat -c %b /etc/hosts # %b显示文件占用的block块数量8[root@C7-Server01 kkutysllb]# stat -c %B /etc/hosts # %B显示文件占用的block块单位大小512 du：统计磁盘空间使用情du命令可以用于统计磁盘空间的使用情况，这个命令有助于我们找出哪个文件过多地占用了磁盘空间。 语法格式：du [option] [file] 重要选项参数 【使用示例】 1）通过参数-a显示所有当前目录或文件所占空间 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@C7-Server01 kkutysllb]# du -a4 ./.bash_logout4 ./.bash_profile4 ./.bashrc4 ./.bash_history0 ./mytest0 ./data/stu010 ./data/stu02/test010 ./data/stu02/test020 ./data/stu02/test030 ./data/stu02/test040 ./data/stu020 ./data/stu03/test010 ./data/stu03/test020 ./data/stu03/test030 ./data/stu03/test040 ./data/stu030 ./data0 ./image0080 ./image0090 ./202012312234.550 ./image0100 ./data_tmp/stu010 ./data_tmp/stu02/test010 ./data_tmp/stu02/test020 ./data_tmp/stu02/test030 ./data_tmp/stu02/test040 ./data_tmp/stu020 ./data_tmp/stu03/test010 ./data_tmp/stu03/test020 ./data_tmp/stu03/test030 ./data_tmp/stu03/test040 ./data_tmp/stu030 ./data_tmp4 ./hard_link0 ./soft_link4 ./data0014 ./data0024 ./test.sh4 ./data0034 ./data0044 ./test01.sh0 ./test014 ./data00552 . 上面占用大小单位的K字节。 2）显示当前目录的总大小 1234[root@C7-Server01 kkutysllb]# du -s52 .[root@C7-Server01 kkutysllb]# du -sh52K . 3）显示指定层次的目录大小 12345678910111213[root@C7-Server01 kkutysllb]# du -h --max-depth=1 /usr/local0 /usr/local/bin0 /usr/local/etc0 /usr/local/games0 /usr/local/include0 /usr/local/lib0 /usr/local/lib640 /usr/local/libexec0 /usr/local/sbin0 /usr/local/share0 /usr/local/src182M /usr/local/python3182M /usr/local 4）显示/usr/local下的第一和第二层子目录大小，但是不含python3 123456789101112131415[root@C7-Server01 kkutysllb]# du -h --max-depth=2 --exclude=/usr/local/python3 /usr/local0 /usr/local/bin0 /usr/local/etc0 /usr/local/games0 /usr/local/include0 /usr/local/lib0 /usr/local/lib640 /usr/local/libexec0 /usr/local/sbin0 /usr/local/share/applications0 /usr/local/share/info0 /usr/local/share/man0 /usr/local/share0 /usr/local/src0 /usr/local date：显示与设置系统时间date命令用于显示当前的系统时间或设置系统时间。 语法格式：date [option] [+FORMAT] 重要参数选项 【使用示例】 1）常用时间格式测试 123456789101112131415161718[root@C7-Server01 kkutysllb]# date +%y # 显示年份（短格式）19[root@C7-Server01 kkutysllb]# date +%Y # 显示年份（长格式）2019[root@C7-Server01 kkutysllb]# date +%m # 显示月份04[root@C7-Server01 kkutysllb]# date +%d # 显示日期22[root@C7-Server01 kkutysllb]# date +%H # 显示小时23[root@C7-Server01 kkutysllb]# date +%M # 显示分44[root@C7-Server01 kkutysllb]# date +%S # 显示秒52[root@C7-Server01 kkutysllb]# date +%F # 显示标准日期格式（年-月-日）2019-04-22[root@C7-Server01 kkutysllb]# date +%T # 显示标准时间格式（时:分:秒）23:45:02 2）通过参数-d显示指定字符串所描述的时间示例 12345678910[root@C7-Server01 kkutysllb]# date +%F -d '-1day' # 显示昨天2019-04-21[root@C7-Server01 kkutysllb]# date +%F -d '+1day' # 显示明天2019-04-23[root@C7-Server01 kkutysllb]# date +%F -d '1month' # 显示1个月后2019-05-22[root@C7-Server01 kkutysllb]# date +%F -d '1year' # 显示1年后2020-04-22[root@C7-Server01 kkutysllb]# date +%F -d '24hour' # 显示24小时后2019-04-23 3）时间格式转换 12[root@C7-Server01 kkutysllb]# date -d "Thu Jul 6 21:41:16 CST 2017" "+%Y-%m-%d %H:%M:%S"2017-07-06 21:41:16 -d选项后面接上需要转化的时间，最后再接上你想要输出的时间格式。 4）设置系统时间 12[root@C7-Server01 kkutysllb]# date -s "Mon Apr 22 23:56:53 CST 2019"Mon Apr 22 23:56:53 CST 2019 whereis：显示命令及其相关文件全路径whereis命令用于定位指定命令的可执行文件、源码文件及man帮助文件的路径。whereis命令用于在PATH环境变量里查找指定的命令。 语法格式：whereis [option] [filename] 重要选项参数 【使用范例】 1）将与python相关的文件都查找出来 12[root@C7-Server01 kkutysllb]# whereis pythonpython: /usr/bin/python /usr/bin/python2.7 /usr/bin/python2.7-config /usr/bin/python.bak /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 /usr/share/man/man1/python.1.gz 2）只查找python相关的可执行文件 12[root@C7-Server01 kkutysllb]# whereis -b pythonpython: /usr/bin/python /usr/bin/python2.7 /usr/bin/python2.7-config /usr/bin/python.bak /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 3）查找python的man帮助文件 12[root@C7-Server01 kkutysllb]# whereis -m pythonpython: /usr/share/man/man1/python.1.gz 4）查找python的源代码文件 12[root@C7-Server01 kkutysllb]# whereis -s pythonpython: # 为空表示没有找到 如果只是想查找命令的全路径，Linux中还有个which命令更常用，请大家自行研究。 who：显示已登录用户信息who命令能够显示已经登录系统的用户，以及系统的启动时间等信息。 语法格式：who [option] 重要选项参数 【使用示例】 1）显示已登录用户信息的不同参数实践例子 123456789[root@C7-Server01 kkutysllb]# whoroot pts/0 2019-04-22 22:53 (192.168.101.1)[root@C7-Server01 kkutysllb]# who -b system boot 2019-04-22 22:53[root@C7-Server01 kkutysllb]# who -lLOGIN tty1 2019-04-22 22:53 9468 id=tty1[root@C7-Server01 kkutysllb]# who -HNAME LINE TIME COMMENTroot pts/0 2019-04-22 22:53 (192.168.101.1) 2）显示最全的登录用户的信息 123456[root@C7-Server01 kkutysllb]# who -H -aNAME LINE TIME IDLE PID COMMENT EXITsystem boot 2019-04-22 22:53LOGIN tty1 2019-04-23 00:17 10515 id=tty1run-level 3 2019-04-22 22:53root + pts/0 2019-04-22 22:53 . 10337 (192.168.101.1) df：报告文件系统磁盘空间的使用情况显示文件系统磁盘空间的使用情况。如果不指定命令后面的文件参数，则会显示所有磁盘分区的使用情况，如果给定文件，则显示此文件所在磁盘分区的使用情况。 语法格式：df [option] [file] 重要选项参数 【使用示例】 1）显示磁盘的使用情况 123456789[root@C7-Server01 kkutysllb]# dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/sda3 43608268 2712880 40895388 7% /devtmpfs 3984384 0 3984384 0% /devtmpfs 3995140 0 3995140 0% /dev/shmtmpfs 3995140 11920 3983220 1% /runtmpfs 3995140 0 3995140 0% /sys/fs/cgroup/dev/sda1 406180 164980 241200 41% /boottmpfs 799032 0 799032 0% /run/user/0 2）以人类可读的方式显示磁盘的使用情况 123456789[root@C7-Server01 kkutysllb]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0 3）显示磁盘inode的使用情况 123456789[root@C7-Server01 kkutysllb]# df -hiFilesystem Inodes IUsed IFree IUse% Mounted on/dev/sda3 21M 113K 21M 1% /devtmpfs 973K 399 973K 1% /devtmpfs 976K 1 976K 1% /dev/shmtmpfs 976K 1.3K 975K 1% /runtmpfs 976K 16 976K 1% /sys/fs/cgroup/dev/sda1 200K 335 200K 1% /boottmpfs 976K 1 976K 1% /run/user/0 4）显示指定文件系统类型的磁盘 1234[root@C7-Server01 kkutysllb]# df -t xfsFilesystem 1K-blocks Used Available Use% Mounted on/dev/sda3 43608268 2712880 40895388 7% //dev/sda1 406180 164980 241200 41% /boot 5）列出系统文件系统的类型 123456789[root@C7-Server01 kkutysllb]# df -TFilesystem Type 1K-blocks Used Available Use% Mounted on/dev/sda3 xfs 43608268 2712880 40895388 7% /devtmpfs devtmpfs 3984384 0 3984384 0% /devtmpfs tmpfs 3995140 0 3995140 0% /dev/shmtmpfs tmpfs 3995140 11920 3983220 1% /runtmpfs tmpfs 3995140 0 3995140 0% /sys/fs/cgroup/dev/sda1 xfs 406180 164980 241200 41% /boottmpfs tmpfs 799032 0 799032 0% /run/user/0 top：实时显示系统中各个进程的资源占用状况top命令用于实时地对系统处理器状态进行监控，它能够实时地显示系统中各个进程的资源占用状况。该命令可以按照CPU的使用、内存的使用和执行时间对系统任务进程进行排序显示，同时top命令还可以通过交互式命令进行设定显示。 语法格式：top [option] 重要选项参数 交互式命令 【使用示例】 1）显示进程信息 1[root@C7-Server01 kkutysllb]# top 2）显示多核不同核CPU的信息 在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况。 图中显示系统有4个逻辑CPU 3）将进程按照批处理方式排序 1[root@C7-Server01 kkutysllb]# top -b 4）显示进程的完整路径 1[root@C7-Server01 kkutysllb]# top -c 5）显示指定的进程 1[root@C7-Server01 kkutysllb]# top -p 10560 6）交互式例子 默认进入top命令模式，各进程是按照CPU的使用量来排序的 场景1：敲击键盘“b”和“x” 场景2：敲击键盘“z”和“x” 场景3：通过“&gt;”或“&lt;”可以向右或向左改变排序列 敲击”&gt;”变为按内存排序 一致敲击“&lt;”变为按PID排序]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-21-x86架构基础]]></title>
    <url>%2F2019%2F04%2F21%2F2019-04-21-x86%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[标准服务器技术是网络功能虚拟化（NFV）实现的一个关键因素，了解一些x86架构的基础知识对大家后续了解电信云关键技术，尤其是掌握虚拟化技术原理和关键优化方案是必须具备的。本文主要从x86架构的CPU指令集增强，内存管理、中断和异常、IO架构等部分进行阐述，以及包含一些基础IT的基本概念的讲解。 x86-64指令集的增强Intel 的x86体系结构是世界上最流行的处理器架构，从1978年8086/8088处理器问世到现在的Core i7和Core i9，以及Xeon系列处理器，Intel x86体系结构已经在CPU领域叱咤40多年。 x86-64是x86架构的延伸产品，是一种64位微处理器架构及其相应的指令集。在x86-64出现以前，Intel与惠普联合推出IA-64架构，此架构不与x86兼容，且市场反应冷淡。于是，与x86兼容的x86-64架构应运而生。1999年，AMD 首次公开64位指令集为IA-32 提供扩展，称为x86-64（后来改名为AMD64）。此架构后来也为Intel 所采用，也就是现在的Intel 64。 x86-64能有效地把x86架构移植到64位环境，并且兼容原有的x86应用程序，市场前景广阔。外界使用x84-64或者x64称呼这个64位架构，以保持中立，不偏袒任何一家厂商。 AMD 64指令集主要特点有：支持64 位通用寄存器、64 位整数及逻辑运算和64 位虚拟地址。 Intel 64架构加入了额外的寄存器和其他改良的指令集，可使处理器直接访问超过4GB的内存，允许运行更大的应用程序。通过64位的存储器地址上限，其理论存储器容量上限达16EB，目前大多数操作系统和应用程序已基本支持完整的64位地址。 x86的内存架构硬件架构中最复杂、最核心的部分就是其内存架构。此部分详细内容因为篇幅有限无法详细展开，面向的人员主要包括CPU架构设计、操作系统开发和内核底层优化等领域，至于运维方面后期如果不做内核优化的同事了解即可，如感兴趣可参考《手把手教你设计CPU-RISC-V处理器》、《嵌入式操作系统原理》和《处理器虚拟化技术》等书籍。 地址空间地址空间是所有可用资源的集合，我们姑且将它看做一个大大的数组，那么地址就是这个数组的索引。地址空间可以划分为物理地址空间和线性地址空间两大类。 （1）物理地址空间 硬件平台通常划分为CPU、内存和其他硬件设备三个部分。其中，CPU 是整个硬件平台的主导者，内存和其他硬件设备都是CPU 可以使用的资源。这些资源组合在一起，分布在CPU的物理地址空间内，CPU使用物理地址索引这些资源。物理地址空间的大小由CPU实现的物理地址位数所决定，物理地址位数由CPU经过MMU（Memory Management Unit，内存管理单元）转换后的外地址总线位数决定。外地址总线位数与CPU处理数据的能力（即CPU 位数）没有必然的联系，例如：16位的8086 CPU具有20位地址空间。 一个硬件平台只有一个物理地址空间，但每个程序都认为自己独享整个平台的硬件资源。为了让多个程序能够有效地相互隔离，也为了它们能够有效地使用物理地址空间的资源，引入了线性地址空间的概念。 （2）线性地址空间 线性地址空间的大小由CPU实现的线性地址位数决定，线性地址位数由CPU的内地址总线位数决定。由于CPU的内地址总线与CPU的执行单元直连，所以，内地址总线位数往往与CPU位数一致，如果是32 位处理器，那么它就实现了32 位线性地址，其线性地址空间为4GB，如果是64位处理器，那么它的线性地址空间的为2的64次方，即16384GB。需要注意的是，线性地址空间的大小与物理地址空间的大小没有必然联系，Intel的PAE平台具有4GB 的线性地址空间，而其物理地址空间为64GB。但是，线性地址空间会被映射到某一部分物理地址空间或整个物理地址空间。也就是说，线性地址空间小于等于物理地址空间。 CPU负责将线性地址空间转换成物理地址空间，保证程序能够正确访问到该线性地址空间所映射到的物理地址空间。在现代操作系统中，每个进程通常都拥有自己的私有线性地址空间。一个典型的线性地址空间构造如下图所示。 地址地址是访问地址空间的索引。根据访问地址空间的不同，索引可以分为物理地址和线性地址。但由于x86特殊的段机制，还存在一种额外的地址—逻辑地址。 （1）逻辑地址 逻辑地址是程序直接使用的地址（x86无法禁用段机制，逻辑地址一直存在）。逻辑地址由一个16位的段选择符和一个32位的偏移量（32位平台）构成。下面以具体程序为例进行解释。比如：我们写下面一段c语言代码： 12int a = 100; # 定义一个整型变量aint *p = &amp;a; # 定义一个整型指针p，指向变量a在内存中的地址 上述语句中的指针变量p存储的就是变量a的逻辑地址。实际上，p中存储的仅是逻辑地址的偏移部分，而偏移对应的段选择符位于段寄存器中，并没有在程序中显示。 （2）线性地址 线性地址又称虚拟地址。线性地址是逻辑地址转换后的结果，用于索引线性地址空间。当CPU使用分页机制时，还需要将线性地址转换成物理地址才能访问物理平台内存或其他硬件设备；当分页机制未启用时，线性地址与物理地址相同。 （3）物理地址 物理地址是物理地址空间的索引，是CPU提交到总线用于访问物理平台内存或其他硬件设备的最终地址，在x86下，物理地址有时也被称为总线地址。 根据上面的描述，我们可以总结如下： 分段机制启用，分页机制未启用：逻辑地址—&gt;线性地址=物理地址 分段机制、分页机制同时启用：逻辑地址—&gt;线性地址—&gt;物理地址 x86内存管理机制x86架构的内存管理机制分为两部分：分段机制和分页机制。分段机制为程序提供彼此隔离的代码区域、数据区域、栈区域，从而避免了同一个处理器上运行的多个程序互相影响。 分页机制实现了传统的按需分页、虚拟内存机制，可以将程序的执行环境按需映射到物理内存。此外，分页机制还可以用于提供多任务的隔离。 分段机制和分页机制都可以通过配置，支持简单的单任务系统、多任务系统或共享内存的多处理器系统。需要强调的一点是，处理器无论在何种运行模式下都不可以禁止分段机制，但是分页机制却是可选选项。 （1）分段机制 分段机制是x86架构下的朴素内存管理机制，不可以禁用。了解分段机制有利于对后续内存虚拟化原理和优化方案有更深的了解。 分段机制将内存划分成以基地址（Base）和长度（Limit）描述的块。段可以与程序最基本的元素联系起来，程序可以简单地划分为代码、数据和栈，段机制就有相应的代码段、数据段和栈段。 一个程序根据分段机制在内存中由逻辑地址、段选择符、段描述符和段描述符表4 个基本部分构成。 1）当程序使用逻辑地址访问内存的某个部分时，CPU通过逻辑地址中的段选择符索引段描述符表，进而得到该内存对应的段描述符（段描述符描述段的基地址、长度以及读/写、访问权限等属性信息） 2）根据段描述符中的段属性信息检测程序的访问是否合法，如果合法，再根据段描述符中的基地址将逻辑地址转换为线性地址。 这个流程可以用如下图示进行总结。 段选择符是逻辑地址的一个组成部分，用于索引段描述符表以获得该段对应的段描述符。段选择符作为逻辑地址的一部分，对应用程序是可见的。但是，正如前面在逻辑地址中介绍的，应用程序中只存储和使用逻辑地址的偏移部分，段选择符的修改和分配由连接器和加载器完成。 为了使CPU能够快速地获得段选择符，x86架构提供了6个段寄存器存放当前程序中各个段的段选择符。这6 个段寄存器分别如下： CS（Code-Segment，代码段）：存放代码段的段选择符。 DS（Data-Segment，数据段）：存放数据段的段选择符。 SS（Stack-Segment，栈段）：存放栈的段选择符。 ES、FS、GS：可以存放额外三个数据段的段选择符，由程序自由使用。 由于段选择符的存在最终是为了索引段描述符表中的段描述符，为了加速段描述符的访问，x86架构在不同的段寄存器后增加了一个程序不可见的段描述符寄存器。当相应段寄存器中加入一个新的段选择符后，CPU自动将该段选择符索引的段描述符加载到这个不可见的段描述符寄存器中。各个段寄存器的构造如下。 段描述符描述某个段的基地址、长度以及各种属性（例如，读/写属性、访问权限等）。这是分段机制的核心思想。当CPU通过一个逻辑地址的段选择符获得该段对应的段描述符后，会使用段描述符中各种属性字段对访问进行检查，一旦确认访问合法，CPU将段描述符中的基地址和程序中逻辑地址的偏移量相加就得到程序的线性地址。 正如前面讲到的，x86架构在每个段寄存器后增加了一个程序不可见的段描述符寄存器，每当段寄存器加入一个新的段选择符后，CPU自动将该段选择符索引的段描述符加载到这个段描述符寄存器中。后续只要不发生段寄存器的更新操作，CPU就不再查询段描述符表而是直接使用这个段描述符寄存器中的值，从而加快CPU的执行效率。 x86架构提供了两种段描述符表：GDT（全局段描述符表Global Descriptor Table）和LDT（本地段描述符表Local Descriptor Table）。具体选择哪个段描述符表，由段选择符中的TI字段决定，当TI=0时，索引GDT，当TI=1时索引LDT。系统中至少有一个GDT可以被所有的进程访问。与此同时，系统中可以有一个或多个LDT，可以被某个进程私有，也可以被多个进程共享。 GDT是内存中的一个数据结构。简单地讲，可以将GDT看成是一个数组，由基地址（Base）和长度（Limit）描述。 LDT是一个段，需要用一个段描述符来描述。LDT的段描述符存放在GDT中，当系统中有多个LDT时，GDT中必须有对应数量的段描述符。 为了加速对GDT和LDT的访问，x86架构提供了GDTR寄存器和LDTR寄存器。关于这两种寄存器的具体描述如下： GDTR：包括一个32位的基地址（Base）和一个16 位的长度（Limit）。 LDTR：结构与段寄存器相同（同样包含对程序不可见的段描述符寄存器）。 通过段选择符索引GDT/LDT的过程如下图所示： x86架构内存管理中分段机制总结： 1）在程序加载阶段，该进程LDT的段选择符首先索引GDT，获得LDT的段描述符并将其加载到LDTR寄存器中。此外，该进程的CS、DS、SS中加入相应的段选择符，CPU根据段选择符的TI字段索引相应的段描述符表，获得相应的段描述符，并加载入CS、DS、SS对应的程序不可见的段描述符寄存器。 2）程序执行到读/写内存中的数据时，把程序中相应变量的逻辑地址转换为线性地址： 进行必要的属性、访问权限检查； 从DS对应的段描述符寄存器获得该段的基地址； 将变量的32位偏移量和段描述符中的基地址相加，获得该变量的线性地址。 2）分页机制 分段机制的目的是将内存中的线性地址空间划分成以基地址和长度描述的多个段进行管理，程序对应的逻辑地址以基地址和偏移量来描述，实现逻辑地址到线性地址空间的映射。而分页机制是使用单位“页”来管理线性地址空间和物理地址空间的映射关系。同时，分页机制允许一个页面存放在物理内存中或磁盘的交换区域（如Linux下的Swap分区，Windows下的虚拟内存文件）中，程序可以使用比 机器物理内存更大的内存区域，从而实现现代操作系统中虚拟内存机制。（注意：操作系统的虚拟内存原理和映射关系和后面要讲的计算虚拟化技术中内存虚拟化技术基本一致，只是VMM实现时又多了一层嵌套）。 在x86架构下，页的典型大小为4KB，于是一个4GB的线性地址空间被划分成1024×1024个页面，参见本文线性地址空间示意图。物理地址空间的划分与此类似。x86架构允许大于4KB的页面大小（如2MB、4MB、1GB）等。 分页机制的核心思想是通过页表将线性地址转换为物理地址，并配合旁路转换缓冲区（Translation Lookaside Buffer，后面简称为TLB）来加速地址转换的过程。分页机制主要由页表、CR3 寄存器和TLB三个部件构成，如下图所示。 页表是用于将线性地址转换成物理地址的主要数据结构。一个地址对齐到页边界后的值称为页帧号（或者页框架），它实际上就是该地址所在页面的基地址。比如：一个页大小为4kB，那么第一个页帧号就是0，第二个页帧号就是4097，依次类推。线性地址对应的页帧号叫做虚拟页帧号（Virtual Frame Number，下面简称为VFN），物理地址对应的页帧号叫做物理页帧号（Physical Frame Number，下面简称为PFN）或机器页帧号。页表实际上是存储VFN到PFN映射的数据结构。 在传统的32位的保护模式中（未启用物理地址扩展PAE功能），x86处理器使用两级转换方案，在这种方案中，CR3寄存器指向一个4KB大小的页目录表，页目录中共有1024个记录，每一项记录大小4B空间，都指向一个4KB大小的页表，页表中也有1024项，每项大小4B空间，所以，最后整个线性地址空间大小就是1024 个长为4KB的页，即总共4GB大小的空间。未启用PAE 的4KB大小的页面如下图所示。 页目录项（Page Directory Entry，下面简称为PDE），包含页表的物理地地址，PDE存放在页目录表中。 页表项（Page Table Entry，下面简称为PTE）：包含该线性地址对应的物理页帧号PFN，PTE存在页表中，确定物理页帧号PFN 后，再将线性地址的0~11位偏移量与其相加，就可以确定该线性地址对应的物理地址。 虚拟内存实现的关键在于PDE和PTE都包含一个P（Present）字段： 当P=1时，物理页面存在于物理内存中，CPU完成地址转换后可以直接访问该页面。 当P=0时，物理页面不在物理内存中（在硬盘的交换分区中），当CPU访问该页面时，会产生一个缺页错误中断，由操作系统的缺页处理机制将存放在硬盘上的页面调入物理内存，使访问可以继续。同时，由于程序的局部性特点，操作系统会将该页面附近的页面一起调入物理内存，方便CPU的访问。所以，为了减少内存占用，要求程序开发人员尽量少的使用全局索引或递归调用等机制。 P=0时的PDE和PTE的1~31位都将为操作系统提供物理页面在硬盘上的信息，这些位存储着物理页面在硬盘上的位置。 启用物理地址扩展（之后简称为PAE）后，页表结构将发生相应的变化。页表和页目录的总大小仍是4KB，但页表和页目录中的表项都从32位扩为64位，以使用附加的地址位。这样，页表和页目录都只有512个表项，变成了原来方案的一半，所以又加入了一个级：CR3 指向页目录指针表，即一个包含4个页目录指针的表。启用PAE 的4KB大小的页面使用的三级页表如下图所示： CR3寄存器也称为页目录基地址寄存器（Page-Directory Base Register，PDBR），存放着页目录的物理地址。一个进程在运行前，必须将其页目录的基地址存入CR3，而且，页目录的基地址必须对齐到4KB页边界。启用PAE时，CR3指向页目录指针表，每一项都指向一个页目录表，共有4个页目录表。 为了提高地址转换的效率，x86架构使用TLB对最近用到的页面映射进行缓存。TLB中存放着VFN到PFN的转换记录，当CPU访问某个线性地址时，如果其所在页面的映射存在于TLB中，无须查找页表，即可得到该线性地址对应的PFN，CPU 再将它与线性地址的偏移相加，就能得到最后的物理地址。 x86架构内存管理中心分页机制总结： 1）CPU访问一个线性地址，在TLB中进行匹配，如果地址转换在TLB中，则跳到步骤6。否则，发生了一次TLB Miss（TLB 缺失），继续步骤2。 2）查找页表，如果页面在物理内存中，则跳到步骤4。 3）如果页面不在物理内存中，则产生缺页错误，由操作系统的缺页错误处理程序进行以下处理。 将页面从磁盘复制到物理内存中。 更改对应的PTE，设置P 位为1，并对其他字段进行相应的设置。 刷新TLB 中对应的PTE。 从缺页错误处理程序中返回。 4）此时，页面已经存在于物理内存中，并且页表也已经包含了这个映射。重新在TLB中进行匹配，如果地址转换在TLB中，则跳到步骤6。否则，发生了一次TLB Miss（TLB 缺失），继续步骤5。 5）CPU重新查页表，把对应的映射插入到TLB中。 6）此时，TLB已经包含了该线性地址对应的PFN。将PFN和线性地址中的偏移量相加，就得到了对应的物理地址。 中断与异常程序的执行往往不只是按顺序执行那么简单，一些异常和中断会打断顺序执行的程序流，转而进入一条完全不同的执行路径。中断提供给外部设备一种“打断CPU当前执行任务，并响应自身服务”的手段。中断(interrupt)是异步的事件，典型的比如由I/O设备触发；异常(exception)是同步的事件，典型的比如处理器执行某条指令时发现出错了等等，其实异常的本质就是同步中断。 中断通常被定义为一个打断CPU芯片指令执行的事件，该事件对应到CPU芯片内部或者外部的电路产生的电子信号。 中断信号可以被划分为同步中断和异步中断： 同步中断，该类型中断由CPU的控制单元在执行指令的时候产生，并且是在当前指令执行完毕下一个指令执行之前产生。 异步中断，该类型中断由其他硬件设备在任意的时间产生，并且遵循CPU的时钟信号传递给CPU。 对于Intel的CPU而言，它将同步中断称作异常，而将异步中断称作中断。 通常中断（即异步中断）由时钟定时器或者其他I/O设备产生，如键盘接收到敲击某个按键的信号后产生的中断信号。而异常（即同步中断）则通常由于编程错误或者由CPU检测到异常条件需要内核进行处理而产生，如上面讲到的Page Fault Exception（缺页异常），异常可以由程序通过int或者sysenter指令主动产生。 对于Intel x86 CPU而言，它将中断和异常进行了如下归类： 中断，即异步中断，中断信息随着CPU的时钟信号传递到CPU内部。中断分为可屏蔽中断和不可屏蔽中断两类。 可屏蔽中断，所有由I/O设备产生的IRQ请求都被归为可屏蔽中断。一个可屏蔽中断可以有两种状态，屏蔽或者不屏蔽，当一个中断被屏蔽时，该中断信号将被对应的控制单元所忽略。 不可屏蔽中断，即控制单元无法忽略该类型的中断信号，CPU肯定会接收到该类型的中断，一般对应到一些紧要的事件，比如硬件错误。 异常，即同步中断，中断信号在CPU执行完某个指令后产生并接收到。处理器检测到的异常，即当CPU执行指令的时候检测到硬件上存在一些异常条件的时候就会产生该信号。这种类型的异常根据产生时在内核堆栈中保存的EIP寄存器的值（即异常恢复后CPU重新执行的位置）进行细分： Faults，该异常可以被内核正确纠正，并且纠正后重新执行引起该异常的指令时不会造成程序的中断或者功能的异常。这时候保存到EIP寄存器的值是引起异常的指令的地址，故异常恢复的时候会重新执行该指令，如Page Fault Exception（缺页异常），当访问的内存地址没有被映射到物理内存时，产生异常，内核分配新的物理内存页并建立映射关系，然后异常处理完毕后，CPU重新访问该地址，即可访问到正确的物理内存。 Traps，该异常发生时，内核堆栈EIP寄存器保存的地址指向引起该异常的指令的下一条指令，即当该异常处理返回后会继续程序的执行，而不是重新执行引起异常的指令。x86 CPU的硬件虚拟化功能就是利用陷入（Traps）再模拟的方法，当CPU执行虚拟机指令的时候，如果执行的是敏感指令，就会触发Traps类型的异常，让VMM（Virtual Machine Monitor）对该敏感指令进行模拟，然后继续恢复虚拟机的运行。 Aborts，当发生严重的错误时，CPU已经无法保证内核堆栈中EIP寄存器存放的值是引起该异常的指令的地址。该异常用于汇报严重的错误，如硬件错误或者是内存的不一致性。该异常信号让CPU切换到相应的abort exception handler，该处理函数由于无法确认错误，只能结束当前进程。 我们在写程序时，经常会在容易产生错误的地方进行异常抛出，然后针对抛出的异常定义执行策略。这类编程产生的异常，由程序主动执行int或者int3之类的指令产生。CPU像处理Traps一样处理这些程序主动产生的异常，该类异常通常被称为软件中断（software interrupt）。这类异常主要有两种用途：实现系统调用和通知某个debugger特定的事件发生。 这些异常或中断由0~255的数字唯一标识，也就是经常说的中断信号量。对于不可屏蔽中断和异常来说，相应的中断信号量是固定的，而可屏蔽中断对应的中断信号量则可以通过设置中断控制器来更改。 x86系统的I/O架构计算机所处理的任务其实只有两种：CPU运算和I/O操作。这部分内容是后续学习计算虚拟化中I/O虚拟化的基础。I/O（输入/输出）是CPU访问外部设备的方法。设备通常通过寄存器和设备RAM将自身功能展现给CPU，CPU通过读/写这些寄存器和RAM完成对设备的访问及其他操作。按访问方式的不同，x86架构的I/O分为如下两类： 1）端口I/O（后文简称为Port I/O）：即通过I/O端口访问设备寄存器。x86有65536个8位的I/O端口，编号为0x0~0xFFFF。CPU将端口号作为设备端口的地址，进而对设备进行访问。这65536个端口构成了64KB的I/O端口地址空间。I/O端口地址空间是独立的，不是线性地址空间或物理地址空间的一 部分。需要使用特定的操作命令IN/OUT对端口进行访问，此时CPU通过一个特殊的芯片管脚标识这是一次I/O端口访问，于是芯片组知道地址线上的地址是I/O端口号并相应地完成操作。此外，2个或4个连续的8位I/O端口可以组成16位或32位的I/O端口。 2）内存映射I/O（Memory Map I/O，后文简称为MMIO）：即通过内存访问的形式访问设备寄存器或设备RAM。MMIO要占用CPU的物理地址空间，它将设备寄存器或设备RAM映射到物理地址空间的某段地址，然后使用MOV等访存指令访问此段地址，即可访问到映射的设备。MMIO方式访问设备也需 要进行线性地址到物理地址的转换，但是这个转换过程中的MMIO地址不可缓存到TLB中。MMIO是一种更普遍、更先进的I/O访问方式，很多CPU 架构都没有Port I/O，采用统一的MMIO方式。 DMA技术直接内存访问（Direct Memory Access，后文简称为DMA）是所有现代计算机的重要特色。DMA允许设备绕开CPU直接向内存中复制或读取数据。如果设备向内存复制数据都经过CPU，则CPU会有大量中断负载，中断过程中，CPU对其他任务来讲无法使用，不利于系统性能的提高。通过DMA，CPU只负责初始化这个传输动作，而传输动作本身由DMA 控制器（简称为DMAC）来实行和完成。在实现DMA传输时，由DMAC直接控制总线，在DMA传输前，CPU要把总线控制权交给DMAC，结束DMA传输后，DMAC立即把总线控制权交回给CPU。 一个完整的DMA 传输过程的基本流程如下： 1）DMA请求：CPU对DMAC进行初始化，并向I/O端口发出操作命令，I/O端口提出DMA请求。 2）DMA响应：DMAC对DMA请求进行优先级判别和屏蔽判别，然后向总线控制芯片提出总线请。CPU执行完当前总线周期后释放总线控制权。此时，总线控制芯片发出总线应答，表示DMA请求已被响应，并通过DMAC通知I/O端口开始DMA传输。 3）DMA传输：DMAC获得总线控制权后，CPU即可挂起或只执行内部操作，由DMAC发出读/写命令，直接控制RAM与I/O端口进行DMA传输。 4）DMA结束：当完成规定的成批数据传送后，DMAC释放总线控制权，并向I/O端口发出结束信号。当I/O端口接收到结束信号后，停止I/O设备的工作并向CPU提出中断请求，使CPU执行一段检查本次DMA传输操作正确性判断的代码，并从不介入的状态退出。 由此可见，DMA无须CPU直接控制传输，也没有中断处理方式那样保留现场和恢复现场的过程，通过硬件（DMAC）为RAM与I/O设备开辟了一条直接传送数据的通路，极大地提高了CPU效率。需要注意的是，DMA操作访问的必须是连续的物理内存。DMA 传输的过程如下图所示。 进程、线程和协程什么是进程和线程 进程是什么呢？大白话讲，进程就是应用程序的启动实例。比如我们运行一个游戏，打开一个软件，就是开启了一个进程。进程拥有代码和打开的文件资源、数据资源、独立的内存空间。 线程又是什么呢？线程从属于进程，是程序的实际执行者。一个进程至少包含一个主线程，也可以有更多的子线程。线程拥有自己的栈空间。 对操作系统来说，线程是最小的执行单元，进程是最小的资源管理单元。无论进程还是线程，都是由操作系统所管理的。线程一般具有五种状态：初始化&gt;&gt;&gt;可运行&gt;&gt;&gt;运行中&gt;&gt;&gt;阻塞&gt;&gt;&gt;销毁。线程不同状态之间的转化均需要CPU开销来完成。 什么是协程协程 携程英文Coroutines，是一种比线程更加轻量级的存在。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。最重要的是，协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行）。这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。 在Python语言中有个生成器的概念，里面有个关键字yield，当程序执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。但是，yield让程序暂停，和线程的阻塞是有本质区别的。通过yield关键字的暂停完全由程序控制，线程的阻塞状态是由操作系统内核来进行切换。大家可以在Python脚本中写入如下代码并执行体验下： 1234567891011def consume(): while True: # consume等待接收数据 number = yield print("我要执行啦。。。。开始计数：",number)consumer = consume()next(consumer)for num in range(0,100): print("开始执行：",num) consumer.send(num) 在《流畅的Python》一书中还有个例子，可以更好地来说明协程的特点。如下面示例是一个计算移动平均值的协程函数。实现的功能说明如下： 1）函数体中是一个无限循环，表明只要调用方不断把值发给这个协程，它就会一直接收值，然后生成结果。仅当调用方在协程上调用 .close() 方法，或者没有对协程的引用而被垃圾回收程序回收时，这个协程才会终止。 2）yield 表达式用于暂停执行协程，把结果发给调用方；还用于接收调用方后面发给协程的值，恢复无限循环。 使用协程的好处是，total 和 count 声明为局部变量即可，无需使用实例属性或闭包在多次调用之间保持上下文。 123456789101112131415161718192021def averager(): total = 0.0 count = 0 average = None while True: term = yield average total += term count += 1 average = total/count# 创建协程对象coro_avg = averager()# 调用next函数，激活协程next(coro_avg)# 计算移动平均值：多次调用 .send(...) 方法，产出当前的平均值print(coro_avg.send(10)) # 10.0print(coro_avg.send(20)) # 15.0print(coro_avg.send(30)) # 20.0print(coro_avg.send(50)) # 27.5]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-20-Linux系统命令-第三篇《文件过滤及内容编辑处理》]]></title>
    <url>%2F2019%2F04%2F20%2F2019-04-20-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%B8%89%E7%AF%87%E3%80%8A%E6%96%87%E4%BB%B6%E8%BF%87%E6%BB%A4%E5%8F%8A%E5%86%85%E5%AE%B9%E7%BC%96%E8%BE%91%E5%A4%84%E7%90%86%E3%80%8B%2F</url>
    <content type="text"><![CDATA[cat：合并文件或查看文件内容cat命令可以理解为英文单词concatenate的缩写，其功能是连接多个文件并且打印到屏幕输出，或者重定向到指定的文件中。此命令常用来显示单个文件内容，或者将几个文件内容连接起来一起显示，还可以从标准输入中读取内容并显示，生产环境中它常与重定向或追加符号配合使用。 cat命令具备5大常用功能 语法格式：cat [option] [file] 重要选项参数 【使用示例】 1）在/home/kkutysllb目录下生成一个data001文件 123456789101112&gt; # EOF这里要按回车才能结束，另外，EOF必须成对出现，但也可以用别的成对标签来替换。&gt; &gt; [root@C7-Server01 kkutysllb]# cat &gt;&gt; data001 &lt;&lt; EOF&gt; &gt; &gt; day001...&gt; &gt; day002...&gt; &gt; day003...&gt; &gt; day004...&gt; &gt; ...&gt; &gt; day00n...&gt; &gt; EOF&gt; 123456789101112131415# 使用cat -n查看刚编辑生成的文件data001 [root@C7-Server01 kkutysllb]# cat -n data001 1 day001... 2 day002... 3 day003... 4 day004... 5 ... 6 day00n... 2）直接执行cat命令，不带任何选项 1234567# 查看当前目录下soft_link软链接文件的内容 [root@C7-Server01 kkutysllb]# cat soft_link 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 3）执行cat命令，分别带-n及-b选项，并对比区别 1234567891011121314&gt; # 新生成data002文件，输入以下内容，注意其中的空行要保留&gt; &gt; [root@C7-Server01 kkutysllb]# cat &gt;&gt; data002 &lt;&lt; EOF&gt; &gt; &gt; www.sina.com.cn&gt; &gt;&gt; &gt; www.baidu.com&gt; &gt;&gt; &gt; www.chinamobile.com&gt; &gt;&gt; &gt; sn.chinamobile.com&gt; &gt;&gt; &gt; EOF&gt; 1234567891011121314&gt; # 使用-n选项查看data002文件的内容&gt; &gt; # 文件内容中的空行也被标识行号&gt; &gt; [root@C7-Server01 kkutysllb]# cat -n data002&gt; 1 www.sina.com.cn&gt; 2 &gt; 3 www.baidu.com&gt; 4 &gt; 5 www.chinamobile.com&gt; 6 &gt; 7 sn.chinamobile.com&gt; 8 &gt; 12345678&gt; # 使用-b选项查看data002文件的内容，会忽略显示空白行行号&gt;&gt; [root@C7-Server01 kkutysllb]# cat -b data002&gt; 1 www.sina.com.cn&gt; 2 www.baidu.com&gt; 3 www.chinamobile.com&gt; 4 sn.chinamobile.com&gt; 3）执行cat命令，带-E选项 123456789101112131415# -E选项会在显示文件的每行行尾加上$符号 [root@C7-Server01 kkutysllb]# cat -E data001 day001...$ day002...$ day003...$ day004...$ ...$ day00n...$ 4）cat的一个特殊小用法（没基础的可以先大致了解，后续学了shell脚本后再看这部分） 1234567891011121314151617# 利用cat在shell脚本中显示帮助菜单 #!/bin/bash exportfs_usage() &#123; cat &lt;&lt; END USAGE: $0 &#123;start|stop|monitor|status|validate-all&#125; END&#125;exportfs_usage 执行上述脚本的输出如下： 为了实现自动化运维，有时运维人员不得不通过脚本把操作写好，然后让其他同事，通过傻瓜式的菜单选择，来完成相应的工作，进而提升工作效率。因此，cat命令在shell编程中常用来描写菜单输出操作。 小练习：其他选项用法请自行练习，将结果贴在讨论区。提示：与cat命令作用相反的命令为tac，其作用是方向显示文件的内容，请自行研究练习。 补充知识：LInux系统中的重定向 重定向简介 重定向是Linux中一个重要知识点，对于它的作用，直白点儿说，就是可以让数据从一个地方（文件或工具）无损地流到另一个地方。 标准输入/输出/错误输出 标准输入是一个名称，它表示数据的一个流入方向，通常表示数据从文件等流入到处理的工具或命令中，用代码0表示，使用&lt;或&lt;&lt;符号来指示数据朝箭头所指的方向流动。 标准输出也是一个名称，也表示数据的一个流入方向，通常用代码1表示，使用&gt;或&gt;&gt;符号来指示数据朝箭头的方向流动。和标准输入不同的是，1表示将命令等处理的一般信息输出到文件。 标准错误输出是另一个名称，也是表示数据的一个流入方向，通常用代码2表示，使用&gt;或&gt;&gt;符号来指示数据朝箭头的方向流动。和标准输出不同的是，标准错误输出2表示将错误的信息输出到文件等，不输出正确的普通信息（仅输出错误信息）。 more：分页显示文件内容more命令的功能类似于cat，但cat命令是将整个文件的内容一次性显示在屏幕上，而more则会一页一页地显示文件内容。但more的功能还是比较简单的，有一个增强版的命令是less，将在后面讲解。（其实，学会了less命令，大家就可以把more命令忘记了） 语法格式：more [option] [file] 重要选项参数 在交互模式下，使用more命令打开文本之后，会进入一个基于vi的交互界面，在这里可以使用部分vi编辑器的功能，如搜索功能，还可以切换到vi编辑器。常用操作方式如下： 【使用示例】 1）more命令后面不接任何参数 123# 显示/etc/services文件内容 [root@C7-Server01 kkutysllb]# more /etc/services 2）显示指定行数的内容 12345678910111213# 显示/etc/service文件的前5行内容[root@C7-Server01 kkutysllb]# more -5 /etc/services # /etc/services:# $Id: services,v 1.55 2013/04/14 ovasik Exp $## Network services, Internet style# IANA services version: last updated 2013-04-10 3）从指定的行数开始显示内容 12345678910111213141516171819202122232425262728293031323334353637383940414243# 从465行开始显示/etc/service文件的内容[root@C7-Server01 kkutysllb]# more -5 /etc/services # /etc/services:# $Id: services,v 1.55 2013/04/14 ovasik Exp $## Network services, Internet style# IANA services version: last updated 2013-04-10[root@C7-Server01 kkutysllb]# more +465 /etc/services bpjava-msvc 13722/tcp # BP Java MSVC Protocolbpjava-msvc 13722/udp # BP Java MSVC Protocolvnetd 13724/tcp # Veritas Network Utilityvnetd 13724/udp # Veritas Network Utilitybpcd 13782/tcp # VERITAS NetBackupbpcd 13782/udp # VERITAS NetBackupvopied 13783/tcp # VOPIED Protocolvopied 13783/udp # VOPIED Protocol# This port is registered as wnn6, but also used under the unregistered name# "wnn4" by the FreeWnn package.wnn6 22273/tcp wnn4wnn6 22273/udp wnn4quake 26000/tcpquake 26000/udpwnn6-ds 26208/tcpwnn6-ds 26208/udptraceroute 33434/tcptraceroute 33434/udp## Datagram Delivery Protocol services。。。 4）分页显示目录下的内容 12345678910111213141516171819202122232425# 分页显示/etc/目录下的内容，每页显示10行内容 [root@C7-Server01 kkutysllb]# ls -l /etc/|more -10 total 1108 -rw-r--r--. 1 root root 16 Apr 7 20:39 adjtime -rw-r--r--. 1 root root 1518 Jun 7 2013 aliases -rw-r--r--. 1 root root 12288 Apr 7 20:43 aliases.db drwxr-xr-x. 2 root root 236 Apr 7 20:35 alternatives -rw-------. 1 root root 541 Apr 11 2018 anacrontab -rw-r--r--. 1 root root 55 Apr 11 2018 asound.conf drwxr-x---. 3 root root 43 Apr 7 20:35 audisp drwxr-x---. 3 root root 83 Apr 7 20:43 audit drwxr-xr-x. 2 root root 33 Apr 7 20:35 bash_completion.d --More-- 。。。 小练习：more的其它用法请自行练习，可在讨论区进行讨论 less：分页显示文件内容如果使用man less查看less的帮助文档，会发现官方的解释是less为more的反义词（opposite of more）。但less命令的名称只是个文字游戏，它是more命令的高级版本（less这个名称来自俗语“越简单就越丰富”，即less is more）。 less命令的基本功能类似于more命令，可以分页显示文件内容，但比more的功能更强大。less命令在读取文件内容时，并不是像more、vi命令一样，要一次性将整个文件加载之后再显示，而是会根据需要来加载文件的内容，这样打开文件的速度会更快。而且less命令支持[page up]、[page down]等按键的功能，可以通过该功能向前或向后翻看文件，这样更容易查看一个文件的内容。 语法格式：less [option] [file] 重要选项参数 在交互模式下，less命令也是基于more命令和vi命令的，在这里可以使用vi编辑器的部分功能，如搜索功能，还可以切换到vi编辑器。表3-7给出了less命令的交互式子命令。 less交互式命令说明 【使用示例】 1）查看文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 使用less查看/etc/services文件内容[root@C7-Server01 kkutysllb]# less /etc/services # /etc/services:# $Id: services,v 1.55 2013/04/14 ovasik Exp $## Network services, Internet style# IANA services version: last updated 2013-04-10## Note that it is presently the policy of IANA to assign a single well-known# port number for both TCP and UDP; hence, most entries here have two entries# even if the protocol doesn't support UDP operations.# Updated from RFC 1700, ``Assigned Numbers'' (October 1994). Not all ports# are included, only the more common ones.## The latest IANA port assignments can be gotten from# http://www.iana.org/assignments/port-numbers# The Well Known Ports are those from 0 through 1023.# The Registered Ports are those from 1024 through 49151# The Dynamic and/or Private Ports are those from 49152 through 65535## Each line describes one service, and is of the form:## service-name port/protocol [aliases ...] [# comment]tcpmux 1/tcp # TCP port service multiplexertcpmux 1/udp # TCP port service multiplexerrje 5/tcp # Remote Job Entryrje 5/udp # Remote Job Entryecho 7/tcpecho 7/udpdiscard 9/tcp sink nulldiscard 9/udp sink nullsystat 11/tcp userssystat 11/udp usersdaytime 13/tcpdaytime 13/udp。。。 2）显示行号 123456789101112131415161718192021222324252627282930313233343536373839404142# 使用less -N查看文件/etc/services的内容[root@C7-Server01 kkutysllb]# less -N /etc/services 1 # /etc/services:2 # $Id: services,v 1.55 2013/04/14 ovasik Exp $3 #4 # Network services, Internet style5 # IANA services version: last updated 2013-04-106 #7 # Note that it is presently the policy of IANA to assign a single well-known8 # port number for both TCP and UDP; hence, most entries here have two entries9 # even if the protocol doesn't support UDP operations.10 # Updated from RFC 1700, ``Assigned Numbers'' (October 1994). Not all ports11 # are included, only the more common ones.12 #13 # The latest IANA port assignments can be gotten from14 # http://www.iana.org/assignments/port-numbers15 # The Well Known Ports are those from 0 through 1023.16 # The Registered Ports are those from 1024 through 4915117 # The Dynamic and/or Private Ports are those from 49152 through 6553518 #19 # Each line describes one service, and is of the form:20 #21 # service-name port/protocol [aliases ...] [# comment]22 23 tcpmux 1/tcp # TCP port service multiplexer24 tcpmux 1/udp # TCP port service multiplexer25 rje 5/tcp # Remote Job Entry26 rje 5/udp # Remote Job Entry27 echo 7/tcp28 echo 7/udp29 discard 9/tcp sink null30 discard 9/udp sink null31 systat 11/tcp users32 systat 11/udp users33 daytime 13/tcp34 daytime 13/udp35 qotd 17/tcp quote36 qotd 17/udp quote37 msp 18/tcp # message send protocol (historic)38 msp 18/udp # message send protocol (historic)39 chargen 19/tcp ttytst source 小练习：使用less分页显示/etc/目录下的内容。（提示：可参照more使用示例） head：显示文件内容头部head命令用于显示文件内容头部，它默认输出文件的开头10行。如果指定了多个文件，则在每一段输出前会给出文件名作为文件头。 语法格式：head [option] [file] 重要选项参数 【使用示例】 1）不带任何参数，默认显示文件的前10行内容 12345678910111213# 显示/etc/passwd文件的前10行内容[root@C7-Server01 kkutysllb]# head /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltmail:x:8:12:mail:/var/spool/mail:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologin 2）显示文件的前n行内容 12345678910# 显示/etc/passwd文件的前5行内容# 选项n可带也可不带，通常情况下，为简化指令一般不带选项n，直接输入数字即可[root@C7-Server01 kkutysllb]# head -5 /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 3） 读取文件的前10个字节 123456# 使用-c选项，读取/etc/hosts文件的前10个字节# 注意：空格符也统计在内[root@C7-Server01 kkutysllb]# head -c 10 /etc/hosts127.0.0.1 # 这行最后有一个空格 4） 显示多个文件 12345678910111213# 同时显示/etc/hosts和/etc/passwd文件的前5行内容[root@C7-Server01 kkutysllb]# head -c 10 /etc/hosts127.0.0.1 [root@C7-Server01 kkutysllb]# head -5 /etc/hosts /etc/passwd==&gt; /etc/hosts &lt;== # 每个文件名作为文件头127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6==&gt; /etc/passwd &lt;== # 每个文件名作为文件头root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 小练习：使用head命令只显示/etc/services文件的最后一行内容 tail：显示文件内容尾部tail命令用于显示文件内容的尾部，它默认输出文件的最后10行。如果指定了多于一个文件，则在每一段输出前会给出文件名作为文件头。 语法格式：tail [option] [file] 重要选项参数 【使用示例】 1）不带任何参数，默认显示文件的最后10行 12345678910111213# 显示/etc/passwd文件的最后10行内容[root@C7-Server01 kkutysllb]# tail /etc/passwdnobody:x:99:99:Nobody:/:/sbin/nologinsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologindbus:x:81:81:System message bus:/:/sbin/nologinpolkitd:x:999:998:User for polkitd:/:/sbin/nologintss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologinsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologinpostfix:x:89:89::/var/spool/postfix:/sbin/nologinchrony:x:998:996::/var/lib/chrony:/sbin/nologinntp:x:38:38::/etc/ntp:/sbin/nologinkkutysllb:x:1000:1000::/home/kkutysllb:/bin/bash 2）显示文件的末尾5行内容 12345678# 与head命令一样，可以省略-n选项，直接填写数字[root@C7-Server01 kkutysllb]# tail -5 /etc/services com-bardac-dw 48556/tcp # com-bardac-dwcom-bardac-dw 48556/udp # com-bardac-dwiqobject 48619/tcp # iqobjectiqobject 48619/udp # iqobjectmatahari 49000/tcp # Matahari Broker 3）实时监控文件的变化 123# 使用-f选项实时监控系统安全日志/var/log/secure[root@C7-Server01 kkutysllb]# tail -f /var/log/secure 初始状态如下 当我再次打开一个终端时，文件实时发生变化 需要说明的是：使用tail -f实时跟踪日志文件结束后，必须使用Ctrl+C退出。同时，在Linux系统还有一个专门跟踪系统日志文件的命令tailf，功能几乎等同于tail-f，与tail-f不同的是，如果文件不增长，那么它不会去访问磁盘文件，也不会更改文件的访问时间。 cut：从文本中提取一段文字并输出cut命令从文件的每一行剪切字节、字符或字段，并将这些字节、字符或字段输出至标准输出。 语法格式：cut [option] [file] 重要选项参数 【使用示例】 1）以字节为分隔符 123456789101112131415161718192021222324252627282930313233343536373839# 显示当前目录下data001文件的内容[root@C7-Server01 kkutysllb]# cat data001day001...day002...day003...day004......day00n...# 以-b选项，只显示data001文件每行的第3个字节内容[root@C7-Server01 kkutysllb]# cut -b 3 data001yyyy.y# 显示data001文件每行的第3到第5个字节内容[root@C7-Server01 kkutysllb]# cut -b 3-5 data001y00y00y00y00.y00# 显示data001文件每行从第1个字节到第3个字节的内容[root@C7-Server01 kkutysllb]# cut -b -3 data001daydaydayday...day 2）以字符为分隔符 123456789# 显示data001文件每行的第2到5个字符[root@C7-Server01 kkutysllb]# cut -c 2-5 data001ay00ay00ay00ay00..ay00 发现与-b选项的输出没有区别，这是英文是以单字母为字符的，如果换成中文就看出区别了，请大家自行练习。 3）自定义分隔符 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 使用：作为分隔符，显示/etc/passwd文件所有行的第一列的内容# 此方法和后续三剑客中awk使用场景类似[root@C7-Server01 kkutysllb]# cut -d : -f 1 /etc/passwdrootbindaemonadmlpsyncshutdownhaltmailoperatorgamesftpnobodysystemd-networkdbuspolkitdtsssshdpostfixchronyntpkkutysllb# 以：为分隔符，显示/etc/passwd文件每行的3-5列内容[root@C7-Server01 kkutysllb]# cut -d : -f 3-5 /etc/passwd0:0:root1:1:bin2:2:daemon3:4:adm4:7:lp5:0:sync6:0:shutdown7:0:halt8:12:mail11:0:operator12:100:games14:50:FTP User99:99:Nobody192:192:systemd Network Management81:81:System message bus999:998:User for polkitd59:59:Account used by the trousers package to sandbox the tcsd daemon74:74:Privilege-separated SSH89:89:998:996:38:38:1000:1000: sort：文本排序sort命令将输入的文件内容按照指定的规则进行排序，然后将排序结果输出。 语法格式：sort [option] [file] 重要选项参数 【使用示例】 1）默认以行为单位进行比较 默认比较的原则是从首字符向后，依次按ASCII码值进行比较，输出默认按升序进行排列。 123456789101112131415161718192021# 生成一个实验文件，写入如下内容[root@C7-Server01 kkutysllb]# cat &gt;&gt; data003 &lt;&lt; EOF&gt; 10.0.2.1&gt; 10.0.2.56&gt; 10.0.2.3&gt; 10.0.2.5&gt; 10.0.2.14&gt; 10.0.2.11&gt; EOF# 使用sort默认对data003文件进行升序排序输出[root@C7-Server01 kkutysllb]# sort data00310.0.2.110.0.2.1110.0.2.1410.0.2.310.0.2.510.0.2.56 2）使用-n选项使输出按数字从小到大的顺序进行排列 1234567[root@C7-Server01 kkutysllb]# sort -n data00310.0.2.110.0.2.1110.0.2.1410.0.2.310.0.2.510.0.2.56 3）结合-r选项进行反向排序 1234567[root@C7-Server01 kkutysllb]# sort -nr data00310.0.2.5610.0.2.510.0.2.310.0.2.1410.0.2.1110.0.2.1 4）使用-t和-k按照指定格式要求进行排序 1234567891011121314151617181920212223242526272829# 对/etc/passwd文件按要求进行排序# 使用-t选项指定分割符为：# 使用-k数字选项指定按照分割后的第3列进行排序[root@C7-Server01 kkutysllb]# sort -t : -k 3 /etc/passwdroot:x:0:0:root:/root:/bin/bashkkutysllb:x:1000:1000::/home/kkutysllb:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologinbin:x:1:1:bin:/bin:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologinftp:x:14:50:FTP User:/var/ftp:/sbin/nologinsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinntp:x:38:38::/etc/ntp:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/synctss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologinshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologinmail:x:8:12:mail:/var/spool/mail:/sbin/nologindbus:x:81:81:System message bus:/:/sbin/nologinpostfix:x:89:89::/var/spool/postfix:/sbin/nologinchrony:x:998:996::/var/lib/chrony:/sbin/nologinpolkitd:x:999:998:User for polkitd:/:/sbin/nologinnobody:x:99:99:Nobody:/:/sbin/nologin 5）使用-k选项的进阶用法 1234567891011# 使用-k选项指定字符范围进行排序# -k4.1,4,2表示按照第4列的第一个字符到第二个字符范围整体排序[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data00310.0.2.110.0.2.310.0.2.510.0.2.1110.0.2.1410.0.2.56 uniq：去除重复行uniq命令可以输出或忽略文件中的重复行。在工作中，我们常用的场景是使用sort命令对文件排序，然后使用uniq命令去重并计数。 语法格式：uniq [option] [INPUT] 重要选项参数 【使用示例】 1）去重测试 123456789101112131415161718192021222324252627# 创建测试文件[root@C7-Server01 kkutysllb]# cat data00310.0.2.110.0.2.5610.0.2.310.0.2.510.0.2.1410.0.2.1110.0.2.210.0.2.210.0.2.310.0.2.1110.0.2.110.0.2.110.0.2.1# 使用uniq指令去重，结合sort指令一起使用[root@C7-Server01 kkutysllb]# sort data003 | uniq10.0.2.110.0.2.1110.0.2.1410.0.2.210.0.2.310.0.2.510.0.2.56 2）显示重复行的个数 12345678[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data003 | uniq -c4 10.0.2.12 10.0.2.22 10.0.2.31 10.0.2.52 10.0.2.111 10.0.2.141 10.0.2.56 3）只显示重复行 12345[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data003 | uniq -cd4 10.0.2.12 10.0.2.22 10.0.2.32 10.0.2.11 4）只显示唯一行 1234[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data003 | uniq -u10.0.2.510.0.2.1410.0.2.56 wc：统计文件的行数、单词数或字节数wc命令用于统计文件的行数、单词数或字节数。 语法格式：wc [option] [file] 重要选项参数 【使用示例】 1）查看文件的字节数、字数、行数等 123456# 查看/etc/hosts的统计值（字节数、字数、行数等）# 什么参数都不加时，显示的三个数字从左到右分别是行数，单词数和字节数[root@C7-Server01 kkutysllb]# wc /etc/hosts2 10 158 /etc/hosts 1234# 显示文件的字节数[root@C7-Server01 kkutysllb]# wc -c /etc/hosts158 /etc/hosts 1234# 显示文件的单词数[root@C7-Server01 kkutysllb]# wc -w /etc/hosts10 /etc/hosts 1234# 显示文件的行数[root@C7-Server01 kkutysllb]# wc -l /etc/hosts2 /etc/hosts 1234# 显示文件最长行的长度[root@C7-Server01 kkutysllb]# wc -L /etc/hosts78 /etc/hosts 2）在shell脚本常用来判断某个软件包是否安装 diff：比较两个文件的不同diff命令可以逐行比较纯文本文件的内容，并输出文件的差异。只能同时比较2个文件。 语法格式：diff [option] [file1] [file2] 重要选项参数 【使用示例】 1）比较两个文本文件 123456789101112131415161718192021222324252627282930313233343536&gt; # 将data003文件去重后生成一个新文件data004&gt; &gt; [root@C7-Server01 kkutysllb]# sort data003 | uniq &gt; data004&gt; [root@C7-Server01 kkutysllb]# cat data004&gt; 10.0.2.1&gt; 10.0.2.11&gt; 10.0.2.14&gt; 10.0.2.2&gt; 10.0.2.3&gt; 10.0.2.5&gt; 10.0.2.56&gt; &gt; # 比较data003和data004两个文件&gt; &gt; [root@C7-Server01 kkutysllb]# diff data003 data004&gt; 2,5d1 # 以data003为基准，data003比data004多了2,3,4,5行，所以data003的2-5行需要删除&gt; &lt; 10.0.2.56&gt; &lt; 10.0.2.3&gt; &lt; 10.0.2.5&gt; &lt; 10.0.2.14&gt; 7c3 # 以data003为基准，其第7行对应data004的第3行，两者结果不一致，需要修改&gt; &gt; ## &lt; 10.0.2.2&gt; &gt; &gt; 10.0.2.14&gt; &gt; 10,13c6,7 # 以data003为基准，其第10,11行对应data004的第6,7行，两者结果不一致，需要修改。同时，data003多了12,13两行内容&gt; &gt; &lt; 10.0.2.11&gt; &gt; &lt; 10.0.2.1&gt; &gt; &lt; 10.0.2.1&gt; &gt; &lt; 10.0.2.1&gt; &gt; ------&gt; &gt; &gt; 10.0.2.5&gt; &gt; 10.0.2.56&gt; 2） 并排格式输出 12345678910111213141516# 使用-y选项就可以并排格式输出对比[root@C7-Server01 kkutysllb]# diff -y data003 data00410.0.2.1 10.0.2.110.0.2.56 &lt;10.0.2.3 &lt;10.0.2.5 &lt;10.0.2.14 &lt;10.0.2.11 10.0.2.1110.0.2.2 | 10.0.2.1410.0.2.2 10.0.2.210.0.2.3 10.0.2.310.0.2.11 | 10.0.2.510.0.2.1 | 10.0.2.5610.0.2.1 &lt;10.0.2.1 &lt; 3） 使用-c选项就可以上下文输出 1234567891011121314151617181920212223242526272829303132333435# -表示data004比data003少的行数# +表示data004比data003多的行数# !表示两者不一样的行数[root@C7-Server01 kkutysllb]# diff -c data003 data004*** data003 2019-04-20 15:44:51.488782009 +0800--- data004 2019-04-20 17:21:03.569053069 +0800------ *** 1,13 **** 10.0.2.1- 10.0.2.56 - 10.0.2.3 - 10.0.2.5 - 10.0.2.14 10.0.2.11 ! 10.0.2.2 10.0.2.2 10.0.2.3 ! 10.0.2.11 ! 10.0.2.1 ! 10.0.2.1 ! 10.0.2.1 --- 1,7 ---- 10.0.2.1 10.0.2.11 ! 10.0.2.14 10.0.2.2 10.0.2.3 ! 10.0.2.5 ! 10.0.2.56 ​ 小练习：除了diff指令可以比较两个文件外，vimdiff指令还可以可视化比较两个文件，大家自行研究。 tee：多重定向tee命令用于将数据重定向到文件，同时提供一份重定向数据的副本作为后续命令的标准输入。简单地说就是把数据重定向到给定文件和屏幕上。 语法格式：tee [option] [file] 重要选项参数 【使用示例】 1）tee命令允许标准输出同时把内容写入（覆盖）到文件中 12345678910111213141516171819202122232425262728293031323334353637383940# 显示当前目录下内容详细信息，并将结果写入到data005文件中（如果data005文件不存在，则直接创建，否则将内容覆盖原文件内容）[root@C7-Server01 kkutysllb]# ls -lhi | tee data005total 28K1231037 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.5534100198 drwxr-xr-x 5 root root 45 Apr 17 11:59 data1231031 -rw-r--r-- 1 root root 54 Apr 19 17:14 data0011231047 -rw-r--r-- 1 root root 73 Apr 19 17:31 data0021231030 -rw-r--r-- 1 root root 121 Apr 20 15:44 data0031231056 -rw-r--r-- 1 root root 66 Apr 20 17:21 data0041231024 -rw-r--r-- 1 root root 0 Apr 20 17:41 data0051231057 drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp33583813 -rw-r--r--. 1 root root 158 Jun 7 2013 hard_link1231054 -rw-r--r-- 1 root root 0 Apr 16 17:56 image0081231055 -rw-r--r-- 1 root root 0 Apr 16 17:56 image0091231032 -rw-r--r-- 1 root root 0 Apr 17 11:46 image0101231039 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts1231022 -rw-r--r-- 1 root root 0 Apr 20 17:10 test011231025 -rw-r--r-- 1 root root 82 Apr 20 17:08 test01.sh1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 test.sh[root@C7-Server01 kkutysllb]# cat -n data0051 total 28K2 1231037 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.553 34100198 drwxr-xr-x 5 root root 45 Apr 17 11:59 data4 1231031 -rw-r--r-- 1 root root 54 Apr 19 17:14 data0015 1231047 -rw-r--r-- 1 root root 73 Apr 19 17:31 data0026 1231030 -rw-r--r-- 1 root root 121 Apr 20 15:44 data0037 1231056 -rw-r--r-- 1 root root 66 Apr 20 17:21 data0048 1231024 -rw-r--r-- 1 root root 0 Apr 20 17:41 data0059 1231057 drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp10 33583813 -rw-r--r--. 1 root root 158 Jun 7 2013 hard_link11 1231054 -rw-r--r-- 1 root root 0 Apr 16 17:56 image00812 1231055 -rw-r--r-- 1 root root 0 Apr 16 17:56 image00913 1231032 -rw-r--r-- 1 root root 0 Apr 17 11:46 image01014 1231039 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest15 1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts16 1231022 -rw-r--r-- 1 root root 0 Apr 20 17:10 test0117 1231025 -rw-r--r-- 1 root root 82 Apr 20 17:08 test01.sh18 1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 test.sh 2）tee命令允许标准输出同时把内容追加到文件中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 使用-a选项，会将内容追加到目标文件最后[root@C7-Server01 kkutysllb]# ls -lhi /boot | tee -a data005total 123M 72 -rw-r--r--. 1 root root 145K Apr 21 2018 config-3.10.0-862.el7.x86_64 81 -rw-r--r-- 1 root root 149K Mar 18 23:10 config-3.10.0-957.10.1.el7.x86_64 67 drwxr-xr-x. 3 root root 17 Apr 7 20:33 efi 68 drwxr-xr-x. 2 root root 27 Apr 7 20:34 grub786496 drwx------. 5 root root 132 Apr 20 16:09 grub2 76 -rw-------. 1 root root 50M Apr 7 20:38 initramfs-0-rescue-e344b139f44946638783478bcb51f820.img 75 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-862.el7.x86_64.img 78 -rw------- 1 root root 11M Apr 20 16:08 initramfs-3.10.0-862.el7.x86_64kdump.img 84 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-957.10.1.el7.x86_64.img 73 -rw-r--r--. 1 root root 298K Apr 21 2018 symvers-3.10.0-862.el7.x86_64.gz 82 -rw-r--r-- 1 root root 307K Mar 18 23:10 symvers-3.10.0-957.10.1.el7.x86_64.gz 71 -rw-------. 1 root root 3.3M Apr 21 2018 System.map-3.10.0-862.el7.x86_64 80 -rw------- 1 root root 3.4M Mar 18 23:10 System.map-3.10.0-957.10.1.el7.x86_64 77 -rwxr-xr-x. 1 root root 6.0M Apr 7 20:38 vmlinuz-0-rescue-e344b139f44946638783478bcb51f820 74 -rwxr-xr-x. 1 root root 6.0M Apr 21 2018 vmlinuz-3.10.0-862.el7.x86_64 83 -rwxr-xr-x 1 root root 6.4M Mar 18 23:10 vmlinuz-3.10.0-957.10.1.el7.x86_64[root@C7-Server01 kkutysllb]# [root@C7-Server01 kkutysllb]# cat -n data005 1 total 28K 2 1231037 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.55 3 34100198 drwxr-xr-x 5 root root 45 Apr 17 11:59 data 4 1231031 -rw-r--r-- 1 root root 54 Apr 19 17:14 data001 5 1231047 -rw-r--r-- 1 root root 73 Apr 19 17:31 data002 6 1231030 -rw-r--r-- 1 root root 121 Apr 20 15:44 data003 7 1231056 -rw-r--r-- 1 root root 66 Apr 20 17:21 data004 8 1231024 -rw-r--r-- 1 root root 0 Apr 20 17:41 data005 9 1231057 drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp 10 33583813 -rw-r--r--. 1 root root 158 Jun 7 2013 hard_link 11 1231054 -rw-r--r-- 1 root root 0 Apr 16 17:56 image008 12 1231055 -rw-r--r-- 1 root root 0 Apr 16 17:56 image009 13 1231032 -rw-r--r-- 1 root root 0 Apr 17 11:46 image010 14 1231039 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest 15 1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts 16 1231022 -rw-r--r-- 1 root root 0 Apr 20 17:10 test01 17 1231025 -rw-r--r-- 1 root root 82 Apr 20 17:08 test01.sh 18 1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 test.sh 19 total 123M 20 72 -rw-r--r--. 1 root root 145K Apr 21 2018 config-3.10.0-862.el7.x86_64 21 81 -rw-r--r-- 1 root root 149K Mar 18 23:10 config-3.10.0-957.10.1.el7.x86_64 22 67 drwxr-xr-x. 3 root root 17 Apr 7 20:33 efi 23 68 drwxr-xr-x. 2 root root 27 Apr 7 20:34 grub 24 786496 drwx------. 5 root root 132 Apr 20 16:09 grub2 25 76 -rw-------. 1 root root 50M Apr 7 20:38 initramfs-0-rescue-e344b139f44946638783478bcb51f820.img 26 75 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-862.el7.x86_64.img 27 78 -rw------- 1 root root 11M Apr 20 16:08 initramfs-3.10.0-862.el7.x86_64kdump.img 28 84 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-957.10.1.el7.x86_64.img 29 73 -rw-r--r--. 1 root root 298K Apr 21 2018 symvers-3.10.0-862.el7.x86_64.gz 30 82 -rw-r--r-- 1 root root 307K Mar 18 23:10 symvers-3.10.0-957.10.1.el7.x86_64.gz 31 71 -rw-------. 1 root root 3.3M Apr 21 2018 System.map-3.10.0-862.el7.x86_64 32 80 -rw------- 1 root root 3.4M Mar 18 23:10 System.map-3.10.0-957.10.1.el7.x86_64 33 77 -rwxr-xr-x. 1 root root 6.0M Apr 7 20:38 vmlinuz-0-rescue-e344b139f44946638783478bcb51f820 34 74 -rwxr-xr-x. 1 root root 6.0M Apr 21 2018 vmlinuz-3.10.0-862.el7.x86_64 35 83 -rwxr-xr-x 1 root root 6.4M Mar 18 23:10 vmlinuz-3.10.0-957.10.1.el7.x86_64 3）tee还有个用法类似cat的创建/追加文件的用法，区别是不需要添加输出重定向符号 12345678910# 创建docker.service.d目录mkdir /etc/systemd/system/docker.service.d# 在docker.service.d/目录下创建一个kolla.conf文件，并完成相应配置tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; 'EOF'[Service]MountFlags=sharedEOF tr：替换或删除字符tr命令从标准输入中替换、缩减或删除字符，并将结果写到标准输出。 语法格式：tr [option] [SET1] [SET2] 重要选项参数 【使用示例】 1）将文件中出现的“www”替换为“xyz” 1234567891011121314151617181920212223242526272829# 生成测试文件[root@C7-Server01 kkutysllb]# tee data005 &lt;&lt; 'EOF'&gt; www.sina.com.cn&gt; www.aliyun.com&gt; www.chinamobile.com&gt; www.openstack.org&gt; www.tsinghua.edu.cn&gt; kkutysllb.cn&gt; EOF&gt; www.sina.com.cn&gt; www.aliyun.com&gt; www.chinamobile.com&gt; www.openstack.org&gt; www.tsinghua.edu.cn&gt; kkutysllb.cn# 不使用任何选项参数时，默认按照set1和set2的对应字符位置替换# 下面的例子w对应z，所有全部替换为z[root@C7-Server01 kkutysllb]# tr 'www' 'xyz' &lt; data005zzz.sina.com.cnzzz.aliyun.comzzz.chinamobile.comzzz.openstack.orgzzz.tsinghua.edu.cnkkutysllb.cn 2）使用tr命令“统一”字母大小写 123456789# 这个例子中使用了正则的匹配规则，后续正则表达式会专门讲解[root@C7-Server01 kkutysllb]# tr '[a-z]' '[A-Z]' &lt; data005WWW.SINA.COM.CNWWW.ALIYUN.COMWWW.CHINAMOBILE.COMWWW.OPENSTACK.ORGWWW.TSINGHUA.EDU.CNKKUTYSLLB.CN 3）删除所有的点”.” 1234567[root@C7-Server01 kkutysllb]# tr -d "." &lt; data005wwwsinacomcnwwwaliyuncomwwwchinamobilecomwwwopenstackorgwwwtsinghuaeducnkkutysllbcn 4）将所有的非w字符替换为&amp; 12345[root@C7-Server01 kkutysllb]# tr -c 'w' '&amp;' &lt; data005# 除w字符外，其它所有字符被替换为&amp;，包括换行符\n和制表符\twww&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; 小练习：想想怎么使用br指令完成整单词替换？ vi/vim编辑器vi是Linux命令行界面下的文字编辑器，几乎所有的Linux系统都安装了vi，只要学会了vi这个编辑工具，就可以在任何Linux系统上使用它。而vim是vi命令的增强版（Vi IMproved），与vi编辑器完全兼容，此外还有很多增强功能，例如用不同颜色高亮显示代码。因此，如果系统有vim命令，那么建议大家就使用vim编辑文本。 语法格式：vim [option] [file] 【vim的三种模式】 一般来说，vim可分为三种模式：普通模式、编辑模式、命令模式。这三种模式的作用分别如下。 （1）普通模式 用vim命令打开一个文件，默认的状态就是普通模式。在这个模式中，不能进行编辑输入操作，但可以按“上下左右”键来移动光标，也可以执行一些操作命令进行如删除、复制、粘贴等之类的工作。 （2）编辑模式 在普通模式下不能进行编辑输入操作，只有按下“i，I，o，O，a，A，r，R，s，S”（其中“I”最常用）等字母进入编辑模式之后才可以执行录入文字等编辑操作。看文件是否处于编辑模式状态有一个重要的特征，那就是在窗口的左下角要有插入的标记“——INSERT——”或“——插入——”，如下图所示： （3）命令模式 在普通模式下，输入“：”或“/”或“?”时，光标会自动定位在那一行，在这个模式中，可以执行保存、退出、搜索、替换、显示行号等相关操作。如下图所示： 【vi/vim的操作示意图】 1）进入编辑模式指令 i：在当前光标所在处输入文字 a：在当前光标所在的下一个字符处插入文字 I：在当前所在行的行首第一个非空格字符处开始插入文字、和A相反 A：在当前所在行的行尾最后一个字符处开始插入文字，和I相反 O：在当前所在行的上一行处插入新的一行 o：在当前所在行的下一行处插入新的一行 Esc：退出编辑模式，回到命令模式中。 2）命令行模式下指令 :wq：退出并保存 :wq!：退出并强制保存。 :q!：强制退出，不保存 :n1,n2,w filename：n1、n2为数字，将n1行到n2行的内容保存成filename这个文件。 :n1,n2 co n3：n1、n2为数字，将n1行到n2行内容拷贝到n3位置下。 :n1,n2 m n3：n1、n2为数字，将n1行到n2行的内容移动到n3位置下。 :!command：暂时离开vi到命令行模式下执行command的显示结果 :set nu：显示行号 :set nonu：与set nu相反，取消行号 :vs filename：垂直分屏显示，同时显示当前文件和filename对应文件的内容 :sp filename：水平分屏显示，同时当前文件和filename对应文件的内容 I+#+ESC：在可视模式下（Ctrl+v），一次性注释所选的多行，取消注释可用n1,n2s/#//gc。这里操作是一个通用操作，#可以换成tab键，这样可以实现批量缩进 Del：在可视块模式下（Ctrl+v），一次性删除所选内容 r：在可视块模式下（Ctrl+v），一次性替换所选内容 【常用操作】 1）普通模式下移动光标的操作 G：将光标自动文件的最后一行 gg：将光标移动文件的第一行 0：数字0，将光标从所在位置移动到当前行的开头 $：从光标所在位置移动到当前行的结尾 n：n为数字，为回车键，将光标从当前位置向下移动n行 ngg：n为数字，移动光标到文件的第n行 H：光标移动当前窗口最上方那一行 M：光标移动到当前窗口中间的那一行 L：光标移动到当前窗口最下方的那一行 h或向左箭头：光标向左移动一个字符 j或向下箭头：光标向下移动一个字符 k或向上箭头：光标向上移动一个字符 l或向右箭头：光标向右移动一个字符 2）普通模式下搜索与替换操作： /关键字：从光标位置开始向下搜索关键字行 ?关键字：从光标位置开始向上搜索关键字 n：从光标位置开始，向下重复前一个搜索的动作 N：从光标位置开始，向上重复前一个搜索的动作 :g/A/s/B/g：把符合A的内容全部替换为B，左斜线为分隔符，可以用@、#等替换 :%s/A/B/g：把符合A的内容全部替换为B，左斜线为分隔符，可以用@、#等替换。 :n1,n2s/A/B/gc：n1、n2为数字，在第n1行和n2行间寻找A，用B替换 3）普通模式下复制、粘贴、删除等操作 yy：复制光标所在行的全部内容 nyy：n为数字，复制光标开始向下共n行的内容 p/P：p将已复制的数据粘贴到光标的下一行，P则为粘贴到光标的上一行 dd：删除所在的当前行 ndd：n为数字，删除从光标开始向下的共n行。 u：恢复（回滚）前一个执行过的操作 .：点号。重复前一个执行过的动作 %：向后删除字符 X：向前删除字符 d1G：删除当前行至第一行 dG：删除当前行至最后一行 d0：删除当前光标文本至行首 d$：删除当前光标文本至行尾 vi/vim编辑器是Linux运维、开发、测试中最常用的工具，需要大家重点掌握。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-19-中国移动NovoNet2020]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-19-%E4%B8%AD%E5%9B%BD%E7%A7%BB%E5%8A%A8NovoNet2020%2F</url>
    <content type="text"><![CDATA[2015年7月，中国移动发布了NovoNet 2020愿景，推出下一代革新网络NovoNet，2016—2020年将成为中国移动SDN/NFV技术的关键部署期。中国移动希望利用SDN、NFV等新技术，构建一张资源可全局调度、能力可全面开放、容量可弹性伸缩、架构可灵活调整的新一代网络，满足“互联网+”、物联网等创新型业务对通信网络的需求。 中国移动网络重构战略中国移动NovoNet的核心发展理念是实现网络的“三化”和“三可”，即网络功能部署的软件化、虚拟资源的共享化、硬件基础设施的通用化，同时达到网络开放可编程、控制转发可解耦、功能编排可调度的能力，以便在移动网络、IP承载网络、传送网络、数据中心等领域实现网络和业务的虚拟化、软件化。 中国移动NovoNet的愿景是实现网络可编程、部署更灵活、调度更高效、网络更智能、服务更开放、成本更低廉的总体效果，通过SDN和NFV两大基础技术，在网络架构、运营管理和网络开放等层面持续优化网络和服务。 网络架构 1、 网络功能软件化 将软件和硬件解耦，网元功能以软件部署在通用硬件平台上，实现业务的快速部署和升级，有利于运营商快速满足客户的业务需求。 2、资源共享化 借助NFV技术，实现硬件资源的通用化和虚拟资源的共享化，可以降低硬件件成本，实现资源的灵活配置和调度，有利于运营商开发创新业务并灵活部署。 3、 网络可编程 应用SDN技术，将网络控制与物理网络拓扑分离，实现网络可编程，有利于快速响应和满足用户的业务需求，最大化利用运营商的网络资源。 网络运营管理 在2017全球未来网络发展峰会网络重构与转型论坛上，中国移动通信研究院副院长杨志强明确指出，5G和固定宽带业务将会成为运营商未来网络的核心应用，在开源软件和硬件的基础上进行自主研发和运营将成为新的主流运营模式。 1、集中控制 网络功能的控制和调度通过软件完成，提升网络的调度优化能力，可以逐步实现面向全局最优的网络管理和简化网络运维。 2、灵活调度 实现业务部署、业务资源的动态调度，可快速、灵活调度网络资源，应对网络故障、突发事件等。 3、绿色节能 根据业务量需求动态调度网络资源，并可实现高效率集中控制，大幅提升网络资源的利用效率，构建绿色节能的通信网络。 网络服务 1、全面开放的服务 提供全面的开放能力，构建与第三方业务开发者合作共赢的生态环境，全面服务“互联网+”。 2、高效敏捷的服务 具备完善的业务部署和调度能力，能够有效疏导网络流量、对业务提供完善的生命周期管理能力，支持业务的快速迭代、灵活部署、高效使用。 3、按需调度的服务 计算、存储、网络资源可全局调度，网络可动态编程，可根据用户、业务的需求动态调配资源。 中国移动网络重构目标中国移动将SDN/NFV作为网络重构的技术基础，实现基础设施云化、网元功能软件化以及运营管理智能化。未来网络的核心是通过SDN/NFV的引入，实现网络的软件化、资源池化、集中控制、灵活的编排与调度，构建云化的部署形态、智能化的网络调度、全局化的网络编排管理。 （1）引入网络功能虚拟化（NFV）技术，采用IT通用服务器构建资源池，电信设备软硬解耦，以软件形成电信云，并支撑内容分发、边缘计算等。 （2）引入软件定义网络（SDN）技术，采用控制转发分离和路由集中计算，实现网络灵活、智能调度和网络能力的开放。 （3）引入协同编排技术，实现跨领域、端到端的全网资源、网元和流量流向的管理编排与调度。 中国移动网络重构主要包括节点重构、架构重构、网元功能重构以及网络管理与业务运营重构四个方面。 节点重构 构建云化数据中心，替代传统的核心网机房。云化数据中心采用标准化和微模块方式构建，易于快速复制部署，是电信云的基本组件和满足电信网络要求的关键基础设施，可以承载各类虚拟化的电信类软件应用。 ① 标准化的组网：以电信标准为基准的更为严格的网段隔离和网络平面划分原则，业务、管理、基础设施平面独立。 ② 标准化的基础设施：硬件采用通用的X86硬件架构，增强性能要求和电信级管理要求；以统一的云操作系统支持统一的虚拟层指标要求；以电信级增强的OpenStack/VIM实现云资源的管理和分配。 ③ 统一的管理编排体系：以整合的NFVO和SDN编排器和控制器作为统一的管理编排体系。 架构重构 ① 构建基础设施资源池：分布式部署云化数据中心，形成基础设施资源池，承载不同的网络功能。 ② 控制功能集中化：网络控制功能集中在核心云数据中心。 ③ 媒体面下沉：靠近接入点设置边缘云数据中心，将大流量的媒体内容调度到网络边缘，实现快速疏导，提升用户体验。 网元功能重构 通过基于服务的网络架构、切片、控制转发分离等，结合云化技术，实现网络的定制化、开放化、服务化，支持大流量、大连接和低时延的万物互联需求。 ① 用户面功能可实现灵活部署和独立扩缩容，采用集中式部署支持广域移动性和业务连续性，采用网络边缘部署，实现流量本地卸载，支持端到端毫秒级时延，降低回传网络和集中式用户面的处理压力。 ② 网络切片提供端到端资源隔离的逻辑专网，可基于行业用户的需求，实现专网专用，基于业务场景灵活组合网络功能，灵活扩缩容，功能扩展敏捷。 ③ 固网宽带业务控制设备通过转发控制分离，应对固网发展中面临的运维复杂、资源利用率低及业务开通慢等挑战。 网络管理与业务运营重构 构建下一代网络编排器，实施对网络资源池的管理，完成网络和网元部署以及生命周期管理，协调SDN控制器完成网络的统一调度，实现业务的统一编排和对外开放。 中国移动网络重构研发与实践为了验证NovoNet的理念，中国移动已开展了相关研发和测试，包括研发SDN应用和控制器、SDN交换机、多厂家vIMS、vEPC软硬件解耦测试及MANO实验室验证、基于SDN网元功能重构的EPC原型验证、三层解耦的Nanocell网关测试等实验室验证。同时，中国移动也在数据中心、RCS、IMS、SPTN等领域开展了现网的试点及试商用的探索。NovoNet试验网由陕西、安徽、山东、河北、浙江、广东6个省组成，试验网的每个节点均构建两层云数据中心，用全局的编排器实现全网资源和网络的编排调度，以统一的资源池支持数据中心SDN、广域网SDN、SPTN、固网宽带、物联网、VoLTE、vCDN等多业务环境。 基于NovoNet的发展理念，中国移动加快推动SDN/NFV开源开放和相关产业发展，积极参与相关标准组织的项目推进。从2013年开始，中国移动牵头推动成立了Carrier Grade SDN工作组并任主席，参与无线和移动、光传送网、北向接口工作组标准化工作。2014年5月，中国移动与华为联合发起3GPP第一个NFV虚拟网络管理研究项目，并得到23家单位的支持。中国移动在SA1牵头发起Service Chaining立项，研究Gi-LAN场景业务链的场景和需求。2014年9月，中国移动联合国际运营商和厂商发起成立开源组织OPNFV，并出任董事会要职。2016年9月，中国移动加入领先的开源可编程、软件定义网络平台OpenDaylight项目，并利用该平台和OpenStack在其NovoDC项目中部署企业私有云服务产品。 2014年，中国移动完成7家厂商的OpenStack+SDN数据中心解决方案测试，以及2家厂商的广域网方案测试。2015年，中国移动成立Open NFV实验室，引入了15个合作厂商，进行三层解耦测试，搭建云数据中心平台，就NFV部署展开了相关实践。目前，实验室和外场均已按计划开展三层解耦的集成和测试、OPNFV开源平台测试，已搭建了多套基于多厂家NFV组件互操作的云数据中心平台，可模拟核心云数据中心和边缘云数据中心。 2015年，中国移动完成业界首次SDN+NFV的面向商用外场测试，在中国移动自有数据中心采用该架构进行了国内第一次比较全面的组合测试。 ① 在VoLTE和vIMS方面，中国移动进行了NFV实践，2015年-2019年连续5年在陕西、安徽、河北、广东、浙江和山东共6个城市验证了3个系统厂商以及自研产品的核心网云化的快速上线、弹性伸缩、网络快速更新等能力。 ② 在虚拟化RCS（Rich Comunication Suite，富媒体通信套件）方面，中国移动融合通信业务平台（新消息、VoWiFi、业务管理功能）基于NFV架构进行部署，采用软硬件解耦方案，硬件由惠普和思科提供，中兴提供虚拟层和虚拟网元功能并完成系统集成，目前已在中国南北基地进行部署商用。 ③ 在Nanocell商用方面，2016年中国移动基于NFV的4G一体化小基站（Nanocell）开始部署商用。 中国移动网络重构小结NovoNet是中国移动面向2020年的网络愿景和行动计划，NovoNet的目标形态是通过NFV和SDN技术的结合，打造以DC为部署核心、以MANO/SDN控制器体系为管理控制核心、以虚拟化和软件化实现网元功能、以SDN技术实现网络灵活调度的电信云。NovoNet是中国移动下一代网络发展的愿景，也是中国移动下一代网络发展的重大项目和平台，是中国移动对未来网络的重新定义。 标准化和开源系统是SDN和NFV发展的两个重要基石，国际合作和产业推进是NovoNet发展的助推器。中国移动正面向未来网络发展的方向，围绕着NovoNet的引入策略、技术攻关、试验验证、产业推进、测试认证五个方面全面展开工作，以开放共赢的理念从顶层设计、标准推动、开源开放等多个层面全面推动产业发展，积极构建NovoNet试验网，推动NovoNet网络技术和管理成熟。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-19-电信云落地过程若干问题]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-19-%E7%94%B5%E4%BF%A1%E4%BA%91%E8%90%BD%E5%9C%B0%E8%BF%87%E7%A8%8B%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[NFV技术从诞生起，从根本上来说就是为了解决运营商网络演进中部署成本高，迭代更新慢，架构僵化等痛点问题。同时，在引入NFV技术前，旧有产业链相对单一，核心成员主要包括设备制造商、芯片制造商等，而NFV引入后拉长了整体通信产业链条，传统设备制造商面临严峻的挑战，原本软硬件一体化设备销售模式被拆解为通用硬件、虚拟化平台和网元功能三部分销售模式。这也直接决定了运营商期望的多层解耦部署模式推行困难。同时，在NFV的转发性能提升、MANO管理模式选型、VNFM选型和NFVO部署等方面也多多少少存在影响电信云落地的问题。 NFV部署模式选型NFV通过软硬件解耦，使得网络设备开放化，软硬件可以独立演进，避免厂家锁定。基于NFV分层解耦的特性，根据软硬件解耦的开放性不同，可将集成策略分为单厂家、共享资源池、硬件独立和三层全解耦4种方案，如下图所示。 方案1：单厂家方案，优点就是可以实现快速部署，整体系统的性能、稳定性与可靠性都比较理想，不需要进行异构厂商的互通测试与集成。缺点是与传统网络设备一样，存在软硬件一体化和封闭性问题，难以实现灵活的架构部署，不利于实现共享；与厂商存在捆绑关系，不利于竞争，会再次形成烟囱式部署，总体成本较高，也不利于自主创新以及灵活的迭代式部署升级。目前，中国电信的4G/VoLTE/IMS网络就是采用这种方式，在短期内对中国移动的业务发展形成较大压力。 方案2：倾向于IT化思路，选择最好的硬件平台和虚拟机产品，要求上层应用向底层平台靠拢。只对VNF与NFVI层解耦，VNF能够部署于统一管理的虚拟资源之上，并确保功能可用、性能良好、运行情况可监控、故障可定位；不同供应商的VNF可灵活配置、可互通、可混用、可集约管理。其中，VNFM与VNF通常为同一厂商（即“专用VNFM”），这种情况下VNF与VNFM之间的接口不需标准化；特殊场景下采用跨厂商的“VNFM”（即“通用VNFM”）。VMware的解决方案就是典型的方案二厂商A的定位，考虑到中国移动苏州研发中心与VMware的战略合作情况，可以预期不远的将来中国移动的NFV网络架构中会出现类似部署方案。 方案3：倾向于电信思路，通用硬件与虚拟化层软件解耦，基础设施全部采用通用硬件，实现多供应商设备混用；虚拟化层采用商用/开源软件进行虚拟资源的统一管理。可以由电信设备制造商提供所有软件，只是适配在IT平台上。目前，中国移动大区集中化网络建设就是采用此部署方案。 方案4：全解耦的好处是可以实现通用化、标准化、模块化、分布式部署，架构灵活，而且部分核心模块可选择进行定制与自主研发，也有利于形成竞争，降低成本，实现规模化部署；不利的地方是需要规范和标准化，周期很长，也需要大量的多厂商互通测试，需要很强的集成开发能力，部署就绪时间长，效率较低，后续的运营复杂度高，故障定位和排除较为困难，对运营商的运营能力要求较高。该模式是中国移动一直不遗余力推广的模式，目前在陕西移动已初步完成苏研VIM+分布式存储、华为VNFM和研究院NFVO+的标准三层部署模式验证，并打通了标准三层组网下FirstCall。 另外，以上各方案都涉及MANO的解耦，涉及运营商自主开发或者第三方的NFVO与不同厂商的VNFM、VIM之间的对接和打通，屏蔽了供应商间的差异，统一实现网络功能的协同、面向业务的编排与虚拟资源的管理。但是，NFVO+的解耦目前还停留在实验验证阶段，在中国移动的电信云一阶段还是采用NFVO与VNFM同厂商捆绑的模式。 NFV转发性能的提升NFV设计的初衷是针对部分低转发流量类业务功能，x86服务器在配备高速网卡（10Gbit/s）后，业务应用不经特殊优化，基本也可以满足大多数低速率转发业务的处理要求（即使后续随着SDN技术的推动，引入了40Gbit/s的高速转发能力，但目前也只是实验验证阶段，并未实际部署）。 传统硬件网元能够通过专用芯片实现高转发性能，而x86环境下的虚拟化网元尚不具备万兆以上端口的小包线速转发能力，在同等业务量的情况下，虚拟化网元和传统设备相比存在一定的性能差距。x86服务器采用软件转发和交换技术，报文在服务器各层面间传递，会受到CPU开销等多方面因素的影响，因此服务器的内部转发性能是NFV系统的主要瓶颈。 NFV中的网络业务应用运行于服务器的虚拟化环境中，单个应用业务流量的收发要经过虚拟化层、服务器I/O通道、内核协议栈等多个处理流程，而多个应用业务之间又可以用复杂的物理或虚拟网络相连接。因此，NFV系统的整体性能取决于单服务器转发性能与业务组链转发性能两个方面。如下所示： 业务应用流量的收发I/O通道依次包括物理网卡、虚拟交换机、虚拟网卡3个环节（见上图左半部分）；从软件结构上看，报文的收发需要经过物理网卡驱动、宿主机内核网络协议栈、内核态虚拟交换层、虚拟机网卡驱动、虚拟机内核态网络协议栈、虚拟机用户态应用等多个转发通道（见上图右半部分），存在着海量系统中断、内核上下文切换、内存复制、虚拟化封装/解封等大量CPU开销操作过程。 影响NFV转发性能的主要因素整理如下： 1. 网卡硬件中断 目前大量流行的PCI/PCIe（Peripheral Component Interconnect，外设部件互连标准/PCI-Express）网卡在收到报文后，一般采用DMA（Direct Memory Access，直接存储器存取）方式直接写入内存并产生CPU硬件中断，在低速转发应用中此方法十分有效。 但是，当网络流量激增时，CPU的大部分时间阻塞于中断响应。在多核系统中，可能存在多块网卡绑定同一个CPU核的情况，使其占用率达到100%。中断处理方式在低速网络I/O场景下非常有效。然而，随着高速网络接口等技术的迅速发展，10Gbit/s、40Gbit/s甚至100Gbit/s的网络端口已经出现。随着网络I/O速率的不断提高，网卡面对大量高速数据分组引发频繁的中断，中断引起的上下文切换开销将变得不可忽视，造成较高的时延，并引起吞吐量下降。因此，网卡性能改进一般采用减少或关闭中断（如轮询取代中断、零复制技术、大页内存技术等）、多核CPU负载均衡等优化措施。 2. 内核网络协议栈 在Linux或FreeBSD系统中，用户态程序调用系统套接字进行数据收发时，会使用内核网络协议栈。这将产生两方面的性能问题：一是系统调用导致的内核上下文切换，会频繁占用CPU周期；二是协议栈与用户进程间的报文复制是一种费时的操作。 NFV系统中，业务应用报文处理从物理网卡到业务应用需要完成收发操作各1次，至少经过4次上下文切换（宿主机2次以及VM内2次）和4次报文复制。将网络协议栈移植到用户态是一种可行的思路，但这种方法违反了GNU协议。GNU是GNU GPL（GNU General Public License，通用公共许可证）的简称，Linux内核受GNU GPL保护，内核代码不能用于Linux内核外。因此，弃用网络协议栈以换取转发性能，是唯一可行的办法，但需要付出大量修改业务应用代码的代价。 3. 虚拟化层的封装效率 业务应用中存在两类封装：服务器内部的I/O封装和网络层对流量的虚拟化封装。前者是由于NFV的业务应用运行于VM中，流量需要经历多次封装/解封装过程，包括：宿主机虚拟化软件对VM的I/O封装、虚拟交换机对端口的封装、云管理平台对虚拟网络端口的封装；后者是为实现NFV用户隔离，在流量中添加的用户标识，如VLAN、VxLAN（Virtual Extensible Local Area Network，可扩展虚拟局域网）等。这两类封装/解封均要消耗CPU周期，会降低NFV系统的转发效率。 4. 业务链网络的转发效率 NFV的业务链存在星形和串行两种组网方式，如下图所示。 星形连接依赖于物理网络设备的硬件转发能力，整体转发性能较优，但当应用的数量较大时，会消耗大量网络设备端口。因此，在业务链组网范围不大时，如在IDC内部，为简化组网和节约端口，更多地采用串行连接。 当串行连接时，NFV控制器需要在多个业务应用中选择合适位置的应用进程或进程组来处理流量，以均衡各应用负荷并兼顾业务链网络性能。不合适的负载均衡算法会造成流量在不同进程组的上下行链路之间反复穿越，严重降低业务链网络的带宽利用率。 5. 其他开销 （1）缓存未命中开销：缓存是一种能够有效提高系统性能的方式，然而，由于设计的不合理造成频繁的缓存未命中，则会严重削弱NFV数据平面的性能。 （2）锁开销：当多个线程或进程需要对某一共享资源进行操作时，往往需要通过锁机制来保证数据的一致性和同步性，而加锁带来的开销会显著降低数据处理的性能。 （3）上下文切换开销：NFV的扩展需要多核并行化的支持，然而在该场景下，数据平面需要进行资源的分配调度，调度过程中涉及多种类型的上下文切换。在网卡中断、系统调用、进程调度与跨核资源访问等上下文切换过程中，操作系统均需要保存当前状态，而这一类的切换开销往往相当昂贵，严重影响系统性能。 以上3种开销对于NFV转发性能的影响较大，在实际的转发过程中，开销不止这3种。 针对以上影响转发性能的挑战，NFV在落地过程引入不同开源技术进行应对，具体的实现原理会在第二部分《NFV关键技术》中详细阐述，这里只是做一个简单的介绍，使初学者有个概念性的了解。 1. 轮询取代中断 作为I/O通信的另一种方式，轮询不存在中断所固有的开销。以网卡接收分组为例，在轮询模式下，系统会在初始化时屏蔽收发分组中断，并使用一个线程或进程来不断检测收取分组描述符中的收取分组成功标志是否被网卡置位，以此来判断是否有数据分组。整个收取过程没有发生上下文切换，因此也就避免了相应的开销。 当I/O速率接近CPU速率时，中断的开销变得不可忽略，轮询模式的优势明显；相反，如果数据吞吐率很低，中断能有更好的CPU利用率，此时不宜采用轮询模式。基于以上分析，针对网络流量抖动较大的场景，可以选用中断与轮询的混合模式，即在流量小时使用中断模式，当遇到大流量时切换为轮询模式。目前Linux内核与DPDK都支持这种混合中断轮询模式。 2. 零复制技术 零复制技术主要用以避免CPU将数据从一个内存区域复制到另一个内存区域带来的开销。在NFV数据平面操作的场景下，零复制指的是除网卡将数据DMA复制进内存外（非CPU参与），从数据分组接收到应用程序处理数据分组，整个过程中不存在数据复制。零复制技术对于高速网络而言是十分必要的。 DPDK、Netmap、PF-ring等高性能数据分组处理框架都运用了零复制技术，可以实现在通用平台下高效的网络处理，大幅提升单服务器内的报文转发性能。进一步地，DPDK不仅实现了网卡缓冲区到用户空间的零复制，还提供虚拟环境下的虚拟接口、兼容OpenvSwitch虚拟交换机、专为短小报文提供的hugepage访问机制等实用技术。 上述开源方案能很好地满足NFV中DPI（Deep Packet Inspection，深度数据包检测）、防火墙、CGN（Carrier-Grade NAT ，运营商级网络地址转换）等无需协议栈的网络业务功能，但存在着大量改写原有业务应用套接字的问题，应用中需要在性能提升与代码改动之间进行取舍。 3. 高效虚拟化技术 目前在NFV领域常用的高效虚拟化技术大致可以归为以下两类。 （1）基于硬件的虚拟化技术 I/O透传与SR-IOV是两种经典的虚拟化技术。I/O透传指的是将物理网卡直接分配给客户机使用，这种由硬件支持的技术可以达到接近宿主机的性能。不过，由于PCIe设备有限，PCI研究组织提出并制定了一套虚拟化规范——SR-IOV，即单根I/O虚拟化，也就是一个标准化的多虚机共享物理设备的机制。完整的带有SR-IOV能力的PCIe设备，能像普通物理PCIe设备那样被发现、管理和配置。 SR-IOV主要的应用还是在网卡上，通过SR-IOV，每张虚拟网卡都有独立的中断、收发队列、QoS等机制，可以使一块物理网卡提供多个虚拟功能（VF），而每个VF都可以直接分配给客户机使用。 SR-IOV使虚拟机可以直通式访问物理网卡，并且同一块网卡可被多个虚拟机共享，保证了高I/O性能，但SR-IOV技术也存在一些问题。由于VF、虚端口和虚拟机之间存在映射关系，对映射关系的修改存在复杂性，因此除华为外，大部分厂商目前还无法支持SR-IOV场景下的虚拟机迁移功能。另外，SR-IOV特性需要物理网卡的硬件支持，并非所有物理网卡都提供支持。 （2）半虚拟化技术 半虚拟化无需对硬件做完全的模拟，而是通过客户机的前端驱动与宿主机的后端驱动一同配合完成通信，客户机操作系统能够感知自己处在虚拟化环境中，故称为半虚拟化。由于半虚拟化拥有前后端驱动，不会造成VM-exit，所以半虚拟化拥有更高的性能。主流虚拟化平台Xen就使用了半虚拟化的驱动，半虚拟化比起SR-IOV的优势在于支持热迁移，并且可以与主流虚拟交换机对接。但是，在大流量转发场景下，前后端驱动中Domain0也是最大的瓶颈。 4. 硬件分流CPU能力 CPU具有通用性，需要理解多种指令，具备中断机制协调不同设备的请求，因此CPU拥有非常复杂的逻辑控制单元和指令翻译结构，这使得CPU在获得通用性的同时，损失了计算效率，在高速转发场景下降低了NFV的转发性能。 业界普遍采用硬件分流方法来解决此问题，CPU仅用于对服务器进行控制和管理，其他事务被卸载到硬件进行协同处理，降低CPU消耗，提升转发性能。 网卡分流技术是将部分CPU事务卸载到硬件网卡进行处理，目前大多数网卡设备已经能够支持卸载特性。网卡卸载的主要功能有：数据加解密、数据包分类、报文校验、有状态流量分析、Overlay报文封装和解封装、流量负载均衡，以及根据通信协议最大传输单元限制，将数据包进行拆分或整合。 除此之外，CPU+专用加速芯片的异构计算方案也是一种硬件分流思路。异构计算主要是指使用不同类型指令集（X86、ARM、MIPS、POWER等）和体系架构的计算单元（CPU、GPU、NP、ASIC、FPGA等）组成系统的计算方式。在NFV转发性能方面，使用可编程的硬件加速芯片（NP、GPU和FPGA）协同CPU进行数据处理，可显著提高数据处理速度，从而提升转发性能。 5．整体优化方案DPDK PCI直通、SR-IOV方案消除了物理网卡到虚拟网卡的性能瓶颈，但在NFV场景下，仍然有其他I/O环节需要进行优化，如网卡硬件中断、内核协议栈等。开源项目DPDK作为一套综合解决方案，对上述问题进行了优化与提升，可以应用于虚拟交换机和VNF。DPDK是Intel提供的数据平面开发工具集，为Intel处理器架构下用户空间高效的数据包处理提供库函数和驱动的支持。它不同于Linux系统以通用性设计为目的，而是专注于网络应用中数据包的高性能处理。有关DPDK的详细介绍，大家可参见《深入浅出DPDK》这本书。 一般来说，服务器上的每个CPU核会被多个进程/线程分时使用，进程/线程切换时，会引入系统开销。DPDK支持CPU亲和性技术，优化多核CPU任务执行，将某进程/线程绑定到特定的CPU核，消除切换带来的额外开销，从而保证处理性能。 同时，DPDK支持巨页内存技术。一般情况下，页表大小为4KB，巨页技术将页表尺寸增大为2MB或1GB，使一次性缓存内容更多，有效缩短查表消耗时间。同时，DPDK提供内存池和无锁环形缓存管理机制，加快了内存访问效率。 报文通过网卡写入服务器内存的过程中，会产生CPU硬件中断，在数据流较大的情况下，硬件中断会占用大量时间。DPDK采用轮询机制，跳过网卡中断处理过程，释放了CPU处理时间。服务器对报文进行收发时，会使用内核网络协议栈，由此产生内核上下文频繁切换和报文拷贝问题，占用了CPU周期，消耗了处理时间。DPDK使用户态进程可直接读写网卡缓冲区，旁路了内核协议栈处理。 DPDK以用户数据I/O通道优化为基础，结合Intel虚拟化技术（主要是VT-d技术）、操作系统、虚拟化层与虚拟交换机等多种优化方案，形成了完善的转发性能加速架构，并开放了用户态API供用户应用程序访问。DPDK已逐渐演变为业界普遍认可的完整NFV转发性能优化技术方案。但目前DPDK还无法达到小包线速转发，仍需进行性能提升研究和测试验证工作。 运营商如何推动三层解耦落地？在NFV方面，解耦是首当其冲的问题，目前业界有不解耦、软硬件解耦和三层解耦这3种思路，其中软硬件解耦又分为共享虚拟资源池和硬件独立两种方案，不同方案的对比介绍在本文的NFV部署模式部分已有介绍，这里不再赘述。 不解耦无法实现硬件共享，运营商依赖厂商，网络开放能力弱，不支持自动化部署，显然不符合NFV技术的初衷；而仅硬件解耦不支持多厂商VNF在同一云平台部署，运营商仍旧依赖厂商；三层解耦可以解决上述问题，但其涉及多厂商垂直互通，系统集成和维护难度大，部署周期长。NFV三层解耦要求在部署NFV时不同组件由不同的厂商提供，需要比传统电信网络更复杂的测试验证、集成和规划部署工作。 NFV分层解耦的方式由于缺乏主集成商（苏研努力的目标，陕西目前试点的主要目的）和完整验证，距离开放的全解耦目标还有相当距离，运营商会面临一定的运维风险和技术挑战。NFV分层解耦的技术挑战主要有以下几点： （1）不同厂商的硬件设备之间存在管理和配置的差异，如存储设备管理配置、安全证书、驱动、硬件配置等方面的问题，会导致统一资源管理困难、自动化配置失效；另一方面，各类VNF和虚拟化软件部署于不同的硬件设备上，在缺乏预先测试验证的情况下，硬件板卡或外设之间，如PCIe网卡、RAID卡硬件、BIOS，存在兼容性不一致问题。因此，NFV三层解耦规模商用前，需要运营商细化服务器安全证书、硬件选型方面的规范要求，重点关注硬件可靠性和兼容性问题，在商用前进行软硬件兼容性和可靠性验证。以上问题需要通过大量的适配、验证和调优来解决。 （2）不同基础软件之间存在兼容性问题，如操作系统与驱动层之间、虚拟交换机与操作系统之间、虚拟化软件与VNF之间，不同的模块和不同的版本，以及不同的配置参数、优化方法，都会造成性能、稳定性、兼容性的较大差异，有待进一步测试与验证。为此，需要尽量减少虚拟化层类型，适时引入自主研发虚拟化层软件，减少持续不断的三层解耦测试工作量。采用集中的云管平台（统一VIM），降低NFVO与VIM集成的复杂度。 （3）分层之后，从NFV各层之间的接口定义与数据类型，到层内功能的实现机制，乃至层间的协同处理均需要运营商去推动和完善。如VNF在发生故障时，涉及VM迁移与业务倒换机制以及NFVI、NFVO和VIM的处理流程；又如VNF对配置文件管理和存储设备使用不当，同样会导致VM实例化失效。因而，在VNF多厂家集成过程中，集成方或者运营商需要需要有角色对问题定界、定位进行裁决，在集成和运维的过程中，对技术问题进行端到端的管理，对各层的功能进行详细定义或者详细规范。 （4）NFV系统集成涉及多厂商、多软硬组件的高度集成，由于虚拟化环境的存在，在初期的测试验证、中期的系统部署、后期的运维过程中，进行系统评测与管理部署都较为困难。这就要求运营商在提升DevOps能力的基础上，依托持续集成与持续部署和运维自动化技术，形成NFV系统的持续集成、测试和部署能力，大白话就是要求运营商亟待需要提升自主开发、自主集成和自主测试能力。同时，MANO架构需要全网统一。由于目前VNFM通常与VNF是绑定的厂商组件，而实际上真正的VIM也是厂商提供的，因此VNFM、VIM仍然是与VNF、NFVI就近部署。所以需要尽早明确NFVO的架构（例如，采用集团NFVO+区域NFVO两层架构），明确VNFM和VIM的跨专业、跨地域部署能力和部署位置，明确已部署的云管平台与VIM架构的关系，以及已有的EMS、NMS与VNFM架构的关系。 对于运营商来说，三层解耦会是一个较长的过程，与厂商的博弈也需要时间，再加上自主能力（研发、测试、集成）也需要时间，在实现最终目标之前可以先选择过渡方案，例如厂商一体化方案（不适合作为商业化规模部署方案）、部分解耦方案（硬件与软件解耦、MANO中的NFVO解耦出来）等，在试点和小规模部署过程中培育能力，逐渐实现最终的解耦目标，并在解耦基础上逐步提升自主研发比例，增强对网络NFV化的掌控力。 MANO管理模式利弊分析EISI NFV对MANO的资源管理提出直接模式和间接模式两种方案。NFV-MANO允许NFVO和VNFM两者都能管理VNF生命周期所需的虚拟化资源，直接和间接是相对VNFM而言的。 （1）直接（Direct）模式：VNFM直接通过VIM分配VNF生命周期管理所需的虚拟化资源。VNFM向NFVO提出对VNF的生命周期管理操作进行资源授权，NFVO根据操作请求及整体资源情况返回授权结果；VNFM根据授权结果直接与VIM交互完成资源的调度（分配、修改、释放等）；VNFM向NFVO反馈资源变更情况。如下图所示： （2）间接（Indirect）模式：VNFM向NFVO提出对VNF的生命周期管理操作进行资源授权，NFVO根据操作请求及整体资源情况返回授权结果；VNFM根据授权结果向NFVO提出资源调度（分配、修改、释放等）请求，NFVO与VIM交互完成实际的资源调度工作；NFVO向VNFM反馈资源变更情况。如下图所示： 总体而言，两者都由VNFM提供VNF生命周期管理。在执行VNF生命周期管理操作之前，无论该操作新增资源，还是修改或者释放已分配的资源，VNFM都需要向NFVO请求资源授权；资源容量和状态等信息由NVFO统一维护管理。两种模式的不同主要体现在：直接模式下，VNFM和NFVO都需要与VIM交互，将VIM的虚拟资源管理接口暴露给VNFM使用；间接模式下，VFNM不需要和VIM进行交互，NFVO需要提供VIM代理能力。 两种模式在架构、业务成效、性能、集成复杂度以及安全性方面的对比分析如下所示： 综合以上分析，从功能、落地部署、安全性、未来演进角度考虑，间接模式较好；性能方面，直接模式占优；系统集成复杂度两者相当。考虑网络的未来发展，从运营商对网络的自主掌控能力出发，要求厂商必须支持间接模式，以推进分层解耦、实现对虚拟资源的统一管控。 VNFM如何选型？通用VNFM和专用VNFM是ETSI定义的两种架构选项。 （1）通用VNFM：通用VNFM可以服务于不同类型或不同厂商提供的VNF，对它所管理的多种类型、多厂商VNF的操作没有依赖性，但它必须能够在VNF包中定义的不同VNF的特定脚本。按照管理要求，可能有多个通用VNFM，每个VNFM管理一定VNF的子集。在这种情况下，NFVO需要同时处理多个通用VNFM。下图展示了通用VNFM的架构。 （2）专用VNFM：专用VNFM与它所管理的VNF之间具有依赖性，一般管理由同一厂商提供的VNF。NFV架构框架同时也允许一个或多个专用VNFM连接到单个NFVO。在VNF生命周期管理过程复杂，且一些管理特性与这些VNF紧耦合的场景下，就需要使用专用VNFM。下图展示了专用VNFM的架构。 两种架构选项具有相同的VNFM功能要求，如VNFD解析，获得部署VNF所需资源要求及所需部署的业务软件；NFVI告警与VNF告警关联、VNF弹性策略执行；VNF生命周期管理，包括实例化、查询、扩/缩容、终止等。但是，两种架构在技术实现难度、运维复杂度等方面却存在着差异。 NFVO如何部署？目前，ETSI NFV进一步细化了NFVO功能模块的具体功能要求。按照MANO规范，NFVO可以分解为网络服务编排（Network Service Orchestrator，NSO）和资源编排（Resource Orchestrator，RO）。网络服务生命周期的管理功能，即NSO功能；跨VIM的NFVI资源编排功能，即RO功能。NFVO作为MANO的一个功能实体，在部署时，可以有如下两种部署形态。 1. NFVO功能不分解部署 NFVO作为一个独立的实体部署，可采用级联的方式来部署。如下图所示，每个管理域可以被当作一个或多个数据中心，在该管理域中部署一套独立的NFVO，以及VNFMs、VIMs，用来管理该域中的网络服务。另外，再部署一套顶层NFVO，用来管理域间的网络服务，它并不管理下层管理域中的网络服务，不过它可以接收下层管理域中网络服务实例化、弹性伸缩，以及终止操作的请求，并将此请求直接传递给下层管理域中的NFVO，由下层管理域的NFVO来完成实际的操作。 2. NFVO功能分解部署 NFVO可以分为两个独立的实体来部署，NSO主要完成NS的生命周期管理，包括NS模板以及VNF包的管理，如下图所示。NSO不再关注资源的状态以及资源所在的管理域，仅关注资源的额度。RO主要完成管理域内资源的管理和编排，如资源的预留、分配、删除等操作，以及支持资源的使用率和状态的监控。 NFVO功能不分解部署时，资源申请效率高；集成难度相对较低；若NFVO故障，则只会影响该NFVO管理域的业务和资源。NFVO分解后，VNFM访问或申请资源的效率会降低；如果RO出现故障，则只会影响该RO管理的资源；但是，一旦NSO出现故障，则将影响所有整个NFV的业务功能；NFVO分解为NSO、RO之后，或增加NSO-RO之间的接口，增加系统集成难度。 根据分析比较，在一定的业务规模下，将NFVO分解为NSO、RO难以带来明显的优势或收益，反而会导致性能降低、集成复杂。因此，建议NFVO采用不分解架构。另外，考虑后续的演进和发展，在技术架构上可将NSO和RO进行内部功能解耦，并实现微服务化，以增强未来NFVO部署的灵活性。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-19-NFV网络参考架构]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-19-NFV%E7%BD%91%E7%BB%9C%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[ETSI定义的NFV网络参考架构ETSI作为NFV的发起标准组织，于2015年年初发布了NFV参考架构等系列文稿，具体包括：用例文档、架构框架、虚拟化需求、NFV基础设施、NFV MANO、VNF、服务质量、接口、安全、PoC框架、最佳实践等内容。虽然ETSI NFV阶段成果不是强制执行的标准，但是得到了业界的普遍认可，已经成为了业界的事实标准。 目前，NFV标准框架已基本稳定，如下图所示。NFV标准框架主要包括NFV 基础设施、虚拟网络功能和NFV管理与编排（NFV Management and Orchestration，NFV MANO）。ETSI定义的NFV技术架构同当前网络架构（独立的业务网络+OSS系统）相比，NFV从纵向和横向上进行了解耦，纵向分为3层：基础设施层、虚拟网络层和运营支撑层。横向分为两个域：业务网络域和管理编排域。 ​ VIM + NFVI 基本等同于OpenStack管理下的IaaS NFVI包括各种计算、存储、网络等硬件设备，以及相关的虚拟化控制软件，将硬件相关的计算、存储和网络资源全面虚拟化，实现资源池化。NFVI物理基础设施可以是多个地理上分散的数据中心，通过高速通信网连接起来，实现资源池统一管理。 VNF运行在NFVI之上。VNF旨在实现各个电信网络的业务功能，将物理网元映射为虚拟网元 VNF，VNF所需资源需要分解为虚拟的计算、存储、交换资源。VNF作为一种软件功能，部署在一个或多个虚机上，并由NFVI来承载。VNF之间可以采用传统网络定义的信令接口进行信息交互。VNF的性能和可靠性可通过负载均衡和HA等软件措施以及底层基础设施的动态资源调度来解决。 EMS（Element Management System，网元管系统）可以管理VNF，厂商通常对原网管系统进行扩展，统一管理虚拟化和非虚拟化的网元。 运营支撑层OSS/BSS，就是目前的OSS/BSS系统，需要为虚拟化进行必要的修改和调整。为了适应 NFV 发展趋势，未来的业务支撑系统（BSS)与运营支撑系统（OSS）将进行升级（中国移动目前正推出OSS4.0系统），实现与VNF Manager和网元编排管理的互通。 NFV MANO（NFV Management and Orchestration，NFV管理与编排）负责对整个NFVI资源的管理和编排，业务网络和NFVI资源的映射和关联，OSS业务资源流程的实施等。MANO内部包括编排管理（Orchestrator）、虚拟化的网络功能管理（VNF Manager，VNFM）和虚拟化的基础设施管理（Virtualised Infrastructure Manager，VIM）3个实体，分别完成对NFVI、VNF和NS（Network Service，业务网络提供的网络服务）3个层次的管理。其中，Orchestrator编排管理NFV基础设施和软件资源，在NFVI上实现网络服务的业务流程和管理。VNFM实现VNF生命周期管理，如实例化、更新、查询和弹性等。VIM控制和管理VNF所需要的与算、存储和网络资源的管理和调度，即所谓的Cloud OS。 NFVI到底是个什么鬼？NFVI能够同时为一个或多个VNF实例提供基础设施资源，并实现不同VNF资源的动态配置。ETSI对NFVI功能架构和接口进行了定义，将NFVI细分为计算域、管理域和网络域3个功能域。每个域自主管控，通过标准接口进行信息交互，协作实现对VNF的具体承载。 计算域包括通用高容量服务器和存储设备。 管理域包括各种Hypervisor，对硬件进行抽象以支撑软件应用在不同服务器之间的可移植性；为虚拟机VM分配计算域资源；为编排和管理系统提供管理界面，允许虚拟机VM的加载和监控。 网络域包括所有的通用高容量交换机，这些交换机互联形成一个可配置网络，提供基础设施网络服务。 NFVI内部3个域之间接口示意如下图所示： 计算域包括各种服务器与存储设备，其作用是结合Hypervisor域的一个管理程序，负责VNF各个组件的需求，提供COTS计算和存储资源。概括地说，计算域提供与网络基础设施领域的接口，但其自身不支持网络连接。根据给定计算域的元素：服务器、存储和网络结构，各种接口可以被归类为：物理网络接口、内部和外部域接口、管理和编排接口。 物理网络接口：有几种不同类型的设备接口可能被利用，这些接口包括以太网、光纤通道、无线接入（这几个接口是比较常用的，但不仅限于这3类）。 内部和外部域接口：该部分主要是两个接口，一个是NFVI与VIM的内部接口，另一个是与VNF应用的外部接口。NFVI与VIM（Nf-Vi）的接口主要负责VIM与NFVI相连，被用作管理接口，是一个内部接口。而VIM被作为一个单独的外部服务，这样做既考虑了逻辑上的隔离，同时也顾及到安全方面的因素。Nf-Vi接口是唯一授权的NFV管理界面和交互接口，其管理组件就是VIM。同时，在NFVI内，Vl-Ha/CSr连接到外部计算域，包括所有的基础设施，是计算域和Hypervisor域之间的接口，该接口被Hypervisor/OS用于监控计算域内可用的物理资源。 管理和编排接口：NFVI的编排和管理是严格通过Nf-Vi接口的。为了正常工作，应当有某种类型的接口，物理接口的类型并不重要，重要的是需要有一个专门的安全接口，该接口将被用来建立基础设施的鉴权和授权。除此接口外，每个底层域组件（如计算、存储、网络）都应当有相应的代理。 Hypervisor域本身是抽象硬件和实现服务的软件环境，如启动/终止虚拟机，作用于策略、缩放、实时迁移和高可用性。仅当VIM被告知或者指示Hypervisor域的主要接口时，服务才能被启用。Hypervisor域的主要接口包括NF-Vi、Vi-Ha和Vn-NF接口，其内部结构如下图示。 NF-Vi接口：对接VIM的接口，虚拟机监控程序服务请求需通过该接口。只有VIM或NFVO才能通过这些接口与Hypervisor交互。Hypervisor不得自主执行业务，除非是在VIM应用策略的范围内。 Vi-Ha接口：通过该接口Hypervisor获取硬件信息，并创建虚拟机需要使用的虚拟硬件组件。 Vn-NF接口：该接口是VM到VNF的逻辑接口。一个VNF由一个或多个虚拟机创建。虚拟机在本质上是软件运行某种函数、算法、应用，而忽略其类型、模型、实际物理单元的数量。 网络域的主要功能包括：分布式VNF的虚拟网络功能组件之间进行通信的通道、不同VNF之间通信的通道、VNF编排和管理模块之间的通信通道、NFVI编排和管理功能模块之间的通信通道、VNFC远程部署的方案和现有运营商网络之间交互通信的方案。 网络域的网络架构应该提前设置好VNF的网络结构，同时提供足够的网络连接。为了实现上述功能，网络架构需要包含足够多的组件来提供网络域的功能。包含的组件如下： 1）网络架构的寻址机制，可以有多于一种的寻址机制，同时该机制还包括地址定位和管理功能； 2）路由机制，该机制用来生成网络拓扑，并进行路由转发； 3）带宽定位机制； 4）OAM集合，用来检测可靠性、可用性、连接服务的完整性。 网络域中的虚拟网络由NFVI中网络组件提供，通常在数据中心内部或数据中心之间使用使用L2或L3的overlay网络来实现。所谓的overlay网络，就是重叠在数据中心内部TOR/EOR交换机组成的物理网络之上的网络，通过在多个虚拟网络功能实体之间建立隧道来完成互联。 网络域的内部接口可以南北接口和东西接口，按类型分为管理协议接口、管理模块/信息接口、数据模型和流监控协议接口三类。南北向接口可以参见传统网络SNMP接口模型，通过MIB模块中的数据模块提供管理协议接口。新的南北向接口是NetConf接口，使用YANG数据模型，定义简单，支持多种协议，典型应用就是SDN控制器的管理。东西向接口主要分为前向关联和后向反射两种模式。前向关联模式主要用来探测服务故障，后向反射模式主要用来进行故障状态的上报和恢复检测。 网络域的外部接口主要有：不同网络资源之间的接口[Vi-Ha]/Nr，例如网络资源、端口资源之间就是采用这类接口；[Vn-Nf]/N接口用于VNF（虚拟网元）接入到虚拟网络的接口；Ha/CSr-Ha/Nr计算域、网络域与物理资源的接口；Ex-Nf是NFVI与现有网络之间的永久接口。 虚拟化场景下的逻辑组网如下 虚拟层 DVS：虚拟分布式交换机，与物理交换机一样，构建起虚拟机之间的网络，并提供与外部网络互通的能力. 接入层：划分为存储网络，业务网络和管理网络，各个网络层面间，通过划分不同的vlan将管理、业务、存储三个平面逻辑隔离。为简化组网提高组网可靠性，建议接入交换机采用堆叠方式 汇聚层：接入交换机上行到汇聚层交换机，采用ETH-TRUNK上行至汇聚交换机，汇聚交换机堆叠之后，无需启用VRRP功能，如果需要汇聚交换机提供网关功能，则直接将VLANIF接口作为用户网关地址。建议采用集群方式。. 核心层：汇聚交换机上行接入核心层交换机，核心交换机采用OSPF或者静态路由的方式同上层设备进行对接.建议采用集群方式。 虚拟网元层VNF虚拟网元功能VNF旨在通过虚拟化技术实现各种电信业务网络功能，将物理网元映射为虚拟网元VNF，VNF所需资源可以分解为虚拟的计算、存储和交换资源，由NFVI来承载。如下图所示： ETSI NFV架构中，VNF是NFV总体架构的一个功能单元，通过接口参考点Vn-Nf和Ve-Vnfm，分别与NFVI模块和MANO 的VNFM模块进行信息交互。VNF以软件模块形式部署在NFVI提供的资源上，从而实现网络功能虚拟化。 VNF之间的接口指两个实体之间的交互点，这些实体可以是软硬件服务或者资源。软件接口将各个软件功能实体分开，它们被创建并用于软件实体之间的相互作用与交互，同时限制各软件实体之间的通信。ETSI和3GPP为VNF定义了以下5种接口： SWA-1接口：这是各种网络功能之间明确定义的通信接口，表示网络功能上信令或媒体的接口。SWA-1接口是在2个VNF，或者一个VNF和PNF，或者一个VNF和一个节点之间。一个VNF可以支持多个SWA-1接口。 SWA-2接口：属于VNF内部接口，如VNFC到VNFC的通信。这些接口由VNF厂商定义，因此是私有接口。这些接口对底层虚拟化基础设施有明确要求，但对一个 VNF 而言用户是不可见的。SWA-2 接口利用底层的通信机制连接到SWA-5接口上网络连接模块。 SWA-3接口：VNF与NFV管理和编排器间的接口，特别是与VNFM间的接口。管理接口用来为一个或多个 VNF 执行生命周期管理，SWA-3 接口可以使用IP/ L2连接。 SWA-4接口：EM使用SWA-4接口与VNF通信，用于VNF的运行期的管理。 SWA-5接口：对应VNF-NFVI接口，存在于每个VNF和底层NFVI之间的接口。每个不同的VNF依赖于一组不同的SWA-5接口提供的NFVI资源，如网络、计算、存储等。SWA-5接口描述了一种用于VNF的可部署实例的执行环境，是NFVI和VNF本身之间的所有子接口的抽象，SWA-5子接口为：通用计算功能、专用功能、存储、网络I/O。 NFV管理与编排层在传统的网络中，网络功能实现常常与它们运行的架构、资源等紧紧联系在一起。NFV引入了一个新的虚拟化层，让云化后的网络管理格局变得更加复杂，需要更多编排工作，以管理 VNF 和网络服务（Network Service, NS）的全生命周期。VNF能够被其他的VNF或者物理网络关联来实现一个网络服务。VNF软件和NFVI之间的解耦，NS可以包括相关虚拟链路、物理网络功能、虚拟网络功能、网络功能虚拟化基础设置以及相互之间的关系等等这些，它们的协作处理需要一组新的管理和业务流程功能。为此，在NFV网络架构引入了MANO这个逻辑实体，实现NFV网络架构在横向的业务网络管理和编排管理。 在ETSI的定义中，对引入NFV-MANO架构框架，要求遵循一些原则： 1、编排由多个功能块提供，功能块不进行优先级区分，即没有哪个功能块是优先于其他功能块。 2、在架构功能的分类上要求遵循总体平等的架构原则。 3、NFV-MANO功能可能以不同的方式实现，如作为整体的单一实例、作为一个带有多负载均衡的可扩展系统、作为一个分布式协作系统、一个功能上分解的分布式的系统。它也可以实现为云网一体化管理的扩展，或者作为一个实现 NFV功能的与云网管理系统交互的单独系统。 基于上述原则，NFV-MANO既可以通过虚拟化以纯软件的方式实现，也可以异构管理跨地域的NFVI，实现基于策略的分布式统一纳管和自动化响应能力。 参考ETSI NFV文稿，MANO内部包括虚拟基础设施管理、虚拟网络功能管理和网元/服务编排器3个实体，分别实现对NFVI、VNF和NS这3个层次的管理。MANO的架构如下图右边所示 NFVO：编排器，具有两个主要职责，一是执行资源编排功能，对多VIM之间的NFVI资源进行编排；二是完成网络服务生命周期管理，执行网络服务编排功能。 VNFM：虚拟网元管理单元，负责VNF实例的生命周期管理，每个VNF均有一个与之关联的 VNF Manager，一个VNF Manager可以管理一个或多个VNF。 VIM：虚拟化基础设施管理单元，主要实现对NFVI纳管的计算、存储和网络资源的控制与管理，通常在一个运营商的基础设施域内。 NS Catalogue：网络服务目录，代表所有加载的网络服务资源，通过 NFVO所公开的接口支持创建和管理NS部署模板，包括网络服务描述（Network Service Descriptor，NSD）、虚拟链路描述（Virtual Link Descriptor，VLD）和VNF转发图描述（VNF Forwarding Graph Descriptor，VNFFGD）。 VNF Catalogue：虚拟网络功能服务目录，代表了所有加载的VNF网元资源，通过NFVO公开的接口来创建和管理VNF网元，包括虚拟网元功能描述（VNF Descriptor，VNFD）、软件镜像、清单文件和策略文件等。NFVO和VNFM均可通过查询VNF Catalogue来发现和检索VNFD以支持不同的操作，如鉴权授权或检查实例的可行性。 NFV Instances Repository：NFV实例库，记录所有VNF实例和网络服务NS实例的信息。每个VNF实例由一条VNF记录来表示，每个NS实例由一条NS记录来表示。这些记录在每个实例的生命周期中被反复刷新，用于反映执行NS生命周期管理操作或VNF生命周期管理操作的变化。 NFVI Resources Repository：NFVI资源库，记录可用的、保留的及可分配的NFVI资源信息，支持资源预留、分配和用于监控的有用信息。NFVI资源库主要用于NFVO的资源编排和管理，允许根据与NS和VNF实例相关联的资源来追踪NFVI保留和分配资源，如某个VNF实例在其生命周期内的任意时刻所使用VM的数量、配额、端口和弹性IP等信息。 NFV-MANO与多个模块或系统具有接口关系，包括资源管理、虚拟化网络功能、运营系统支撑、业务系统支撑功能和NFV基础设施等，例如：Os-Ma-nfvo为OSS/BSS和NFVO之间的接口；Ve-Vnfm-em是EM和VNFM之间的接口等等，通过这些定义良好的接口，实现MANO和外围模块或系统之间良好协作，华为IES，中国移动OSS4.0等都是基于MANO的这些接口实现。 参考上述ETSI NFV架构，结合电信业务特点，给出一种NFV-MANONFV-MANO系统实现方案。如下图所示，该系统的主要功能模块包括：网络服务编排管理模块（NFVO）、虚拟化网络功能管理模块（VNFM）、虚拟资源管理模块（VIM）、视图管理、接口管理和Rest API。 结合上图实现虚拟化编排的主要系统流程。 1）用户上传VNF以及NS模板； 2）选取需要实例化NS模板，如IMS； 3）系统解析模板，根据策略配置决定如何初始化VNF； 4）根据VNF模板配置初始化需要的虚拟网络环境，如子网、路由、ACL、NAT等； 5）创建VNF需要的云主机； 6）实例化NS所需的VNF； 7）监控脚本监控VNF服务是否启动，并发送监控数据； 8）通过监控脚本判断IMS是否启动成功。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-18-NFV基础概念]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-18-NFV%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[NFV技术的起源和概念在移动互联网时代，运营商面临内外困局。就自身而言，采用的流量增长—网络扩容—收入增长的商业模型正在失效，庞大、僵化的电信基础网络，不能够满足用户的丰富需求；就竞争对手而言，互联网企业以天为计的业务迭代时间，能够很好地贴合用户需求，飞速发展的OTT业务，使运营商越来越趋向于管道工的角色。 随着云计算普及和x86服务器性能提高，各大运营商为了避免进一步成为哑管道的尴尬，由全球各大运营商牵头提出网络功能虚拟化技术（Network Function Virtualization，NFV）。NFV的思路是通过虚拟化技术降低成本，实现业务的灵活配置。对运营商来说，NFV是一次改变困局、实现跨越发展的难得机遇，一方面可以降低CAPEX和OPEX成本，降低整体的TCO；另一方面也可以加速新产品推出和业务创新。 所谓的网络功能虚拟化就是利用IT虚拟化技术将现有网络设备功能整合进标准x86服务器、存储和网络数通设备，以软件的形式实现网络功能，以此取代目前网络中私有、专有和封闭的网元设备。NFV网络功能示意图如下所示： 维基百科对NFV的定义是：NFV是一种网络架构概念，给予IT虚拟化技术将网络功能节点虚拟化为可以链接在一起提供通信服务的功能模块。 OpenStack基金会对NFV的定义是：通过软件和自动化替代专用网络设备来定义、创建和管理网络功能的新方式。 ETSI NFV标准化组织对NFV的定义是：NFV致力于改变网络运营商构建网络的方式，通过IT虚拟化技术将各种网元变成独立的应用，可以灵活部署在基于标准服务器、存储、交换机构建的统一平台上，实现在数据中心、网络节点和用户端等各个位置的部署与配置。 NFV并非只是简单地在设备中部署虚拟机，其重要特征在于引入虚拟化层之后，虚拟功能网元与硬件完全解耦，改变了电信领域软件、硬件绑定的设备提供模式。打破了传统电信设备的竖井式体系，其核心是网元的分层解耦和引入新的MANO管理体系实现全生命周期管理。 NFV的组织NFV的工作开展涉及标准化和开源两条线，需要多组织的协作，促进NFV技术架构的成熟。其中，标准化线中3GPP SA5、TMF等组织负责NFV流程与接口的设计，ETSI NFV ISG以及ITU-T等组织负责NFV的需求和框架，并为流程和接口提供管理功能。而开源线中包括开源集成软件NFV开放平台项目（OPNFV,Open Platform for NFV)以及相应的开源组件（OpenStack，KVM，OVS等）。 标准化线条 ETSI：在2012年，由全球13家网络运营商（AT&amp;T、BT、Century Link、中国移动、Colt、Deutsche、Telecom、KDDI、NTT、Orange、Telecom Italia、Telefonica、Telstra、Verizon）提出NFV的目标与行动计划，并主要负责NFV接口参考点定义、流程、需求和信元定义。 3GPP：在2013年Rel-13中也开始关注NFV移动网络虚拟化的研究，其主要负责NFV技术在5G业务和市场的应用。 DMTF：2015年开始将自身研究与NFV工作结合，开始支持NFVD需求，支持VNF的管理和网络管理。 CCSA：国内主导NFV技术的组织，主要负责承载网、核心网、接入网等网络功能虚拟化技术研究，编排和接口功能需求以及虚拟化管理技术研究。 开源线条 OPNFV：2014年，由AT&amp;T、NTT、中国移动、RedHat、爱立信等厂商发起OPNFV开源社区，目的是为NFV提供一个统一的开源基础平台，集成OpenStack、OpenDaylight、OVS、ceph等上游社区的成果，推动上游社区加速接纳NFV架构。 OPEN-O：2016年，由华为与Linux基金会、中国移动共同举办OPEN-O新闻发布会，携手中国电信、韩国电信、爱立信、因特尔、RedHat、F5等15家产业领导者发起全球首个统一SDN和NFV开源协同器OPEN-O，主要负责NFV的管理和编排方面的研究。 NFV的技术基础NFV的目标是降低运营商成本的同事提供服务的灵活性和资源的利用率。近几年，标准服务器技术、虚拟化技术、云计算、SDN、开源项目的发展都推动了NFV技术产生和应用。 1、标准服务器 移动通信网络是一个“标准先行”的网络。在传统网络下，由于所有的网络流程、协议信元都经过深入讨论形成标准之后，各设备厂商才设计产品、实现功能。因此，不同的硬件根据实现的功能不同其设计标准、布局和元器件选择等均不相同，不同硬件之间无法兼容替换。同时，由于移动通信协议是固定的，厂商产品所处理的内容不会超过协议定义的范围，一般采用专有芯片来处理完成。因此，这类硬件无法承担较复杂的计算任务，虽然性能优越，但部署成本较高。如下图所示两种专有硬件设计： 在网络功能虚拟化技术中，行业标准服务器的使用是一个关键，不仅软硬件兼容可替换，且供应商市场充分竞争。近几年，x86、ARM架构的快速发展，使得服务器平台多CPU、多核、多线程技术非常成熟。同时，随着SR-IOV网卡的广泛应用，DPDK技术的开源化，使得通用芯片的计算能力，通用网卡的转发能力也越来越强（已经达到40Gbit/s等）。 2、虚拟化技术 在计算机科学中，虚拟化技术是一种资源管理技术，将物理资源抽象，提供给用户使用，打破物理资源的实现方式、地理位置、封装等限制。不仅提高了资源的最优化利用，而且还可实现资源的负载均衡、节能减排和自愈等功能。主要涉及计算虚拟化、存储虚拟化和网络虚拟化三大领域虚拟化技术。 在虚拟化技术实现中，Hypervisor是所有虚拟化技术实现的核心，它是运行基础服务器和操作系统之间的中间层软件，如VMware的ESX。通过它，多个操作系统和应用程序可以共享硬件资源，协调和管理服务器上所有物理设备和虚拟机，因此，也称为虚拟机监视器VMM（Virttual Machine Monitor）。主流的Hypervisor有VMware vSphere、Hyper-V、XenServer、PowerVM、KVM、FusionCompute等。 3、云计算 目前，云计算没有一个统一的定义。维基百科的定义是：云计算是一种基于互联网的计算方式，通过这种方式，共享软硬件资源和信息可以按需提供为计算机和其它设备。云计算的核心思想是将大量用网络连接的计算资源统一管理和调度，构成一个计算资源池向用户按需服务。提供资源的网络就称为“云”。 云计算提供三种服务模式，分为基础设施即服务（IaaS）、平台即服务（PaaS）和软件即服务（SaaS）。网络功能虚拟化NFV主要用于电信网络领域，因此也被称为电信网络云化实现，简称电信云。既然是“云”，那就必然离不开云计算的三种服务模式。在NFV的网路架构中，VIM/NFVI是真正的与“云”有关联的部分，其对应云计算三种服务模式的IaaS。而NFV的VNFM和NFVO部分其本质只是“云”上的APP应用。 云计算部分详见《云计算的技术架构》。 提到云计算必然要提及OpenStack，这一个开源云操作系统从诞生起，社区活跃度就非常高。目前，除了Amazon、微软外，几乎所有的私有云和公有云解决方案都是基于开源OpenStack的改造和增强版。那么NFV，也就是电信云的VIM+NFVI是否也可以采用OpenStack？从NFV的技术特点和应用场景来看，其对OpenStack有三点基本诉求： 1、业务面的高性能要求：涉及虚拟机性能和网络转发性能两个方面。 2、高可用和高可靠性要求：涉及控制平面、业务平面和存储平面三个方面。 3、易运维要求：涉及云化电信网络统一管理、统一调度、自动化运维等方面。 为此，为了适用NFV，OpenStack需要在业务平面进行相应增强，具体如下图所示： 上述增强技术主要涉及VIM层面和OpenStack+KVM的场景，主要改变在Nova和Neutron中，本质上是对虚拟化性能优化技术的充分利用。 4、SDN 网络设备一般由控制平面和数据平面组成。控制平面为数据平面制定转发策略，规划转发路径，如路由协议、网关协议等。数据平面则是执行控制平面策略的实体，包括数据的封装/解封装、查找转发表等。目前，设备的控制面和转发面都是由设备厂商自行设计和开发，不同厂家实现的方式不尽相同。并且，软件化的网络控制面功能被固化在设备中，使设备使用者没有任何控制网络的能力。这种控制平面和数据平面紧耦合的方式带来了网络管理复杂、网络测试繁杂、网络功能上线周期漫长等问题。因而，软件定义网络应运而生。 SDN技术是软件定义网络，本质是把网络软件化，提高网络可编程能力和易修改性。SDN没有改变网络的功能，而是重构了网络的架构。SDN的价值在于：网络业务自动化和网络自治，更快部署网络业务实例。更快在网络中增加新业务，大量需求仅需要升级控制器软件就可以实现。同时，简化了网络协议，大量网络业务协议逐渐消失，用户的策略处理集中在控制器实现。通过集中控制，对网络资源进行统筹调度和深度挖掘，提高网络资源利用率，接入更多业务，从垂直整合走向水平整合，使得芯片、设备、控制器各层可以独立分层充分竞争。 NFV没有改变设备的功能，而是改变了设备的形态。NFV的本质是把专用硬件设备变成一个通用软件设备，共享硬件基础设施。NFV的价值在于：软件设备的发行安装速度远比硬件设备快，容量伸缩更快，避免硬件采购安装的长周期，可按需实时扩容。实现新需求新业务更快，避免了硬件的冗长开发周期。同时，简化了设备形态，统一了底层硬件资源，都是服务器和交换机。采用通用服务器作为和交换机作为基础设施，大大降低设备成本。水平整合改变了原来的竞争格局，各个层次可以分层竞争。 SDN与NFV有什么关系？NFV的软件设备（统称VNF）快速部署以及VNF之间网络快速建立，需要支持网络自动化和虚拟化能力，这需要SDN网络提供支持。在SDN网络情况下的一些网络诉求，比如能够快速提供虚拟网络，快速部署增值业务处理设备和网络设备等这些快速业务上线需求，需要NFV的软件网络设备（FW、vRouter）才能达成目的。 所以，SDN是面向网络的，SDN没有改变网络的功能，而是重构了网络的架构。NFV是面向设备的，NFV没有改变设备的功能，而是改变设备的形态。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-16-Linux系统命令-第二篇《文件或目录操作命令》]]></title>
    <url>%2F2019%2F04%2F17%2F2019-04-16-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%BA%8C%E7%AF%87%E3%80%8A%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[pwd：显示当前所在的位置pwd命令是“print working directory”中每个单词的首字母缩写，其功能是显示当前工作目录的绝对路径。在实际工作中，我们在命令行操作命令时，经常会在各个目录路径之间进行切换，此时可使用pwd命令快速查看当前我们所在的目录路径。 语法格式：pwd [option] 通常情况下，执行pwd命令不需要带任何参数。 重要选项参数 【使用举例】1）不带任何选项执行pwd命令。 1[root@C7-Server01 ~]# pwd #显示当前目录为root用户的家目录 2）对比使用-L和-P参数 1[root@C7-Server01 ~]# ls -l /etc/rc0.d # 显示/etc/目录下rc0.d的目录信息 显示结果为/etc/rc0.d目录为/etc/rc.d/rc0.d目录的软链接 1[root@C7-Server01 ~]# cd /etc/rc0.d/ # 进入/etc/rc0.d目录 在该目录下分别执行pwd -L和pwd -P，对比执行结果。 当前系统命令提示符的显示格式是系统默认的格式，受环境变量PS1限制，查看环境变量PS1的方法为： 1[root@C7-Server01 ~]# echo $PS1 如果要修改系统提示符的显示格式，可以根据下表自行调整/etc/profile文件中PS1环境变量的设置，修改完成保存退出，通过source /etc/profile加载新设置的环境变量使配置生效。 cd：切换目录cd命令是“change directory”中每个单词的首字母缩写，其功能是从当前工作目录切换到指定的工作目录。 语法格式：cd [option] [dir] cd命令后面的选项和目录等参数都可以省略。默认情况下，单独执行cd命令，可切换到当前登录用户的家目录（由系统环境变量HOME定义）。cd是bash shell的内置命令，查看该命令对应的系统帮助需要使用help cd。在使用cd命令时，如果使用键盘上“Tab”键的自动补齐功能，可以提高输入速度和准确度。这个“Tab”键的自动补齐功能同样也适用于其他命令。 要了解路径的概念，比如，相对路径是不从“/”（斜线）开始的路径，而是从当前目录或指定的目录开始，如：data/、mnt/oldboy；绝对路径是从“/”（斜线）根开始的路径，如：/root/mydata/、/mnt/VMware。 重要选项参数 需熟练掌握带*选项的用法：当需要切换到当前用户上一次所在的目录时，请使用“cd -”（注意空格）；当需要切换到当前用户的家目录时，请使用“cd～”（注意空格）；当需要切换到当前目录的上一级目录所在的路径时，请使用“cd ..”（注意空格）。 【使用举例】 1）进入系统/etc目录 1[root@C7-Server01 ~]# cd /etc/ 2）切换到/usr/local目录下 1[root@C7-Server01 etc]# cd /usr/local/ 3）切换到当前目录的上一级目录 1[root@C7-Server01 local]# cd .. # ..表示当前目录的父目录，.表示当前目录 4）进入当前目录的父目录的父目录 1[root@C7-Server01 local]# cd ../.. # 当前目录为/usr/local，其父目录就是/usr，其爷爷目录就是根目录/ 只要目录有足够多的层次，可以一直这样继续下去“cd../../../..”，直到退到“/”为止。 5）返回当前用户上一次所在的目录 1[root@C7-Server01 /]# cd - # -就是表示返回到进入当前目录前的那个目录 6）进入当前用户的家目录 1[root@C7-Server01 local]# cd ~ tree：以树形结构显示目录下的内容tree命令的中文意思为“树”，功能是以树形结构列出指定目录下的所有内容，包括所有文件、子目录及子目录里的目录和文件。 语法格式：tree [option] [directory] tree命令后若不接选项和目录就会默认显示当前所在路径目录的目录结构。tree命令可能需要单独安装，首先检查系统是否安装了tree命令，如果采用的是最小化安装Linux系统的方式，那么tree命令有可能没有安装，此时可用yum命令安装tree命令。 1、检查系统是否安装tree命令，如果没有输出，就表示没有安装，此时通过第二步命令进行安装。 1[root@C7-Server01 ~]# rpm -aq tree 2、安装tree命令 1[root@C7-Server01 ~]# yum install -y tree 重要选项参数 【使用示例】 1）不带任何参数执行tree命令 1[root@C7-Server01 ~]# tree 2）以树形结构显示目录下的所有内容（-a的功能） 在Linux系统中，以“.”点号开头的文件为隐藏文件，默认不显示。 1[root@C7-Server01 bin]# tree -a # 在/usr/bin目录下执行 3）只列出根目录下第一层目录的结构（-L功能） 1[root@C7-Server01 /]# tree -L 1 # -L参数后接数字，表示查看目录的层数，不带-L选项默认显示所有层 4）只显示所有的目录（但不显示文件） 1[root@C7-Server01 ~]# tree -d 1[root@C7-Server01 ~]# tree -dL 1 # -d参数只显示目录，-L参数显示层数，组合起来就是只显示当前目录下第一层目录。 5） -f选项和-i选项的使用 1[root@C7-Server01 ~]# tree -L 1 -f /boot # 只显示/boot目录下第一层的所有文件的全路径 1[root@C7-Server01 ~]# tree -L 1 -fi /boot # 加上-i选项，显示结果不带树枝，便于复制粘贴 6）使用tree命令区分目录和文件的方法（常用） 1[root@C7-Server01 ~]# tree -L 1 -F /boot # 使用-F参数会在目录后面添加“/”，方便区分目录。 123[root@C7-Server01 ~]# tree -L 1 -F /etc/ | grep /$ # 结合管道符|和grep，可以方便将目录过滤出来#/$是Linux的通配符，表示以/结尾的字符#管道符的作用在于指令拼接，上面指令的意思就是将tree指令的执行结果交给grep指令处理。 小练习：通过tree指令将/usr/local/下的第一级文件全部过滤出来，结果只能包含文件，不能包含目录（答案贴在本文的讨论区） mkdir：创建目录mkdir命令是“make directories”中每个单词的粗体字母组合而成，其功能是创建目录，默认情况下，如果要创建的目录已存在，则会提示此文件已存在；而不会继续创建目录。 语法格式：mkdir [option] [directory] mkdir命令可以同时创建多个目录，格式为mkdir dir1 dir2…，也可以使用{}来完成指定序列的批量目录创建。使用mkdir创建多级目录时，建议直接使用-p参数，可以避免出现“No such file or directory”这样没有文件或目录的报错了，且不会影响已存在的目录。 重要选项参数： 【使用示例】 1）不使用任何命令参数创建目录用法示例 1[root@C7-Server01 kkutysllb]# mkdir mytest 1[root@C7-Server01 kkutysllb]# mkdir mytest # 当再次创建时，会提示目标目录已存在 2）使用-p参数递归创建目 当我们创建多级目录时，如果第一级目录（data）不存在，那么创建结果会报错，导致无法创建成功，操作如下： 1[root@C7-Server01 kkutysllb]# mkdir data/stu01 此时，可以指定-p参数递归创建多级目录： 1[root@C7-Server01 kkutysllb]# mkdir -p data/stu01 3）加-v参数显示创建目录的过程 12# 在data目录下同时创建2个子目录stu02和stu03，并且同时在两个子目录下再分别创建四个子目录test01-test04 [root@C7-Server01 kkutysllb]# mkdir -pv data/stu&#123;02,03&#125;/test&#123;01..04&#125; 其实这个-v没有什么实际用途，它只是用来显示创建目录的过程。 大括号（{}）的特殊用法，见如下图示： 4）创建目录时可使用-m参数设置目录的默认权限 如果创建目录是不设置目录的权限，所创建目录的默认权限是755，如下所示：（目录的权限问题在讲解ls指令会讲到） 1[root@C7-Server01 kkutysllb]# mkdir -m 333 dir01 这个指令在创建特殊安全要求的目录时会用到，平时一般不用。因为，目录的权限还决定了目录中子目录及其文件的权限，所以在创建目录时要综合考虑。 touch：创建空文件或改变文件的时间戳属性touch命令有两个功能：一是创建新的空文件；二是改变已有文件的时间戳属性。 语法格式：touch [option] [file] 注意区分touch和mkdir命令的功能，mkdir命令是创建空目录，而touch是创建空文件。在Linux中，一切皆文件。虽然touch命令不能创建目录，但是可以修改目录的时间戳。 重要选项参数 【使用示例】 1）创建文件示例（文件事先不存在的情况） 1[root@C7-Server01 kkutysllb]# touch image001 12#可以使用&#123;&#125;同时创建多个文件[root@C7-Server01 kkutysllb]# touch image&#123;002..009&#125; 2）更改文件的时间戳属性 12#查看文件时间戳属性，可以使用stat指令[root@C7-Server01 kkutysllb]# stat image001 文件的时间戳属性分为访问时间、修改时间、状态改变时间。 1[root@C7-Server01 kkutysllb]# touch -a image001 # -a选项更改文件的访问时间为当前时间 1[root@C7-Server01 kkutysllb]# touch -m image001 # -m选项更改文件的修改时间为当前时间 无论是-a选项修改文件的访问时间，还是-m选项修改文件的修改时间，文件的状态改变时间ctime也同步进行改变。 3）指定时间属性创建/修改文件 12# 将文件image001的访问时间和修改时间统一修改为2020年10月21日 [root@C7-Server01 kkutysllb]# touch -d 20201021 image001 这个功能经常用于黑客入侵后的操作。 12#使用image002文件的时间戳为模板改为image001文件的时间戳[root@C7-Server01 kkutysllb]# touch -r image002 image001 12#还可以利用-t选项将image001的时间修改为202012312234.55的格式[root@C7-Server01 kkutysllb]# touch -t 202012312234.55 image001 知识扩展： 这里扩展一点有关时间戳属性的知识。GNU/Linux的文件有3种类型的时间戳： 12345Access：2015-07-30 17:48:20.502156890 +0800 #&lt;==最后访问文件的时间。 Modify：2015-07-30 17:48:45.006106223 +0800 #&lt;==最后修改文件的时间。 Change：2015-07-30 17:48:45.006106223 +0800 #&lt;==最后改变文件状态的时间。 ls：显示目录下的内容及相关属性信息ls命令可以理解为英文单词list的缩写，其功能是列出目录的内容及其内容属性信息（list directorycontents）。该命令有点类似于DOS系统下的dir命令，有趣的是，Linux下其实也有dir命令，但我们更习惯于使用ls。 语法格式：ls [option] [file] ls命令后面的选项和目录文件可以省略，表示查看当前路径的文件信息。 重要选项参数 【使用示例】 1）直接执行ls命令，不带任何参数 12# 列出/home/kkutysllb目录下的文件信息 [root@C7-Server01 kkutysllb]# ls 2）使用-a参数显示所有文件，特别是隐藏文件 12# 加了-a参数，就会把以“.”（点号）开头的内容显示出来了。这里显示的第一个点号，表示当前目录，即/home/kkutysllb/目录本身，而两个点号则表示当前目录的上级目录，此处就代表/home/目录了 [root@C7-Server01 kkutysllb]# ls -a 3）使用-l参数显示详细信息 1[root@C7-Server01 kkutysllb]# ls -l 4）显示完整时间属性的参数 –time-style=long-iso 1[root@C7-Server01 kkutysllb]# ls -l --time-style=long-iso 小练习：关于–time-style=long-iso的其它参数大家自行练习，将答案贴在本文的讨论区。 5）执行ls命令，带显示内容的访问时间属性的参数 12# 首先通过stat指令查看image001文件的访问时间信息 [root@C7-Server01 kkutysllb]# stat image001 12# 通过选项--time=atime显示文件的访问时间[root@C7-Server01 kkutysllb]# ls -l --time-style=long-iso --time=atime image001 同理，通过选项–time=mtime可以查看文件的修改时间，选项–time=ctime可以查看文件状态的改变时间，大家可以自行练习。（国际惯例，答案贴在本文的讨论区） 6）执行ls命令，带-F参数（这一点与tree命令的-F很类似） 12#加了-F,我们可以清晰地看到所有目录的结尾都被加上了斜线/[root@C7-Server01 kkutysllb]# ls -lF 12# 过滤出当前目录下所有子目录[root@C7-Server01 kkutysllb]# ls -lF | grep /$ 123# 过滤出当前目录下所有普通文件# grep命令的-v选项是反向过滤的意思[root@C7-Server01 kkutysllb]# ls -lF | grep -v /$ 7）使用-d参数只显示目录本身的信息 12# 有时候我们想查看目录本身的信息，但是若使用“ls目录”命令，就会显示目录里面的内容，比如： [root@C7-Server01 kkutysllb]# ls -l data/ 12# 通过-d选项可以查看目录的详细信息[root@C7-Server01 kkutysllb]# ls -ld data/ 8）使用-R参数递归查看目录 12# tree指令没有递归方式查看多级目录下详细信息的选项，但是ls指令就有，通过-R选项实现 [root@C7-Server01 kkutysllb]# ls -lR data/ 小练习：通过ls命令迅速查找/boot目录下最近更新过的文件（不知文件名的情况下） 提示： 1、利用-t选项对目录下文件按照修改时间排序。 2、利用-r选项对时间排序后的文件进行倒序排序。 9）ls命令列出文件的属性解读 123456# 利用-h和-i选项可以列出某个目录下所有文件和子目录的详细信息# -h选项是将文件的大小通过人类可读的方式显示出来，不加-h选项系统默认是字节单位# -i选项是文件的inode值，在linux系统中一个文件分为inode和block两部分，无论哪一部分占用满，都会显示磁盘空间不足的提示。# 因此，有时你会发现磁盘还有很大空间，但是提示磁盘占用满，这就是因为inode占用满的缘故。[root@C7-Server01 kkutysllb]# ls -lhi /boot 第一列：inode索引节点编号。第二列：文件类型及权限（第一个字符为类型，后9个字符为文件权限符号）。第三列：硬链接个数（详细请参看ln命令的讲解）。第四列：文件或目录所属的用户（属主）。第五列：文件或目录所属的组。第六列：文件或目录的大小。第七、八、九列：文件或目录的修改时间。第十列：实际的文件名或目录名。 cp：复制文件或目录cp命令可以理解为英文单词copy的缩写，其功能为复制文件或目录。 语法格式：cp [option] [source] [dest] 重要参数选项 【使用示例】 1）无参数和带参数-a的比较 12# 查看当前目录下的文件信息 [root@C7-Server01 kkutysllb]# ls -l 12# 通过cp指令复制image001文件为image010文件[root@C7-Server01 kkutysllb]# cp image001 image010 12# 使用-a选项复制image001文件为image011[root@C7-Server01 kkutysllb]# cp -a image001 image011 2）-i参数的例子 123# 使用-i选项覆盖已存在文件时，会有提示确认 [root@C7-Server01 kkutysllb]# cp -i image001 image011 请大家思考下：为什么不使用-i选项覆盖已存在文件时，也会有提示确认？（提示：利用Google搜索下Linux系统的别名功能） 3）使用-r参数复制目录 12345# 如果要复制一个目录，可以用-r参数递归复制**# 在linux系统中，没有单独复制目录的命令 [root@C7-Server01 kkutysllb]# cp -r data/ data_tmp/ 通过diff指令或vimdiff指令对比复制后的两个文件（diff和vimdiff指令后面会讲） mv：移动或重命名文件mv命令可以理解为英文单词move的缩写，其功能是移动或重命名文件（move/rename files）。 语法格式：mv [option] [source] [dest] 重要参数选项 【使用示例】 1）给文件改名的例子 1234567891011121314151617181920212223242526272829303132333435363738394041# 将image001文件修改为image_t001 [root@C7-Server01 kkutysllb]# ls -l total 0 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.55 drwxr-xr-x 5 root root 45 Apr 17 11:59 data drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp d-wx-wx-wx 2 root root 6 Apr 16 15:09 dir01 -rw-r--r-- 1 root root 0 Dec 31 2020 image001 -rw-r--r-- 1 root root 0 Apr 16 17:56 image002 -rw-r--r-- 1 root root 0 Apr 16 17:56 image003 -rw-r--r-- 1 root root 0 Apr 16 17:56 image004 -rw-r--r-- 1 root root 0 Apr 16 17:56 image005 -rw-r--r-- 1 root root 0 Apr 16 17:56 image006 -rw-r--r-- 1 root root 0 Apr 16 17:56 image007 -rw-r--r-- 1 root root 0 Apr 16 17:56 image008 -rw-r--r-- 1 root root 0 Apr 16 17:56 image009 -rw-r--r-- 1 root root 0 Apr 17 11:46 image010 -rw-r--r-- 1 root root 0 Dec 31 2020 image011 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest [root@C7-Server01 kkutysllb]# mv image001 image_t001 [root@C7-Server01 kkutysllb]# ls -l 2）移动文件的例子 12345# 将image_t001文件移动到dir01目录下 # 源目录下已不存在刚移动过的文件 [root@C7-Server01 kkutysllb]# mv image_t001 dir01/ 在dir01目录下存在刚移动进来的文件image_t001 12345# 移动多个文件# 第一种方式，源文件在前，目标目录在后[root@C7-Server01 kkutysllb]# mv image002 image003 image004 dir01/ 123# 第二种方式，使用-t选项，目标目录在前，源文件在后[root@C7-Server01 kkutysllb]# mv -t dir01/ image005 image006 3）关于mv命令的使用小结 rm：删除文件或目录rm命令可以理解为英文单词remove的缩写，其功能是删除一个或多个文件或目录（remove files or directories）。这是Linux系统里最危险的命令之一，请慎重使用。 语法格式：rm [option] [file] 重要选项参数 【使用示例】 1）不带参数删除例子实践 1234567#删除/home/kkutysllb/image011这个文件 #不带参数时，删除前默认要求确认，原因与cp命令一致 [root@C7-Server01 kkutysllb]# rm image011 rm: remove regular empty file ‘image011’? y # 回复y删除，回复n不删除 2）强制删除例子实践 1234567# 使用-f选项强制删除image007文件 # 直接删除，不提示确认 [root@C7-Server01 kkutysllb]# rm -f image007 [root@C7-Server01 kkutysllb]# 3） 使用-r选项递归删除目录 1234567891011121314151617181920212223# 使用-r选项可以递归删除目录，原理是先删除目录中文件和子目录，然后再删除目录本身 # 不带-f参数时，每删除一个文件都需要确认 # linux还有个删除目录的命令rmdir，但是只能删除空目录 [root@C7-Server01 kkutysllb]# rm -r dir01/ rm: descend into directory ‘dir01/’? y rm: remove regular empty file ‘dir01/image_t001’? y rm: remove regular empty file ‘dir01/image002’? yrm: remove regular empty file ‘dir01/image003’? y rm: remove regular empty file ‘dir01/image004’? y rm: remove regular empty file ‘dir01/image005’? y rm: remove regular empty file ‘dir01/image006’? y rm: remove directory ‘dir01/’? y 4）关于删除的实践经验 常在河边走，哪有不湿鞋！但是如果能遵守下面的要领就可以少湿鞋甚至不湿鞋！ 1）用mv替代rm，不要急着删除，而是先移动到回收站/tmp。 2）删除前务必备份，最好是异机备份，若出现问题随时可以还原。 3）如果非要删除，那么请用find完成要删除文件查找，然后通过管道符接rm指令进行精准删除，最大限度避免误删除，包括通过系统定时任务等清理文件方法。比如下面的指令： 1find ．-type f -name "*.txt" -mtime +7|xargs rm –f 上面指令的意思是查找当前目录下7天以前的所有以.txt结尾的文件（注意，linux没有文件扩展名的概念，在linux中一切皆是文件），然后将结果通过管道符“|”传递给xargs rm -f指令组合进行删除（xargs是一个标准输入指令，其后面接rm指令就是模拟用户在终端侧直接输入rm指令的用法） ln：硬链接与软链接ln命令可以理解为英文单词link的缩写，其功能是创建文件间的链接（make links between files），链接类型包括硬链接（hard link）和软链接（符号链接，symbolic link）。 硬链接（Hard Link）：创建语法为“ln源文件目标文件”，硬链接生成的是普通文件（-字符）。 软链接或符号链接（Symbolic Link or Soft Link）：创建语法为“ln-s源文件目标文件（目标文件不能事先存在）”，软链接生成的是符号链接文件（l类型）。 硬链接是指通过索引节点（Inode）来进行链接。在Linux（ext2、ext3、ext4）文件系统中，所有文件都有一个独有的inode编号。在Linux文件系统中，多个文件名指向同一个索引节点（inode）是正常且允许的。这种情况下的文件就称为硬链接。硬链接文件相当于文件的另外一个入口。它的作用之一就是允许一个文件拥有多个有效路径名（多个入口），这样用户就可以建立硬链接到重要文件，以防止误删源数据，只有删除了源文件以及源文件所有对应的硬链接文件，文件实体才会被删除。 软链接或符号链接（Symbolic Link or Soft Link）有点像Windows里的快捷方式。软链接类似于一个文本文件，里面存放的是源文件的路径，指向源文件实体。即使删除了源文件，软链接文件也还是依然存在，但是无法访问指向的源文件路径内容了。软链接和源文件是不同类型的文件，也是不同的文件，inode号也不相同。 语法格式：ln [option] [source] [target] 重要选项参数 【使用示例】1）创建一个硬链接文件 1234567891011121314151617181920212223242526[root@C7-Server01 kkutysllb]# ln /etc/hosts hard_link# 通过ls -i查看源文件和硬链接文件的inode值[root@C7-Server01 kkutysllb]# ls -li /etc/hosts hard_link 33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 /etc/hosts33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 hard_link# 可以看见硬链接文件与原文件inode值一样# 删除源文件/etc/hosts[root@C7-Server01 kkutysllb]# rm -f /etc/hosts[root@C7-Server01 kkutysllb]# cat /etc/hostscat: /etc/hosts: No such file or directory # 提示文件不存在# 可以通过刚才的硬链接文件进行误删除文件的找回，即在做一个硬链接[root@C7-Server01 kkutysllb]# ln hard_link /etc/hosts[root@C7-Server01 kkutysllb]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@C7-Server01 kkutysllb]# ls -li hard_link /etc/hosts33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 /etc/hosts33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 hard_link#找回后的文件inode值与删除前一致，就好像误删除从没发生过。 2）创建一个软连接文件 12345678910111213141516#使用-s选项创建一个软连接文件[root@C7-Server01 kkutysllb]# ln -s /etc/hosts soft_link# 通过ls -li指令可以看出软连接文件和源文件的inode值不一致，说明这是两个不同的文件[root@C7-Server01 kkutysllb]# ls -li /etc/hosts soft_link 33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 /etc/hosts1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts#查看软连接文件的内容就是查看原文件的内容[root@C7-Server01 kkutysllb]# cat soft_link 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6#通过readlink命令可以查看软链接中的存放的具体信息（非原文件内容）[root@C7-Server01 kkutysllb]# readlink soft_link /etc/hosts#可以看出软链接中存放的就是源文件的绝对路径 软链接和源文件的关系图示： *注意一点：**目录不可以创建硬链接，但是可以创建软链接。因为，前面讲过Linux系统的磁盘需要挂载到一个目录才可以使用，但是多个磁盘可能对应分区格式和文件系统不一样，因此如果两个目录之间如果是硬链接关系，但是对应的磁盘分区格式又不一致就会出现问题。*** find：查找目录下的文件find命令用于查找目录下的文件，同时也可以调用其他命令执行相应的操作。 语法格式：find [-H] [-L] [-P] [-D debugopts] [-Olevel] [pathname] [expression] 注意子模块的先后顺序。 find命令用法的使用说明： 重要选项参数 【使用示例】 1）查找指定时间内修改过的文件 123# 查找当前目录下2天内访问过的文件 [root@C7-Server01 kkutysllb]# find . -atime -2 2）用-name指定关键字查找 123#查找/var/log下5天以前修改过的以.log结尾的所有文件 [root@C7-Server01 kkutysllb]# find /var/log -mtime +5 -name "*.log" 3）利用“！”反向查找 123#查找当前目录下非目录文件 [root@C7-Server01 kkutysllb]# find . ! -type d 小练习： 1、按照文件大小来查找文件 2、按照文件或目录的权限来查找文件 3、查找文件时希望忽略一个或多个目录的方法 4、按照文件的更新时间来差找文件 5、按照文件或目录的归属用户和用户组来查找文件 至此，Linux系统文件和目录的操作命令部分已经写完了。但是，这并不是所有命令（Linux系统核心命令使用频率较高的越有150条），而是和运维工作息息相关的最最最常用的几个命令，作为初学者必须首先掌握上述这些命令，再结合实际工作中的需要去掌握其它命令。其它命令章节也是遵循此原则来写，后续不再赘述。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-10-Linux系统命令-第一篇《关机、重启命令》]]></title>
    <url>%2F2019%2F04%2F15%2F2019-04-10-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%B8%80%E7%AF%87%E3%80%8A%E5%85%B3%E6%9C%BA%E3%80%81%E9%87%8D%E5%90%AF%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[Linux命令行的概述众所周知，Linux是一个主要通过命令行来进行管理的操作系统，即通过键盘输入指令来管理系统的相关操作，包括但不限于编辑文件、启动停止服务等。这和初学者曾经使用的Windows系统使用鼠标点击的可视化管理大不相同。 使用鼠标可视化管理的优势是简单、容易上手，但缺点是不便于快速、批量、自动化管理系统，而且感觉系统很臃肿，这个时候Linux系统的命令行管理优势就凸显了。使用Linux命令行管理，不但可以批量、自动化管理，而且还可以实现智能化、可视化管理；当然，后者需要开发人员配合开发管理界面来完成。但是无论如何，Linux系统的优势基因还是快速、批量、自动化、智能化管理系统及处理业务。 Linux命令行介绍安装Linux系统时，无论是使用文本模式（命令行）安装，还是使用图形模式安装，最终管理系统的任务都会落到命令行之上。 大多数互联网企业在安装系统时甚至不会安装图形管理软件包，而是直接使用文本模式安装，因此登录后直接面对的就是命令行的界面，如下图所示： Linux命令行结尾的提示符有“#”和“$”两种不同的符号，代码如下所示： 1[root@C7-Server01 ～]# # 这是超级管理员root用户对应的命令行。 1[kkutysllb@C7-Server01 ～]$ # 这是普通用户kkutysllb对应的命令行。 Linux命令提示符由PS1环境变量控制。示例代码如下： 1[root@C7-Server01 ~]# set | grep PS1 这里的PS1=[\u@\h\W]\$，可以通过全局配置文件/etc/bashrc或/etc/profile进行按需配置和调整。 Linux命令行常用快捷键这里需要特别说明一下的是，在企业工作中，管理Linux时一般不会直接采用键盘、显示器登录系统，而是会通过网络在远程进行管理，因此，需要通过远程连接工具连接到Linux系统中。目前最常用的Linux远程连接工具为：SecureCRT和Xshell客户端软件，因此，本节涉及的常用命令快捷键也是基于这两款客户端软件的，其他软件的快捷键使用情况与此基本类似。提高Linux运维效率的30个命令行常用快捷键如下： Linux重启、关机、注销命令1、shutdown 命令常用操作shutdown是一个用来安全关闭或重启Linux系统的命令，系统在关闭之前会通知所有的登录用户，系统即将关闭，此时所有的新用户都不可以登录，与shutdown类似功能的命令还有init、halt、poweroff、reboot。 语法格式为：shutdown [OPTION]…… TIME [MESSAGE] 通常情况下，我们执行的shutdown命令为shutdown-h now或shutdown-r now shutdown命令常用的参数选项如下： 【使用举例】 1）10分钟后关闭或重启系统 12#关机[root@C7-Server01 ~]# shutdown -h +10 12#重启[root@C7-Server01 ~]# shutdown -r +10 2）11点整定时关闭或重启系统 12#11点整重启 [root@C7-Server01 ~]# shutdown -r 11:00 12#11点整关闭[root@C7-Server01 ~]# shutdown -h 11:00 3）立即关闭或重启Linux系统的命令如下： 12#立即关机 [root@C7-Server01 ~]# shutdown -h now 12#立即重启[root@C7-Server01 ~]# shutdown -r now 2、halt/poweroff/reboot命令的常用操作从RedHat或CentOS 6开始，你会发现halt、poweroff、reboot这三个命令对应的都是同一个man帮助文档。在CentOS 6时代，而halt、poweroff是reboot命令的链接文件，而在CentOS 7时代，这三个命令都是systemctl的连接文件。 语法格式为：halt/poweroff/reboot [OPTIONS] 通常情况下，我们执行这三个命令时都不带任何参数。因此，这三个命令的选项参数也就没什么好研究的。 【使用举例】 1）使用halt命令完成关机 1[root@C7-Server01 ~]# halt 2）使用poweroff命令完成关机 1[root@C7-Server01 ~]# poweroff 3）使用reboot命令重启系统 1[root@C7-Server01 ~]# reboot 除此之外，还可以使用init指令完成关机或重启指令，执行init 0为关机，执行init 6为重启，这是因为0和6是系统的两个运行级别，分别对应关机和重启。你们可以在自己的实验环境尝试执行看看。。。 3、常见不常见关机、重启和注销的命令列表 Linux命令正是组成Linux系统最核心、重要的基础之一，因此，大家只要牢牢掌握基础命令，在日后linux运维、shell编程、云计算/大数据、甚至Python自动化运维都能如鱼得水。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-05-CentOS操作系统6.x版本与7.x版本的区别]]></title>
    <url>%2F2019%2F04%2F15%2F2019-04-05-CentOS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F6-x%E7%89%88%E6%9C%AC%E4%B8%8E7-x%E7%89%88%E6%9C%AC%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Centos7与6之间最大的差别就是初始化技术的不同，7采用的初始化技术是Systemd,并行的运行方式，除了这一点之外，服务启动、开机启动文件、网络命令方面等等也存在不同。 1.系统初始化技术在Linux系统常用系统初始化技术主要有三种：Sysvinit技术、Upstart技术和Systemd技术。在CentOS 6时代，主要使用sysvinit技术完成系统的初始化，其实现原理通过调用各种服务的shell脚本完成初始化工作。而在CentOS 7时代，开始采用Systemd技术来完成系统的初始化工作，其最大的特点的各类服务可以并发完成。同时，在CentOS 7时代也保留Sysvinit技术。三种不同的初始化技术的优缺点如下： Sysvinit技术特点： 1.系统第1个进程为init; 2.init进程是所有进程的父进程，不可kill； 3.大多数Linux发行版的init系统是和SystemV相兼容的，被称为sysvinti 4.代表系统：CentOS5 CentOS6 优点： sysvinit运行非常良好，概念简单清晰，它主要依赖于shell脚。所以，如果大家开始学习shell编程时，建议安装一个CentOS6系统。 缺点： 1.按照一定顺序执行——&gt;启动太慢。 2.很容易hang住，fstab与nfs挂载问题 Upstart技术CentOS6采用了upstart技术代替sysVinit进行引导，Upstart对rc.sysinit脚本做了大量的优化，缩短了系统初始化的启动时间。但是CentOS6为了简便管理员的操作，upstart的很多特性并没有凸显或直接不支持。代表系统：CentOS6, Ubuntu14, 从CentOS7, Ubuntu15开始使用systemd。 Systemd技术新系统都会采用的技术（RedHat7,CentOS7,Ubuntu15等），设计目标是克服sysvinit固有的缺点，提高系统的启动速度。和Sysvinit兼容，降低迁移成本，最主要优点：并行启动，Pid为1的进程。 2.在yum源上的优化在centos6的时候，默认是从官方源下载rpm包的，由于是国外的yum源很慢不能用，CentOS7在这里做了优化，当我们使用yum安装软件的时候，默认不会再从官方下载，而是自动寻找离自己地理位置最近的yum源开始下载。 3.网络命令如果在安装系统的时候选择minimal，会比之前6的时候以更小的包来安装，比如：vim、ifconfig、route、setup、netstat等等都不会安装。因此，使用CentOS7通过最小化安装后，还需挂在ISO完成上述常用工具的安装。如果采用脚本自动化安装，可以将上述的工具的安装命令写入到安装到脚本中。 4.字符集修改 1234567891011/etc/locale.conf #字符集配置文件 localectl set-locale LANG=zh_CN.UTF-8 # 命令行一步到位 [root@CentOS7 ~]# localectl set-locale LANG=zh_CN.UTF-8 [root@CentOS7 ~]# localectl status System Locale: LANG=zh_CN.UTF-8 VC Keymap: us X11 Layout: us 5.开机启动管理/etc/rc.local 这个文件还是存在，不过如果我们还想继续使用这种方式需要给它加执行权限chmod +x /etc/rc.d/rc.local system一统天下 snapshot(支持快照) 12345678systemctl status cron.service #查看定时任务状态systemctl stop cron.service#关闭定时任务systemctl status cron.service#查看操作情况systemctl list-unit-files|grep enable #查看当前正在运行的服务systemctl disable postfix.service #关闭邮件服务systemctl list-unit-files|grep postfix #查看邮件服务是否开启systemctl stop firewalld.service #关闭防火墙systemctl is-enable #开启的服务 systemctl disable#关闭的服务 6.运行级别runlevel/etc/inittab 是无效的，由system target 替代。12345678910#永久生效下次登录生效 systemctl get-default graphical.target # 切换到5 systemctl get-default multi-user.target # 切换到3 ##临时生效的话 init3 七种种运行级别如下1[root@centos7 ~]# ls -lh /usr/lib/systemd/system/runlevel*.target 7.网卡名称CentOS7的网卡名称太长，这不符合我们的使用习惯，增加了管理难度，最简单粗暴的方法是在安装系统的时候就把网卡名改了。当然，安装好的系统也是可以修改的。下面分别介绍两种方法。 方法一：(推荐）在进入安装界面的时候把光标移动到Install CentOS7,按下tab键，在后面输入“net ifnames=0 biosdevname=0”回车即可。 方法二：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#进入网卡配置文件目录[root@centos7 ~]# cd /etc/sysconfig/network-scripts/#重命名网卡[root@centos7 network-scripts]# mv ifcfg-eno16777736 ifcfg-eth0#修改配置文件NAME、DEVICE[root@centos7 network-scripts]# vim ifcfg-eth0TYPE=EthernetBOOTPROTO=staticTYPE=EthernetBOOTPROTO=staticDEFROUTE=yesTYPE=EthernetBOOTPROTO=staticDEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noNAME=eth0UUID=552c01f7-fd9d-4f19-913e-379a2bf5a467DEVICE=eth0ONBOOT=yesIPADDR=10.0.0.111"ifcfg-eth0" 14L, 239C written#修改grubsed -i.bak 's#crashkernel=auto rhgb quiet#crashkernel=auto rhgb net.ifnames=0 biosdevname=0 quiet#g' /etc/sysconfig/grub[root@centos7 network-scripts]# vim /etc/sysconfig/grub GRUB_TIMEOUT=5GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT="console"GRUB_CMDLINE_LINUX="crashkernel=auto rhgb net.ifnames=0 biosdevname=0 quiet""/etc/sysconfig/grub" 7L, 263C written#生成启动菜单[root@centos7 network-scripts]# grub2-mkconfig -o /boot/grub2/grub.cfg Generating grub configuration file ...Found linux image: /boot/vmlinuz-3.10.0-327.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-327.el7.x86_64.imgFound linux image: /boot/vmlinuz-0-rescue-7ed5d4eebe4c43e3aadbda68cd0ef311Found initrd image: /boot/initramfs-0-rescue-7ed5d4eebe4c43e3aadbda68cd0ef311.imgdone#重启系统生效[root@centos7 network-scripts]# reboot 8.桌面系统1234567[CentOS6] GNOME 2.x[CentOS7] GNOME 3.x（GNOME Shell） 9.文件系统1234567[CentOS6] ext4 [CentOS7] xfs 10.内核版本1234567[CentOS6] 2.6.x-x [CentOS7] 3.10.x-x 11.启动加载器1234567[CentOS6] GRUB Legacy (+efibootmgr)[CentOS7] GRUB2 12.防火墙1234567[CentOS6] iptables[CentOS7] firewalld 13.默认数据库1234567[CentOS6] MySQL [CentOS7] MariaDB 14.文件结构1234567[CentOS6] /bin, /sbin, /lib, and /lib64在/下 [CentOS7] /bin, /sbin, /lib, and /lib64移到/usr下 15.主机名123[CentOS7] /etc/hostname 16.时间同步1234567891011[CentOS6] $ ntp $ ntpq -p [CentOS7] $ chrony $ chronyc sources 17.修改时间1234567891011[CentOS6] $ vim /etc/sysconfig/clock ZONE="Asia/Shanghai" UTC=fales $ sudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime [CentOS7] $ timedatectl set-timezone Asia/Shanghai $ timedatectl status 18.修改地区1234567891011[CentOS6] $ vim /etc/sysconfig/i18n LANG="zh_CN.utf8" $ /etc/sysconfig/i18n $ locale [CentOS7] $ localectl set-locale LANG=zh_CN.utf8 $ localectl status 19.服务相关1）启动停止123456789101112131415[CentOS6] $ service service_name start $ service service_name stop $ service sshd restart/status/reload [CentOS7] $ systemctl start service_name $ systemctl stop service_name $ systemctl restart/status/reload sshd 2) 自启动123456789[CentOS6] $ chkconfig service_name on/off [CentOS7] $ systemctl enable service_name $ systemctl disable service_name 3) 服务一览123456789[CentOS6] $ chkconfig --list [CentOS7] $ systemctl list-unit-files $ systemctl --type service 4) 强制停止1234567[CentOS6] $ kill -9 &lt;PID&gt; [CentOS7] $ systemctl kill --signal=9 sshd 20.网络1）网络信息123456789101112131415[CentOS6] $ netstat $ netstat -I $ netstat -n [CentOS7] $ ip n $ ip -s l $ ss 2）IP地址MAC地址1234567[CentOS6] $ ifconfig -a [CentOS7] $ ip address show 3）路由1234567891011[CentOS6] $ route -n $ route -A inet6 -n [CentOS7] $ ip route show $ ip -6 route show 21.重启关闭1）关闭123456789[CentOS6] $ shutdown -h now [CentOS7] $ poweroff $ systemctl poweroff 2）重启1234567891011[CentOS6] $ reboot $ shutdown -r now [CentOS7] $ reboot $ systemctl reboot 3）单用户模式1234567[CentOS6] $ init S [CentOS7] $ systemctl rescue 4）启动模式123456789101112131415161718192021[CentOS6] $ vim /etc/inittab id:3:initdefault: $ startx [CentOS7] $ systemctl isolate multi-user.target $systemctl isolate graphical.target #默认 $ systemctl set-default graphical.target $ systemctl set-default multi-user.target #当前 $ systemctl get-default]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-03-30-Linux的文件系统]]></title>
    <url>%2F2019%2F04%2F15%2F2019-03-30-Linux%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[Linux目录结构的特点Windows系统目录和磁盘是强对应的关系，联系很紧密，例如c盘下的目录和文件不可能和D盘的目录有关联或交集。而Linux系统目录与之不同，Linux系统的目录和磁盘等设备是不直接关联的，每个目录都可以关联（官称：挂载）在不同的设备（例如磁盘）上，例如：看似有包含关系的几个目录/、/etc、/boot、/var很可能是在不同的分区或磁盘上。 在Linux中，一切皆是文件。Linux是以文件的方式来管理系统中各个组件，所有的文件存储都以根（”/“）开始，整个文件系统结构就像一颗倒挂的”树“。Linux文件系统的目录结构如图所示： 在Linux中，一切从“根”开始，“/”是所有目录的起点（顶点），Linux根下面的目录是一个有层次的树状结构。我们熟悉的Windows系统，目录和磁盘是强对应的关系，联系很紧密，例如c盘下的目录和文件不可能和D盘的目录有关联或交集。而Linux系统目录与之不同，Linux系统的目录和磁盘等设备是不直接关联的，每个目录都可以关联（官称：挂载）在不同的设备（例如磁盘）上，例如：看似有包含关系的几个目录/、/etc、/boot、/var很可能是在不同的分区或磁盘上。 在逻辑上，所有的目录（包括目录下的子目录）都在最高级别的目录“/”下，根（“/”）目录是所有目录的起始点（顶点），而实际上访问目录/、/etc、/boot、/var时，可能是在访问完全不同的分区和磁盘。 Linux下面的设备（磁盘），如果不挂载是看不到入口的，就像没窗没门的房间，是不能被正常使用的。如果要访问设备，就必须为设备开一个入口，这个入口就是挂载点。挂载点实质就是一个目录，开入口的过程就是将挂载点和磁盘设备关联，即挂载。 Linux的根目录特点 1、根目录“/“是所有目录的顶点。 2、目录结构像一颗倒挂的“树”。 3、目录和分区没有关联，因此不同目录可以映射不同的分区。 4、分区或设备要想访问，必须将其与目录挂载，此时挂载的目录相当于分区的入口。 5、挂载目录的指令为mount。 6、系统开机自动挂载的配置文件为/etc/fstab。 Linux目录的层次标准FHSFHS全称为Filesystem Hierarchy Standard，中文意思是目录层次标准，是Linux的目录规范标准。详细见http://www.pathname.com/fhs/。FHS定义了两层规范： 第一层是“/”目录下的各个目录应该放什么文件数据。 第二层是针对/usr（Unix software resource）和/var(Variable data)这两个目录的子目录在定义的。 参考资料： http://www.pathname.com/fhs/ http://www.ibm.com/developerworks/linux/library/l-proc/index.html 所有目录的命名和结构是有规范，在Linux的目录结构遵照FHS规范。常用目录和子目录的作用如下： /bin ：存放普通用户或管理员使用所有二进制命令文件。 /sbin：存放管理员使用的所有二进制命令文件。 /boot : 存放Linux系统启动引导的安装文件存放目录。 /dev ： 存放特殊存储文件或设备文件存放的目录，比如：cpu、内存、硬盘、光驱、鼠标、键盘等。 /etc ： 存放应用程序配置文件，比如：yum或rpm安装软件的配置文件存放路径，很多服务的启动程序存放的路径。 /home ：非关键性目录，是可选目录，是普通用户的家目录。一般每个用户的家目录，默认为此目录下与用户名同名的目录。 /lib：存放基本的共享库文件或Linux内核模块文件,为系统启动或根文件系统上的应用程序（/bin,/sbin等）提供共享库，以及为内核提供内核模块。 ​ libc.so.：*动态链接的C库，在64位系统下，有可能位于/lib64目录下。 ​ ld：*运行时链接器/加载器，在64位系统下，有可能位于/lib64目录下。 ​ modules：用于存储内核模块的目录，在64系统下，也位于/lib目录下。 /lib64：存放64位操作系统基本的共享库文件或Linux内核模块文件。 /media：便携式设备挂载点。比如：U盘，光盘，移动硬盘等。 /mnt：其他文件系统的临时挂载点。 /opt：附件应用程序的安装位置，比如第三方应用程序devstack。 /root ：管理员root用户的家目录，也是可选目录，并非必选。因为，管理员root在生产环境下一班不允许登录。 /srv：存放此系统专门提供给运行此系统应用程序上的数据。 /tmp：为那些会产生临时文件的程序用于存储临时文件的目录，可供所有用户执行写入操作，有有特殊权限。 /usr：是一个层级结构目录，存放全局共享只读数据。 ​ /usr/bin：普通用户命令。 ​ /usr/sbin：管理员命令，并非系统必须，用于扩展系统功能的命令。 ​ /usr/lib64：存放系统的扩展共享库文件。 ​ /usr/include：存放c程序的头文件位置。 ​ /usr/share：命令手册页和自带文档等架构特有的文件存放位置。 ​ /usr/local：又一个层级目录，让系统管理员安装本地应用程序或者第三方的应用程序。 ​ /usr/X11R6：存放X window的程序文件。 ​ /usr/src：程序源码文件的存位置。 /var：层级目录，存储常发生变化的数据的目录。 ​ /var/cache：应用程序缓存数据。 ​ /var/lib：应用程序的状态信息。 ​ /var/local：为/usr/local提供经常变化的数据。 ​ /var/lock：锁文件存放位置。 ​ /var/log：应用程序的日志文件。 ​ /var/opt：为/opt目录下应用程序提供变化的数据。 ​ /var/run：为运行中的进程提供相关数据。 ​ /var/spool：为应用程序提供的管道数据。 ​ /var/tmp：系统重启后依然需要留存的数据。 /proc：存放内核和进程的基于内存的虚拟文件系统。说白了，就是为内核和进程存储运行的相关数据，多为内核参数。例如：net.ipv4.ip_forward，虚拟化为net/ipv4/ip_forward，存储于/proc/sys/下，因此其完整路径为/proc/sys/net/ipv4/ip_foward /sys：挂载sysfs虚拟文件系统的挂载点，比proc更为理想的访问内核数据的途径，也是一个基于内存的虚拟文件系统。为管理LInux设备提供一种统一文件系统。在Linux内核2.6版本开始才出现此目录，以前并没有。 Linux系统的文件类型： -：表示普通文件，在其他命令中用f表示普通文件 d：表示目录文件，完成路径映射，与windows功能不同。 b：表示块设备文件，完成块设备映射的文件，支持以block为单位随机访问设备 c：表示字符设备文件，完成字符设备映射的文件，支持以字符为单位线性访问设备 ​ major number：主设备号，用于表示设备类型，进而确定要加载的驱动程序 ​ minor number：次设备号，用于表示同一种类型设备下不同的设备，进而确定要驱动的嗯对象 l：表示符号链接文件，类似windows上快捷方式，也成为软链接文件。 p：表示管道文件 s：表示套接字文件，用于两个进程之间进行通信时套接的数据。 Linux系统重要的配置文件/etc/sysconfig/network-scripts/ifcfg-eth : 网卡配置文件 /etc/resolv.conf : DNS配置文件 /etc/hosts : 主机名与IP的映射关系配置文件 /etc/sysconfig/network : 配置主机名的目录 /etc/fstab : 实现开机要挂载的文件系统的一个文件 /etc/rc.local : 实现开机启动的配置或软件 /etc/inittab : 实现开机后系统运行的级别，加载相关的启动文件 /etc/issue : 用户登录时的系统提示 /etc/motd : 用户登录后的系统提示 /etc/redhat-release：系统发行版本信息 /usr/local : 通过源码编译的文件 /var/log/message：系统日志文件]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-03-18-Linux系统的内存管理]]></title>
    <url>%2F2019%2F04%2F15%2F2019-03-18-Linux%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[内存管理中的基本概念内存管理一向是所有操作系统书籍不惜笔墨重点讨论的内容，无论市面上或是网上都充斥着大量涉及内存管理的教材和资料。因此，我们这里所要写的Linux内存管理采取避重就轻的策略，从理论层面就不去班门弄斧，贻笑大方了。 遵循“理论来源于实践”的“教条”，我们先不必一下子就钻入内核里去看系统内存到底是如何管理，那样往往会让你陷入似懂非懂的窘境（我当年就犯了这个错误！）。所以最好的方式是先从外部（用户编程范畴）来观察进程如何使用内存，等到大家对内存的使用有了较直观的认识后，再深入到内核中去学习内存如何被管理等理论知识。 每个程序在操作系统中都对应一个进程（例如：QQ，微信等），所有进程都必须占用一定数量的内存，它或是用来存放从磁盘载入的程序代码，或是存放取自用户输入的数据等等。不过进程对这些内存的管理方式因内存用途不一而不尽相同，有些内存是事先静态分配和统一回收的，而有些却是按需要动态分配和回收的。 对任何一个普通进程来讲，它都会涉及到5种不同的数据段。稍有编程知识的朋友都能想到这几个数据段中包含有“程序代码段”、“程序数据段”、“程序堆栈段”等。不错，这几种数据段都在其中，但除了以上几种数据段之外，进程还另外包含两种数据段。下面我们来简单归纳一下进程对应的内存空间中所包含的5种不同的数据区。 代码段：代码段是用来存放可执行文件的操作指令，也就是说它是可执行程序在内存中的镜像。代码段需要防止在运行时被非法修改，所以只准许读取操作，而不允许写入（修改）操作——它是不可写的。 数据段：数据段用来存放可执行文件中已初始化全局变量，换句话说就是存放程序静态分配的变量和全局变量。 BSS段：BSS段包含了程序中未初始化的全局变量，在内存中 bss段全部置零。 堆（heap）：堆是用于存放进程运行中被动态分配的内存段，每一个程序当开始执行，就会在内存中划出一片空间作为程序运行时代码和数据存放地方。它的大小并不固定，可动态扩张或缩减。当进程调用malloc等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用free等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）。 栈：栈是用户存放程序临时创建的局部变量，也就是说我们函数括弧“{}”中定义的变量（但不包括static声明的变量，static意味着在数据段中存放变量）。除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中，并且待到调用结束后，函数的返回值也会被存放回栈中。由于栈的先进先出特点，所以栈特别方便用来保存/恢复调用现场。从这个意义上讲，我们可以把堆栈看成一个寄存、交换临时数据的内存区。 机器语言指令中出现的内存地址，都是逻辑地址，需要转换成线性地址，再经过MMU(CPU中的内存管理单元)转换成物理地址才能够被访问到。我们写个最简单的hello world程序，用gccs编译，再反编译后会看到以下指令： 1mov 0x8747ab0, %eax 这里的内存地址0x8747ab0 就是一个逻辑地址，必须加上隐含的DS 数据段的基地址，才能构成线性地址。也就是说 0x8747ab0 是当前任务的DS数据段内的偏移量offset。 在x86保护模式下，段的信息（段基线性地址、长度、权限等）即段描述符占8个字节，段信息无法直接存放在段寄存器中（段寄存器只有2字节）。Intel的设计是段描述符集中存放在GDT或LDT中，而段寄存器存放的是段描述符在GDT或LDT内的索引值(index)。 Linux系统逻辑地址=线性地址为什么这么说呢？因为Linux所有的段（用户代码段、用户数据段、内核代码段、内核数据段）的线性地址都是从 0x00000000 开始，长度4G，这样，线性地址=逻辑地址+ 0x00000000，也就是说逻辑地址等于线性地址了。 这种情况下Linux只用到了GDT，不论是用户任务还是内核任务，都没有用到LDT。GDT的第12和13项段描述符是 KERNEL_CS 和KERNEL_DS，第14和15项段描述符是 USER_CS 和USER_DS。内核任务使用KERNEL_CS 和KERNEL_DS，所有的用户任务共用USER_CS 和USER_DS，也就是说不需要给每个任务再单独分配段描述符。内核段描述符和用户段描述符虽然起始线性地址和长度都一样，但DPL(描述符特权级)是不一样的。KERNEL_CS 和KERNEL_DS 的DPL值为0（最高特权），USER_CS 和USER_DS的DPL值为3。 用gdb调试程序的时候，用info reg 显示当前寄存器的值： 1234cs 0x73 115ss 0x7b 123ds 0x7b 123es 0x7b 123 可以看到ds值为0x7b, 转换成二进制为 00000000 01111011，TI字段值为0,表示使用GDT，GDT索引值为 01111，即十进制15，对应的就是GDT内的__USER_DATA 用户数据段描述符。 从上面可以看到，Linux在x86的分段机制上运行，却通过一个巧妙的方式绕开了分段。 Linux主要以分页的方式实现内存管理。如下图所示： 前面说了Linux中逻辑地址等于线性地址，那么线性地址怎么对应到物理地址呢？这个大家都知道，那就是通过分页机制，具体的说，就是通过页表查找来对应物理地址。 准确的说分页是CPU提供的一种机制，Linux只是根据这种机制的规则，利用它实现了内存管理。 在保护模式下，控制寄存器CR0的最高位PG位控制着分页管理机制是否生效，如果PG=1，分页机制生效，需通过页表查找才能把线性地址转换物理地址。如果PG=0，则分页机制无效，线性地址就直接做为物理地址。 分页的基本原理是把内存划分成大小固定的若干单元，每个单元称为一页（page），每页包含4k字节的地址空间（为简化分析，我们不考虑扩展分页的情况）。这样每一页的起始地址都是4k字节对齐的。为了能转换成物理地址，我们需要给CPU提供当前任务的线性地址转物理地址的查找表，即页表(page table)。注意，为了实现每个任务的平坦的虚拟内存，每个任务都有自己的页目录表和页表。 为了节约页表占用的内存空间，x86将线性地址通过页目录表和页表两级查找转换成物理地址。32位的线性地址被分成3个部分：最高10位 Directory 页目录表偏移量，中间10位 Table是页表偏移量，最低12位Offset是物理页内的字节偏移量。 页目录表的大小为4k（刚好是一个页的大小），包含1024项，每个项4字节（32位），项目里存储的内容就是页表的物理地址。如果页目录表中的页表尚未分配，则物理地址填0。页表的大小也是4k，同样包含1024项，每个项4字节，内容为最终物理页的物理内存起始地址。 每个活动的任务，必须要先分配给它一个页目录表，并把页目录表的物理地址存入cr3寄存器。页表可以提前分配好，也可以在用到的时候再分配。 还是以mov 0x8747ab0, %eax中的地址为例分析一下线性地址转物理地址的过程。 前面说到Linux中逻辑地址等于线性地址，那么我们要转换的线性地址就是 0x8747ab0。转换的过程是由CPU自动完成的，Linux所要做的就是准备好转换所需的页目录表和页表（假设已经准备好，给页目录表和页表分配物理内存的过程很复杂，这里不做展开讨论，喜欢内核优化的可自行研究）。 线性地址 0x8747ab0 转换成二进制后是 1000 0111 0100 0111 1010 1011 0000 0000，最高10位1000 0111 01的十进制是541，CPU查看页目录表第541项，里面存放的是页表的物理地址。线性地址中间10位00 0111 1010 的十进制是122，页表的第122项存储的是最终物理页的物理起始地址。物理页基地址加上线性地址中最低12位的偏移量，CPU就找到了线性地址最终对应的物理内存单元。 我们知道Linux中用户进程线性地址能寻址的范围是0 － 3G，那么是不是需要提前先把这3G虚拟内存的页表都建立好呢？一般情况下，32位机器的物理内存是小于3G的，加上同时有很多进程都在运行，根本无法给每个进程提前建立3G的线性地址页表。Linux利用CPU的一个缺页机制解决了这个问题。进程创建后我们可以给页目录表的表项值都填0，CPU在查找页表时，如果表项的内容为0,则会引发一个缺页异常，进程暂停执行，Linux内核这时候可以通过一系列复杂的算法给分配一个物理页，并把物理页的地址填入表项中，进程再恢复执行。当然进程在这个过程中是被蒙蔽的，它自己的感觉还是正常访问到了物理内存。]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-03-02-Linux系统基础介绍]]></title>
    <url>%2F2019%2F04%2F15%2F2019-03-02-Linux%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文作为Linux操作系统开篇文章，主要介绍一些操作系统的基础概念和原理，然后简要介绍Linux的发展历史，以及市面上常见的Linux系统版本。 操作系统基本概念和原理操作系统可以说是目前所有现代人都了解的一个名词，大家平时的日常工作和生活都离不开操作系统。目前，世界上主要流行的操作系统有三类：Linux、Windows和Mac OS。其中，后两个操作系统是大家熟悉的，大家的日常工作和生活大部分都会跟这两种操作系统打交道，对于Linux操作系统可能普通只是处在听说过的阶段，很少有人去详细了解和使用。 那么如果有人问你什么是操作系统？虽然大家平时都在用，但是估计很多人都会一脸懵逼。其实，操作系统就是处在计算机硬件和人之间的一个重要的中间部件，它存在意义有2个：一是通过在其上部署应用软件，满足人们操作计算机硬件的需求。二是将上层应用软件与底层硬件进行解耦，满足人们随时随地、随心所欲的操作计算机硬件的要求。 作系统的官方定义是：英文名Operating System，简称OS。是计算机系统中必不可少的基础系统软件，它是应用程序运行以及用户操作必备的基础环境支撑，是计算机系统的核心。**从官方定义不难看出，操作系统（后统称为OS）首先是一个软件，是一个支撑软件，支撑应用软件和人们的操作。所以，它是一种特殊软件，主要由内核+库两部分实现（如下图所示）。 内核存在的目的就是将底层的硬件进行软件化封装，方便上层调用来操作硬件。而库的作用就是将内核的软件封装再构造成一个个标准函数，供上层应用去调用从而避免应用直接操作内核的风险。其次，它还是计算机系统的核心。我们现在讲的计算机系统都是冯诺依曼架构（如下图所示），由控制器、运算器、存储设备和输入输出设备四部分构成，OS就是这些部件协调运作赖以支撑的基础，所以它也是计算机系统的核心。 综上，用一句话概括操作系统的概念就是：操作系统就是位于用户与计算机系统硬件之间用于传递信息系统程序软件。 什么是Linux Linux系统组成如上图 内核：是系统的心脏，是运行程序和管理像磁盘和打印机等硬件设备的核心程序。 Shell：是系统的用户界面，提供了用户和内核进行交互操作的一种接口。它接收用户输入的命令并把它送入内核去执行，是一个命令解释器。但它不仅是命令解释器，而且还是高级编程语言，shell编程。 文件系统：文件系统是文件存放在磁盘等存储设备上的组织方法，Linux支持多种文件系统，如ext3,ext2,NFS,SMB,iso9660等。 应用程序：标准的Linux操作系统都会有一套应用程序例如X-Window,Open Office等。 和windows一样，Linux也是一种操作系统。与windows的商业不同的是，Linux是一套完全开源的操作系统，其实现代码全部呈现给使用者。就像你亲自下厨做饭一样，所有的原材料和工序你都清楚，因此做出饭菜自然比外卖要安全可靠。所以，国内一些企业自研操作系统均是由Linux源码封装改造而来，其宣称”自主可控“，也无可厚非。 Linux在设计之初，是基于Intel x86 PC架构的，是一套多任务、多用户并支持多线程和多CPU的操作系统。其设计的本意就是打破商业软件版权的限制（因为当初unix系统被AT&amp;T回收版权，禁止向学生群体开放源码，从而引起版权纠纷），全世界都能使用的类unix系统兼容产品。从其1991年诞生到现在约30年的时间，Linux操作系统主要用于服务器领域、嵌入式开发，其在个人PC桌面领域应用较少，这也是其不被大众熟悉的原因。（目前，在个人PC桌面领域做的较好由国外的ubuntu、fedora，国内的深度deepin，中兴的新起点等操作系统）。其实，现在windows在个人桌面操作系统的优势除了软件生态以外，也不再剩下什么。如果Linux的软件生态得到广大发展，那么可以预期个人桌面系统将发生颠覆性的变化。这也是微软为什么在即将发布的windows 10的1903版本推出同时兼容读写Linux系统文件的原因，微软现在已经很着急啦：）。 全球超算99.9%都是使用Linux，全球和国内排名前1000的互联网公司90%的服务器也使用的是Linux，就是因为Linux系统优越性使其在服务器领域一举奠定了霸主地位，无论是windows还是mac os都无法与其竞争。Linux的优越性主要都继承自unix系统，主要由以下几点： 具备开放源代码的程序软件，可自由修改。 Unix系统兼容，具备几乎所有Unix系统的优点。 可自由传播，无任何商业化版权制约。 适合Intel x86CPU系列架构的计算机。 严格来说，Linux这个词本身只表示Linux内核，但是人们已经习惯用Linux来形容整个基于Linux内核的操作系统，并且是一种使用GNU通用公共许可证（GUN general public，GPL）工程的，包括各种工具和数据库的操作系统。 Linux内核除系统调用外，由五个主要的子系统组成：进程调度、内存管理、虚拟文件系统、网络和进程间通信(IPC)。 各个子系统的主要功能为： 1、进程调度：它控制着进程对CPU的访问，当需要选择一个进程开始运行时，由调度程序选择最应该运行的进程； 2、内存管理：它允许多个进程安全地共享主内存区域，支持虚拟内存；从逻辑上可以分为硬件无关的部分和硬件相关的部分； 3、虚拟文件系统(VFS)：它隐藏了各种不同硬件的具体细节，为所有设备提供统一的接口，支持多达数十种不同的文件系统，分为逻辑文件系统和设备驱动程序； 4、网络：它提供了对各种网络标准协议的存取和各种网络硬件的支持，分为网络协议和网络驱动程序两部分； 5、进程间通信：支持进程间各种通信机制，包括共享内存、消息队列和管道等。 GNU的全称为GNU’s not Unix，意思是“GNU不是Unix”，GNU计划，又称革奴计划，是由Richard Stallman在1984年公开发起的，是FSF（自由软件基金会）的主要项目，这个项目成立的本意就是建立一套完全自由的可移植的类unix操作系统。在Linux内核发布的时候，GUN项目已经完成了除系统内核之外的各种必备软件的开发。在Linus Torvalds和其他开发人员的努力下，GNU项目的部分组件又运行到了Linux内核之上，例如：GNU项目里的Emacs、gcc、bash、gawk等，至今都是Linux系统中很重要的基础软件。因此，如今我们说的Linux操作系统实际上是GNU/Linux操作系统。 而GPL是一个最著名的开源许可协议，其核心是保证任何人有共享和修改自由软件的自由，任何人有权取得、修改和重新发布自由软件的源代码权利，但都必须同时给出具体更改的源代码。正是因为该协议的存在，才使得开源软件有如今如火如荼的发展局面。 Linux系统启动顺序和基本概念开机自检（BIOS)—&gt;MBR引导（512字节，其中前446字节是Grub菜单，后64字节是分区表）—&gt;GRUB菜单（选择启动系统）—-&gt;加载内核Kernel—&gt;运行INIT进程。其中，INIT进程在Linux系统的用PID编号为1来表示，意思为所有进程的“大佬”。 BIOS：基本输入输出系统（basic input output system，BIOS），是一组固话到计算机主板ROM上的程序，保存着计算计算最重要的基本输入输出程序、系统设置信息、开机自检程序和系统自启动程序，为计算机提供最底层的、最直接的硬件设置和控制。 MBR：一种硬盘分区格式。目前，硬盘分区格式主要有两种，分别是MBR和GTP。MBR，即主引导记录扇区（master boot record），位于整块硬盘的0磁道0煮面1扇区，用于操作系统对硬盘读写时，判断分区的合法性以及分区引导信息的定位。总共512字节，前446字节用于主引导记录，后64字节用于存储硬盘分区表（DPT），每个分区表大小16字节，共4个分区表，所以采用MBR分区格式的硬盘最多只能分出4个分区（主分区+扩展分区），最后两个字节”55，AA“是分区的结束标志。MBR分区表的格式如下： GPT：全局唯一分区格式，正在逐渐取代MBR成为新标准。它和统一的可扩展固件接口（unified extensible firnware interface，UEFI）相辅相成。UEFI用于取代BIOS，而GPT用于取代MBR。在GPT硬盘中，分区表中的位置信息存储在GPT头中，第一个扇区同样有一个与MBR类似的标记，叫做受保护的主引导记录（protected main boot record，PMBR）。其作用是当使用不支持GPT的分区工具时，整个硬盘将显示为一个受保护的分区，防止数据被破坏，其中存储的内容与MBR一样，之后才是GPT头部信息。与MBR相比，支持2TB以上的磁盘，如果使用fdisk分区，最大只能建立2TB大小的分区，创建大于2TB的分区时，需使用parted工具，同时必须使用64位操作系统。以下是GPT分区表的数据格式： GRUB：多操作系统启动程序（GRand unified bootloader）。支持多操作系统引导，当系统中装载多操作系统时，在系统启动时便于用户选择。GRUB还可用于选择操作系统分区上的不同内核，也可用于向这些内核传递启动参数。Linux常见的引导程序包括：LILO、GRUB、GRUB2。CentOS 6.x系统和Ubuntu系统默认采用GRUB引导程序，所以当我们有需要编译GRUB菜单时，执行如下命令： 1grub-mkconfig -o /boot/grub/grub.cfg CentOS 7.x系统默认采用GRUB2引导程序，所以当我们有需要编译GRUB菜单时，执行如下命令： 1grub2-mkconfig -o /boot/grub2/grub.cfg GRUB加载引导程序的流程如下： GRUB2是基于GRUB开发的更加安全强大的多系统引导程序，同时采用模块化设计，使得GRUB2核心更加尽量，使用更加灵活，也不需要像GRUB那样分为stage1、stage1.5和stage2三个阶段。 init：就是系统的不同运行级别对应加载的启动文件。在Linux跟目录下的/etc/目录下，有与系统运行级别对应的rc开头的目录，里面存在对应系统运行级别的脚本文件。Linux内核加载完成后，通过加载这些启动文件完成系统的初始化。]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-10-10-云计算的技术架构]]></title>
    <url>%2F2018%2F10%2F10%2F2018-10-10-%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[云计算总体技术架构下图是一张从云计算上下文参考架构引申出的云计算总体技术架构图。其实，当我们面临一个复杂系统的认知学习，首先需要从全局对其做鸟瞰式概览，将其关键部分抽象为几个关键模块。然后，分析每个关键模块之间的关联，也就是上下文对接关系。最后，再由上下文参考架构拓展出具体关键流程和服务模块。做到上述“收放自如”，才算真正入门，才具备继续深钻的基础条件。：） 上图左边是云计算的参考架构，主要由4个关键子模块和1个关键服务组成。4个关键子模块包括：CloudDevice（云设备）、User（用户&amp;开发者）、ServiceCenter（运营服务中心）和OperatorCenter（运维中心）。而将4个关键子模块进行衔接的关键服务就是我们常说的IaaS、PaaS和SaaS三层平台的逻辑抽象，由IaaS层的CloudOS统一完成纳管和呈现。 对上下文架构参考图进一步拓展，就是右边的云计算解决方案的整体技术架构图。这张整体技术架构图向上可以支撑公有云、私有云、电信云和混合云的各种方案部署。虽然，其涉及的技术方案很多，但其本质上还是底层四个关键技术领域。即，计算、网络、存储和安全。说白了，云计算要想彻底精通，必须同时精通计算、网络、存储和安全四个领域。 云计算的核心技术识别 虚拟化及资源调度平台 虚拟化软件：高性能、高可靠性、智能调度算法 数据中心的一体化自动管控 分布式计算/存储框架 虚拟化的硬件加速 计算与存储平台 定制化的服务器与存储：讲话涉及大内存，高网络/存储IOPS 数据中心安全性：可信赖、完整性、可用性 网络平台 高密度、低成本的10GE互联 网络的集群与虚拟化 基础设施平台 E2E的集成交付能力 绿色节能的工程设计 从物理设备（服务器、存储和网络设备）、虚拟化软件平台、分布式计算和存储资源调度、一体化自动化管控软件、虚拟化数据中心的安全和E2E的集成交付能力，都是构建高效绿色云数据中心的关键技术。 简化设计的大内存、高网络和存储IOPS的服务器，可以为云数据中心提供强大的计算能力。 高IOPS，支持链接克隆、精简置备、快照等功能的存储设备，可以为数据中心提供强大的存储能力。 高密度、低成本，支持大二层网络技术的交换设备为数据在二层网络流动提供交换能力。 虚拟化软件平台，可以抽象物理资源为资源池，给云用户配置不同规格虚拟机提供底层支撑。 灵活、高效的分布式计算或存储框架，为云计算的资源调度和调整提供支撑。 从门禁监控、网络接入、虚拟化平台软件安全、经过安全加固的OS和DB到用户的分权分域管理，保证数据中心的放心使用。 一体化自动化的管控软件，提升维护人员的效率，降低企业成本。 云计算的关键技术云计算的单点技术都是“老”技术，组合起来却有无与伦比的的价值。马云有句话说的好，从技术层面来讲，云计算的就是新瓶装旧酒。 计算架构：支持scale out模式，整体性能最优，基于软件可靠性和可扩展性。 云计算硬件 服务器：高可靠性、高性能 网络：高密度以太网交换机 存储：低成本、多备份 云计算软件 并行计算技术 分布式存储 分布式文件管理 虚拟化技术 智能化云计算系统管理技术 通过对多项核心技术进行归类汇总，可归结为三个方面：整体的计算架构、承载的硬件设备和软件系统。 整体的计算架构：需要涵盖高性能、高可靠和可扩展。 云计算硬件包括：高可靠和高性能的计算服务器提供计算资源；低成本、数据安全的存储设备提供数据存储空间；支持大二层网络的高密度交换机进行数据的通信和交流。 云计算软件包括：用于大数据的并行分析计算技术；整合存储资源提供动态可伸缩资源池的分布式存储技术；用于数据管理的分布式文件管理；计算、存储等资源池化的虚拟化技术；简化运维人员工作，方便高效智能运维的系统管理技术。 云计算的硬件技术：计算架构 早起许多IT系统开始很简单，但当需要进行系统扩展时就会变得复杂。升级系统最常见的原因是需要更多的容量，以支持更多的用户、文件、应用程序或连接的服务器。常见的系统扩展方式有Scale up和Scale out两种。 Scale up 纵向扩展架构主要是利用现有的系统，通过不断增加存储容量来满足数据增长的需求。但是这种方式只增加了容量，而带宽和计算能力并没有相应的增加。所以，整个系统很快就会达到性能瓶颈，需要继续扩展。 Scale out 横向扩展架构的升级通常是以节点为单位，每个节点往往将包含容量、处理能力和I/O带宽。一个节点被添加到系统，系统中的三种资源将同时升级。而且，Scale out架构的系统在扩展之后，从用户的视角看起来仍然是一个单一的系统。所以Scale out方式使得系统升级工作大大简化，用户能够真正实现按需购买，降低TCO。 云计算的设计思想是以最低成本构建出整体的性能最优，与传统电信设备和IT设备（服务器、大型机、企业存储等）追求设备可靠性和性能的思路完全不同。 云计算的硬件技术：存储系统 企业存储一般采用专用的存储设备，成本高。 分布式存储系统把使用便宜IDE/SATA硬盘的服务器本地存储构建存储资源池，既降低了服务器的成本，也降低了存储成本，构建最低成本的计算和存储。 通过“分布式存储和多副本备份”来解决海量信息的存储和系统可靠性，数据存储可以配置多份副本，保证数据的安全性。 云计算的硬件技术：数据中心的联网 东西向流量增长 并行计算业务(如：搜索)需要服务器集群协同运算，产生大量横向交互流量 虚拟机的自由部署和动态迁移，虚机间需要实时同步大量的数据 随着云计算的发展，越来越多业务承载在数据中心的虚拟机上，业务数据的流动从南北向转变为东西向，对数据中心网络的需求和冲击提出了很大挑战。 数据中心内部虚拟机的迁移促进了大二层网络虚拟交换技术的发展，支持大容量数据的通信和超高的端口密度，可以连接更多的服务器提升数据中心的处理能力。 云计算的软件技术：集群管理 云计算虚拟化平台软件，支持分布式的集群管理。可以针对业务模型，对物理服务器创建不同的业务集群，并在集群内实现资源调度和负载均衡，在业务负载均衡的基础上实现资源的动态调度，弹性调整。 云计算虚拟化平台需要支持各种不同的存储设备，包括本地存储、SAN存储、NAS存储和分布式本地存储，保证业务的广适配性。 同时，提供链接克隆、资源复用、精简置备和快照功能，降低企业成本并提供高效率、高可靠性的资源池。 结束语截止目前，云计算基础入门部分已更新完毕，此部分主要是针对打算入坑的新人，给其一个总体上概括认知。后续本打算重写虚拟化技术。但是，考虑到OpenStack以及Docker容器涉及很多Linux基础知识，故临时调整更新内容为重写Linux部分。主要涉及：Linux系统组成、常用命令总结（这部分是我自己总结，大家可下载留存参考）、三剑客基本使用教程和shell编程基础。 完成Linux部分更新后，再继续重写虚拟化技术入门，涉及计算、存储和网络虚拟化三部分。以上与目前更新云计算基础统一构成基础概念部分。所谓“基础不牢，地动山摇”，因此这部分虽然是入门，其实还是很重要的部分。]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-25-云计算带来哪些变化]]></title>
    <url>%2F2018%2F09%2F25%2F2018-09-25-%E4%BA%91%E8%AE%A1%E7%AE%97%E5%B8%A6%E6%9D%A5%E5%93%AA%E4%BA%9B%E5%8F%98%E5%8C%96%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[传统的IT架构与云计算架构投资决策的差异 传统的IT架构俗称三层架构，即，表示层、业务逻辑层和数据访问层。虽然三层架构将系统在逻辑上分层了三层，但它并不是物理意义上的分层。也就是说对于不同层的代码而言，经历编译、打包、部署之后，所有的代码最终还是运行在同一个进程之中。 对于这种功能集中、代码中心化、一个发布包、部署后运行在同一个进程中应用程序，我们称之为单块架构应用。企业采用这种架构开发某个业务时，对计算、网络、存储、服务器资源的需求都是独占的，不同业务之间是一种烟囱式的条块化分割。随着业务的增加，需求功能的迭代，单块架构只能通过增加自己独占资源组内的资源来实现，即使其他应用资源占用很低，也无法共享其它应用的空闲资源。因此，这种架构模式已经很难满足业务快速变化的需要。 一方面，代码的可维护、扩展性、灵活性在降低； 另一方面，系统的修改成本、构建以及维护成本在增加，因此单块架构的改造与重构势在必行。 随着云计算的出现，在技术上面其实有四点最关键的技术： 第一个技术就是服务器虚拟化，前面讲到了，就是把一台物理的服务器，当成很多逻辑的服务器来用，这种分割的目的就是那提升资源共享利用率，达到业务可以快速的部署，代码重构维护简单的目的。 第二个技术就是分布式的存储，就是把一些原来的专用大存储服务器，比如存储9000那种大的设备大的柜子。把它里面设计成一台一台小的服务器，通过软件方式融合在一起进行管理，当成是一个“存储池”来管理这个资源，这种就是分布式的存储。分布式的存储的好处是可以提高存储的速度，同时以按照客户的需要去扩容。比如：可以像虚拟CPU和虚拟内存那样，实现存储资源的弹性部署。同时，随着Server SAN概念的提出，分布式存储的硬件基于x86通用服务器，远比专用存储服务器的硬件成本低。 第三个技术就是软件定义的网络，也就是现在炒的火热的SDN。我们都知道网络这一块管理起来很复杂，需要提前做好规划，提前分配IP地址。那么，如何通过软件来实现呢？就是通过云计算实现的云资源池，利用里面基础虚拟网络资源，分析这类资源的随时变化，通过编程的方式将这种变化通过协议流去控制实现，通过软件去管理，自动化完成。这就是软件定义网络概念的本质。 第四个技术就是REST，它本质上是一个协议，一种资源状态转移协议。前面说过传统IT的三层架构，主要分为表示层、业务逻辑层和数据访问层。由于在云计算架构里，它的资源是池化的，资源之间是共享的（隔离的本质其实是最大的共享！详见后面OpenStack Neutron部分论述），交互很快，用传统的三层架构软件去实现这种调度根本不现实。因此，就出现了REST协议，它其实是将传统三层架构的功能做了整合，通过它应用可以直面底下的虚拟资源池。这相当于是一个资源调度程序，由应用程序调用，直接去使用这种云的资源。当这种弹性的资源调度模式与上层应用关联起来之后，应用不够或者坏掉了，都没关系，直接再起一个新应用，让应用扩展起来，底层的虚拟机资源也随之增加。这里面有几个特点，第一个就是这种协议是一种API接口协议，所有做开发的人都必须遵循的一种接口协议，为分布式开发和CI/CD(持续集成，持续交付）创造了条件。第二个是云计算采用轻量级的云OS，其主要职责是纳管，并不是具体实现。其上应用恢复与扩展的速度比传统的三层架构更快，为敏捷开发创造了条件。但是，它唯一的遗憾还是未能有一个组织出来进行标准化定义，因此兼容性方面还有缺陷。直到现在容器技术的兴起，才使上述两个场景（CI/CD和敏捷开发）真正落地。 云时代商业模式的变化：从自建变为租用 企业由纯粹的自建IT系统逐步转向混合云的模式。 部分核心应用私有云，一般应用租用公有云服务。 云服务又分为：IaaS、PaaS、SaaS三种方式。 以前，IT企业提供业务需要自己建设机房，买了一堆服务器、交换机部署一个数据中心，然后在上面再部署自己的软件来提供服务。往往这都是一笔不小的花费，从而导致好多创业公司的启动经费过高。 随着IDC机房的出现，云计算技术的普及，这种模式发生了变换。好多公司完全可以通过租借云DC的方式来部署自己的业务软件。所谓的云DC，其实就是通过云计算技术将IDC机房中基础设施通过虚拟化资源的方式来统一提供，租用的费用远远比自建少得多，同时可以节省一笔维护开销，从而导致创业的门槛大大降低。 往往这些企业只需要购买很少一部分服务器和交换机来部署自己的核心业务，存储自己的核心数据，同样也使用云计算来提高资源利用率，降低成本，这部分往往我们称为企业私有云。非核心的企业业务软件和数据，往往通过公有云提供商提供云DC/云主机部署，这部分往往我们称为公有云。两者的结合，就诞生了混合云的概念。因此，混合云是从企业业务的全局部署角度提出的，并不是云计算的新技术分支，现在IT企业提供业务大部分都是通过混合云的方式提供。与混合云类似的还有行业云，两者之间的区别非常小，可以合并为一种。 那么租用云DC到底租用的是什么？这个从需求者不同需求角度来说，分为IaaS（基础设施即服务），PaaS（平台即服务），SaaS（软件即服务）。有些企业，软件开发能力很强，具备平台级的开发能力，但是缺少运行软件平台的服务器、存储和交换机。为了节省成本，他们往往都是向云服务提供租用云主机，将自己的软件部署在云主机上，这种租用的就是IaaS。另一些企业，具备应用层软件的开发能力，但不具备数据之间逻辑处理的能力（即，算法），同时为了节省成本，他们往往都是云服务提供上租用开发平台（如大数据处理），也连带底层基础设施（云主机）一起租用，这种租用的就是IaaS+PaaS。目前还没有哪家企业只租用PaaS，自建IaaS的方式来部署自己的业务。至于SaaS的租用，目前只涉及个人办公（如web版office）或开发领域（如Mob等应用），很少有面向企业的服务应用。 云时代建设模式的变化：从烟囱变为水平 芯片、新介质取得突破，以及CPU、硬盘、网络性能大幅提升为IT架构的水平化演进提供了技术支撑。云计算经历了从虚拟化—云资源池—平台&amp;应用的逐步发展阶段。 云服务提供商为了提供各种云服务，必然要建设相应的数据中心，这种建设是去IOE化的。去IOE化的概念，最早是阿里巴巴提出的，其本意是，在阿里巴巴的IT架构中，去掉IBM的小型机、Oracle数据库、EMC存储设备，代之以自己在开源软件基础上开发的系统。说白了就是结合开源软件技术，将IOE的各层级功能通过通用硬件平台，分布式架构，软硬件解耦和软件定义存储的方式实现，通过Cloud OS进行纳管，并统一对外提供服务。 前面也提到了，云计算的出现不是突然出现的，详见博文 云计算的前世今生。随着软硬件技术的发展，他主要经历了三个发展阶段。第一个阶段就是虚拟化阶段，代表技术就是VMware和Xen，主要通过虚拟化技术来提升企业资源利用率，从而降低成本。第二个阶段是云DC阶段，代表技术就是OpenStack，但是OpenStack最成熟的部分还是其IaaS服务提供，这个阶段主要通过提供云主机和云DC来降低企业运营成本。第三个阶段就是现在火热的容器化应用阶段，其实容器也是一种虚拟化技术，属于轻量级操作系统级别的虚拟化。它是以软件进程的方式来封装各个应用模块，通过共享底层操作系统内核，来提供各种平台服务和应用服务（PaaS+SaaS）。通过这种技术，使得前几年DevOps概念完美落地并真正意义上实现敏捷开发，这是一种企业应用现有管理模式的变革。 云计算给企业带来哪些变化 要说云计算技术给企业带来了哪些变化，最明显的还要数银行。 银行采用基于云计算的新架构以后，它的第一个变换就是IT资源池化了，即使有一些资源损坏了，它还可以继续工作，并不影响它的业务。所以这样提高了就是业务的质量，减少了业务中断，保证这个业务的连续性，全年的业务停机时间就大大减少。 第二个变化就是业务提供的速度，当业务需要很多IT资源时，可以通过资源弹性扩缩容的方式，来满足业务需求。不像以前，一个业务办理的好慢好烦，甚至死机，影响客户的体验，客户就可能去选择其他家更快业务。所以通过把资源管理好，也让IT人员有更多的创新机会，可以开发包括手机端的应用，让更多的人通过手机随时随地去办理业务。不知你注意到没有，国内这么多银行，招商银行在这方面一直走在其他银行前面。 还有更重要的一点对于企业来说，它的业务成本的降低。银行对散户，对个人的业务，实际上都是亏钱的，个人取钱的交易或者是存钱的交易，银行都要投入人力和场地等费用，而现今，通过手机银行和互联网银行，可以把这个散户交易成本降下来，通过云计算等互联网技术，免除了场地网点的费用而且业务处理速度非常的快，用户体验也提升了。]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-12-云计算的前世今生]]></title>
    <url>%2F2018%2F09%2F12%2F2018-09-12-%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[云计算的发展历程 云计算的历史也是蛮好玩的… 一开始就是在2003年之前，大家都在在提升CPU的计算能力，提升存储的空间，使得CPU性能越来越高、存储变得更大。 到了2003年以后呢，企业的数据中心里面发现资源已经很空闲，因为架构设计的原因，让这些服务器很多时间都处在平均不到5%的使用率，大部分99%都是空闲的。相当云投资一千块，其实只用了五块钱，实际上产生了很大的浪费。同时，机房空间、动力等基础设施资源被占用过多，导致数据中心内部无法放置更多机柜、机架和服务器，需要买地建设新的机房（按照国内的地价/房价，这基本是不可能的）。因此，在2003年VMware这个厂商提供了服务器虚拟化的技术，就是把一台服务器一台设备，切成很多虚拟的小块服务器，也就是俗称的“虚拟机”。而让业务应用在虚拟机上面去工作，这就叫服务器虚拟化。 这个技术解决了数据中心里面机房空间和动力资源不够的问题，原来需要十台服务器的，现在在一台高级的服务器里面切了十份，分别给10个业务应用使用，即使这样也就才用了40%的CPU，电力节省了接近9/10，而且一个机房里面容量就可以做二十倍数的的提升。原来部署一百台物理的服务器，现在可以部署两千台虚拟服务器，大大的提升了资源的利用率。而虚拟化的另一个好处就是还大幅提升了客户业务的创新的积极性，因为以前他们服务器都摆不下，好多东西不能做，现在摆下了，所以他要做什么东西都可以试试。但是这个虚拟化技术只解决了资源的共享问题，还没解决运营效率问题。 到了2006年的时候，亚马逊提出了公有云的服务，那时还不叫云计算，叫Amazon Web Service, 简称AWS（现在亚马逊的云计算项目还是沿用这个叫法）。亚马逊大家都知道，它原本是一个做超市的，超市的概念就是你需要你去拿，拿了就到门口结帐，结帐你就拿走。然后亚马逊它把这个计算资源也当成一种，把服务器也当成资源去卖，但是它卖的不是物理的，不是让你搬一台服务器走。而是建好一个数据中心，当你用的时候，你要多少台我们就给你多少台，你只要刷信用卡在网上做支付，你就可以使用这些逻辑上提供给你的资源，这也是虚拟化资源第一次被当成商品出售的案例。随后，Amazon在此基础上提出了云计算的概念，并开发出了公有云业务。亚马逊的这种公有云的这种服务对企业而言，想做的时候，只要付钱就可以做了。而且一般企业业务部门都很有钱，所以业务部门的愿意投钱去做这个事情。 到了2010年的时候，出现了另外一种云计算架构，叫OpenStack。它是NASA就是美国太空总署那个NASA和RackSpace联合做的。RackSpace也是做公有云服务的运营商，就像我们的阿里云一样，它也是想学习亚马逊。但是它的人没有亚马逊的团队这么大，所以它把这个架构给开源出来，形成一个OpenStack开源社区，大家都在OpenStack上面去开发完善，然后让这个框架变得更好，目前OpenStack会聚了全球的研发力量，并吸引了几乎全球所有主流IT厂商的支持，帮它完善这个架构，形成了云计算领域一个全新的强大生态系统。OpenStack社区把这个架构公开出来以后，各个企业都看到了一个新的技术，可以运用到他们自己企业里面，所以很多企业都开始基于OpenStack做自己的私有云，让自己家的IT，变得效率更高，资源更有效。 然后，到了2014年以后，很多企业经过这么多年的建设以后，基本都走在了公有云和私有云这两条路上，私有云就是在家里，自建自用，不对外。公有云就是公开对外出售虚拟资源（如服务器、负载均衡器、防火墙等）在亚马逊、阿里或者华为都有这类服务，只要花钱买就可以。很多企业，尤其是中小企业，无力承担基础设施建设费用，就是通过购买EC2（云计算服务中资源服务器）来部署它的业务。还有一些企业，将一部分业务部署在公有云上，另一部分私密性业务部署在私有云上，这样就合成了一个混合云的状态。 云计算技术的演进历程 随着云计算技术走向成熟，在混合云时代，企业对于云计算的相关技术成熟度的问题已经不太关注了。企业新的追求是，让IT的人员怎么在一个团队的模式下面，对公有云和私有云的混合云资源进行统一的管理。这是一个管理的概念，也是混合云构建的核心。 云计算（Cloud Computing）是分布式计算（Distributed Computing）、并行计算（Parallel Computing）和网格计算（Grid Computing）的发展或者说是这些计算机科学概念的商业实现。 并行计算（Paralled Computing）：同时使用多种计算资源解决计算问题的过程，主要目的是快速解决大型且复杂的问题。 特点：把计算任务分派给系统内多个计算单元。 分布式计算（Distributed Computing）：把一个需要巨大计算能力才能解决的问题分成多个小部分，把这些小部分分配给多个计算进行处理，最后综合这些计算结果得到最终的结果。 特点：分配计算任务到网络中多台独立的机器。 网格计算（Grid Computing）：利用互联网把地理上广泛分布的各种资源连成一个逻辑的整体，就像一台超级计算机。 特点：分布式计算的一种，为用户提供一体化的信息和应用服务。 云计算的部署模式 私有云（Private Cloud）： 企业利用自有或租用的基础设施资源自建的云。 社区云/行业云（Community cloud）：为特定社区或行业所构建的共享基础设施的云。 公有云（Public cloud）：出租给公众的大型的基础设施的云。 混合云（Hybrid cloud）：由两种或两种以上部署模式组成的云。 从部署模式上看，云计算又分为私有云、社区云也就是行业云、公有云、和混合云，这几种形态，这里也给出了具体的定义。简单理解： 私有云就是企业建在自家院子里的，只给自己用的云，有些公司特别是公有云厂商喜欢把“私有云”称作“专有云”，名字不同，但含义基本相同，虽然这些厂商不愿意承认。 社区云既行业云，应用范围要比私有云广泛，更像是由家庭和家族的关系，往往应用在某个特定的区域或特定行业，主要给特殊血缘关系的行业或社区家族成员使用。 公有云的使用范围则更加宽泛，基本上只要给钱谁都可以用，因此公有云的规模往往也更大。 但公有云的用户鱼龙混杂，像火车站一样，而且目标明显，不仅使用的好人多，惦记着甚至使用的坏人也不少，所以经常成为黑客及非法集团的攻击或信息偷窃目标。但瑕不掩瑜，人们不能因为火车上有小偷，就不坐火车了。但贵重的东西，比如大额现金最好就别随身带了，放家里由家人看着最保险，也就是在使用公有云的同时还使用私有云。而且希望公有云与私有云能很好的协同。这就是混合云了。 云计算当前的商业模式 云基础设施即服务（IaaS）— 出租处理能力、存储空间、网络容量等基本计算资源。 云平台即服务（PaaS）— 为客户开发的应用程序提供可部署的云环境。 云软件即服务（SaaS）— 在网络上提供可直接使的应用程序。 现在不止这几种，还有存储即服务，桌面即服务，数据中心即服务等等。 在服务模式上，云计算分为三种，包括：IaaS 基础设施即服务、PaaS 开发平台即服务、SaaS 软件即服务这三种模式。 IaaS，就是以前买IBM、HP、DELL、华为服务器存储跑企业业务（比如：生产管理业务、财务、市场客户管理、邮箱等），现在企业把业务放到Amazon的网站或华为企业云服务（公有云）网站就全搞定了。简单讲就是由“买”变“租”。大型企业的IT部门也可以以IaaS服务的形式，对内部业务部门提供服务，然后进行内部结算。 PaaS，就是以前企业需要业务软件的时候，软件开发商在自己公司开发完在卖给企业客户，还要派人去安装软件。现在开发、安装都在企业客户那里全做了。当然给的钱也不再是软件费而是人头费了。 SaaS，就是以前写汇报胶片用微软PPT软件，现在登录www.prezi.com，在那个网站上就可以写胶片了，很酷。 云计算的8个通用特点 大规模（Massive scale） 同质化（Homogeneity） 虚拟化（Virtualization） 弹性计算（Resilient computing） 低成本软件（Low cost software） 地理分布（Geographic distribution） 服务定位（Service orientation） 先进安全技术（Advanced security technologies） 这是美国国家标准与技术研究院给云计算列出的8个通用特点： 大规模（Massive scale），因为云计算服务把IT的资源供应集中化了，自然规模很大。也正因为如此，量变导致质变，使得云计算与传统IT有了众多的区别。 同质化（Homogeneity），也可以理解成标准化，这点倒是和用电很类似，大家要保持相同电压、插座接口，这样人们的电器和各种设备才能被广泛使用。 虚拟化（Virtualization），有两层含义，一个是计算单元的精细化，一块蛋糕太大，一个人吃不了，那最好切成小块，大家分着吃，也就是让每个计算单元更小，这样可以充分利用IT资源；另外一层含义是软硬件的分离，虚拟化之前软件和指定硬件是绑在一起的，虚拟化之后软件在所有硬件上可以自由的迁移，这跟人们由买房变成租房是一样的，既然北上广深的房价太高，很多人便租房住了，拎个箱子想住哪就住哪。 弹性计算（Resilient computing），在前面已经说过，指的是IT资源供给可弹性伸缩。 低成本软件（Low cost software），是从竞争与市场需求发展的角度说的。云计算降低了人们使用IT的门槛，不仅仅在个人技术能力上，而且在资金能力上，很多小微的初创企业，本身就没啥钱，希望能够用最少的钱使用最多的IT服务，要想打开这部分市场，自然需要低成本的软件，通过薄利多销的形式赚到更多的钱。 地理分布（Geographic distribution）：前文我们提到了泛在接入，也就是能够在任意时间任意地点提供IT服务。从使用者的角度看，就是地理分散的，由于各地网络带宽的优劣差异，那么IT提供者，也就是云计算数据中心的部署，自然也是呈现出地理分布式特征的。大的公有云厂商都有几十个甚至数百个数据中心或服务节点，面向全球提供云计算服务。 服务定位（Service orientation）：因为云计算是一种服务模式，它的整个体系的设计也就是面向服务的。 先进安全技术（Advanced security technologies），林子大了，什么鸟都有，公有云大了，什么用户也都有，包括好的坏的，自然先进的安全技术保障是一个云计算必须的条件了。 到此为止，对云计算的总结就是：4部署-3服务-5特性-8个通用点，简称4358。 云计算的流派 从实现方式来看，云计算有两个典型的流派：大分小模式和小聚大模式。（现在这个概念已经很少有人提了，因为从资源的使用看，两种流派的本质是相同的） 大分小模式：也称为Amazon流派，不同的应用在使用资源时，通过时分复用算法来调用。 关键技术点包括：计算、存储和网络虚拟化以及虚拟机监控、调度和迁移。 典型代表：Amazon，alibaba，华为的EC2等。 小聚大模式：也称为Google流派，资源在多个应用间贡献，通过将应用划分多个子任务，结合调度算法来实现某个子引用在资源上的独占。 关键技术点包括任务分解、调度、分布式通信总线和全局一致性。 典型代表：Google，我国的天河2号等]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-03-什么是云计算?]]></title>
    <url>%2F2018%2F09%2F03%2F2018-09-03-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%91%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[云计算概念的定义 云计算基础知识入门 云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络服务（即随时随地接入可接入），进入可配置的计算资源共享池（包括：网络、服务器、存储、应用软件、服务），这些资源能够被快速提供，只需投入很少的管理工作，或与服务供应商进行很少的交互。（美国国家标准与技术研究院） 云计算是一种基于互联网的计算方式，通过这种方式，共享软硬件资源和信息可以按需求提供给计算机和其他设备。云计算依赖资源的共享以达成规模经济，类似基础设施。（维基百科） 云计算概念诞生之初，市场上对其概念有很多种理解，经过一段时间的争论，现在大家一般来说都认可的就是美国标准与技术研究院的它给出的一个最标准的定义。它把云计算定义为一种模式，而不是一种技术。这种模式既可以是商业模式，也可以是服务模式。 云计算的关键特征 按需自助服务（On-demand Self-service） 无处不在的网络接入（Broad Network Access） 与位置无关的资源池（Locations independent resources pooling） 快速弹性（Rapid Elastic） 按使用付费（Pay per User） 我们从云计算的基本特质上进一步理解一下云计算的概念和内涵。 从管理层面上说，云计算实现了IT资源的按需自助服务。“按需”是从量的角度来说，这是一种“量体裁衣”的资源使用方式，避免粗放管理带来成本损耗。“自助”是从人的角度来说，减少了资源使用者与资源管理者之间的频繁交互，进一步减少人工成本的损耗。同时，从历史上看，IT的使用难度每降低一个层次，IT产业就会获得一次质飞跃，因为客户数量会因此有指数级的提升。而“自助”这种简单化操作意味着更多的人都可以使用IT产品和服务。 云计算实现了广泛网络接入。这意味着用户可以在全球各地7x24小时的使用IT服务，也就是随时、随地、随心、随意的使用。这极大的提升了用户工作的灵活性和经营工作效率。 云计算实现了资源池化。“池化”就意味着资源的同质化、归一化。无论是网络、服务器、存储、应用还是服务，都是这些同质化、归一化资源的组合、协同实现。使用者和管理者只需考虑需求的资源“量”，无需考虑资源提供商之间的差异性。 云计算资源弹性伸缩。是指资源能够快速的供应和释放，就是说当你要的时候，我能快速的很快就给你，不是说你申请了以后，十天半个月我才给你资源，而是你要我马上就给你，当你不用的时候，我马上就回收，资源释放。 从经营层面上说，云计算实现了可计量的服务。“技术免费、服务收费”是开源社区的一个宗旨。前面也提到云计算的本质就是一种服务，为了实现这类服务收费，就必须要求服务可计量，而计量的依据就是资源使用的可计量。比如：按使用小时为时间单位，以服务器CPU个数、占用存储的空间、网络的带宽等综合计费，当然也可以包时、包天、包月那种套餐模式进行计量。而且，计量的越精细，运营效率越高。 以上，就是美国标准与技术研究院给云计算标准的定义时所诠释的特质。 从技术视角来看：云计算=计算/存储的网络 商业视角：云计算=信息电厂计算和存储：由PC时代的局域网向云时代的互联网迁移。软件：由PC时代的终端向云端迁移。 用户消费模式变化 通过互联网提供软硬件与服务； 用户通过浏览器或轻量级终端获取、使用服务。 商业模式发生变化 从“购买软硬件产品”向“购买信息服务”转变，如同100年前用电的转变。 云计算产生的背景 云计算=需求推动+技术进步+商业模式转变 云计算的产生是需求推动、技术进步、商业模式转变共同促进的结果。 需求推动： 政企客户低成本且高性能的信息化需求。 个人用户的互联网、移动互联网应用需求强烈，追求更好用户体验。 技术进步 虚拟化技术、分布与并行计算、互联网技术的发展与成熟，使得基于互联网提供包括IT基础设施、开发平台、软件应用成为可能。 宽带技术及用户发展，使得基于互联网的服务使用模式逐渐成为主流。 商业模式转变 少数云计算的先行者（例如Amazon的IaaS、PaaS)的云计算服务已开始运营。 市场对云计算商业模式已认可，越来越多的用户接受并使用云计算服务。 生态系统正在形成，产业链开始发展和整合。 几年之内，云计算已从新兴技术发展成为当今的热点技术。从Google公开发布的核心文件到Amazon EC2（亚马逊弹性计算云）的商业化应用，再到美国电信巨头AT&amp;T（美国电话电报公司）推出的Synaptic Hosting（动态托管）服务，云计算从节约成本的工具到盈利的推动器，从ISP（网络服务提供商）到电信企业，已然成功地从内置的IT系统演变成公共的服务。]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么开始写博客？]]></title>
    <url>%2F2018%2F08%2F31%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%80%E5%A7%8B%E5%86%99%E5%8D%9A%E5%AE%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言2018年3月—7月期间，有幸在外培训，时间相对宽松。当时，主要一直在自学SDN部分知识，鉴于同班同学一直询问询问云计算怎么学习？为此，我特意开通微信公众号，主要讲述一点儿OpenStack的基础知识。（如下） 当时，还特意列出一个大纲，从最基础的Linux开始，一直到后续高级服务和应用。比如：Tacker、Workflow、批量热迁移等。 但是，微信公众号的排版实在是太浪费时间，再加上后续回到工作岗位，个人时间实在有限，因此公众号的内容一直处在长期停更，偶尔写写的状态。实在有愧大家的期待-_-!!! 在2018年7月期间，一个偶然的机会让我接触到hexo和markdown两个小伙伴，让我燃起了写技术博客的动力。hexo主要用来生成静态页面，而markdown是一种极简的文本编写工具，其语法格式只有十来种，对编写网页博客有很大助力：-）利用这两个工具，可以提升网文的写作效率。至于怎么使用，网上的教程很多，大家可以自信搜索。 言归正传，后续公众号中已完成的内容我会逐渐搬到个人博客中来，并对前期挖的坑做个填补-_-!!!。新增的内容主要在个人博客中完成，微信公众号只能有空闲时间后，再进行补充，还请大家以后多关注博客内容。 另外，大家有什么疑惑，以后都可以在博客的留言区进行留言。]]></content>
      <categories>
        <category>随笔杂感</category>
      </categories>
      <tags>
        <tag>篇首语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-31-写在前面的话]]></title>
    <url>%2F2018%2F08%2F31%2F2018-08-31-%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2%E7%9A%84%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[学习大纲 鉴于好多人询问云计算怎么学习？我的建议是实践和理论相结合！IT不像CT那么多理论和流程，IT诞生和发展的源动力就是提升工作效率。因此，IT的理论并没有CT那么难，但是要想用好，用巧必须具备相当丰富的实操经验。对于新手，最好的学习方式就是边实践边消化相关理论流程。 OpenStack是目前最成熟并且符合生产部署的一套开源云操作系统，全球各大IT，CT厂商，甚至运营商都为此投入了大量的人力、物力和精力。由于其开源的特性，全世界的云计算爱好者也为此贡献了很多。从openstack的特点来看，它是由python语音开发，是一个提供多种服务，相当灵活和稳定的云操作系统。以前的定义主要集中IaaS层（基础设施层），但是现在随着docker的出现，IaaS和PaaS（平台层）的界限已经非常模糊，同时随着K8s的兴起，甚至与SaaS的界限也不再清晰。从操作系统的组成来看，openstack+docker+k8s的组合更像一个更广泛的云化操作系统，openstack是内核，dokcer是用户空间，K8s是调用各项基础进程的API。 转入正题，由于openstack非常灵活，同时也就引入另一个问题，学习起来非常困难。因此，在学习openstack的同时，如果自己手头有一套实验环境那是相当的完美。我当初学习openstack是从devstack开始学起（openstack开发者版本），同时我也建议初学者从devstack开始学起。接下来我们就开始学习OpenStack的第一步，基础准备工作。 工具准备 学习云计算的硬件和软件条件 1、一台不少于16G内存，2CPU+内核笔记本或二手服务器，且硬件要开启支持虚拟化功能。BIOS中的开启方法如下图所示： 2、下载CentOS 7的光盘安装镜像。下载地址在CentOS官网链接，下载DVD版本即可。https://www.centos.org/download/ 3、去VMWare官网下载VMWare虚拟机软件，需要新注册个账号。下载链接如下：（注意：如果你你的宿主机是win10系统，不要下载VirtualBox，因为win10的无线网络在VirtualBox中设置桥接网络有BUG）https://www.vmware.com/cn/support/workstation.html 以上工具就是我们环境搭建需要的基本工具。还有一些辅助工具如：XShell、XFTP可自行百度下载（个人版在官网申请是免费，记得千万不要升级） 下载地址如下： http://www.netsarang.com/products/xsh_overview.html]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
</search>
