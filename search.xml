<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2019-05-13-计算虚拟化之CPU虚拟化]]></title>
    <url>%2F2019%2F05%2F13%2F2019-05-13-%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B9%8BCPU%E8%99%9A%E6%8B%9F%E5%8C%96%2F</url>
    <content type="text"><![CDATA[CPU虚拟化概述CPU虚拟化的一个很大挑战就是要确保虚拟机发出CPU指令的隔离性。即为了能让多个虚拟机同时在一个主机上安全运行，VMM必须将各个虚拟机隔离，以确保不会相互干扰，同时也不会影响VMM内核的正常运行。尤其要注意的是：由于特权指令会影响到整个物理机，必须要使得虚拟机发出的特权指令仅作用于自身，而不会对整个系统造成影响。例如：当虚拟机发出重启命令时，并不是要重启整个物理机，而仅仅是重启所在的虚拟机。因此，VMM必须能够对来自于虚拟机操作硬件的特权指令 进行翻译并模拟，然后在对应的虚拟设备上执行，而不在整个物理机硬件设备上运行。 软件方式实现的CPU虚拟化—二进制翻译技术二进制翻译（Binary Translation，BT）是一 种软件虚拟化技术，由VMware在Workstations和ESX产品中最早实现。在最初没有硬件虚拟化时代，是全虚拟化的唯一途径。由于BT最开始是用来虚拟化32位平台的，因此，也称为BT32。 二进制翻译，简单来说就是将那些不能直接执行的特权指令进行翻译后才能执行。具体来说，当虚拟机第一次要执行一段指令代码时，VMM会将要执行执行的代码段发给一个称为“Just-In-Time”的BT翻译器，它类似Java中的JVM虚拟机和Python的解释器，实时将代码翻译成机器指令。 翻译器将虚拟机的非特权指令翻译成可在该虚拟机上安全执行的指令子集， 而对于特权指令，则翻译为一组在虚拟机上可执行的特权指令，却不能运行在物理机上。这种机制实现了对虚拟机的隔离与封装，同时又使得x86指令的语义在虚拟机层次上得到保证。 在执行效率上，为了降低翻译指令的开销，VMM会将执行过的二进制指令翻译结果进行缓存。如果虚拟机再次执行同样的指令序列，那么之前被缓存的翻译结果可以被复用。这样就可以均衡整个VM执行指令集的翻译开销。为了进一步降低由翻译指令导致的内存开销，VMM还会将虚拟机的内核态代码 翻译结果和用户态代码直接绑定在一起。由于用户态代码不会执行特权指令，因此这种方法可以保证 安全性。采用BT机制的VMM必须要在虚拟机的地址空间和VMM的地址空间进行严格的边界控制。VMware VMM利用x86CPU中的段检查功能（Segmentation）来确保这一点。但是由于现代操作系统 Windows、Linux以及Solaris等都很少使用段检查功能，因此VMM可以使用段保护机制来限制虚拟机和VMM之间的地址空间边界控制。在极少数情况下，当虚拟机的确使用了段保护机制并且引发了 VMM冲突，VMM可以转而使用软件的段检查机制来解决这一问题。 硬件方式实现的CPU虚拟化—VT-x和AMD-v2003年，当AMD公司将x86从32位扩展到64位时，也将段检查功能从64位芯片上去除。同样的情况也 出现在Intel公司推出的64位芯片上。这一变化意味着基于BT的VMM无法在64位机上使用段保护机制保护VMM。尽管后来AMD公司为了支持虚拟化又恢复了64位芯片的段检查功能，并一直延续到目前所有的AMD 64位芯片，但是Intel公司却并没有简单恢复 ，而是研发了新的硬件虚拟化技术VT-x，AMD公司紧随后也推出了AMD-V技术来提供CPU指令集虚拟化的硬件支持。VT-x与AMD-V尽管在具体实现上有所不同，但其目的都是希望通过硬件的途径来限定某些特权指令操作的权限，而不是原先只能通过二进制动态翻译来解决这个问题。 如前所述，VT-x提供了2个运行环境：根（Root）环境和非根（Non-root）环境。根环境专门为VMM准备，就像没有使用VT-x技术的x86服务器，只是多了对VT-x支持的几条指令。非根环境作为一个受限环境用来运行多个虚拟机。 根操作模式与非根操作模式都有相应的特权级0至特权级3。VMM运行在根模式的特权级0，Guest OS的内核运行在非根模式的特权级0，Guest OS的应用程序运行在非根模式的特权级3。运行环境之间相互转化，从根环境到非根环境叫VM Entry；从非根环境到根环境叫VM Exit。VT-x定义了VM Entry操作，使CPU由根模式切换到非根模式，运行客户机操作系统指令。若在非根模式执行了敏感指令或发生了中断等，会执行VM Exit操作，切换回根模式运行VMM。此外，VT-x还引入了一组新的命令：VMLanch/ VMResume用于调度Guest OS，发起VM Entry；VMRead/ VMWrite则用于配置VMCS。 根模式与非根模式之问的相互转换是通过VMX操作实现的。VMM可以通过VMX ON 和VMX OFF打开或关闭VT-x。如下图所示： VMX操作模式流程： VMM执行VMX ON指令进入VMX操作模式。 VMM可执行VMLAUNCH指令或VMRESUME指令产生VM Entry操作，进入到Guest OS，此时CPU处于非根模式。 Guest OS执行特权指令等情况导致VM Exit的发生，此时将陷入VMM，CPU切换为根模式。VMM根据VM Exit的原因作出相应处理，处理完成后将转到第2步，继续运行GuestOS。 VMM可决定是否退出VMX操作模式，通过执行VMXOFF指令来完成。 这样，就无需二进制翻译和半虚拟化来处理这些指令。同时，VT-x与AMD-V都提供了存放虚拟机状态的模块，这样做的的目的就是将虚拟机上 下文切换状态进行缓存，降低频模式繁切换引入的大量开销。 还是以VT-x解决方案为例，VMX新定义了虚拟机控制结构VMCS(Virtual Machine ControlStructure)。VMCS是保存在内存中的数据结构，其包括虚拟CPU的相关寄存器的内容及相关的控制信息。CPU在发生VM Entry或VM Exit时，都会查询和更新VMCS。VMM也可通过指令来配置VMCS，达到对虚拟处理器的管理。VMCS架构图如下图所示： 每个vCPU都需将VMCS与内存中的一块区域关联起来，此区域称为VMCS区域。对VMCS区域的操纵是通过VMCS指针来实现的，这个指针是一个指向VMCS的64位的地址值。VMCS区域是一个最大不超过4KB的内存块，且需4KB对齐。 VMCS区域分为三个部分：偏移地址0起始存放VMCS版本标识，通过不同的版本号，CPU可维护不同的VMCS数据格式；偏移地址4起始存放VMX终止指示器，在VMX终止发生时，CPU会在此处存入终止的原因；偏移地址8起始存放VMCS数据区，这一部分控制VMX非根操作及VMX切换。 VMCS 的数据区包含了VMX配置信息：VMM在启动虚拟机前配置其哪些操作会触发VM Exit。VMExit 产生后，处理器把执行权交给VMM 以完成控制，然后VMM 通过指令触发VM Entry 返回原来的虚拟机或调度到另一个虚拟机。。 VMCS 的数据结构中，每个虚拟机一个，加上虚拟机的各种状态信息，共由3个部分组成： Gueststate：该区域保存了虚拟机运行时的状态，在VM Entry 时由处理器装载；在VM Exit时由处理器保存。它又由两部分组成： Guest OS寄存器状态：包括控制寄存器、调试寄存器、段寄存器等各类寄存器的值。 Guest OS非寄存器状态：记录当前处理器所处状态，是活跃、停机（HLT）、关机（Shutdown）还是等待启动处理器间中断（Startup-IPI）。 Hoststate：该区域保存了VMM 运行时的状态，主要是一些寄存器值，在VM Exit时由处理器装载。 Control data：该区域包含虚拟机执行控制域、VM Exit控制域、VM Entry控制域、VM Exit信息域和VM Entry信息域。 有了VMCS结构后，对虚拟机的控制就是读写VMCS结构。比如：对vCPU设置中断，检查状态实际上都是在读写VMCS数据结构。 CPU虚拟化管理为了保证电信云/NFV中关键业务虚机运行的性能，就要求同一台物理机上的多个业务虚机实例所获取的资源既能满足其运行的需要，同时不互相产生干扰，这就需要对CPU资源进行精细化的调优和管理，也就是CPU QoS保障技术。在介绍CPU QoS保障技术之前，我们首先来看下CPU虚拟化的本质和超配技术。 CPU虚拟化的本质和超配 vCPU数量和物理CPU对应关系如上图所示，以华为RH2288H V3服务器使用2.6GHz主频CPU为例，单台服务器有2个物理CPU，每颗CPU有8核，又因为超线程技术，每个物理内核可以提供两个处理线程，因此每颗CPU有16线程，总vCPU数量为282=32个vCPU。总资源为32*2.6GHz=83.2GHz。 虚拟机vCPU数量不能超过单台物理服务器节点可用vCPU数量。但是，由于多个虚拟机间可以复用同一个物理CPU，因此单物理服务器节点上运行的虚拟机vCPU数量总和可以超过实际vCPU数量，这就叫做CPU超配技术。 例如，以华为的FusionCompute为例，查询显示的服务器可用CPU的物理个数为2个，每个主频2.4GHz。 这是特权虚机中占用CPU资源，占用了4个vCPU 在资源池性能页，可以查看每个物理CPU有6个核，并且开启了超线程，也就是每个物理CPU有12个核的资源，一台服务器总共vCPU数量为：122=24个。由于在华为的FUSinCompute中CPU的资源统计单位不是逻辑核数，而是频率HZ。因此，可用资源（122-4）*2.4=48GHz（为什么减4个？因为系统DM0占用了4个vCPU，因此单个客户机最多只能使用20个vCPU）。如下所示，当前已经使用了19.15GHZ，占用率为39.39%。 CPU的QoS保障了解了CPU虚拟化的本质和超配，前文提到的CPU的QoS保障技术主要指的是CPU上下限配额及优先级调度技术。 CPU的上下限配额主要指的是vCPU资源管理层面的解决方案。在CPU虚拟化后，根据虚拟化的资源—频率HZ，来对虚拟机进行分配时，为了保证虚拟机的正常运行，特地定义了三种vCPU资源的划分方式，这就是CPU资源的QoS管理。可以按照限额、份额和预留三种方式进行vCPU资源划分，三者之间是有一定依赖和互斥关系的，其定义具体如下： CPU资源限额：控制虚拟机占用物理资源的上限。 CPU资源份额：CPU份额定义多个虚拟机在竞争物理CPU资源的时候按比例分配计算资源。 CPU资源预留：CPU预留定义了多个虚拟机竞争物理CPU资源的时候分配的最低计算资源。 为了描述上述三种vCPU资源划分方式的关系，我们还是举例来说明。比如：单核CPU主频为3GHz，该资源供两个虚拟机VM1和VM2使用。 场景一：当VM1资源限额为2GHz，VM1可用的CPU资源最多为2GHz，也就意味着如果VM2没有设定CPU QoS，那么VM2最多只有1GHZ的vCPU可以使用。 场景二：当VM1和VM2的资源份额分别是1000和2000，在资源紧张场景下发生竞争时，VM1最多可获得1GHz的vCPU，VM2最多可获得2GHz的vCPU。 场景三：当给虚拟机VM1资源预留2GHz的vCPU资源，VM2资源预留为0，在资源紧张发生竞争时，VM1最少可获得2GHz的vCPU，最多可获得3GHZ的vCPU（在没有超配时），而VM2最多可获得1GHz（3-2=1）的vCPU资源，最少为0。 上例只是简单描述了各种vCPU划分方式单独生效时的场景，也是在实际中较常用的场景。同时，在实际中还有一些场景是多种vCPU划分方式共同作用的，虽然不常用，但是却是实实在在有意义的。比如：以一个主频为2.8GHz的单核物理机为例，如果运行有三台单CPU的虚拟机A、B、C，份额分别为1000、2000、4000，预留值分别为700MHz、0MHz、0MHz。当三个虚拟机满CPU负载运行时，每台虚拟机应分配到资源计算如下： 虚拟机A按照份额分配本应得400MHz，由于其预留值大于400MHz，最终计算能力按照预留值700MHz算，剩余的2100MHz资源按照2000:4000也就是1:2的比例在B和C之间进行划分，因此虚拟机B得到700MHz计算资源，虚拟机C得到1400MHz计算资源。 这里有一点需要注意：CPU的份额和预留只在多个虚拟机竞争物理CPU资源时发生，如果没有竞争发生，有需求的虚拟机可以独占物理CPU的资源。 而CPU的优先级调度技术主要指的的服务器虚拟化后资源的重分配机制。从上述虚拟化的结构可以看出，虚拟机和VMM共同构成虚拟机系统vCPU资源的两极调度框架。如下图所示，是一个多核环境下的虚拟机vCPU资源的两级调度框架。 虚拟机操作系统负责第2级调度，即线程或进程在vCPU上的调度（将核心线程映射到相应的vCPU 上）。VMM负责第1级调度，即vCPU在物理处理单元上的调度。两级调度的策略和机制不存在依赖关系。 vCPU调度器负责物理处理器资源在各个虚拟机之间的分配与调度，本质上把各个虚拟机中的vCPU按照一定的策略和机制调度在物理处理单元上，可以采用任意的策略（如上面资源管理方案）来分配物理资源，满足虚拟机的不同需求。vCPU可以调度在一个或多个物理处理单元执行（分时复用或空间复用物理处理单元），也可以与物理处理单元建立一对一绑定关系（限制访问指定的物理理单元）。 NUMA架构感知的调度技术除了基础的两级调度技术外，还有基于NUMA架构的精细化调度技术。在《DPDK技术栈在电信云中的最佳实践（一）》一文中，我们介绍过服务器的NUMA架构，其产生的主要原因就是解决服务器SMP架构扩展性能的问题。同样，在计算虚拟化中的CPU资源调度方面，也有基于服务器NUMA架构的调度技术，这就是Host NUMA和Guest NUMA技术，它们都是虚拟化软件技术。 Host NUMA主要提供CPU负载均衡机制，解决CPU资源分配不平衡引起的VM性能瓶颈问题，当启动VM时，Host NUMA根据当时主机内存和CPU负载，选择一个负载较轻的node放置该VM，使VM的CPU和内存资源分配在同一个node上。 如上图左边所示，Host NUMA把VM的物理内存放置在一个node上，对VM的vCPU调度范围限制在同一个node的物理CPU上，并将VM的vCPU亲和性绑定在该node的物理CPU上。考虑到VM的CPU负载是动态变化，在初始放置的node上，node的CPU资源负载也会随之变化，这会导致某个node的CPU资源不足，而另一个node的CPU资源充足，在此情况下，Host NUMA会从CPU资源不足的node上选择VM，把VM的CPU资源分配在CPU资源充足的node上，从而动态实现node间的CPU负载均衡。 Host NUMA保证VM访问本地物理内存，减少了内存访问延迟，可以提升VM性能，性能提升的幅度与VM虚拟机访问内存大小和频率相关。对于VM的vCPU个数超过node中CPU的核数时，如上图右边所示，Host NUMA把该VM的vCPU和内存均匀地放置在每个node 上，vCPU的调度范围为所有node的CPU。 如果用户绑定了VM的vCPU亲和性，Host NUMA特性根据用户的vCPU亲和性设置决定VM的放置，若绑定在一个node的CPU上，Host NUMA把VM的内存和CPU放置在一个node上，若绑定在多个node的CPU上，Host NUMA把VM的内存均匀分布在多个node 上，VM的vCPU在多个node的CPU上均衡调度。 Host NUMA技术的本质保证了VM访问本地物理内存，减少了内存访问延迟，可以提升VM性能，性能提升的幅度与VM访问内存大小和频率相关。Host NUMA主要应用于针对大规格、高性能虚拟机场景，适用Oracle、 SQL Server等关键应用。 Guest NUMA如上图所示，能够使得虚拟机内部程序运行时针对NUMA结构进行优化，CPU会优先使用同一个Node上的内存，从而减小内存访问延时、提高访问效率，以此达到提升应用性能的目的。目前OS和应用都会有针对NUMA的优化，VMM通过向虚拟机呈现NUMA结构，使Guest OS及其内部应用识别Numa结构， CPU会优先使用同一个Node上的内存，减小内存访问延时、提高访问效率。 Guest NUMA的本质就是VMM保证NUMNA结构的透传，使虚拟机中的关键应用在NUMA方面的优化生效，减少了内存访问延迟，可以提升VM性能。Guest NUMA主要应用于虚拟机中应用程序减小内存访问延时、提高访问效率，以此达到提升应用性能的目的。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-11-计算虚拟化概述]]></title>
    <url>%2F2019%2F05%2F11%2F2019-05-11-%E8%AE%A1%E7%AE%97%E8%99%9A%E6%8B%9F%E5%8C%96%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[所谓计算虚拟化，从狭义角度可理解为对单个物理服务器的虚拟化，主要包括对服务器上的CPU、内存、I/O设备进行虚拟化，目的就是实现多个虚拟机能各自独立、相互隔离地运行于一个服务器之上。从广义角度还可延伸到云资源池下，各类资源池组网场景下的CPU、内存、I/O设备等资源进行整合、抽象和虚拟化。 服务器虚拟化平台概念回顾在上一篇文章《虚拟化基础》中，我们介绍虚拟化基础的一些基本概念，这里我们按照服务器平台虚拟化后的一个分层结构来简单回顾下。如下： 一个完整的服务器虚拟化平台从下到上包括以下几个部分： 底层物理资源：包括网卡、CPU、内存、存储设备等硬件资源，一般将包含物理资源的物理机称为 宿主机（Host）。 虚拟机监控器（Virtual Machine Monitor，VMM）：VMM是位于虚拟机与底层硬件设备之间的虚拟层，直接运行于硬件设备之上，负责对硬件资源进行抽象，为上层虚拟机提供运行环境所需资源，并使每个虚拟机都能够互不干扰、相互独立地运行于同一个系统中。 抽象化的虚拟机硬件：即虚拟层呈现的虚拟化的硬件设备。虚拟机能够发现哪种硬件设施，完全由VMM决定。虚拟设备可以是模拟的真实设备，也可以是现实中并不存在的虚拟设备，如VMware的vmxnet网卡。 虚拟机：相对于底层提物理机，也称为客户机（Guest）。运行在其上的操作系统则称为客户机操作系统（Guest OS）。每个虚拟机操作系统都拥有自己的虚拟硬件，并在一个独立的虚拟环境中执行。通过VMM的隔离机制，每个虚拟机都认为自己作为一个独立的系统在运行。 同时，在上一篇文章《虚拟化基础》中，我们提到过Hypervisor就是VMM。其实，这个说法并不准确，至少在VMware的虚拟化解决方案中不准确，在VMware的ESX产品架构中，VMM和Hypervisor还是有一定区别的，如下图所示。 Hypervisor是位于虚拟机和底层物理硬件之间的虚拟 层，包括boot loader、x86 平台硬件的抽象层，以及内存与CPU调度器，负责对运行在其上的多个虚拟机进行资源调度。而VMM则是与上层的虚机 一一对应 的进程，负责对指令集、内存、中断与基本的I/O设备进行虚拟化。当运行一个虚拟机时，Hypervisor中的vmkernel会装载VMM，虚拟机直接运行于VMM之上，并通过VMM的接口与Hypervisor进行通信。而在KVM和Xen架构中，虚拟层都称为Hypervisor，也就是**VMM=Hypervisor**。 判断一个VMM能否有效确保服务器系统实现虚拟化功能，必须具备以下三个基本特征： 等价性（Equivalence Property）：一个 运行于VMM控制 之下的程序（虚拟机），除了时序和资源可用性可能不一致外， 其行为应该与相同条件下运行在物理服务器上的行为一致。 资源可控 性（Resource Control Property）：VMM必须能够完全控制虚拟化的资源。 效率性（Efficiency Property）：除了特权指令，绝大部分机器指令都可以直接由硬件执行，而无需VMM干涉控制。 上述三个基本特征也是服务器虚拟化实现方案的指导思想。 x86平台虚拟化面临的问题与挑战基于x86的操作系统在一开始就被设计为能够直接运行在裸机硬件环境之上，所以自然拥有整个机器硬件的控制权限。为确保操作系统能够安全地操作底层硬件，x86平台使用了特权模式和用户模式的概念对内核程序与用户应用程序进行隔离。 在这个模型下，CPU提供了4个特权级别，分别是Ring0、1、2和3。如下图所示： Ring 0是最高特权级别，拥有对内存和硬件的直接访问控制权。Ring 1、2和3权限依次降低， 无法执行操作讷河系统级别的指令集合。相应的，运行于Ring 0的指令称为“特权指令”；运行于其他级别的称为“非特权指令”。常见的操作系统如Linux与Windows都运行于Ring 0，而用户级应用程序运行于Ring 3。如果低特权级别的程序执行了特权指令，会引起“ 陷入”（Trap）内核态，并抛出一个异常。 当这种分层隔离机制应用于虚拟化平台 ，为了满足 VMM的“资源可控” 特征，VMM必须处于Ring 0级别控制所有的硬件资源，并且执行最高特权系统调用。而虚拟机操作系统Guest OS则要被降级运行在Ring 1级别，故Guest OS在执行特权指令时都会引起”陷入“。如果VMM能够正常捕获异常，模拟Guest OS发出的指令并执行，就达到了目的。这就是IBM的Power系列所采用的特权解除和陷入模拟的机制，支持这种特性的指令集合通常被认为是“ 可虚拟化的”。 但是。。。但是。。。但是。。。x86平台的指令集是不虚拟化的。为什么这么说？首先我们来看下x86平台指令集分类，x86平台的指令集大致分为以下4类： 访问或修改机器状态的指令。 访问或修改敏感寄存器或存储单元的指令， 比如访问时钟寄存器和中断寄存器。 访问存储保护系统或内存、地址分配系统的指令（段页之类）。 所有I/O指令。 其中，1~4在x86平台都属于敏感指令，第1、4类指令属于敏感指令中的特权指令，由操作系统内核执行，Guest OS在执行两类指令时，因为不处于Ring 0级别，所以会陷入，并抛出异常，这个异常会被VMM捕获，然后模拟Gust OS去执行，并将执行结果返回给Guest OS。到此为止，一切都OK。但是，第2、3类指令属于非特权指令，可以由应用程序调用，也就是可以在Ring 3级别执行，并调用Guest OS内核进程来完成。当应用程序调用这些指令时，由于要修改内存和内部寄存器，这些状态修改需要由Guest OS完成，而Guse OS此时运行在Ring 1级别，虽然也会发生陷入，但是不会抛出异常，这样VMM就捕获不到，也就无法模拟完成。因此，当Guest OS执行这些指令就会导致虚拟机状态异常，甚至影响服务器的状态。在x86平台下，这类指令共有19个，我自己称之为x86平台敏感指令中的边界指令。 就是因为x86平台指令集有上述缺陷，所以为了计算虚拟化技术在x86平台应用，各大虚拟化厂商推出了五花八门的虚拟化技术，其目的都是围绕“如何捕获模拟这19条边界指令”这一命题来设计。在很长一段时间，都是通过软件的方式来解决这个问题，其中包括无需修改内核的全虚拟化与需要修改内核的半虚拟化。尽管半虚拟化要求修改Guest OS内核的方式在一定程度上并不满足“ 等价性”要求，但是在性能上却明显优于全虚拟化。直到2005年Intel与AMD公司分别推出了VT-d与AMD-V，能够在芯片级别支持全虚拟化时，虚拟化技术才得到彻底完善，这就是现在称之为的硬件辅助虚拟化技术。 x86平台计算虚拟化解决方案全虚拟化全虚拟化（Full Virtualization）与半虚拟化（Para- Virtualization）的划分，是相对于是否修改Guest OS而言的。如下图所示，全虚拟化通过一层能够完整模拟物理硬件环境的虚拟软件，使得Guest OS与底层物理硬件彻底解耦。因此，Guest OS无需任何修改，虚拟化的环境对其完全透明，也就是说在全虚方案中，虚拟机感知不到自己处于虚拟化环境中，认为自己一直运行在物理硬件上。如下图所示： 在实现上，通常是结合特权指令的二进制翻译机制与一般指令的直接执行的方式。具体来说， 对于Guest OS发出的特权指令和边界指令，VMM会进行实时翻译，并缓存结果（目的是提高虚拟化性能），而对于一般级别的指令，则无需VMM干涉，可以直接在硬件上执行。异常-捕获-模拟的过程如下图所示： 由于虚拟化环境对Guest OS是完全透明的，全虚拟化模式对于虚拟机的迁移以及可移植性是最佳解决方案，虚拟机可以无缝地从虚拟环境迁移到物理环境中。但是，软件模拟实现的全虚拟化无疑会增加VMM的上下文切换，因为这种方案实现的虚拟机性能不如半虚拟化方案。 VMware的ESX系列产品 和Workstations系列产品是全虚拟化技术的典型产品。 半虚拟化如前所述，x86平台上一直存在一些Ring 3级别可以执行的边界指令，尽管全虚拟化模式通过实时译 这些特殊指令解决了这一问题，但是实现开销较大，性能并不如在实际物理机上运行。为了改善能，半虚拟化技术应运而生， “Para-Virtualization” 可理解为通过某种辅助的方式实现虚拟化。半虚拟化的解决方案如下图所示。 半虚拟化在Guest OS和虚拟层之间增加了一个特殊指令的过渡模块，通过修改Guest OS内核，将执行特权指令和边界指令替换为对虚拟层进行hypercall的调用方式来达到目的。同时，虚拟层也对内存管理、中断处理、时间同步提供了hypercall的调用接口。Hypercall调用过程如下图所示： 通过这种方式，虚拟机运行的性能得以显著提升。但是，对于某些无法修改内核的操作系统，比如：Windows，则不能使其运行于半虚拟化环境中。而且，由于需要修改Guest OS内核，无法保证虚拟机在物理环境与虚拟环境之间的透明切换。开源项目Xen和华为6.3版本之前的虚拟化解决方案Fusion Compute就是通过修改Linux内核以及提供I/O虚拟化操作的Domain 0的特殊虚拟机，使得运行于虚拟化环境上的虚拟机性能可以接近运行于物理环境的性能，属于半虚拟化技术方案的典型产品。但是，随着业务规模的增大，特殊虚机Domain 0是这种解决方案扩展性和性能方面的瓶颈。 硬件辅助虚拟化所谓“解铃还须系铃人”，针对敏感指令引发的一系列虚拟化问题，处理器硬件厂商最终给出了自己的解决方案。2005年Intel与AMD公司都效法IBM大型机虚拟化技术分别推出VT-x和AMD-V技术。如下图所示： 第一代VT-x与AMD-V都试图通过定义新的运行模式，使Guest OS恢复到Ring 0，而让VMM运行在比 Ring 0低的级别（可以理解为Ring -1）。比如： Intel公司的VT-x解决方案中，运行于非根模式下的Guest OS可以像在非虚拟化平台下一样运行于Ring 0级别，无论是Ring 0发出的特权指令还是Ring 3发出的敏感指令都会被陷入到根模式的虚拟层。VT-x解决方案具体如下图所示： VT-x与AMD-V推出之后，完美解决解决x86平台虚拟化的缺陷，且提升了性能，所以各个虚拟化厂商均快速开发出对应的产品版本，用于支持这种技术。比如：KVM-x86、Xen 3.0与VMware ESX 3.0之后的虚拟化产品。随后Intel和AMD在第二代硬件辅助虚拟化技术中均推出了针对I/O的硬件辅助虚拟化技术VT-d和IOMMU。 总结：x86平台下的三种虚拟化技术，都是围绕x86在虚拟化上的一些缺陷产生的。下图对三种虚拟 化技术进行了比较。 从图中可以看出，全虚拟化与半虚拟化的Guest OS的特权级别都被压缩在Ring 1中，而硬件虚拟化则将Guest OS恢复到了Ring 0级别。 在半虚拟化中，Guest OS的内核经过修改，所有敏感指令和特权指令都以Hypercall的方式进行调用，而在全虚拟化与硬件虚拟化中，则无需对Guest OS 进行修改。全虚拟化中对于特权指令和敏感指令采用了动态二进制翻译的方式，而硬件虚拟化由于在芯片中增加了根模式的支持，并修改 了敏感指令的语义，所有特权指令与敏感指令都能够自动陷入到根模式的VMM中。 利用二进制翻译的全虚拟化 硬件辅助虚拟化 操作系统协助/半虚拟化 实现技术 BT和直接执行 遇到特权指令转到root模式执行 Hypercall 客户操作系统修改/兼容性 无需修改客户操作系统，最佳兼容性 无需修改客户操作系统，最佳兼容性 客户操作系统需要修改来支持hypercall，因此它不能运行在物理硬件本身或其他的hypervisor上，兼容性差，不支持Windows 性能 差 全虚拟化下，CPU需要在两种模式之间切换，带来性能开销；但是，其性能在逐渐逼近半虚拟化。 好。半虚拟化下CPU性能开销几乎为0，虚机的性能接近于物理机。 应用厂商 VMware Workstation/QEMU/Virtual PC VMware ESXi/Microsoft Hyper-V/Xen 3.0/KVM Xen]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-03-虚拟化技术基础]]></title>
    <url>%2F2019%2F05%2F03%2F2019-05-03-%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[DPDK技术奠定了NFV领域数据包转发性能提升的基础，那么软硬件解耦后，在通用服务器上实现各功能网元，资源层面的隔离和共生问题就需要虚拟化技术来解决。虚拟化使用软件的方法重新定义划分IT资源，可以实现IT资源的动态分配、灵活调度、跨域共享，提高IT资源利用率，使IT资源能够真正成为社会基础设施，服务于各行各业中灵活多变的应用需求。 什么是虚拟化坦白地说，虚拟化就是欺骗。随着个人计算机的普及，“虚拟化”这个广泛使用的术语已经脱离了其技术本身，成为一种共同语言、流行文化和理念。自20世纪90年代互联网热潮的早期，任何与Web相关的活动均被称为“虚拟”。通过菲利浦•狄克的科幻小说、让•鲍德里亚的后现代主义研究，以及电影（如《黑客帝国》和《盗梦空间》）的影响，模拟现实的概念已经深入人心。 在技术领域，虚拟化是指利用“欺骗”技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。 传统构架是在每台物理机器上仅能拥有一个操作系统，而且多数情况下仅有一个负载。很难在服务器上运行多个主应用程序，否则可能会产生冲突和性能问题（上世纪90年代前，服务器一直面临的问题）。要解决上述冲突，最佳做法是每个服务器仅运行一个应用程序以避免这些问题。但是，这么做的结果是大部分时间资源利用率很低，且成本很高。因此，必须在加大投资和降低风险间寻找平衡。但是，随着业务的增长，随之而来的成本压力也变化，相关管理效率也会变低，需消耗的资源也会变大。 企业实施虚拟化战略的核心目的就是提高IT部门作为业务支撑部门的工作效率，达到节约成本与提高效率并重的目的。虚拟化的重要使命之一就是提高管理效率，从而降低成本、提高硬件使用率，把管理变得更加轻松。虚拟化的主攻方向集中在减少实体服务器的部署数量，并将实体机器上的操作系统及应用程序，无缝转移至虚拟机器上，以便集中管理这些不同平台的虚拟环境。 传统构架下，APP:OS:Phy = 1:1:1。这样子就造成资源利用率低，为不造成资源浪费，会增加APP部署。进而产生的影响就是不同应用之间的资源抢占，隔离性差。而OS主要提供应用运行的环境，在资源调度方面相对薄弱，不能完全有效解决以上问题。 为在不造成冲突的前提下提高资源利用率，最好是在一个OS上部署一个APP，于是就出现了虚拟化的技术的雏形。在一台主机上部署多个虚拟客户机并安装OS，每个OS安装一个APP，这样就解决了问题，达到的效果是APP:OS:Phy = n:n:1。 虚拟化之后实现了上层操作系统与下层硬件的解耦，就是说操作系统不再依赖物理的硬件，而是在VMM层上建立OS，由VMM层来实现OS对硬件的需求。 虚拟化的几个重要概念 宿主机（Host Machine）：指物理机资源，被Hypervisor用来执行一个或多个虚拟机器的电脑称为主机。 客户机（Guest Machine）：指虚拟机资源，在Hypervisor之上运行多个虚拟机器则称为客户机。 Guest OS和Host OS：如果将一个物理机虚拟成多个虚拟机，则称物理机为Host Machine，运行在其上的OS为Host OS；多个虚拟机称为Guest Machine，运行在其上的OS为Guest OS。 Hypervisor：通过虚拟化层的模拟，虚拟机在上层软件看来就是一个真实的机器，这个虚拟化层一般称为虚拟机监控机（Virtual Machine Monitor，VMM）。需要注意一点：在VMware的ESX虚拟化架构中VMM只是Hypervisor中一个进程，因此在这种场景下VMM不等于Hypervisor。 什么是Hypervisor（VMM） 维基百科的定义如下：Hypervisor，又称虚拟机器监视器（英语：virtual machine monitor，缩写为 VMM），是用来建立与执行虚拟机器的软件、固件或硬件。 通俗的讲：hypervisor是一种运行在物理服务器和操作系统之间的中间层软件，可以允许多个操作系统和应用共享一套基础物理硬件。可以将hypervisor看做是虚拟环境中的“元”操作系统，可以协调访问服务器上的所有物理设备和虚拟机，所以又称为虚拟机监视器（virtual machine monitor）。 hypervisor是所有虚拟化技术的核心，非中断的支持多工作负载迁移是hypervisor的基本功能。当服务器启动并执行hypervisor时，会给每一台虚拟机分配适量的内存，cpu，网络和磁盘资源，并且加载所有虚拟机的客户操作系统。当前主流的Hypervisor有微软的Hyper-V，VMware、Xen和KVM，但在电信云NFV领域主要用到的就是KVM，在后续虚拟化技术分类中，会专门讲解KVM的相关部署和优化，同时为加深大家对KVM的理解，也会讲 一点Xen的知识，毕竟在KVM广泛应用之前，云架构底层的虚拟化技术都是Xen。 虚拟化和hypervisor到底什么关系？ 虚拟化就是通过某种方式隐藏底层物理硬件的过程，从而让多个操作系统可以透明地使用和共享它。这种架构的另一个更常见的名称是平台虚拟化。在典型的分层架构中，提供平台虚拟化的层称为 hypervisor （有时称为虚拟机管理程序 或 VMM）。来宾操作系统称为虚拟机（VM），因为对这些 VM 而言，硬件是专门针对它们虚拟化的。如下图示： 在上图中可以看到，hypervisor 是提供底层机器虚拟化的软件层（在某些情况下需要处理器支持）。并不是所有虚拟化解决方案都是一样的（详见Hypervisor分类部分）。客户机操作系统（GuestOS）对机器的底层资源的访问通过VMM来实现。hypervisor 面对的对象不是客户机中的进程，而是整个客户操作系统（GuestOS）。 hypervisor主要可以划分为两大类：类型1和类型2，以及在此基础上衍生出来的混合类型和操作系统类型。 类型1这种hypervisor是可以直接运行在物理硬件之上的。如下图所示，也就是说它不需要宿主机操作系统（HostOS）的支持，本身就可以管理底层硬件的资源，其本质是在Hypervisor中嵌入了一个精简的Linux操作系统内核。Xen 和 VMWare 的 ESXi 都属于这个类型，这种虚拟化类型的模型如下图所示，其特点就是需要硬件的支持，转发性能强（因为少了HostOS这一层转发），VMM就是HostOS。 类型2这种hypervisor运行在另一个操作系统（运行在物理硬件之上）中。如下图所示，也就是说这种类型的Hypervisor是部署在HostOS之上的，从HostOS角度来看，其上层的所有VM都对应Hypervisor这一个进程。从VM的角度来看，其访问底层硬件资源需要Hypervisor和HostOS共同配合完成。KVM、VirtualBox 和 VMWare Workstation 都属于这个类型。这种虚拟化类型的模型如下图所示，其特点就是比较灵活，比如支持虚拟机嵌套（嵌套意味着可以在虚拟机中再运行hypervisor），但是转发性能明显不如类型1。 目前，随着转发性能提升需求和应用的微服务化架构需求，在类型2的基础上又演进出混合类型虚拟化和基于HostOS的操作系统虚拟化。 混合虚拟化：通过在主机的操作系统中增加虚拟硬件管理模块，通过虚拟硬件管理模块来生成各个虚拟机。属于类型2虚拟化的一种增强模型。特点是相对于类型2虚拟化，没有冗余，性能高，可支持多种操作系统。但是，需要底层硬件支持虚拟化扩展功能。现阶段的KVM和Hyper-V都属于这种增强型类型2虚拟化技术。 操作系统虚拟化：没有独立的hypervisor层。相反，主机操作系统本身就负责在多个虚拟服务器之间分配硬件资源，并且让这些服务器彼此独立。最重要的前提是：如果使用操作系统层虚拟化，所有虚拟服务器必须运行同一操作系统(不过每个实例有各自的应用程序和用户账户)，其本质就是操作系统上面应用程序的一个进程，主要在应用的微服务化架构场景中使用。特点是：简单、易于实现，管理成本非常低。但是，隔离性差，多个虚拟化实例共享同一个操作系统。最典型就是目前炙手可热的容器技术Docker和Virtuozzo。 虚拟化的特征和优势从前面描述可知，虚拟化技术就是一个“大块的资源”逻辑分割成“具有独立功能的小块资源”，这个功能通过Hypervisor来实现。对于服务器领域而言，通过Hypervisor将一个物理服务器虚拟化成若干个小的逻辑服务器，每个逻辑服务器具有与物理服务器相同的功能，所有逻辑服务器的资源总和等于物理服务器的全部资源。 因此，运行在Hypervisor上的逻辑服务器，其本质就是由物理服务器上一个个文件组成。相对物理服务器，天生具备分区、隔离、封装和相对硬件独立四大特征。 1）分区：在单一物理服务器同时运行多个虚拟机。分区意味着虚拟化层为多个虚拟机划分服务器资源的能力；每个虚拟机可以同时运行一个单独的操作系统（相同或不同的操作系统），使您能够在一台服务器上运行多个应用程序。每个操作系统只能看到虚拟化层为其提供的“虚拟硬件”（虚拟网卡、CPU、内存等），以使它认为运行在自己的专用服务器上。 2）隔离：在同一服务器虚拟机之间相互隔离。虚拟机是互相隔离的。例如：一个虚拟机的崩溃或故障（例如，操作系统故障、应用程序崩溃、驱动程序故障，等等）不会影响同一服务器上的其它虚拟机；一个虚拟机中的病毒、蠕虫等与其它虚拟机相隔离，就像每个虚拟机都位于单独的物理机器上一样。可以通过资源控制以提供性能隔离，比如：可以为每个虚拟机指定最小和最大资源使用量，以确保某个虚拟机不会占用所有的资源而使得同一系统中的其它虚拟机无资源可用；可以在单一机器上同时运行多个负载/应用程序/操作系统，而不会出现因为传统 x86 服务器体系结构的局限性发生DLL冲突等问题。 3）封装：整个虚拟机都保存在文件中，可以通过移动文件的方式来迁移虚拟机。封装意味着将整个虚拟机（硬件配置、BIOS 配置、内存状态、磁盘状态、CPU 状态）储存在独立于物理硬件的一小组文件中。这样，只需复制几个文件就可以随时随地根据需要复制、保存和移动虚拟机。 4）相对硬件独立：无需修改即可在任意服务器上运行（主要基于全虚技术的虚拟机，半虚技术的虚拟机只支持开源操作系统，如Linux）。因为虚拟机运行于虚拟化层之上，所以只能看到虚拟化层提供的虚拟硬件，无需关注物理服务器的情况。这样，虚拟机就可以在任何 x86 服务器（IBM、Dell、HP等）上运行而无需进行任何修改。这打破了操作系统和硬件以及应用程序和操作系统/硬件之间的约束。 物理资源在经过Hypervisor虚拟化后，在资源利用率、独立性、运行效率和安全性等方面与传统物理服务器相比均有不同的优势。 1）资源利用率：虚拟化前每台主机一个操作系统，系统的资源利用率低。虚拟化后，主机与操作系统不一一对应，按需分配使用，系统的资源利用率高。 2）独立性：虚拟化前软硬件紧密结合，硬件成本高昂且不够灵活。虚拟化后，操作系统和硬件不相互依赖，虚拟机独立于硬件，能在任何硬件上运行。 3）程序运行效率：虚拟化前同一台主机上同时运行多个程序容易产生冲突，运行效率较低。虚拟化后，操作系统和应用程序被封装成单一个体，不同个体间不冲突。同一台机器上运行多个程序，效率高。 4）安全性：虚拟化前，故障影响范围大，安全性较差。虚拟化后，通过资源的池化，有强大的安全和故障隔离机制。 虚拟化技术的发展最近几年，随着云计算技术广泛应用，虚拟化技术也被大家所关注。其实，虚拟化技术的出现要早于云计算技术约半个世纪。在上世纪60年代，虚拟化技术就已经在大型机上有所应用，在1999年小型机上出现逻辑分区的概念，这就是存储虚拟化的雏形。而到了2000年，在x86平台上VMware首先提出了平台虚拟化技术的概念，以及后续随着CPU速度越来越快，Intel和AMD分别在CPU指令架构中引入虚拟化指令，在服务器领域和数据中心范围内虚拟化技术得到的极大发展，从而催生了云计算技术的出现。可以说，虚拟化技术是云计算技术得以实现并推广落地的重要基石，同时，随着云计算技术的演进，虚拟化技术也同样在不断演进，从最早的计算虚拟化发展到目前的应用虚拟化，两者是一种相辅相成，螺旋式推进的关系。 云计算技术从诞生到当前，共经历了3个阶段，分别称为云计算1.0、云计算2.0和云计算3.0，在不同的阶段，虚拟化技术的表现形式和关注点也不相同，两者关系如下图所示： 在云计算1.0时代，主要是将传统IT硬件基础设施转换为虚拟化基础设施，来提升资源利用率。该阶段的关键特征体现为：通过计算虚拟化技术的引入，将企业IT应用与底层的基础设施彻底分离解耦，将多个企业IT应用实例及运行环境(客户机操作系统，GuestOS)复用在相同的物理服务器上，并通过虚拟化集群调度软件，将更多的IT应用复用在更少的服务器节点上，从而实现资源利用效率的提升。 在云计算2.0时代，主要是向云租户提供池化资源服务和精细化自动管理，推动企业业务的云化演进。该阶段的关键特征体现为：不仅通过计算虚拟化完成CPU、内存、裸金属服务器等池化资源的集中管理和自动调度，同时引入存储虚拟化和网络虚拟化技术，实现数据中心内部存储资源和网络资源的池化集中管理和统一调度。面向内部和外部的租户，将原本需要通过数据中心管理员人工干预的基础设施资源复杂低效的申请、释放与配置过程，转变为一键式全自动化资源发放服务过程。这个阶段大幅提升了企业基础设施资源的快速敏捷发放能力，缩短了基础设施资源准备周期，实现资源的按需弹性供给。为企业核心业务走向敏捷，更好地应对瞬息万变的竞争与发展奠定了基础。云计算2.0阶段面向云租户的基础设施资源服务供给，可以是虚拟机形式，可以是容器(轻量化虚拟机)，也可以是物理机形式。该阶段的企业云化演进，暂时还不涉及基础设施层之上的IT应用与中间件、数据库软件架构的变化。 在云计算3.0时代，面向应用开发者及管理维护者提供分布式微服务化应用架构和大数据智能化服务。 该阶段的关键特征体现为：企业IT应用架构逐步开始去IOE化，依托开源增强、跨不同业务应用领域高度共享的数据库、中间件平台服务层以及功能更加轻量化解耦、数据与应用逻辑彻底分离的分布式无状态化架构，从而实现支撑企业业务敏捷化、智能化以及资源利用效率提升。 数据中心内部虚拟化技术分类目前，在数据中心内虚拟服务器、虚拟网络、虚拟存储、虚拟设备和其他“虚拟技术”等已对传统基础设施产生了逆袭。在上述云计算的三个阶段，使得虚拟化技术和云计算技术得到极大发展的关键就是2.0时代，主要的特征就是从计算虚拟化走向存储虚拟化和网络虚拟化。 从支撑云计算按需、弹性分配资源，与硬件解耦的虚拟化技术的角度来看，云计算早期阶段主要聚焦在计算虚拟化领域。事实上，计算虚拟化技术早在IBM 370时代就已经在其大型机操作系统上诞生。技术原理是通过在OS与裸机硬件之间插入虚拟化层，来在裸机硬件指令系统之上仿真模拟出多个370大型机的“运行环境”，使得上层“误认为”自己运行在一个独占系统之上，实际上是由计算虚拟化引擎在多个虚拟机之间进行CPU分时调度，同时对内存、I/O、网络等访问也进行访问屏蔽。 后来，当x86平台演进成为在IT领域硬件平台的主流之后，VMware ESX、XEN、KVM等依托于单机OS的计算虚拟化技术才将IBM 370的虚拟化机制在x86服务器的硬件体系架构下实现，并且在单机/单服务器虚拟化的基础上引入了具备虚拟机动态迁移和HA调度能力的中小集群管理软件，比如：VMware的vCenter/vSphere、Citrix的XEN Center和华为的FusionSphere等，从而形成当前的计算虚拟化主体。 与此同时，作为数据信息持久化载体的存储已经逐步从服务器计算中剥离出来，与必不可少的CPU计算能力一样，在数据中心发挥着至关重要的作用。现在数据中心内部不再封闭，内部服务器互访和对外部互联网访问需求，使得存储和网络也同计算一样，成为数据中心IT基础设施不可或缺的“三大要素”。就数据中心端到端基础设施解决方案而言，不仅需要计算资源的按需分配、弹性伸缩、与硬件解耦的需求，对存储资源和网络资源需求同样如此，因此，存储虚拟化和网络虚拟化技术应运而生。 对于普通x86服务器来说，CPU和内存资源虚拟化后再将其以vCPU/vMemory的方式，按需供给用户/租户使用。计算虚拟化中仅存在资源池的“大分小”的问题。然而对于存储来说，由于硬盘的容量有限，客户/租户对数据容量的需求越来越大，因此必须对数据中心内多个分布式服务器存储资源，比如：服务器内的存储资源、外置SAN/NAS等进行“小聚大”的整合，组成存储资源池。 这个存储资源池，可能是单一厂家提供的同构资源池，也可以是被存储虚拟化层整合成为跨多厂家异构的统一资源池。各种存储资源池均能以统一的块存储、对象存储或者文件存储格式进行访问。数据存储虚拟化示意图如下所示： 对于数据中心网络来说，网络对于业务应用，作为连接服务器节点的计算和存储资源是一种实实在在的资源需求。传统数据中心内部，网络交换功能都是在物理交换机和路由器设备上完成的，网络功能对上层业务应用而言仅仅体现为一个一个被通信链路连接起来的孤立的“盒子”，无法动态感知来自上层业务的网络功能需求，完全需要人工配置的方式来实现对业务层网络组网与安全隔离策略的需要。 在云时代多租户虚拟化的环境下，不同租户对于边缘的路由及网关设备的配置管理需求也存在极大的差异化，即使物理路由器和防火墙自身的多实例能力也无法满足云环境下租户数量的要求，如果采用与租户数量等量的路由器与防火墙设备，成本上无法接受。于是，伯克利大学的Nick Mckeown教授提出将网络自身的功能从专用封闭平台迁移到服务器通用x86平台上来，SDN概念从此诞生。 网络资源虚拟化后，服务器节点的应用VM连接差异化，就可由云操作系统来自动化地创建和销毁，并通过一次性建立起来的物理网络连接矩阵，进行任意两个网络端节点之间的虚拟通讯链路建立，以及必要的安全隔离保障，从而实现业务驱动的网络自动化管理配置，大幅度降低数据中心网络管理的复杂度。从资源利用率来看，任意两个虚拟网络节点之间的流量带宽，都需要通过物理网络来交换和承载，只要不超过物理网络的资源配额上限（一般建议物理网络按照无阻塞的CLOS模式来设计实施)，一旦虚拟节点被释放，其所对应的网络带宽占用也将被同步释放，因此也就相当于实现对物理网络资源的最大限度的“网络资源动态共享”。通俗点讲，网络虚拟化让多个盒子式的网络实体第一次以一个统一整合的“网络资源池”的形态，出现在业务应用层面前，同时与计算和存储资源之间，也有了统一协同机制。网络虚拟化示意图如下图所示： 上面基础设施虚拟化技术的“三要素”是电信云领域需要重点关注的三个分类，属于云计算中IaaS服务部分的内容。 除此之外，还有基于PaaS和SaaS的桌面虚拟化技术，这部分内容因电信云领域目前不涉及，因此在本站的云计算分类中会有相关介绍，这里不再赘述。后续，会在NFV关键技术分类中按照计算虚拟化、存储虚拟化和网络虚拟化三大部分逐一介绍，并会重点KVM的部署和性能调优。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-02-Linux系统命令-第七篇《磁盘和文件系统管理命令》]]></title>
    <url>%2F2019%2F05%2F02%2F2019-05-02-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%B8%83%E7%AF%87%E3%80%8A%E7%A3%81%E7%9B%98%E5%92%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[fdisk：磁盘分区工具fdisk是Linux下常用的磁盘分区工具。受mbr分区表的限制，fdisk工具只能给小于2TB的磁盘划分分区。如果使用fdisk对大于2TB的磁盘进行分区，虽然可以分区，但其仅识别2TB的空间，所以磁盘容量若超过2TB，就要使用parted分区工具（后面会讲）进行分区。 语法格式：fdisk [option] [device] 重要参数选项 【使用示例】 1）显示系统磁盘分区列表 1234567891011[root@C7-Server01 ~]# fdisk -lDisk /dev/sda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d2190Device Boot Start End Blocks Id System/dev/sda1 * 2048 821247 409600 83 Linux/dev/sda2 821248 17598463 8388608 82 Linux swap / Solaris/dev/sda3 17598464 104857599 43629568 83 Linux 上述信息每列功能说明具体如下： Device：分区，这里有三个分区； Boot：启动分区，用*表示的是启动分区； Start：表示开始的柱面； End：表示结束的柱面； Blocks：block块数量； Id：分区类型Id； System：分区类型 2）模拟添加第二块磁盘 #给C7 Server01再挂载一块20G的磁盘 12345678910111213141516171819202122232425262728293031# 重启系统检查磁盘分区状态[root@C7-Server01 ~]# fdisk -l# 新添加的磁盘为/devsdb，表示sata接口的第二块磁盘Disk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/sda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d2190 Device Boot Start End Blocks Id System/dev/sda1 * 2048 821247 409600 83 Linux/dev/sda2 821248 17598463 8388608 82 Linux swap / Solaris/dev/sda3 17598464 104857599 43629568 83 Linux# 如果不想显示其他分区，还可以指定分区查看[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 3）交互式分区实战 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 查看当前系统的分区设备信息[root@C7-Server01 ~]# ls -l /dev/sd*brw-rw---- 1 root disk 8, 0 May 2 09:41 /dev/sdabrw-rw---- 1 root disk 8, 1 May 2 09:41 /dev/sda1brw-rw---- 1 root disk 8, 2 May 2 09:41 /dev/sda2brw-rw---- 1 root disk 8, 3 May 2 09:41 /dev/sda3brw-rw---- 1 root disk 8, 16 May 2 09:41 /dev/sdb# 对/dev/sdb进行交互式分区[root@C7-Server01 ~]# fdisk /dev/sdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0xc0a89c6e.Command (m for help): m # 输入m，打印帮助信息Command action a toggle a bootable flag # 设置引导扇区 b edit bsd disklabel # 编辑bsd卷标 c toggle the dos compatibility flag # 设置dos兼容分区 d delete a partition # 删除一个分区 g create a new empty GPT partition table # 创建一个新的且为空的GPT分区表 G create an IRIX (SGI) partition table # 创建一些IRIX分区表 l list known partition types # 查看分区类型对应的编号列表 m print this menu # 打印帮助菜单 n add a new partition # 新建一个分区 o create a new empty DOS partition table # 创建一个新的空的DOS分区表 p print the partition table # 打印分区表 q quit without saving changes # 退出且不保存更改 s create a new empty Sun disklabel # 创建一个新的空的sun卷标 t change a partition's system id # 更改分区系统的id u change display/entry units # 改变/显示条目的单位 v verify the partition table # 验证分区表 w write table to disk and exit # 将操作写入分区表并退出程序 x extra functionality (experts only) # 额外的功能Command (m for help): n # 输入n，创建一个分区Partition type: p primary (0 primary, 0 extended, 4 free) # 创建主分区，编号1-4 e extended # 创建扩展分区Select (default p): p # 输入p，创建主分区Partition number (1-4, default 1): 1 # 输入1，设置第一个主分区编号为1First sector (2048-41943039, default 2048): # 直接回车，默认采用2048作为起始柱面Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-41943039, default 41943039): +5G # 设置结束柱面，一般情况下如果整个磁盘采用一个分区，这里就直接回车就行，否则，采用+size的方式进行分区大小设置，我们这里给第一个分区设置5G的空间 Partition 1 of type Linux and of size 5 GiB is setCommand (m for help): p # 输入p，打印刚创建的分区信息Disk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xc0a89c6e Device Boot Start End Blocks Id System/dev/sdb1 2048 10487807 5242880 83 LinuxCommand (m for help): # 重复上述步骤再次创建三个主分区，大家自己练习，注意全部完成后要按w保存，否则分区信息丢失。。。# 打印最终的分区信息[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xc0a89c6e Device Boot Start End Blocks Id System/dev/sdb1 2048 10487807 5242880 83 Linux/dev/sdb2 10487808 20973567 5242880 83 Linux/dev/sdb3 20973568 31459327 5242880 83 Linux/dev/sdb4 31459328 41943039 5241856 83 Linux# 不重启情况下通知内核新的分区表已生效[root@C7-Server01 ~]# partprobe /dev/sdb 需要注意一点：用交互指令d删除分区时要小心，要注意分区的序号，如果删除了扩展分区，那么扩展分区之下的逻辑分区都会删除，所以操作时一定要小心。如果不小心操作错了，直接使用交互指令q不保存退出，这样先前的操作就会无效。如果输入w（保存指令）则会保存所有修改。 4）非交互式分区 上面的示例是交互式分区，有时需要在脚本中自动执行分区，这时需要非交互式分区。如果使用fdisk分区工具来完成，可以使用如下两种办法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 首先备份/dev/sdb分区表信息[root@C7-Server01 ~]# dd if=/dev/sdb of=sdb-partb.info bs=1 count=512512+0 records in512+0 records out512 bytes (512 B) copied, 0.00159899 s, 320 kB/s# 然后清除分区数据[root@C7-Server01 ~]# dd if=/dev/zero of=/dev/sdb bs=1 count=512512+0 records in512+0 records out512 bytes (512 B) copied, 0.0017926 s, 286 kB/s# 查看/dev/sdb分区信息，确认是否被清除[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes# 方法一：使用echo指令模拟交互式分区输入过程，自动执行分区# 此方法需要注意最后一次分区时不要输入+size大小，直接回车即可（虚拟机虚拟磁盘原因，在物理机上无此限制）# 脚本中p就是指主分区，如果要分扩展分区，将p改为m[root@C7-Server01 ~]# echo -e "n\np\n1\n\n+5G\nw\n" | fdisk /dev/sdb...[root@C7-Server01 ~]# echo -e "n\np\n2\n\n+5G\nw\n" | fdisk /dev/sdb...[root@C7-Server01 ~]# echo -e "n\np\n3\n\n+5G\nw\n" | fdisk /dev/sdb...[root@C7-Server01 ~]# echo -e "n\np\n4\n\n\nw\n" | fdisk /dev/sdb...# 查看分区信息[root@C7-Server01 ~]# fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xd0b6c715 Device Boot Start End Blocks Id System/dev/sdb1 2048 10487807 5242880 83 Linux/dev/sdb2 10487808 20973567 5242880 83 Linux/dev/sdb3 20973568 31459327 5242880 83 Linux/dev/sdb4 31459328 41943039 5241856 83 Linux 方法二：也是模拟交互式分区的过程，将输入的交互式指令写入一个文本文件，然后通过标准输入的方式传递给fdisk /dev/sdb指令，其中交互式指令中回车在文本中用换行替代。与上面的实现方式类似，请大家自行练习。 partprobe：更新内核的硬盘分区表信息partprobe命令用于在硬盘分区发生改变时，更新Linux内核中的硬盘分区表数据。有时在使用fdisk、part命令对硬盘进行分区后，会发现找不到新分区，此时需要重启系统才能使修改生效，但使用partprobe可以不重启系统就让修改的分区表生效。 语法格式：partprobe [option] 重要参数选项 【使用示例】 12345# 更新分区表信息# 最好加上具体的磁盘，否则可能会报错，那就只能重启系统[root@C7-Server01 ~]# partprobe /dev/sdb parted：磁盘分区工具对于小于2TB的磁盘可以用fdisk和parted命令进行分区，这种情况一般采用fdisk命令，但对于大于2TB的磁盘则只能用parted分区，且需要将磁盘转换为GPT格式。 语法格式：parted [option] [device] 重要参数选项 【分区命令】 通过parted -h或直接parted进入交互模式后，输入h查看帮助信息 123456789101112131415161718192021222324252627282930313233343536373839[root@C7-Server01 ~]# parted -hUsage: parted [OPTION]... [DEVICE [COMMAND [PARAMETERS]...]...]Apply COMMANDs with PARAMETERS to DEVICE. If no COMMAND(s) are given, run ininteractive mode.OPTIONs:-h, --help displays this help message-l, --list lists partition layout on all block devices-m, --machine displays machine parseable output-s, --script never prompts for user intervention-v, --version displays the version-a, --align=[none|cyl|min|opt] alignment for new partitionsCOMMANDs:align-check TYPE N check partition N for TYPE(min|opt)alignmenthelp [COMMAND] print general help, or help onCOMMANDmklabel,mktable LABEL-TYPE create a new disklabel (partitiontable)mkpart PART-TYPE [FS-TYPE] START END make a partitionname NUMBER NAME name partition NUMBER as NAMEprint [devices|free|list,all|NUMBER] display the partition table,available devices, free space, all found partitions, or a particularpartitionquit exit programrescue START END rescue a lost partition near STARTand ENDresizepart NUMBER END resize partition NUMBERrm NUMBER delete partition NUMBERselect DEVICE choose the device to editdisk_set FLAG STATE change the FLAG on selected devicedisk_toggle [FLAG] toggle the state of FLAG on selecteddeviceset NUMBER FLAG STATE change the FLAG on partition NUMBERtoggle [NUMBER [FLAG]] toggle the state of FLAG on partitionNUMBERunit UNIT set the default unit to UNITversion display the version number andcopyright information of GNU PartedReport bugs to bug-parted@gnu.org 【使用示例】 1）显示分区的情况 1234567891011121314151617181920[root@C7-Server01 ~]# parted -lModel: ATA VMware Virtual S (scsi)Disk /dev/sda: 53.7GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 1049kB 420MB 419MB primary xfs boot2 420MB 9010MB 8590MB primary linux-swap(v1)3 9010MB 53.7GB 44.7GB primary xfsModel: ATA VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 1049kB 5370MB 5369MB primary2 5370MB 10.7GB 5369MB primary3 10.7GB 16.1GB 5369MB primary4 16.1GB 21.5GB 5368MB primary 上述信息显示系统两块磁盘的分区信息，包括大小，起始，终止柱面，类型，文件系统类型等。磁盘/dev/sda为系统盘，有3个主分区，其中2个位xfs文件系统，1个位swap分区。磁盘/dev/sdb为刚添加的数据盘，有4个主分区，每个大小5G，因为还没有格式化，所以没有文件系统格式。 2）通过给虚拟机再挂载一块100G的磁盘/dev/sdc，来模拟2TB磁盘用parted分区工具进行分区 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 通过交互式方式完成分区[root@C7-Server01 ~]# parted /dev/sdcGNU Parted 3.1Using /dev/sdcWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) mklabel gpt # 为/dev/sdc磁盘创建GPT分区，大于2T的磁盘必须进行这一步(parted) mkpart primary 0 40G # 创建主分区，大小为40GWarning: The resulting partition is not properly aligned for best performance.Ignore/Cancel? Ignore # 输入Ignore，忽略告警信息(parted) p # 输入p，显示分区表信息 Model: ATA VMware Virtual S (scsi)Disk /dev/sdc: 107GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 17.4kB 40.0GB 40.0GB primary # 第一个主分区已经创建完毕(parted) mkpart logical 40G 50G # 创建第一个逻辑分区，大小为10G(parted) p # 打印分区表信息 Model: ATA VMware Virtual S (scsi)Disk /dev/sdc: 107GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 17.4kB 40.0GB 40.0GB primary 2 40.0GB 50.0GB 10.0GB logical # 第一个逻辑分区创建完毕(parted) mkpart logical 50G 70G # 创建第二个逻辑分区，大小为20G(parted) mkpart logical 70G 100G # 创建第三个逻辑分区，大小为30G (parted) p # 打印分区表信息 Model: ATA VMware Virtual S (scsi)Disk /dev/sdc: 107GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 17.4kB 40.0GB 40.0GB primary 2 40.0GB 50.0GB 10.0GB logical 3 50.0GB 70.0GB 20.0GB logical 4 70.0GB 100GB 30.0GB logical(parted) quit # 退出 Information: You may need to update /etc/fstab.# 查看系统设备信息[root@C7-Server01 ~]# ll -h /dev/sd*brw-rw---- 1 root disk 8, 0 May 2 11:51 /dev/sdabrw-rw---- 1 root disk 8, 1 May 2 11:51 /dev/sda1brw-rw---- 1 root disk 8, 2 May 2 11:51 /dev/sda2brw-rw---- 1 root disk 8, 3 May 2 11:51 /dev/sda3brw-rw---- 1 root disk 8, 16 May 2 11:51 /dev/sdbbrw-rw---- 1 root disk 8, 17 May 2 11:51 /dev/sdb1brw-rw---- 1 root disk 8, 18 May 2 11:51 /dev/sdb2brw-rw---- 1 root disk 8, 19 May 2 11:51 /dev/sdb3brw-rw---- 1 root disk 8, 20 May 2 11:51 /dev/sdb4brw-rw---- 1 root disk 8, 32 May 2 12:01 /dev/sdcbrw-rw---- 1 root disk 8, 33 May 2 12:01 /dev/sdc1brw-rw---- 1 root disk 8, 34 May 2 12:01 /dev/sdc2brw-rw---- 1 root disk 8, 35 May 2 12:01 /dev/sdc3brw-rw---- 1 root disk 8, 36 May 2 12:01 /dev/sdc4 用parted磁盘分区工具非交互式创建分区的方法类似fdisk，唯一区别就是将交互式下输入的命令作为参数传递parted工具，比如：将交互执行的命令直接放在parted /dev/sdb后面就实现非交互分区了。整体上实现其实比fdisk工具简单，请大家自行练习。 mkfs：创建Linux文件系统 （格式化）mkfs命令用于在指定的设备（或硬盘分区等）上格式化并创建文件系统，fdisk和parted等分区工具相当于建房的人，把房子（硬盘），分成几居室（分区），mkfs就相当于对不同的居室装修（创建文件系统）了，只有装修好的房子（有文件系统）才能入住，分区也是一样，只有格式化创建文件系统（存取数据的机制）后，才能用来存取数据。 mkfs只是一个前端命令，它通过-t参数指定文件系统类型后会调用相应的命令mkfs.fstype。因此，也可以直接使用mkfs.ext4、mkfs.xfs这类命令创建相应的文件系统。 语法格式：mkfs [option] [filesys] 重要参数选项 【使用示例】 通过-t选项创建xfs文件系统和ext4文件系统 123456789101112# 创建/dev/sdb1分区的文件系统为xfs[root@C7-Server01 ~]# mkfs -t xfs /dev/sdb1meta-data=/dev/sdb1 isize=512 agcount=4, agsize=327680 blks= sectsz=512 attr=2, projid32bit=1= crc=1 finobt=0, sparse=0data = bsize=4096 blocks=1310720, imaxpct=25= sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=2560, version=2= sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 确认是否创建成功 123456789101112# 使用mkfs.xfs创建/dev/sdb2的文件系统为xfs[root@C7-Server01 ~]# mkfs.xfs /dev/sdb2meta-data=/dev/sdb2 isize=512 agcount=4, agsize=327680 blks= sectsz=512 attr=2, projid32bit=1= crc=1 finobt=0, sparse=0data = bsize=4096 blocks=1310720, imaxpct=25= sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=2560, version=2= sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 再次确认 创建ext4文件系统的方法类似，请大家自行练习。 dumpe2fs：导出ext2/ext3/ext4文件系统信息dumpe2fs命令用于导出ext2/ext3/ext4文件系统内部的相关信息，例如：文件系统的组成包含超级快、块组、inode、block等信息。如果要导出xfs文件系统的信息，需要使用xfs_info指令。 语法格式：dumpe2fs [option] [device] 重要参数选项 【使用示例】 1）查看分区文件系统的inode信息 123456789101112131415161718192021222324252627282930# 创建/dev/sdb3分区为ext4文件系统格式[root@C7-Server01 ~]# mkfs.ext4 /dev/sdb3mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks327680 inodes, 1310720 blocks65536 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=134217728040 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done # 导出/dev/sdb3分区的中inode相关信息[root@C7-Server01 ~]# dumpe2fs /dev/sdb3 | egrep -i "inode size|inode count"dumpe2fs 1.42.9 (28-Dec-2013)Inode count: 327680Inode size: 256 2）在xfs文件系统下，使用xfs_info指令查看/dev/sdb1的inode信息和block信息 12345678910111213# 首先需要个/dev/sdb1分区设置一个挂载点，然后使用xfs_info + 挂载点的方式进行查看[root@C7-Server01 ~]# mount /dev/sdb1 ~/mydata/[root@C7-Server01 ~]# xfs_info ~/mydata/meta-data=/dev/sdb1 isize=512 agcount=4, agsize=327680 blks= sectsz=512 attr=2, projid32bit=1= crc=1 finobt=0 spinodes=0data = bsize=4096 blocks=1310720, imaxpct=25= sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal bsize=4096 blocks=2560, version=2= sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0 fsck：检查并修复Linux文件系统fsck命令用于检查并修复文件系统中的错误，即针对有问题的系统或磁盘进行修复，类似的命令还有e2fsck命令。有关fsck的使用需要特别注意的是：1）文件系统必须是卸载状态，否则可能会出现故障。2）不要对正常的分区使用fsck，在不加参数的情况下，fsck会根据/etc/fstab进行文件系统检查，这相当于fsck-As参数的功能。 注意：必须卸载文件系统后才能对其进行检查，否则可能会出现错误。平时没有必要使用这个命令检查磁盘，只有当系统开机显示磁盘错误时，才需要执行。 语法格式：fsck [option] [filesys] 重要选项参数 【使用示例】 1）系统开机通过fsck自检 Linux在开机过程中系统会自动调用fsck命令对需要自检的磁盘进行自检，如下图： 这是因为系统开机过程中会优先读取/etc/fstab文件，当最后一列设置为1或2时，这个磁盘在开机时就会调用fsck进行自检，fstab的文件（man fstab看帮助）信息如下： 123456789101112131415161718[root@C7-Server01 ~]# cat /etc/fstab ## /etc/fstab# Created by anaconda on Sun Apr 7 20:32:53 2019## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=0887567f-1df6-425f-ba3d-ce58584279e0 / xfs defaults 0 0UUID=26954b3f-dd29-4a25-85ba-471cdbbf82df /boot xfs defaults 0 0UUID=1f9ad06d-860c-4166-b142-6b633ee82851 swap swap defaults 0 0 在CentOS6系统中，系统分区的根分区最后一列一般是1，boot分区最后一列是2，其余是0。但是在CentOS7系统中，为了不影响系统启动，把系统分区最后1列均设为0，即开机不自检。 需要提醒一下：有时我们自己增加硬盘规划分区，一般最后一列都设置为0，即开机过程中不对磁盘检查，否则，一旦自定义挂载的磁盘有问题，会影响系统启动。 如果真有问题，可以在启动系统后人为进行检查。 2）Linux断电后重启故障修复 当Linux系统遭遇突然断电等非正常关机操作时，很容易导致文件系统数据损坏，造成系统不能重新启动，此时，屏幕出现的提示可能是如下内容： 12345* AN error occurred during the file system check*** xxx*** xxxGive root password for maintenance(or type Control-D to continue): 此时根据系统提示输入root用户的密码，注意而不是直接按Control-D继续，会再重启。 当输入正确的密码之后，正常会出现下面的提示： 1(Repair filesystem) 1 # 此时就可以输入fsck或者fsck-A对磁盘进行修复检查，执行后可能出现一堆询问，按yes即可。 12(Repair filesystem) 1 # fsck -A #&lt;==可能会等待一段时间或fsck。(Repair filesystem) 2 # #&lt;==修复完毕会返回到这个提示符，此时就可以试着重启系统看故障是否修复了。 除了按照开机的提示进行修复外，也可以利用系统盘进入救援模式或单用户模式对系统故障进行修复。千万不要在开机正常工作的情况下执行fsck来检查磁盘，因为这样有可能会导致正常的磁盘发生故障。 mount：挂载文件系统mount命令可以将指定的文件系统挂载到指定目录（挂载点），在Linux系统下必须先挂载所有的设备，然后才能被访问，挂载其实就是为要访问的设置开个门（开门才能访问）。挂载的目录必须事先存在且最好为空，如果目录不为空，那么挂载设备后会掩盖以前的目录内容，但原目录下的内容不会受损，所以，如果卸载了相应的设备，那么此前的目录内容又可以访问了。 语法格式：mount [option] [device] [dir] 重要选项参数 其中，-o选项后接的挂载参数如下： 【使用示例】 1）显示系统已挂载的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 不加参数或加-l选项[root@C7-Server01 ~]# mountsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)devtmpfs on /dev type devtmpfs (rw,nosuid,size=3984384k,nr_inodes=996096,mode=755)securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)tmpfs on /run type tmpfs (rw,nosuid,nodev,mode=755)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)configfs on /sys/kernel/config type configfs (rw,relatime)/dev/sda3 on / type xfs (rw,relatime,attr2,inode64,noquota)debugfs on /sys/kernel/debug type debugfs (rw,relatime)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)systemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=32,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=24337)mqueue on /dev/mqueue type mqueue (rw,relatime)/dev/sda1 on /boot type xfs (rw,relatime,attr2,inode64,noquota)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=799032k,mode=700)/dev/sdb1 on /root/mydata type xfs (rw,relatime,attr2,inode64,noquota)[root@C7-Server01 ~]# mount -lsysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)devtmpfs on /dev type devtmpfs (rw,nosuid,size=3984384k,nr_inodes=996096,mode=755)securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)tmpfs on /run type tmpfs (rw,nosuid,nodev,mode=755)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)configfs on /sys/kernel/config type configfs (rw,relatime)/dev/sda3 on / type xfs (rw,relatime,attr2,inode64,noquota)debugfs on /sys/kernel/debug type debugfs (rw,relatime)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)systemd-1 on /proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=32,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=24337)mqueue on /dev/mqueue type mqueue (rw,relatime)/dev/sda1 on /boot type xfs (rw,relatime,attr2,inode64,noquota)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=799032k,mode=700)/dev/sdb1 on /root/mydata type xfs (rw,relatime,attr2,inode64,noquota) 2）对系统的光驱进行挂载 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 不使用-t选项指定类型为 iso9660，但mount命令可以自动识别[root@C7-Server01 ~]# mount /dev/cdrom /mnt# 提示为只读挂载mount: /dev/sr0 is write-protected, mounting read-only# 查看/dev/cdrom文件，发现设备cdrom是sr0的一个软链接[root@C7-Server01 ~]# ll -h /dev/cdromlrwxrwxrwx 1 root root 3 May 2 18:10 /dev/cdrom -&gt; sr0# 查看是否挂载[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata/dev/sr0 4.3G 4.3G 0 100% /mnt# 查看挂载点内容[root@C7-Server01 ~]# ll -h /mnttotal 686K-rw-rw-r-- 1 root root 14 Nov 26 00:01 CentOS_BuildTagdrwxr-xr-x 3 root root 2.0K Nov 26 00:20 EFI-rw-rw-r-- 1 root root 227 Aug 30 2017 EULA-rw-rw-r-- 1 root root 18K Dec 10 2015 GPLdrwxr-xr-x 3 root root 2.0K Nov 26 00:21 imagesdrwxr-xr-x 2 root root 2.0K Nov 26 00:20 isolinuxdrwxr-xr-x 2 root root 2.0K Nov 26 00:20 LiveOSdrwxrwxr-x 2 root root 648K Nov 26 07:52 Packagesdrwxrwxr-x 2 root root 4.0K Nov 26 07:53 repodata-rw-rw-r-- 1 root root 1.7K Dec 10 2015 RPM-GPG-KEY-CentOS-7-rw-rw-r-- 1 root root 1.7K Dec 10 2015 RPM-GPG-KEY-CentOS-Testing-7-r--r--r-- 1 root root 2.9K Nov 26 07:54 TRANS.TBL# 卸载挂载点[root@C7-Server01 ~]# umount /mnt/ 在实际中，我们经常会挂载NFS（网络文件系统），这就要用到mount命令的-o选项，来保证性能和安全性。具体的说明，请参见本站Linux常用工具分类中的NFS文件系统一文。 umount：卸载文件系统umount命令可以卸载已经挂载的文件系统，如上文中示例2的卸载挂载点。umount卸载可以接挂载点目录，也可以接设备文件。 语法格式：umount [option] [dir|device] 重要选项参数 【使用示例】 1）卸载光驱挂载 12345678910111213141516171819202122# 先挂载光驱[root@C7-Server01 ~]# mount -t iso9660 /dev/cdrom /mntmount: /dev/sr0 is write-protected, mounting read-only# 查看系统挂载信息[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata/dev/sr0 4.3G 4.3G 0 100% /mnt# 使用设备文件卸载[root@C7-Server01 ~]# umount /dev/cdrom 2）强制卸载 1234567891011121314151617181920212223242526# 挂载光驱，并进入挂载点[root@C7-Server01 ~]# mount /dev/cdrom /mnt &amp;&amp; cd /mntmount: /dev/sr0 is write-protected, mounting read-only[root@C7-Server01 mnt]# # 此时尝试卸载光驱，会提示设备忙，无法卸载[root@C7-Server01 mnt]# umount /mntumount: /mnt: target is busy. (In some cases useful info about processes that use the device is found by lsof(8) or fuser(1))# 使用-lf选项强制卸载[root@C7-Server01 mnt]# umount -lf /mnt[root@C7-Server01 mnt]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata sync：刷新文件系统缓冲区sync命令会将内存缓冲区内的数据强制刷新到磁盘。Linux内核为了达到最佳的磁盘操作效率，默认会先在内存中将需要写入到磁盘的数据缓存起来，然后等待合适的时机将它们真正写入到磁盘中，这在绝大多数情况下都是没有任何问题的，而且还提高了系统的效率，但是如果系统出现宕机、掉电等情况，就可能会导致有些文件内容没能保存下来。当然，在Linux系统正常关机或者重启时，会将缓冲区中的内容自动同步到磁盘中。我们也可以手工执行sync命令，将内存中的文件缓冲内容强制写到磁盘中。 但是通常情况下没有必要执行这个命令，一是Linux内核会尽快让内存中的数据自动同步到磁盘上去，二是我们也无法预计什么时候会宕机、掉电。 语法格式：sync [option] 【使用示例】 手动将数据从缓冲区刷到磁盘中并重启系统 ** 123# 写一个测试循环脚本来完成三次同步，每次间隔1秒，然后重启系统**[root@C7-Server01 mnt]# for i in `seq 3`;do sync;sleep 1; done &amp;&amp; reboot; dd：转换或复制文件dd命令具有复制文件、转换文件和格式化文本的功能。 语法格式：dd [option] 重要选项参数 【使用示例】 1）将/dev/sda1分区复制（备份）到文件中 1234567891011121314151617181920212223# 查看磁盘使用情况[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0# 备份分区表信息[root@C7-Server01 ~]# dd if=/dev/sda1 of=sda1-partb.info819200+0 records in819200+0 records out419430400 bytes (419 MB) copied, 5.68928 s, 73.7 MB/s# 查看输出文件的信息[root@C7-Server01 ~]# ll -h sda1-partb.info -rw-r--r-- 1 root root 400M May 2 19:48 sda1-partb.info 2）删除/dev/sdb1分区数据 12345678910111213141516171819202122232425262728# 挂载/dev/sdb1分区到root用户家目录下的mydata/目录[root@C7-Server01 ~]# mount /dev/sdb1 mydata/# 在mydata/目录下创建1000个文件[root@C7-Server01 ~]# touch mydata/file&#123;01..1000&#125;# 查看系统挂载信息，发现/dev/sdb1已使用33M，占比1%[root@C7-Server01 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 3.0G 39G 8% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0/dev/sdb1 5.0G 33M 5.0G 1% /root/mydata# /dev/zero设备读取数据，写入到/dev/sdb1中，就会清空/dev/sdb1分区的数据[root@C7-Server01 ~]# dd if=/dev/zero of=/dev/sdb1dd: writing to ‘/dev/sdb1’: No space left on device # 提示磁盘被写满10485761+0 records in10485760+0 records out5368709120 bytes (5.4 GB) copied, 73.561 s, 73.0 MB/s /dev/zero是0字符设备，可产生连续不断的特殊数据流，生成的文件为特殊格式的数据文件（二进制）。 3）生成任意大小的文件 12345678# 生成一个大小为10M的测试文件test01[root@C7-Server01 ~]# dd if=/dev/zero of=test01 bs=1M count=1010+0 records in10+0 records out10485760 bytes (10 MB) copied, 0.136475 s, 76.8 MB/s[root@C7-Server01 ~]# ll -h test01 -rw-r--r-- 1 root root 10M May 2 20:11 test01 生成文件test01的大小为bs*count=1M*10=10M。 4）生成CentOS7的镜像文件 在Windows系统里制作光盘的ISO镜像，还需要安装其他软件。但在Linux系统中只需要dd命令就足够了，可以使用dd命令，将从光驱读取的镜像复制到系统中，相当于光驱与磁盘对拷。使用此类防范可以不用ftp工具或lrzsz工具对镜像文件进行上传。 123456[root@C7-Server01 ~]# dd if=/dev/cdrom of=CentOS74.img8962048+0 records in8962048+0 records out4588568576 bytes (4.6 GB) copied, 30.8377 s, 149 MB/s[root@C7-Server01 ~]# ll -h CentOS74.img -rw-r--r-- 1 root root 4.3G May 2 20:16 CentOS74.img 这样我们就创建了一个用于KVM或OpenStack的母版镜像文件CentOS74.img。 5）使用dd复制文件，并转换大小写 123456789101112131415161718192021222324# 在当前目录下创建测试文件，内容随便编辑[root@C7-Server01 ~]# cat &gt; file01 &lt;&lt; EOFWWW.sn.Chinamobile.com我爱北京天安门！！！1234www.sina.com.CN###!www.openstack.orgEOF# 使用dd复制文件，并将原文件中所有大写字母转换为小写字母&gt; EOF&gt; [root@C7-Server01 ~]# dd if=file01 of=file01_new conv=lcase&gt; 0+1 records in&gt; 0+1 records out&gt; 96 bytes (96 B) copied, 0.000108961 s, 881 kB/s# 产看file01_new文件内容[root@C7-Server01 ~]# cat file01_newwww.sn.chinamobile.com我爱北京天安门！！！1234www.sina.com.cn###!www.openstack.org Linux磁盘与文件系统管理命令掌握上述命令即可，还有三个用于交换分区管理的命令mkswap（创建交换分区）、swapon（激活交换分区）和swapoff（关闭交换分区）很少会被使用到，大家知道即可。如果在实际运维中需要，到时再通过man查询帮助即可。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-01-Linux系统命令-第六篇《用户和权限管理命令》]]></title>
    <url>%2F2019%2F05%2F02%2F2019-05-01-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E5%85%AD%E7%AF%87%E3%80%8A%E7%94%A8%E6%88%B7%E5%92%8C%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[useradd：创建用户useradd命令可用于创建新的用户或者更改用户的信息。在使用useradd命令时，若不加任何参数选项，后面直接跟所添加的用户名，那么系统首先会读取/etc/login.defs（用户定义文件）和/etc/default/useradd（用户默认配置文件）文件中所定义的参数和规则，然后根据所设置的规则添加用户，同时还会向/etc/passwd（用户文件）和/etc/group（组文件）文件内添加新用户和新用户组记录，向/etc/shadow（用户密码文件）和/etc/gshadow（组密码文件）文件里添加新用户和组对应的密码信息的相关记录。最后，系统还会根据/etc/default/useradd文件所配置的信息建立用户的家目录，并将/etc/skel中的所有文件（包括隐藏的环境配置文件）都复制到新用户的家目录中。 当执行useradd带-D参数时，可以更改新建用户的默认配置值（/etc/default/useradd）或者由命令行编辑文件更改预设值。可简单理解该参数（-D）就是用于修改/etc/default/useradduseradd配置文件的内容的，若这个文件的内容被修改，则添加新用户不加参数时默认值就会从该/etc/default/useradd中读取。 语法格式：useradd [options] [login] 或 useradd -D [options] 重要选项参数 useradd不加选项-D的参数选项及说明 useradd加-D选项参数说明：改变新建用户的预设值 【使用示例】 1）不加任何参数添加用户的例子 123456789101112131415161718192021222324252627282930# 不带任何参数创建一个名为ett的用户[root@C7-Server01 ~]# useradd ett# 查看/home目录下，发现多个一个ett的目录[root@C7-Server01 ~]# ls -l /home/total 4drwx------ 2 ett ett 62 May 1 16:35 ettdrwx------ 7 kkutysllb kkutysllb 4096 Apr 28 00:00 kkutysllb# 查看/etc/passwd文件关于新建用户ett的记录[root@C7-Server01 ~]# cat /etc/passwd | grep -w ettett:x:1001:1001::/home/ett:/bin/bash # 这里设置根据文件/etc/login.defs里面预设内容设置# 查看/etc/shadow文件关于新建用户ett的记录[root@C7-Server01 ~]# cat /etc/shadow | grep -w ettett:!!:18017:0:99999:7::: # 虽然没有设置密码，但是还有相关一行记录# 查看/etc/group文件中关于新建ett的记录[root@C7-Server01 ~]# cat /etc/group | grep -w ettett:x:1001:# 查看/etc/gshadow文件中关于新建用户ett的记录[root@C7-Server01 ~]# cat /etc/gshadow | grep -w ettett:!:: 根据上面的结果，我们将会发现/etc/shadow、/etc/group和/etc/gshadow几个文件都存在与ett用户相关的记录。 2）useradd的-g、-u参数，执行useradd[参数]username添加用户 123456789# 首先创建一个用户组sa，并设置组id为801（创建用户组的命令groupadd后面会将）[root@C7-Server01 ~]# groupadd -g 801 sa# 创建用户user01，使其属于sa用户组，uid为901[root@C7-Server01 ~]# useradd -g sa -u 901 user01[root@C7-Server01 ~]# id user01uid=901(user01) gid=801(sa) groups=801(sa) 3）useradd的-M、-s参数的例子 123456789# 创建一个虚拟化vuser01，不自动创建其家目录，并禁止其登录# /sbin/nologin参数可以设置某个用户没有登录权限[root@C7-Server01 ~]# useradd -M -s /sbin/nologin vuser01[root@C7-Server01 ~]# ls -ld /home/vuser01ls: cannot access /home/vuser01: No such file or directory # 家目录不存在，-M选项作用[root@C7-Server01 ~]# cat /etc/passwd | grep -w vuser01vuser01:x:1002:1002::/home/vuser01:/sbin/nologin # 该用户没有登录权限，-s选项作用 4）useradd的-c、-u、-G、-s、-d、-m、-e、-f等多个参数组合的综合例子 123456789101112131415161718192021# 创建用户user02，并设置用户注释信息为“SysUser”# UID指定为806，同时归属root和sa用户组，登录shell使用/bin/sh# 设置其家目录为/tmp/user02，用户过期时限为2019-10-31，过期后2天停权[root@C7-Server01 ~]# useradd -u 801 -s /bin/sh -c SysUser -d /tmp/user02 -G root,sa -e "2019-10-31" -f 2 user02[root@C7-Server01 ~]# tail -1 /etc/passwd # 查看user02的家目录，注释信息，登录shell等信息user02:x:801:1003:SysUser:/tmp/user02:/bin/sh[root@C7-Server01 ~]# id user02 # 查看user02的uid和gid，所属用户组信息uid=801(user02) gid=1003(user02) groups=1003(user02),0(root),801(sa)[root@C7-Server01 ~]# chage -l user02 # 查看user02的账号期限信息Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Oct 31, 2019Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7[root@C7-Server01 ~]# tail -1 /etc/shadow # 查询用户超期停权时限user02:!!:18017:0:99999:7:2:18200: 5）useradd-D参数的使用说明及案例实践 使用useradd-D参数的结果实际上就是修改用户的初始配置文件/etc/default/useradd，我们首先看下用户初始配置文件内容，如下： 12345678910111213141516171819202122232425262728293031[root@C7-Server01 ~]# cat /etc/default/useradd # useradd defaults file# 依赖于/etc/login.defs的USERGROUP_ENAB参数，如果参数值为no，则组设置由此处控制GROUP=100 # 默认把用户家目录创建在/home/目录下HOME=/home # 是否启用用户过期停权，-1表示不使用INACTIVE=-1 # 用户终止日期设置，默认不设置EXPIRE= # 新建用户默认使用shell类型SHELL=/bin/bash # 配置新建用户家目录的默认文件存放路径。/etc/skell就是在这里配置生效的，即当我们使用useradd创建用户时，用户家目录下的文件（主要是隐藏文件）都是从这里配置的目录下复制过去的SKEL=/etc/skel # 是否创建mail文件CREATE_MAIL_SPOOL=yes 修改实践： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 良好习惯，一般修改系统配置文件，首先创建好备份[root@C7-Server01 ~]# cp /etc/default/useradd&#123;,.bak&#125;# 修改默认登录的shell为/bin/sh[root@C7-Server01 ~]# useradd -D -s /bin/sh# 比对修改前后的两个文件[root@C7-Server01 ~]# diff /etc/default/useradd&#123;,.bak&#125;# 表示前后两个文件第6行不同，新文件已经修改成为/bin/sh6c6## &lt; SHELL=/bin/sh&gt; SHELL=/bin/bash# 增加新建用户失效时限为2039-12-31[root@C7-Server01 ~]# useradd -D -e "2039-12-31"# 比对前后两个文件区别[root@C7-Server01 ~]# diff /etc/default/useradd&#123;,.bak&#125;5,6c5,6&lt; EXPIRE=2039-12-31## &lt; SHELL=/bin/sh&gt; EXPIRE=&gt; SHELL=/bin/bash&gt;&gt; # 新建一个用户user03&gt;&gt; [root@C7-Server01 ~]# useradd user03# 查看新建用户user03的配置信息，发现登陆后默认使用/bin/sh作为shell解释器[root@C7-Server01 ~]# tail -1 /etc/passwduser03:x:1003:1004::/home/user03:/bin/sh# 查看新建用户user03的账号期限，发现为2039年12月31日[root@C7-Server01 ~]# chage -l user03Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Dec 31, 2039Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7# 完成上述验证后，需要恢复现场[root@C7-Server01 ~]# cp /etc/default/useradd&#123;.bak,&#125;cp: overwrite ‘/etc/default/useradd’? y # 输入y确认 usermod：修改用户信息usermod命令用于修改系统已经存在的用户的账号信息。usermod的作用是修改用户，而useradd的作用是添加用户，本质上都是对用户进行操作，因此，参数作用大部分都是类似的，只不过命令不同，就是添加和修改的区别。 语法格式：usermod [options] [login] 重要参数选项 【使用示例】 usermod的-c、-u、-G、-s、-d、-m、-e、-f等多个参数组合的例子 12345678910111213141516171819202122232425262728293031323334353637# 修改上面例子创建的user02用户，注释信息修改为TmpUser# UID修改为1999，归属用户组修改为只归属sa用户组# shell类型修改为/bin/sh# 家目录修改为/home/user02# 用户过期时限修改为2020-01-01，过期后30天停权[root@C7-Server01 ~]# usermod -u 1999 -s /bin/sh -d /home/user02 -G sa -c TmpUser -e "2020-01-01" -f 30 user02# 查询用户登录shell和注释信息[root@C7-Server01 ~]# cat /etc/passwd | grep -w user02user02:x:1999:1003:TmpUser:/home/user02:/bin/sh# 查询用户uid ,gid和归属用户组信息[root@C7-Server01 ~]# id user02uid=1999(user02) gid=1003(user02) groups=1003(user02),801(sa)# 查询用户user02的过期时限[root@C7-Server01 ~]# chage -l user02Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Jan 01, 2020Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7# 查询用户停权的时限[root@C7-Server01 ~]# cat /etc/shadow | grep -e user02user02:!!:18017:0:99999:7:30:18262: userdel：删除用户userdel命令用于删除指定的用户及与该用户相关的文件。 语法格式：userdel [options] [login] 重要选项参数 【使用示例】 1）不加参数删除用户 123456789101112131415# 查询刚才添加的虚拟用户vuser01的信息[root@C7-Server01 ~]# grep -e vuser01 /etc/passwdvuser01:x:1002:1002::/home/vuser01:/sbin/nologin# 不带任何选项参数删除虚拟用户vuser01[root@C7-Server01 ~]# userdel vuser01# 再次查看vuser01在系统配置文件/etc/passwd中的信息，结果什么也没有输出# 为显示效果，在后面追加wc -l命令用于统计屏幕输出行数信息，结果为0[root@C7-Server01 ~]# grep -e vuser01 /etc/passwd | wc -l0 2） 加-r参数删除用户及家目录 12345678910# 在不加任何选项删除用户时，用户信息虽然从/etc/passwd文件删除，但是创建的家目录仍然存在# 加-r选项删除用户user01，可以同步删除用户家目录信息[root@C7-Server01 ~]# userdel -r user01[root@C7-Server01 ~]# grep -w user01 /etc/passwd | wc -l0[root@C7-Server01 ~]# grep -w user01 /home/user01 | wc -lgrep: /home/user01: No such file or directory0 3）强制删除用户示例（此命令请谨慎使用） 我们复制一个终端，在宿主机使用kkutysllb用户登录，如下： 12345678910111213141516171819# 我们在老的终端下，使用root用户删除kkutysllb用户# 使用who命令查询当前登录的用户[root@C7-Server01 ~]# whoroot pts/0 2019-05-01 16:35 (192.168.101.1)kkutysllb pts/1 2019-05-01 18:17 (192.168.101.1)# 因为本机/home/kkutysllb/目录下有私有配置文件，所以本次演示不使用-r选项# 使用-f选项强制删除已登录用户kkutysllb[root@C7-Server01 ~]# userdel -f kkutysllbuserdel: user kkutysllb is currently used by process 10633# 虽然上面提示用户正在登录，但其实用户信息已被删除，当退出后查询/etc/passwd文件已经没有kkutysllb用户的配置信息[root@C7-Server01 ~]# grep -w kkutysllb /etc/passwd | wc -l0 groupadd：创建新的用户组groupadd命令用于创建新的用户组。但groupadd命令的用途一般不大，因为useradd命令在创建用户的同时还会创建与用户同名的用户组。 语法格式：groupadd [options] [group] 重要参数选项 groupadd命令用于创建新的用户组。但groupadd命令的用途一般不大，因为useradd命令在创建用户的同时还会创建与用户同名的用户组。 语法格式：groupadd [options] [group] 重要参数选项 【使用示例】 指定gid添加用户组的例子 12345678910111213# 使用-g选项指定新增用户组的gid为1024[root@C7-Server01 ~]# groupadd -g 1024 kkutysllb# 查看新增用户组在/etc/group配置文件的信息[root@C7-Server01 ~]# tail -1 /etc/groupkkutysllb:x:1024:# 虽然没有指定组密码，但是在系统配置/etc/gshadow文件中仍有默认配置[root@C7-Server01 ~]# tail -1 /etc/gshadowkkutysllb:!:: groupadd的命令在工作场景中的应用绝大多数情况下仅限于此，一般掌握上述用法即可。 groupdel：删除用户组 groupdel命令用于删除指定的用户组，此命令的使用频率极低，了解基本用法即可。需要注意一点：groupdel不能删除还有用户归属的主用户组。 语法格式：groupdel [group] 【使用示例】 删除用户组的例子 12345678910111213# 尝试删除root用户组，会提示失败，因为root用户还存在[root@C7-Server01 ~]# groupdel rootgroupdel: cannot remove the primary group of user 'root'# 删除sa用户组[root@C7-Server01 ~]# groupdel sa# 查询系统配置文件sa用户组的信息[root@C7-Server01 ~]# grep -w sa /etc/group | wc -l0 passwd：修改用户密码passwd命令可以修改用户密码及密码过期时间等内容，是实际中很常用的命令。普通用户和超级用户都可以运行passwd命令，但普通用户只能更改自身的用户密码，超级用户root则可以设置或修改所有用户的密码。root用户修改密码时，如果不符合系统密码规则，则给出警告信息，但密码设置仍然生效。普通用户修改密码时，如果使用弱密码，则给出告警信息，且修改无效。 语法格式：passwd [option] [username] 重要选项参数 【使用示例】 1）新增/修改用户的密码 123456789101112131415161718192021222324252627282930313233# 创建kkutysllb用户（刚才已删除）[root@C7-Server01 ~]# useradd -u 1024 -d /home/kkutysllb -g kkutysllb -c AdminUser -e "2199-12-31" -f 30 kkutysllb[root@C7-Server01 ~]# tail -1 /etc/passwdkkutysllb:x:1024:1024:AdminUser:/home/kkutysllb:/bin/bash# root用户使用passwd命令新增kkutysllb用户的密码[root@C7-Server01 ~]# passwd kkutysllbChanging password for user kkutysllb.New password: # 输入新密码Retype new password: # 确认新密码passwd: all authentication tokens updated successfully.# 使用su命令切换到kkutysllb用户下（从root用户切换到非root用户不用输入密码）[root@C7-Server01 ~]# su - kkutysllbLast login: Wed May 1 19:32:00 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ # 在kkutysllb用户下修改自身密码为123456，会提示三次密码过于简单，然后退出，设置失败[kkutysllb@C7-Server01 ~]$ passwdChanging password for user kkutysllb.Changing password for kkutysllb.(current) UNIX password: # 首先输入用户当前密码New password: # 然后输入新密码BAD PASSWORD: The password is shorter than 8 charactersNew password: BAD PASSWORD: The password is shorter than 8 charactersNew password: BAD PASSWORD: The password is shorter than 8 characterspasswd: Have exhausted maximum number of retries for service 2）显示账号密码 12345678910# 只能在root用户下使用，所以先切换回root用户[kkutysllb@C7-Server01 ~]$ su -Password: # 从非root用户切换回root用户要求输入密码Last login: Wed May 1 19:35:06 CST 2019 on pts/0# 使用-S选项查看kkutysllb用户密码简单信息[root@C7-Server01 ~]# passwd -S kkutysllbkkutysllb PS 2019-05-01 0 99999 7 30 (Password set, SHA512 crypt.) 3）使用一条命令设置密码 12345678910# 使用--stdin选项从标准输入中读取密码，并设置给用户user03[root@C7-Server01 ~]# echo "Adsds#@123"| passwd --stdin user03Changing password for user user03.passwd: all authentication tokens updated successfully.# 查看user03用户密码简单信息[root@C7-Server01 ~]# passwd -S user03user03 PS 2019-05-01 0 99999 7 -1 (Password set, SHA512 crypt.) 4）要求kkutysllb用户7天内不能更改密码，60天以后必须修改密码，过期前10天通知用户，过期后30天后禁止用户登录。 12345678[root@C7-Server01 ~]# passwd -n 7 -x 60 -w 10 -i 30 kkutysllbAdjusting aging data for user kkutysllb.passwd: Success# 查看kkutysllb密码简单信息[root@C7-Server01 ~]# passwd -S kkutysllbkkutysllb PS 2019-05-01 7 60 10 30 (Password set, SHA512 crypt.) chage：修改用户密码有效期 chage命令用于查看或修改用户密码的有效期，有些参数和passwd的功能相同。 语法格式：chage [option] [login] 重要选项参数 【使用示例】 1）要求kkutysllb用户7天内不能更改密码，60天以后必须修改密码，过期前10天通知用户，过期后30天后禁止用户登录。 1234567891011121314# 使用chage命令实现[root@C7-Server01 ~]# chage -m 7 -M 60 -W 10 -I 30 kkutysllb# 使用-l选项，查看kkutysllb账号密码期限信息[root@C7-Server01 ~]# chage -l kkutysllbLast password change : May 01, 2019Password expires : Jun 30, 2019Password inactive : Jul 30, 2019Account expires : Dec 31, 2199Minimum number of days between password change : 7Maximum number of days between password change : 60Number of days of warning before password expires : 10 2） 测试-E选项 1234567891011# 使用-E选项，将user03用户的账号有限期设置为2019-12-31[root@C7-Server01 ~]# chage -E "2019-12-31" user03[root@C7-Server01 ~]# chage -l user03Last password change : May 01, 2019Password expires : neverPassword inactive : neverAccount expires : Dec 31, 2019Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7 chpasswd：批量更新用户密码chpasswd命令用于从标准输入中读取一定格式的用户名、密码来批量更新用户的密码，其格式为“用户名：密码”。 语法格式：chpasswd [option] 重要参数选项 【使用示例】 1）命令行批量修改密码 1234567# 输入chapasswd后，按照格式 用户名：密码逐行修改密码，每行一个# 修改密码的用户必须存在，修改完成后按ctrl+d退出[root@C7-Server01 ~]# chpasswd user02:123456 # 修改user02的密码为123456user03:123456 # 修改user03的密码为123456 2）在不使用shell循环下，批量创建10个用户stu01-stu10，并且设置8位随机密码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 批量创建10个用户[root@C7-Server01 ~]# echo test&#123;01..10&#125;|xargs -n 1 useradd# 查看刚创建的10个用户[root@C7-Server01 ~]# tail /etc/passwdtest01:x:2000:2000::/home/test01:/bin/bashtest02:x:2001:2001::/home/test02:/bin/bashtest03:x:2002:2002::/home/test03:/bin/bashtest04:x:2003:2003::/home/test04:/bin/bashtest05:x:2004:2004::/home/test05:/bin/bashtest06:x:2005:2005::/home/test06:/bin/bashtest07:x:2006:2006::/home/test07:/bin/bashtest08:x:2007:2007::/home/test08:/bin/bashtest09:x:2008:2008::/home/test09:/bin/bashtest10:x:2009:2009::/home/test10:/bin/bash# 建立账号:密码格式文件[root@C7-Server01 ~]# echo test&#123;01..10&#125;:$((RANDOM+10000000))|tr " " "\n" &gt; userpass.txt# 检验刚创建用户名:密码格式文件[root@C7-Server01 ~]# cat userpass.txt test01:10013080test02:10008930test03:10025107test04:10009300test05:10012914test06:10029118test07:10018810test08:10024358test09:10003684test10:10008929# 使用chpasswd命令从格式文件中读取内容，进行批量设置[root@C7-Server01 ~]# chpasswd &lt; userpass.txt# 从root用户先切换到test01用户下（无需密码），再切换到test10用户下（需要输入密码）[root@C7-Server01 ~]# su - test01[test01@C7-Server01 ~]$ su - test10Password: # 输入testt10用户的密码[test10@C7-Server01 ~]$# 使用whoami指令检验[test10@C7-Server01 ~]$ whoami test10 su：切换用户su命令用于将当前用户切换到指定用户或者以指定用户的身份执行命令或程序。若省略了命令后面的用户名，则默认切换为root用户。）从root用户切换到普通用户时，不需要任何密码；从普通用户切换到root用户时，需要输入root密码。 语法格式：su [option] [user] 重要选项参数 【使用示例】 1）切换用户忘记-选项，导致环境变量未同步切换 1234567891011121314151617181920212223# 首先查询当前root用户下环境变量设置[root@C7-Server01 ~]# env | egrep "USER|MAIL|PWD|LOGNAME"USER=rootMAIL=/var/spool/mail/rootPWD=/rootLOGNAME=root# 先从root用户切换到kkutysllb用户，再切回，不带-选项[root@C7-Server01 ~]# su - kkutysllbLast login: Wed May 1 19:36:49 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ suPassword: [root@C7-Server01 kkutysllb]# # 再次查询当前root用户的环境变量，发现是kkutysllb用户的环境变量设置[root@C7-Server01 kkutysllb]# env | egrep "USER|MAIL|PWD|LOGNAME"USER=kkutysllbMAIL=/var/spool/mail/kkutysllbPWD=/home/kkutysllbLOGNAME=kkutysllb 上述示例告诉我们，在切换用户时要保持好的习惯，带-和用户名切换，确保环境变量与当前用户设置一致。 2）向shell传递单个命令示例 在后续我们不熟openstack的各项服务时，经常会进行数据库同步操作，完成此操作的语句如下： 1su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron 上述语句的意思就是：切换到neutron用户下，以/bin/sh作为shell解释器，执行neutron-db-manage –config-file /etc/neutron/neutron.conf –config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head命令完成neutron数据的库表创建工作。 因此，如果仅希望在某用户下执行命令，而不用直接切换到该用户下来操作，可以使用su - 用户名 -c”命令”的方式。 visudo：编辑sudoers文件visudo命令是专门用来编辑/etc/sudoers这个文件的，同时提供语法检查等功能。/etc/sudoers文件是sudo命令的配置文件。 语法格式：visudo [option] 重要选项参数 【使用示例】 1）执行visudo对普通用户kkutysllb授权 执行visudo命令，按照vim编辑器的使用方法在101行添加如下内容，使得kkutysllb用户拥有root用户的权限。 上述授权内容对应的说明如下： visudo 相当于直接执行vim /etc/sudoers编辑，但用命令方式更安全，推荐使用该命令。同时，通过sudo进行系统授权管理的目的：即能让运维人员干活，又不会威胁系统安全，还可以审计用户使用sudo的提权操作命令，默认的用户是无法获得root权限的。 2）检查sudoer文件语法 有的时候用户不是使用visudo（保存时会自动检查语法）编辑的sudoer文件，而是使用vim或者echo等命令编辑的sudoer文件，此时就需要执行如下命令来检查编辑文件的语法是否正确，如果语法不正确，则可能会导致授权无法生效的问题。 12345678910# 通过echo命令将kkutysllb用户组也授权为root权限[root@C7-Server01 kkutysllb]# echo "%kkutysllb ALL=(ALL) ALL" &gt;&gt; /etc/sudoers[root@C7-Server01 kkutysllb]# tail -1 /etc/sudoers%kkutysllb ALL=(ALL) ALL# 使用-c选项检查sudoer文件语法合规性[root@C7-Server01 kkutysllb]# visudo -c/etc/sudoers: parsed OK users：显示已登录用户users命令可以显示已经登录系统的用户。如果是同一用户登录多次，则登录几次就会显示几次用户名。 语法格式：users 【使用示例】 显示已登录用户，如果是一个用户登录多次，就显示几个同名用户 1234567[root@C7-Server01 kkutysllb]# usersroot # 再通过root用户登录系统2次[root@C7-Server01 kkutysllb]# usersroot root root whoami：显示当前登录的用户名 whoami命令用于显示当前登录的用户名，这个命令可以看作英文短句who am i的简写。 语法格式：whoami 【使用示例】 显示当前登录的用户名 123456[root@C7-Server01 kkutysllb]# whoami root[root@C7-Server01 kkutysllb]# su - kkutysllbLast login: Wed May 1 22:17:04 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ whoamikkutysllb last：显示用户登录列表 last命令能够从日志文件/var/log/wtmp读取信息并显示用户最近的登录列表。 语法格式：last [option] 重要选项参数 【使用示例】 1）显示用户最近登录的列表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495[root@C7-Server01 ~]# lastroot pts/2 192.168.101.1 Wed May 1 22:51 still logged in root pts/1 192.168.101.1 Wed May 1 22:51 still logged in kkutysll pts/1 192.168.101.1 Wed May 1 18:17 - 22:51 (04:33) root pts/1 192.168.101.1 Wed May 1 18:13 - 18:17 (00:04) root pts/0 192.168.101.1 Wed May 1 16:35 still logged in reboot system boot 3.10.0-957.10.1. Wed May 1 16:35 - 22:57 (06:22) root pts/0 192.168.101.1 Mon Apr 29 16:42 - 18:05 (01:23) reboot system boot 3.10.0-957.10.1. Mon Apr 29 16:41 - 22:57 (2+06:16) root pts/1 192.168.101.1 Sun Apr 28 19:14 - crash (21:27) root pts/0 192.168.101.1 Sun Apr 28 16:53 - crash (23:48) reboot system boot 3.10.0-957.10.1. Sun Apr 28 16:53 - 22:57 (3+06:04) root pts/0 192.168.101.1 Sat Apr 27 21:43 - down (04:27) reboot system boot 3.10.0-957.10.1. Sat Apr 27 21:43 - 02:11 (04:27) root pts/0 192.168.101.1 Sat Apr 27 21:42 - down (00:00) reboot system boot 3.10.0-957.10.1. Sat Apr 27 21:42 - 21:43 (00:01) root pts/0 192.168.101.1 Sat Apr 27 12:00 - down (00:56) reboot system boot 3.10.0-957.10.1. Sat Apr 27 12:00 - 12:57 (00:57) root pts/0 192.168.101.1 Sat Apr 27 11:13 - crash (00:46) reboot system boot 3.10.0-957.10.1. Sat Apr 27 10:51 - 12:57 (02:05) root pts/0 192.168.101.1 Fri Apr 26 17:24 - down (01:04) reboot system boot 3.10.0-957.10.1. Fri Apr 26 17:24 - 18:29 (01:04) root pts/1 192.168.101.1 Tue Apr 23 19:56 - down (01:39) root pts/0 192.168.101.1 Tue Apr 23 18:25 - down (03:10) reboot system boot 3.10.0-957.10.1. Tue Apr 23 18:24 - 21:36 (03:11) root pts/0 192.168.101.1 Tue Apr 23 12:51 - 13:14 (00:23) reboot system boot 3.10.0-957.10.1. Tue Apr 23 12:50 - 13:14 (00:23) root pts/0 192.168.101.1 Tue Apr 23 12:42 - down (00:06) reboot system boot 3.10.0-957.10.1. Tue Apr 23 12:41 - 12:48 (00:06) root pts/0 192.168.101.1 Tue Apr 23 10:29 - crash (02:12) reboot system boot 3.10.0-957.10.1. Tue Apr 23 10:28 - 12:48 (02:19) kkutysll tty1 Tue Apr 23 00:17 - 00:17 (00:00) root pts/0 192.168.101.1 Mon Apr 22 22:53 - crash (11:35) reboot system boot 3.10.0-957.10.1. Mon Apr 22 22:53 - 12:48 (13:55) root pts/0 192.168.101.1 Mon Apr 22 22:52 - down (00:00) reboot system boot 3.10.0-957.10.1. Mon Apr 22 22:52 - 22:53 (00:01) root pts/0 192.168.101.1 Mon Apr 22 22:38 - down (00:13) reboot system boot 3.10.0-957.10.1. Mon Apr 22 22:23 - 22:52 (00:28) reboot system boot 3.10.0-957.10.1. Sun Apr 21 16:27 - 22:52 (1+06:24) root pts/1 192.168.101.1 Sat Apr 20 18:22 - crash (22:05) root pts/0 192.168.101.1 Sat Apr 20 15:57 - crash (1+00:29) reboot system boot 3.10.0-862.el7.x Sat Apr 20 15:57 - 22:52 (2+06:54) root pts/1 192.168.101.1 Sat Apr 20 12:42 - down (03:14) root pts/1 192.168.101.1 Sat Apr 20 12:40 - 12:40 (00:00) root pts/1 192.168.101.1 Sat Apr 20 12:37 - 12:37 (00:00) root pts/0 192.168.101.1 Sat Apr 20 11:06 - down (04:50) reboot system boot 3.10.0-862.el7.x Sat Apr 20 11:06 - 15:57 (04:50) root pts/0 192.168.101.1 Fri Apr 19 17:13 - down (01:16) reboot system boot 3.10.0-862.el7.x Fri Apr 19 16:37 - 18:30 (01:52) root pts/1 192.168.101.1 Wed Apr 17 13:12 - crash (2+03:25) root pts/0 192.168.101.1 Wed Apr 17 10:41 - crash (2+05:55) reboot system boot 3.10.0-862.el7.x Wed Apr 17 10:38 - 18:30 (2+07:51) root pts/0 192.168.101.1 Tue Apr 16 23:04 - down (01:13) reboot system boot 3.10.0-862.el7.x Tue Apr 16 22:30 - 00:18 (01:48) root pts/1 192.168.101.1 Tue Apr 16 14:29 - down (04:09) root pts/0 192.168.101.1 Tue Apr 16 10:42 - 16:39 (05:57) reboot system boot 3.10.0-862.el7.x Tue Apr 16 10:41 - 18:38 (07:57) reboot system boot 3.10.0-862.el7.x Tue Apr 16 02:05 - 18:38 (16:32) root pts/0 192.168.101.1 Tue Apr 16 02:05 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 02:05 - 02:05 (00:00) root pts/0 192.168.101.1 Tue Apr 16 02:02 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 02:01 - 02:02 (00:00) root pts/0 192.168.101.1 Tue Apr 16 02:00 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 01:59 - 02:00 (00:00) root pts/0 192.168.101.1 Tue Apr 16 01:59 - down (00:00) reboot system boot 3.10.0-862.el7.x Tue Apr 16 01:59 - 01:59 (00:00) root pts/0 192.168.101.1 Tue Apr 16 01:49 - down (00:08) root pts/0 192.168.101.1 Tue Apr 16 01:45 - 01:49 (00:03) root pts/0 192.168.101.1 Tue Apr 16 01:34 - 01:45 (00:10) reboot system boot 3.10.0-862.el7.x Tue Apr 16 01:34 - 01:57 (00:23) root pts/0 192.168.101.1 Mon Apr 15 23:45 - down (00:41) reboot system boot 3.10.0-862.el7.x Mon Apr 15 23:44 - 00:27 (00:42) reboot system boot 3.10.0-862.el7.x Mon Apr 15 18:00 - 00:27 (06:26) root pts/0 192.168.101.1 Mon Apr 15 17:59 - crash (00:00) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:59 - 00:27 (06:27) root pts/0 192.168.101.1 Mon Apr 15 17:58 - crash (00:01) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:57 - 00:27 (06:29) root pts/0 192.168.101.1 Mon Apr 15 17:45 - down (00:09) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:45 - 17:55 (00:10) root pts/0 192.168.101.1 Mon Apr 15 17:44 - down (00:00) reboot system boot 3.10.0-862.el7.x Mon Apr 15 17:43 - 17:44 (00:01) root pts/0 192.168.101.1 Mon Apr 15 17:06 - crash (00:36) root pts/0 192.168.101.1 Mon Apr 15 16:45 - 17:06 (00:21) reboot system boot 3.10.0-862.el7.x Mon Apr 15 16:43 - 17:44 (01:00) root pts/0 192.168.101.1 Sat Apr 13 15:44 - 04:36 (12:51) reboot system boot 3.10.0-862.el7.x Sat Apr 13 15:44 - 11:54 (20:10) reboot system boot 3.10.0-862.el7.x Sat Apr 13 15:43 - 11:54 (20:11) reboot system boot 3.10.0-862.el7.x Sat Apr 13 15:42 - 15:42 (00:00) root pts/0 192.168.101.1 Tue Apr 9 17:58 - down (01:04) reboot system boot 3.10.0-862.el7.x Tue Apr 9 17:58 - 19:03 (01:04) root pts/0 192.168.101.1 Mon Apr 8 01:05 - down (00:27) root pts/0 192.168.101.1 Sun Apr 7 23:35 - 01:05 (01:29) reboot system boot 3.10.0-862.el7.x Sun Apr 7 23:34 - 01:32 (01:57) root pts/0 192.168.101.1 Sun Apr 7 20:43 - down (00:03) reboot system boot 3.10.0-862.el7.x Sun Apr 7 20:43 - 20:47 (00:04) 2）只显示前10行信息 1234567891011[root@C7-Server01 ~]# last -10root pts/2 192.168.101.1 Wed May 1 22:51 still logged in root pts/1 192.168.101.1 Wed May 1 22:51 still logged in kkutysll pts/1 192.168.101.1 Wed May 1 18:17 - 22:51 (04:33) root pts/1 192.168.101.1 Wed May 1 18:13 - 18:17 (00:04) root pts/0 192.168.101.1 Wed May 1 16:35 still logged in reboot system boot 3.10.0-957.10.1. Wed May 1 16:35 - 22:59 (06:24) root pts/0 192.168.101.1 Mon Apr 29 16:42 - 18:05 (01:23) reboot system boot 3.10.0-957.10.1. Mon Apr 29 16:41 - 22:59 (2+06:17) root pts/1 192.168.101.1 Sun Apr 28 19:14 - crash (21:27) root pts/0 192.168.101.1 Sun Apr 28 16:53 - crash (23:48) 3）显示指定用户登录情况 1234[root@C7-Server01 ~]# last kkutysllbkkutysll pts/1 192.168.101.1 Wed May 1 18:17 - 22:51 (04:33) kkutysll tty1 Tue Apr 23 00:17 - 00:17 (00:00) wtmp begins Sun Apr 7 20:43:09 2019 lastb：显示用户登录失败的记录lastb命令可以从日志文件/var/log/btmp中读取信息，并显示用户登录失败的记录，用于发现系统登录异常。 语法格式：lastb [option] 重要选项参数： 【使用示例】 显示用户登录失败的列表 12345[root@C7-Server01 ~]# lastbkkutysll ssh:notty 192.168.101.1 Wed May 1 18:21 - 18:21 (00:00) kkutysll ssh:notty 192.168.101.1 Wed May 1 18:20 - 18:20 (00:00) kkutysll ssh:notty 192.168.101.1 Wed May 1 18:20 - 18:20 (00:00) btmp begins Wed May 1 18:20:46 2019 需要多加注意这个命令执行的结果，如果发现未知的登录失败信息，那就要考虑系统是否遭到暴力破解登录。 lastlog：显示所有用户的最近登录记录lastlog命令从日志文件/var/log/lastlog中读取信息，并显示所有用户的最近登录记录，用于查看系统是否有异常登录。 语法格式：lastlog 【使用示例】 显示所有用户的最近登录记录 12345678910111213141516171819202122232425262728293031323334353637[root@C7-Server01 ~]# lastlog Username Port From Latestroot pts/0 Wed May 1 22:57:53 +0800 2019bin **Never logged in**daemon **Never logged in**adm **Never logged in**lp **Never logged in**sync **Never logged in**shutdown **Never logged in**halt **Never logged in**mail **Never logged in**operator **Never logged in**games **Never logged in**ftp **Never logged in**nobody **Never logged in**systemd-network **Never logged in**dbus **Never logged in**polkitd **Never logged in**tss **Never logged in**sshd **Never logged in**postfix **Never logged in**chrony **Never logged in**ntp **Never logged in**ett **Never logged in**user02 **Never logged in**user03 **Never logged in**kkutysllb pts/0 Wed May 1 22:54:01 +0800 2019test01 pts/0 Wed May 1 22:02:48 +0800 2019test02 **Never logged in**test03 **Never logged in**test04 **Never logged in**test05 **Never logged in**test06 **Never logged in**test07 **Never logged in**test08 **Never logged in**test09 **Never logged in**test10 pts/0 Wed May 1 22:03:22 +0800 2019 上面显示Never logged in的表示从未登录过系统的用户，一般都是系统服务的虚拟用户。 chown：改变文件或目录的用户和用户组chown命令用于改变文件或目录的用户和用户组。要授权的用户和组名，必须是Linux系统实际存在的。 常用格式： chown 用户 文件或目录 #&lt;==仅仅授权用户。 chown :组 文件或目录 #&lt;==仅仅授权组。 语法格式：chown [option] [OWNER][:[GROUP]] [file]，其中的“：”可以用“.”来代替 【重要选项参数】 【使用示例】 1）更改文件所属的用户属性 12345678910# 查询userpass.txt文件的用户和用户组[root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 root root 160 May 1 21:59 userpass.txt# 更改userpass.txt文件用户为test01[root@C7-Server01 ~]# chown test01 userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 test01 root 160 May 1 21:59 userpass.txt 2）更改文件所属的用户组 12345# 更改userpass.txt文件的所属用户组为kkutysllb[root@C7-Server01 ~]# chown .kkutysllb userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 test01 kkutysllb 160 May 1 21:59 userpass.txt 3 ）同时更改文件的用户和用户组 12345# 更改userpass.txt文件的用户为kkutysllb，用户组为root[root@C7-Server01 ~]# chown kkutysllb:root userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rw-r--r-- 1 kkutysllb root 160 May 1 21:59 userpass.txt 4）递归更改目录及目录下的所有子目录及文件的用户和用户组的属性 1234567891011# 查询/home/test目录的所属用户和用户组信息[root@C7-Server01 ~]# ls -ld /home/testdrwxr-xr-x 5 root root 45 May 1 23:16 /home/test# 使用-R选项，递归修改/home/test目录及其子目录的用户和用户组为kkutysllb:kkutysllb[root@C7-Server01 ~]# chown -R kkutysllb:kkutysllb /home/test[root@C7-Server01 ~]# ls -ld /home/test /home/test/stu01drwxr-xr-x 5 kkutysllb kkutysllb 45 May 1 23:16 /home/testdrwxr-xr-x 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu01 chmod：改变文件或目录权限chmod命令是用来改变文件或目录权限的命令，但是只有文件的属主和超级用户root才能够执行这个命令。模式有两种格式：一种是采用权限字母和操作符表达式；另一种是采用数字。权限示意图如下： 语法格式：chmod [option] [mode] [file] 重要选项参数 【使用示例】 1）权限字母和操作符表达式 1234567891011121314151617181920212223242526272829# 设置所有权限为空[root@C7-Server01 ~]# chmod a= userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---------- 1 kkutysllb root 160 May 1 21:59 userpass.txt# 设置文件属主有执行权限[root@C7-Server01 ~]# chmod u+x userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---x------ 1 kkutysllb root 160 May 1 21:59 userpass.txt# 设置文件归属用户组有可写权限[root@C7-Server01 ~]# chmod g+w userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---x-w---- 1 kkutysllb root 160 May 1 21:59 userpass.txt# 设置其他用户对文件有可读权限[root@C7-Server01 ~]# chmod o+r userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ---x-w-r-- 1 kkutysllb root 160 May 1 21:59 userpass.txt# 恢复刚才的设置，即恢复用户的默认权限[root@C7-Server01 ~]# chmod u=rwx,g=rx,o=x userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt -rwxr-x--x 1 kkutysllb root 160 May 1 21:59 userpass.txt 2）文件的数字权限授权 12345# 设置文件userpass.txt为归属用户组可读写执行，其他用户可读可执行[root@C7-Server01 ~]# chmod 075 userpass.txt [root@C7-Server01 ~]# ls -l userpass.txt ----rwxr-x 1 kkutysllb root 160 May 1 21:59 userpass.txt 3） 使用-R选项递归授权目录 12345678# 授权/home/test及其子目录所有用户可读，可写，可执行[root@C7-Server01 ~]# chmod -R 777 /home/test[root@C7-Server01 ~]# ls -ld /home/test /home/test/stu&#123;01..03&#125;drwxrwxrwx 5 kkutysllb kkutysllb 45 May 1 23:16 /home/testdrwxrwxrwx 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu01drwxrwxrwx 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu02drwxrwxrwx 3 kkutysllb kkutysllb 25 May 1 23:16 /home/test/stu03 Linux普通文件的读、写、执行权限说明 Linux目录的读、写、执行权限说明 chgrp：更改文件用户组chgrp命令只用于更改文件的用户组，功能被chown取代了，了解一下即可。 语法格式：chgrp [option] [group] [file] 重要选项参数 【使用示例】 1）修改文件的用户组属性信息 12345678910# 查看文件userpass.txt文件的用户组信息[root@C7-Server01 ~]# ll userpass.txt ----rwxr-x 1 kkutysllb root 160 May 1 21:59 userpass.txt# 修改文件用户组为kkutysllb[root@C7-Server01 ~]# chgrp kkutysllb userpass.txt [root@C7-Server01 ~]# ll userpass.txt ----rwxr-x 1 kkutysllb kkutysllb 160 May 1 21:59 userpass.txt 2）递归修改目录的用户组信息 1234567# 修改/home/test目录及子目录的用户组为root[root@C7-Server01 ~]# chgrp -R root /home/test[root@C7-Server01 ~]# ll -d /home/test/stu&#123;01..03&#125;drwxrwxrwx 3 kkutysllb root 25 May 1 23:16 /home/test/stu01drwxrwxrwx 3 kkutysllb root 25 May 1 23:16 /home/test/stu02drwxrwxrwx 3 kkutysllb root 25 May 1 23:16 /home/test/stu03 umask：显示或设置权限掩码umask是通过八进制的数值来定义用户创建文件或目录的默认权限。 语法格式：umask [option] [mode] 重要选项参数 通过umask计算文件目录权限 文件权限计算：创建文件默认最大的权限为666（-rw-rw-rw-），默认创建的文件没有可执行权限x位。对于文件来说，umask的设置是在假定文件拥有八进制666的权限上进行的，文件的权限就是666减umask（umask的各个位数字也不能大于6，比如077就不符合条件）的掩码数值，如果得到的3位数字其每一位都是偶数，那么这就是最终结果；如果有若干位的数字是奇数，那么这个奇数需要加1变成偶数，最后得到全是偶数的结果。 目录权限计算：创建目录默认最大权限777（-rwx-rwx-rwx），默认创建的目录属主是有x权限的，允许用户进入。对于目录来说，umask的设置是在假定文件拥有八进制777权限上进行，目录八进制权限777减去umask的掩码数值。 【使用示例】 1）查看root用户和非root用户的umask值 123456[root@C7-Server01 ~]# umask 0022 # 超级用户root的umask值是0022[root@C7-Server01 ~]# su - kkutysllbLast login: Wed May 1 22:54:01 CST 2019 on pts/0[kkutysllb@C7-Server01 ~]$ umask 0002 # 普通用户的umask值是0002 上述结果意味着，通过root用户创建文件默认权限是644，目录默认权限是755；通过普通用户（普通用户名和用户组名相同的情况下）创建的文件默认权限是664，目录默认权限是775。详细情况参见示例2。 2）查询umask在系统配置文件的设置规则 123456[kkutysllb@C7-Server01 ~]$ sed -n '59,63p' /etc/profileif [ $UID -gt 199 ] &amp;&amp; [ "`/usr/bin/id -gn`" = "`/usr/bin/id -un`" ]; then umask 002else umask 022fi 上述结果是shell条件判断语句，有两个判断条件，且为与关系： 条件1是判断用户的uid是否大于199，条件2是判断用户名是否和用户组名相同，当两个条件都满足时，则为普通用户，umask取值为002，否则为root用户，umask取值为022。 注意一点：普通用户的umask未必是002，比如满足以下条件，kkutysllb用户属于root组的时候，由于id-gn的执行结果不等于id-un的执行结果，所以umask值为022。 3）使用-p和-S选项 123456789# 使用-p选项，输出的权限掩码可直接作为命令来执行，也就是可以使用umask+数字更改umask的默认值[kkutysllb@C7-Server01 ~]$ umask -pumask 0002# 使用-S选项，[kkutysllb@C7-Server01 ~]$ umask -Su=rwx,g=rwx,o=rx]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-05-01-DPDK技术栈在电信云中的最佳实践（三）]]></title>
    <url>%2F2019%2F05%2F01%2F2019-05-01-DPDK%E6%8A%80%E6%9C%AF%E6%A0%88%E5%9C%A8%E7%94%B5%E4%BF%A1%E4%BA%91%E4%B8%AD%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[DPDK技术基础（3）网络报文转发模式我们来看看发展了十几年的DPDK，从Intel主导开发，到华为、思科、AWS等大厂商的加入，核心玩家都在该圈子里，拥有完善的社区，生态形成闭环。早期，主要是传统电信领域3层以下的应用，如华为、中国电信、中国移动都是其早期使用者，交换机、路由器、网关是主要应用场景。但是，随着上层业务的需求以及DPDK的完善，在更高的未来网络转发性能提升方面的应用也在逐步出现。 在x86服务器中，基于网络包的处理架构如下： Packet input：报文输入。 Pre-processing：对报文进行比较粗粒度的处理。 Input classification：对报文进行较细粒度的分流。 Ingress queuing：提供基于描述符的队列FIFO。 Delivery/Scheduling：根据队列优先级和CPU状态进行调度。 Accelerator：提供加解密和压缩/解压缩等硬件功能。 Egress queueing：在出口上根据QOS等级进行调度。 Post processing：后期报文处理释放缓存。 Packet output：从硬件上发送出去。 可以看到在浅色和阴影对应的模块都是和硬件相关的，因此要提升这部分性能的最佳选择就是去选择网卡上或网络设备芯片上具有网络功能硬件卸载特性的设备，而在深色软件部分可以通过提高算法的效率和结合CPU相关的并行指令来提升网络性能。 传统的Network Processor（专用网络处理器）转发的模型可以分为run to completion（运行至终结，简称RTC）模型和pipeline（流水线）模型。 1）pipeline模型：pipeline模型借鉴于工业上的流水线模型，将一个功能（大于模块级的功能）分解成多个独立的阶段，不同阶段间通过队列传递产品。这样，对于一些CPU密集和I/O密集的应用，通过pipeline模型，可以把CPU密集的操作放在一个微处理引擎上执行，将I/O密集的操作放在另外一个微处理引擎上执行。通过过滤器可以为不同的操作分配不同的线程，通过连接两者的队列匹配两者的处理速度，从而达到最好的并发效率。 如上图所示，在NPA中最主要的就是TOP（Task Optimized Processor）单元，每个TOP单元都是对特定的事务进行过优化的特殊微处理单元，在处理特定的事务时会在性能上有较大的提升，一个报文从Input进入后会经历五个不同的TOP单元，每个TOP的输出又会是下个TOP的输入，这种硬件模型决定了在报文的不同处理中必须按照TOP的顺序来进行，不可能先进行报文修改再进行报文查找。如果需要这种顺序修改处理，必须从TOP修改这个单元将报文再回传到TOP解析上，但这样包的处理速度会大幅下降。 2）run to completion模型：主要将一个程序分为几个不同的逻辑功能，但是这几个逻辑功能会在一个CPU的核上运行，可以进行水平扩展使得在SMP的系统中多个核上执行一样的逻辑，从而提高单位时间内事务处理的量。但是，由于每个核上的处理能力其实都是一样的，并没有针对某个逻辑功能进行优化，因此在这个层面上与pipeline模型比较，run to completion模型是不高效的。 如上图所示，在AMCC 345x的模型中，对于报文的处理没有特殊的运算单元，只有两个NP核，两个NP核利用已烧录的微码进行报文的处理，如果把运行的微码看成处理报文的逻辑，两个NP核上总共可以跑48个任务，每个任务的逻辑都共享一份微码，则同一时刻可以处理48份网络报文。 基于通用IA平台的DPDK中是怎么利用专用网络处理器中的这两种模型来进行高速包处理？如下图所示： 从RTC模型中（上图左边），我们可以清楚地看出，每个IA的物理核都负责处理整个报文的生命周期从RX到TX，这点非常类似前面所提到的AMCC的nP核的作用。 从pipeline模型中（上图右边），我们可以看出，报文的处理被划分成不同的逻辑功能单元A、B、C，一个报文需分别经历A、B、C三个阶段，这三个阶段的功能单元可以不止一个并且可以分布在不同的物理核上，不同的功能单元可以分布在相同的核上（也可以分布在不同的核上）。因此，其对于模块的分类和调用比EZchip的硬件方案更加灵活。 DPDK RTC模型 普通的Linux网络驱动中的扩展方法如下：把不同的收发包队列对应的中断转发到指定核的local APIC（本地中断控制器）上，并且使得每个核响应一个中断，从而处理此中断对应的队列集合中的相关报文。 而在DPDK的轮询模式中主要通过一些DPDK中eal中的参数-c、-l、-lcores来设置哪些核可以被DPDK使用，最后再把处理对应收发队列的线程绑定到对应的核上。每个报文的整个生命周期都只可能在其中一个线程中出现。 和普通网络处理器的RTC模式相比，基于IA平台的通用CPU也有不少的计算资源，比如一个socket上面可以有独立运行的16运算单元（核），每个核上面可以有两个逻辑运算单元（thread）共享物理的运算单元。而多个socket可以通过QPI总线连接在一起，这样使得每一个运算单元都可以独立地处理一个报文，并且通用处理器上的编程更加简单高效，在快速开发网络功能的同时，利用硬件AES-NI、SHA-NI等特殊指令可以加速网络相关加解密和认证功能。RTC模型虽然有许多优势，但是针对单个报文的处理始终集中在一个逻辑单元上，无法利用其他运算单元，并且逻辑的耦合性太强，而流水线模型正好解决了以上的问题。下面我们来看DPDK的流水线模型，DPDK中称为Packet Framework。 DPDK pipeline模型 pipeline的主要思想就是不同的工作交给不同的模块，而每一个模块都是一个处理引擎，每个处理引擎都只单独处理特定的事务，每个处理引擎都有输入和输出，通过这些输入和输出将不同的处理引擎连接起来，完成复杂的网络功能。 DPDK pipeline的多处理引擎实例和每个处理引擎中的组成框图可见如下两个实例的图片：zoom out（多核应用框架）和zoom in（单个流水线模块）。 Zoom out的实例中包含了五个DPDK pipeline处理模块，每个pipeline作为一个特定功能的包处理模块。一个报文从进入到发送，会有两个不同的路径，上面的路径有三个模块（解析、分类、发送），下面的路径有四个模块（解析、查表、修改、发送）。Zoom in的图示中代表在查表的pipeline中有两张查找表，报文根据不同的条件可以通过一级表或两级表的查询从不同的端口发送出去。 DPDK的pipeline是由三大部分组成的，逻辑端口（port）、查找表（table）和处理逻辑（action）。DPDK的pipeline模型中把网络端口作为每个处理模块的输入，所有的报文输入都通过这个端口来进行报文的输入。查找表是每个处理模块中重要的处理逻辑核心，不同的查找表就提供了不同的处理方法。而转发逻辑指明了报文的流向和处理。 用户可以根据以上三大类构建数据自己的pipeline，然后把每个pipeline都绑定在指定的核上从而使得我们能快速搭建属于我们自己的packet framework。DPDK实现报文转发上述两种方案的优缺点比较如下： PCIe与包处理I/OPCI Express（Peripheral Component Interconnect Express）又称PCIe，它是一种高速串行通信互联标准。PCIe规范遵循开放系统互联参考模型（OSI），自上而下分为事务传输层、数据链路层、物理层，如下图所示： 如果在PCIe的线路上抓取一个TLP（Transaction Layer Packet，事务传输层数据包），其格式是一种分组形式，层层嵌套，事务传输层也拥有头部、数据和校验部分。应用层的数据内容就承载在数据部分，而头部定义了一组事务类型。 应用层数据作为有效载荷被承载在事务传输层之上，网卡从线路上接收的以太网包整个作为有效载荷在PCIe的事务传输层上进行内部传输。对于PCIe事务传输层操作而言，应用层数据内容是透明的。一般网卡采用DMA控制器通过PCIe Bus访问内存，除了对以太网数据内容的读写外，还有DMA描述符操作相关的读写。 既然应用层数据只是作为有效载荷，那么PCIe协议的三层栈有多少额外开销呢？下图列出了每个部分的长度。物理层开始和结束各有1B的标记，整个数据链路层占用6B。TLP头部64位寻址占用16B（32位寻址占用12B），TLP中的ECRC为可选位。所以，对于一个完整的TLP包来说，除去有效载荷，额外还有24B的开销（TLP头部以16B计算）。 PCIe逐代的理论峰值带宽都有显著提升，Gen1到Gen2单路传输率翻倍，Gen2到Gen3虽然传输率没有翻倍，但随着编码效率的提升，实际单路有效传输仍然有接近一倍的提升，Gen1和Gen2采用8b/10b编码，Gen3采用128b/130b编码，如下所示： 以8b/10b编码为例，每10个比特传输8个比特（1个字节）的有效数据。以GEN2为例，传输1个字节数据需要500M*10b/s=500MB/s单向每路，对于8路同时传输8个字节数据就有4GB/s的理论带宽。 要查看特定PCIe设备的链路能力和当前速率，可以用Linux工具lspci读取PCIe的配置寄存器（Configuration Space），以下是在虚拟机中读出的信息展示，与实际物理服务器并不一致 除了TLP的协议开销以外，有时还会有实际实现的开销。比如有些网卡可能会要求每个TLP都要从Lane0开始，甚至要求从偶数的时钟周期开始。由于存在这样的实现因素影响，有效带宽还会进一步降低。同时，真实的网卡收发包由DMA驱动，除了写包内容之外，还有一系列的控制操作，这些操作也会进一步影响PCIe带宽的利用。 DMA（Direct Memory Access，直接存储器访问）是一种高速的数据传输方式，允许在外部设备和存储器之间直接读写数据。数据既不通过CPU，也不需要CPU干预。整个数据传输操作在DMA控制器的控制下进行。除了在数据传输开始和结束时做一点处理外，在传输过程中CPU可以进行其他的工作。 网卡DMA控制器通过环形队列与CPU交互。环形队列由一组控制寄存器和一块物理上连续的缓存构成。主要的控制寄存器有Base、Size、Head和Tail。通过设置Base寄存器，可以将分配的一段物理连续的内存地址作为环形队列的起始地址，通告给DMA控制器。同样通过Size寄存器，可以通告该内存块的大小。Head寄存器往往对软件只读，它表示硬件当前访问的描述符单元。而Tail寄存器则由软件来填写更新，通知DMA控制器当前已准备好被硬件访问的描述符单元。 为Intel 82599网卡的收发描述符环形队列为例，硬件控制所有Head和Tail之间的描述符。如下所示： Head等于Tail时表示队列为空，Head等于Next（Tail）时表示队列已满。 环形队列中每一条记录就是描述符。 描述符的格式和大小根据不同网卡各不相同，Intel 82599网卡的一个描述符大小为16B，整个环形队列缓冲区的大小必须是网卡支持的最大Cache line（128B））的整数倍，所以描述符的总数是8的倍数。 环形队列的起始地址也需要对齐到最大Cache line的大小。 无论网卡是工作在中断方式还是轮询方式下，判断包是否接收成功，或者包是否发送成功，都会需要检查描述符中的完成状态位（Descriptor Done，DD）。该状态位由DMA控制器在完成操作后进行回写。 对网络帧的封装及处理有两种方式：将网络帧元数据（metadata）和帧本身存放在固定大小的同一段缓存中；或将元数据和网络帧分开存放在两段缓存里。前者的好处是高效：对缓存的申请及释放均只需要一个指令，缺点是因为缓存长度固定而网络帧大小不一，大部分帧只能使用填0（padding）的方式填满整个缓存，较为耗费内存空间。后者的优点则是相对自由：帧数据的大小可以任意，同时对元数据和网络帧的缓存可以分开申请及释放；缺点是低效，因为无法保证数据存在于一个Cache Line中，可能造成Hit Miss。 为保持包处理的效率，DPDK采用了前者。网络帧元数据的一部分内容由DPDK的网卡驱动写入，包括：VLAN标签、RSS哈希值、网络帧入口端口号以及巨型帧所占的Mbuf个数等。对于巨型帧，网络帧元数据仅出现在第一个帧的Mbuf结构中，其他的帧该信息为空。 Mbuf主要用来封装网络帧缓存，也可用来封装通用控制信息缓存（缓存类型需使用CTRL_MBUF_FLAG来指定）。Mbuf结构报头经过精心设计，原先仅占1个Cache Line。随着Mbuf头部携带的信息越来越多，现在Mbuf头部已经调整成两个Cache Line，原则上将基础性、频繁访问的数据放在第一个Cache Line字节，而将功能性扩展的数据放在第二个Cache Line字节。Mbuf报头包含包处理所需的所有数据，对于单个Mbuf存放不下的巨型帧（Jumbo Frame），Mbuf还有指向下一个Mbuf结构的指针来形成帧链表结构。有兴趣的参见DPDK开发者手册。 当一个网络帧被网卡接收时，DPDK的网卡驱动将其存储在一个高效的环形缓存区中，同时在Mbuf的环形缓存区中创建一个Mbuf对象。这两个行为都不涉及向系统申请内存，因为这些内存已经在内存池被创建时就申请好了。Mbuf对象被创建好后，网卡驱动根据分析出的帧信息将其初始化，并将其和实际帧对象逻辑相连。对网络帧的分析处理都集中于Mbuf，仅在必要的时候访问实际网络帧。这就是内存池的双环形缓存区结构。 IO包处理中轮询和中断的抉择DPDK采用了轮询或者轮询混杂中断的模式来进行收包和发包，而以前网卡驱动程序基本都是基于异步中断处理模式。 异步中断模式：当有包进入网卡收包队列后，网卡会产生硬件（MSIX/MSI/INTX）中断，进而触发CPU中断，进入中断服务程序，在中断服务程序来完成收包的处理。为了改善包处理性能，也可以在中断处理过程中加入轮询，来避免过多的中断响应次数。总之，基于异步中断信号模式的收包，是不断地在做中断处理，上下文切换，每次处理这种开销是固定的，累加带来的负荷显而易见。当有包需要发送出去的时候，基于异步中断信号的驱动程序会准备好要发送的包，配置好发送队列的各个描述符。在包被真正发送完成时，网卡同样会产生硬件中断信号，进而触发CPU中断，进入中断服务程序，来完成发包后的处理，例如释放缓存等。 轮询模式：是指收发包完全不使用中断处理的高吞吐率的方式。DPDK所有的收发包有关的中断在物理端口初始化的时候都会关闭，也就是说，CPU这边在任何时候都不会收到收包或者发包成功的中断信号，也不需要任何收发包有关的中断处理。DPDK的轮询驱动程序负责初始化每一个收包描述符，包括把包缓冲内存块的物理地址填充到收包描述符对应的位置，以及把相应的收包成功标志复位。然后驱动程序修改相应的队列管理寄存器来通知网卡硬件，网卡硬件会把收到的包填充到对应的收包描述符表示的缓冲内存块里，同时标记好收包成功标志。当一个收包描述符所代表的缓冲内存块大小不够存放一个完整的包时，这时候候就需要两个甚至多个收包描述符来处理一个包。 1）每一个收包队列，DPDK都会有一个相应的线程负责轮询里面的收包描述符的收包成功的标志。一旦发现某一个收包描述符的收包成功标志被硬件置位了，就意味着有一个包已经进入到网卡，并且网卡已经存储到描述符对应的缓冲内存块里面，这时候驱动程序会解析相应的收包描述符，把收包缓冲内存块放到收包函数提供的数组里面，同时分配好一个新的缓冲内存块给这个描述符，以便下一次收包。 2）每一个发包队列，DPDK都会有一个相应的线程负责设置需要发送出去的包，DPDK的驱动程序负责提取发包缓冲内存块的有效信息，例如包长、地址、校验和信息、VLAN配置信息等。DPDK的轮询驱动程序根据内存缓存块中的包的内容来负责初始化每一个发包描述符。其中最关键的有两个，一个就是标识完整的包结束的标志EOP（End Of Packet），另外一个就是请求报告发送状态RS（Report Status）。 由于一个包可能存放在一个或者多个内存缓冲块里面，需要一个或者多个发包描述符来表示一个等待发送的包，EOP就是驱动程序用来通知网卡硬件一个完整的包结束的标志。每当驱动程序设置好相应的发包描述符，硬件就可以开始根据发包描述符的内容来发包，那么驱动程序可能会需要知道什么时候发包完成，然后回收占用的发包描述符和内存缓冲块。基于效率和性能上的考虑，驱动程序可能不需要每一个发包描述符都报告发送结果，RS就是用来由驱动程序来告诉网卡硬件什么时候需要报告发送结果的一个标志。不同的硬件会有不同的机制，有的网卡硬件要求每一个包都要报告发送结果，有的网卡硬件要求相隔几个包或者发包描述符再报告发送结果，而且可以由驱动程序来设置具体的位置。 3）发包的轮询就是轮询发包结束的硬件标志位。DPDK驱动程序根据需要发送的包的信息和内容，设置好相应的发包描述符，包含设置对应的RS标志，然后会在发包线程里不断查询发包是否结束。只有设置了RS标志的发包描述符，网卡硬件才会在发包完成时以写回的形式告诉发包结束。当驱动程序发现写回标志，意味着包已经发送完成，就释放对应的发包描述符和对应的内存缓冲块，这时候就全部完成了包的发送过程。 混和中断轮询模式：由于实际网络中可能存在的潮汐效应，在某些时间段网络数据流量可能很低，甚至完全没有需要处理的包，这样完全轮询的方式会让处理器一直全速运行，明显浪费处理能力。因此在DPDK R2.1和R2.2陆续添加了收包中断与轮询的混合模式的支持，类似NAPI的思路，用户可以根据实际应用场景来选择完全轮询模式，或者混合中断轮询模式。而且，完全由用户来制定中断和轮询的切换策略，比如什么时候开始进入中断休眠等待收包，中断唤醒后轮询多长时间等等。所谓混合中断轮询模式，就是应用程序开始就是轮询收包，这时候收包中断是关闭的。但是当连续多次收到的包的个数为0，应用程序可以定义一个简单的策略来决定是否切换到中断以及什么时候让使能收包中断。轮询线程进入休眠之后，对应的核的运算能力就被释放出来，完全可以用于其他任何运算，或者干脆进入省电模式。当后续有任何包收到的时候，会产生一个收包中断，并且最终唤醒对应的应用程序收包线程。线程被唤醒后，就会关闭收包中断，再次轮询收包。应用程序完全可以根据不同的需要来定义不同的策略来让收包线程休眠或者唤醒收包线程。 1）DPDK的混合中断轮询机制是基于UIO或VFIO来实现其收包中断通知与处理流程的。如果是基于VFIO的实现，该中断机制是可以支持队列级别的，即一个接收队列对应一个中断号，这是因为VFIO支持多MSI-X中断号。如果是基于UIO的实现，该中断机制就只支持一个中断号，所有的队列共享一个中断号。 2）混合中断轮询模式相比完全轮询模式，会在包处理性能和时延方面有一定的牺牲，比如：当需要把DPDK工作线程从睡眠状态唤醒并运行，这样会引起中断触发后的第一个接收报文的时延增加。由于时延的增加，需要适当调整Mbuf队列的大小，以避免当大量报文同时到达时可能发生的丢包现象。 综上，在应用场景下如何更高效地利用处理器的计算能力，用户需要根据实际应用场景来做出最合适的选择。 零拷贝技术在每一次网络io过程，数据都要经过几个缓存，再发送出去。如下图： 上图右侧为client，左侧为Server为例： 当Server收到Client发送的index.html文件的请求时，负责处理请求的httpd子进程/线程总是会先发起系统调用，让内核将index.html从存储设备中载入。但是加载到的位置是内核空间的缓冲区kernel buffer，而不是直接给进程/线程的内存区。由于是内存设备和存储设备之间的数据传输，没有CPU的参与，所以这是一次DMA操作。 当数据准备好后，内核唤醒httpd子进程/线程，让它使用read()函数把数据复制到它自己的缓冲区，也就是图中的app buffer。到了app buffer中的数据，已经独属于进程/线程，也就可以对它做读取、修改等等操作。由于这次是使用CPU来复制的，所以会消耗CPU资源。由于这个阶段从内核空间切换到用户空间，所以进行了上下文切换。 当数据修改完成(也可能没做任何操作)后，Server需要发送响应消息给Client，也就是说要通过TCP连接传输出去。但TCP协议栈有自己的缓冲区，要通过它发送数据，必须将数据写到它的buffer中，对于发送者就是send buffer，对于接受者就是recv buffer。于是，通过write()函数将数据再次从app buffer复制到send buffer。这次也是CPU参与进行的复制，所以会消耗CPU。同样也会进行上下文切换。 非本机数据最终还是会通过网卡传输出去的，所以再使用send()函数就可以将send buffer中的数据交给网卡并通过网卡传输出去。由于这次是内存和设备之间的数据传输，没有CPU的参与，所以这次也是一次DMA操作。 当Client的网卡收到响应数据后，将它传输到TCP的recv buffer。这是一次DMA操作。 数据源源不断地填充到recv buffer中，但是Client的应用程序却不一定会去读取，而是需要通知应用程序的进程使用recv()函数将数据从read buffer中取走。这次是CPU操作。 在Server端，每次进程/线程需要一段数据时，总是先拷贝到kernel buffer，再拷贝到app buffer，再拷贝到socket buffer，最后再拷贝到网卡上。也就是说，总是会经过4段拷贝经历。通过分析上述4段拷贝，数据从存储设备到kernel buffer是必须的，从socket buffer到NIC也是必须的，但是从kernel buffer到app buffer却不一定。对于web服务来说，如果不修改http响应报文，数据完全可以不用经过用户空间。也就是不用再从kernel buffer拷贝到app buffer，这就是零复制的概念。 零复制的概念是避免将数据在内核空间和用户空间进行拷贝。主要目的是减少不必要的拷贝，避免让CPU做大量上下文切换。 mmap()函数将文件直接映射到用户程序的内存中，映射成功时返回指向目标区域的指针。这段内存空间可以用作进程间的共享内存空间，内核也可以直接操作这段空间。在映射文件之后，暂时不会拷贝任何数据到内存中，只有当访问这段内存时，发现没有数据，于是产生缺页访问，使用DMA操作将数据拷贝到这段空间中。可以直接将这段空间的数据拷贝到socket buffer中。所以也算是零复制技术。如图： mmap()的代码如下： 123#include void *mmap(void *addr, size_t length, int prot, int flags,int fd, off_t offset); 另外，sendfile()函数也是一种零复制函数。sendfile()函数借助文件描述符来实现数据拷贝：直接将文件描述in_fd的数据拷贝给文件描述符out_fd，其中in_fd是数据提供方，out_fd是数据接收方。文件描述符的操作都是在内核进行的，不会经过用户空间，所以数据不用拷贝到app buffer，实现了零复制。如下图所示： sendfile()的代码如下： 12#include ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 还有一种常用的零拷贝技术就是COW（copy-on-write，写时复制）。当父进程fork生成子进程时，会复制它的所有内存页。这至少会导致两个问题：消耗大量内存；复制操作消耗时间。特别是fork后使用exec加载新程序时，由于会初始化内存空间，所以复制操作几乎是多余的。使用copy-on-write技术，使得在fork子进程时不复制内存页，而是共享内存页(也就是说，子进程也指向父进程的物理空间)，只有在该子进程需要修改某一块数据，才会将这一块数据拷贝到自己的app buffer中并进行修改，那么这一块数据就属于该子进程的私有数据，可随意访问、修改、复制。这在一定程度上实现了零复制，即使复制了一些数据块，也是在逐渐需要的过程进行复制的。 网卡多队列技术多队列与流分类是当今网卡通用的技术。利用多队列及流分类技术可以使得网卡更好地与多核处理器、多任务系统配合，从而达到更高效IO处理的目的。说起网卡多队列，顾名思义，也就是传统网卡的DMA队列有多个，网卡有基于多个DMA队列的分配机制。 网卡多队列技术应该是与处理器的多核技术密不可分的。早期的多数计算机，处理器可能只有一个核，从网卡上收到的以太网报文都需要这个处理器处理。随着多核处理技术的发展，2007年在Intel的82575、82598网卡上引入多队列技术，可以将各个队列通过绑定到不同的核上来满足高速流量的需求。 Linux内核对多队列的支持：众所周知，Linux的网卡由结构体net_device表示，一个该结构体可对应多个可以调度的数据包发送队列，数据包的实体在内核中以结构体sk_buff（skb）表示。网卡驱动程序为每个接收队列设定相应的中断号，通过中断的均衡处理，或者设置中断的亲和性（SMP IRQ Affinity），从而实现队列绑定到不同的核。Linux提供了较为灵活的队列选择机制。dev_pick_tx用于选取发送队列，可以由driver定制其策略，也可以根据队列优先级选取，按照hash来做均衡。也就是利用XPS（Transmit Packet Steering，内核2.6.38后引入）机制，智能地选择多队列设备的队列来发送数据包。为此，需要对CPU核到硬件队列做一个表来记录映射关系，每一个映射记录着专门分配的队列到一个CPU核列表，这个映射的CPU核负责完成队列中的数据传输。这样做的目的一是减少设备队列上的锁竞争，二是增加传输时的缓存命中概率。下面的代码简单说明了在发送时队列的选取是考虑在其中的： 1234567891011121314151617int dev_queue_xmit(struct sk_buff *skb) &#123; struct net_device *dev = skb-&gt;dev; txq = dev_pick_tx(dev, skb); // 选出一个队列 spin_lock_prefetch(&amp;txq-&gt;lock); dev_put(dev); &#125;struct netdev_queue *netdev_pick_tx(struct net_device *dev, struct sk_buff *skb) &#123; int queue_index = 0; if (dev-&gt;real_num_tx_queues！= 1) &#123; const struct net_device_ops *ops = dev-&gt;netdev_ops; if (ops-&gt;ndo_select_queue) queue_index = ops-&gt;ndo_select_queue(dev, skb); // 按照driver提供的策略来选择一个队列的索引 else queue_index = __netdev_pick_tx(dev, skb); queue_index = dev_cap_txqueue(dev, queue_index); &#125; skb_set_queue_mapping(skb, queue_index); return netdev_get_tx_queue(dev, queue_index); &#125; 除此之外，收发队列一般会被绑在同一个中断上，其目的也是为了增加cache命中率。 除了硬件支持的多队列技术外，还有软件支持的流量均衡技术，主要用于单队列网卡，将其上流量均衡的分摊到多个核上，该项技术就是RPS（Receive Packet Steering）。在接收侧，RPS主要是把软中断的负载均衡到CPU的各个core上，网卡驱动对每个流生成一个hash标识，这个hash值可以通过四元组（源IP地址SIP，源四层端口SPORT，目的IP地址DIP，目的四层端口DPORT）来计算，然后由中断处理的地方根据这个hash标识分配到相应的core上去，这样就可以比较充分地发挥多核的能力了。在发送侧，无论来自哪个CPU的数据包只能往这唯一的队列上发送。通俗点来说，就是在软件层面模拟实现硬件的多队列网卡功能，其实现机制如下图所示： DPDK对多队列的支持：观察DPDK提供的一系列以太网设备的API，可以发现其Packet I/O机制具有与生俱来的多队列支持功能，可以根据不同的平台或者需求，选择需要使用的队列数目，并可以很方便地使用队列，指定队列发送或接收报文。根据这样的特性，可以很容易实现CPU核、缓存与网卡队列之间的亲和性，从而达到很好的性能。除此之外，DPDK的队列管理机制还可以避免多核处理器中的多个收发进程采用自旋锁产生的不必要等待。以RTC模型为例，可以从核、内存与网卡队列之间的关系来理解DPDK是如何利用网卡多队列技术带来性能的提升： 1）将网卡的某个接收队列分配给某个核，从该队列中收到的所有报文都应当在该指定的核上处理。 2）从核对应的本地存储中分配内存池，接收报文和对应的报文描述符都位于该内存池。 3）为每个核分配一个单独的发送队列，发送报文和对应的报文描述符都位于该核和发送队列对应的本地内存池中。 可以看出不同的核，操作的是不同的队列，从而避免了多个线程同时访问一个队列带来的锁的开销。那么，网卡是如何将网络中的报文分发到不同的队列呢？常用的方法有微软提出的RSS与英特尔提出的Flow Director技术，前者是根据哈希值希望均匀地将包分发到多个队列中。后者是基于查找的精确匹配，将包分发到指定的队列中。此外，网卡还可以根据优先级分配队列提供对QoS的支持。除此之外，网卡多队列机制还可以应用于虚拟化，详见后文《I/O虚拟化详解》。 高级的网卡设备可以分析出包的类型，包的类型会携带在接收描述符中，应用程序可以根据描述符快速地确定包是哪种类型的包，避免了大量的解析包的软件开销。DPDK的Mbuf结构中含有相应的字段来表示网卡分析出的包的类型，从下面的代码可见Packet_type由二层、三层、四层及tunnel的信息来组成，应用程序可以很方便地定位到它需要处理的报文头部或是内容。 12345678910111213141516struct rte_mbuf &#123; …… union &#123; uint32_t packet_type; /**&lt; L2/L3/L4 and tunnel information．*/ struct &#123; uint32_t l2_type：4; /**&lt; (Outer) L2 type．*/ uint32_t l3_type：4; /**&lt; (Outer) L3 type．*/ uint32_t l4_type：4; /**&lt; (Outer) L4 type．*/ uint32_t tun_type：4; /**&lt; Tunnel type．*/ uint32_t inner_l2_type：4; /**&lt; Inner L2 type．*/ uint32_t inner_l3_type：4; /**&lt; Inner L3 type．*/ uint32_t inner_l4_type：4; /**&lt; Inner L4 type．*/ &#125;; &#125;; ……&#125;; 网卡硬件卸载功能网卡的硬件卸载功能可能是基于端口设置，也有可能是基于每个包设置使能，需要仔细区分。在包粒度而言，每个包都对应一个或者多个Mbuf，DPDK软件利用rte_mbuf数据结构里的64位的标识（ol_flags）来表征卸载与状态。 如果需要使用硬件卸载功能，网卡驱动需要提供相应的API给上层应用，通过调用API驱动硬件完成相应的工作。而驱动硬件的工作实际上是由网卡驱动程序完成的，网卡驱动程序也是通过硬件提供的接口来驱动硬件。硬件提供的接口一般包括寄存器（Register）和描述符（Descriptor）。寄存器是全局的设置，一般用于开启某项功能或者为某项功能设置全局性的参数配置，一般情况下是基于以太网端口为基本单位。描述符可以看做是每个数据包的属性，和数据包一起发送给硬件，一般用于携带单个数据包的参数或设置。 对于各种各样的硬件卸载功能，按照功能的相似性大致可分成三类，分别是计算及更新功能、分片功能、组包功能。 VLAN硬件卸载：VLAN在以太网报文中增加了了一个4字节的802.1q Tag（也称为VLAN Tag），如果由软件完成VLAN Tag的插入将会给CPU带来额外的负荷，涉及一次额外的内存拷贝（报文内容复制），最坏场景下，这可能是上百周期的开销。大多数网卡硬件提供了VLAN卸载的功能，VLAN Tag的插入和剥离由网卡硬件完成，可以减轻服务器CPU的负荷。 1）网卡最典型的卸载功能之一就是在接收侧针对VLAN进行包过滤。在网卡硬件端口设计了VLAN过滤表，无法在过滤表中匹配的VLAN包会被丢弃，没有VLAN信息的以太网则会通过网卡的过滤机制，在DPDK中app/testpmd提供了测试命令与实现代码。 2）网卡硬件能够对接收到的包的VLAN Tag进行剥离。首先硬件能够对VLAN包进行识别，原理上是判断以太帧的以太网类型来确定是否是VLAN包。启动这项硬件特性，需要在网卡端口，或者是属于这个网卡端口的队列上设置使能标志，将VLAN剥离特性打开，对应到软件，是通过驱动将配置写入相应的寄存器。DPDK的app/testpmd提供了如何基于端口使能与去使能的测试命令： 12testpmd&gt; vlan set strip (on|off) (port_id)testpmd&gt; vlan set stripq (on|off) (port_id,queue_id) 3）网卡硬件会将4字节的VLAN tag从数据包中剥离，VLAN Tag中包含的信息对上层应用是有意义的，不能丢弃，此时，网卡硬件会在硬件描述符中设置两个域，将需要的信息通知驱动软件，包含此包是否曾被剥离了VLAN Tag以及被剥离的Tag。软件省去了剥离VLAN Tag的工作负荷，还获取了需要的信息。在DPDK中，驱动会根据硬件描述符信息对每个接收的数据包进行检测，如果剥离动作发生，需要将rte_mbuf数据结构中的PKT_RX_VLAN_PKT置位，表示已经接收到VLAN的报文，并且将被剥离VLAN Tag写入到下列字段，供上层应用处理。 123Struct rte_mbuf&#123; uint16_t vlan_tci; /**&lt; VLAN Tag Control Identifier(CPU order) */ &#125; 4）在发送端口需要在数据包中插入VLAN标识。VLAN Tag由两部分组成：TPID（Tag Protocol Identifier），也就是VLAN的Ether type，和TCI（Tag Control Information）。TPID是一个固定的值，作为一个全局范围内起作用的值，可通过寄存器进行设置。而TCI是每个包相关的，需要逐包设置，在DPDK中，在调用发送函数前，必须提前设置mbuf数据结构中PKT_TX_VLAN_PKT位，同时将具体的Tag信息写入vlan_tci字段。 123struct rte_mbuf&#123; uint16_t vlan_tci; /**&lt; VLAN Tag Control Identifier(CPU order) */ &#125; 5）为了解决VLAN个数局限，业界发展出了采用双层乃至多层VLAN堆叠模式，随着这种模式（也被称为QinQ技术）在网络应用中变得普遍，现代网卡硬件大多提供对两层VLAN Tag进行卸载，如VLAN Tag的剥离、插入。DPDK的app/testapp应用中提供了测试命令，网卡数据手册有时也称VLAN Extend模式。在DPDK相应测试代码如下： 1testpmd&gt; vlan set qinq (on|off) (port_id) checksum硬件卸载功能：checksum计算是网络协议的容错性设计的一部分，基于网络传输不可靠的假设，因此在Ethernet、IPv4、UDP、TCP、SCTP各个协议层设计中都有checksum字段，用于校验包的正确性，checksum不涉及复杂的逻辑，是简单机械的计算，算法稳定，适合固化到硬件中。checksum虽然可以硬件卸载，但依然需要软件的协同配合实现。checksum在收发两个方向上都需要支持，操作并不一致，在接收方向上，主要是检测，通过设置端口配置，强制对所有达到的数据报文进行检测，即判断哪些包的checksum是错误的，对于这些出错的包，可以选择将其丢弃，并在统计数据中体现出来。在DPDK中，和每个数据包都有直接关联的是rte_mbuf，网卡自动检测进来的数据包，如果发现checksum错误，就会设置错误标志。软件驱动会查询硬件标志状态，通过mbuf中的ol_flags字段来通知上层应用。但在发送侧就会复杂一些，硬件需要计算协议的checksum，将且写入合适的位置。。在原理上，网卡在设计之初时就依赖软件做额外设置，软件需要逐包提供发送侧上下文状态描述符，这段描述符需要通过PCIe总线写入到网卡设备内，帮助网卡进行checksum计算。设置上下文状态描述符，在DPDK驱动里面已经实现，对于使用DPDK的程序员，真正需要做的工作是设置rte_mbuf和改写报文头部，保证网卡驱动得到足够的mbuf信息，完成整个运算。 1）从设置mbuf的角度，需要关注如下字段：IPv6头部没有checksum字段，无需计算。 12345678/* fields to support TX offloads */ uint64_t tx_offload; /**&lt; combined for easy fetch */ struct &#123; uint64_t l2_len：7; /**&lt; L2 (MAC) Header Length．*/ uint64_t l3_len：9; /**&lt; L3 (IP) Header Length．*/ uint64_t l4_len：8; /**&lt; L4 (TCP/UDP) Header Length．*/ uint64_t tso_segsz：16; /**&lt; TCP TSO segment size */&#125; 2）对于IPv4的checksum，在发送侧如果需要硬件完成自动运算与插入，准备工作如下： 12ipv4_hdr-&gt;hdr_checksum = 0; // 将头部的checksum字段清零 ol_flags |= PKT_TX_IP_CKSUM; // IP层checksum请求标识置位 3）对于UDP或者TCP，checksum计算方法一样，准备工作如下： 123udp_hdr-&gt;dgram_cksum = 0; // 将头部的checksum字段清零 ol_flags |= PKT_TX_UDP_CKSUM; // UDP层checksum请求标识置位 udp_hdr-&gt;dgram_cksum = get_psd_sum(l3_hdr, info-&gt;ethertype, ol_flags); /* 填入IP层伪头部计算码，具体实现参阅DPDK代码*/ 4）对于SCTP checksum计算方法一样，准备工作如下： 1sctp_hdr-&gt;hdr_checksum = 0; ol_flags |= PKT_TX_SCTP_CKSUM; TSO硬件卸载：TSO（TCP Segment Offload）是TCP分片功能的硬件卸载，显然这是发送方向的功能。TCP会协商决定发送的TCP分片的大小，对于从应用层获取的较大的数据，TCP需要根据下层网络的报文大小限制，将其切分成较小的分片发送。硬件提供的TCP分片硬件卸载功能可以大幅减轻软件对TCP分片的负担。而且这项功能本身也是非常适合由硬件来完成的，因为它是比较简单机械的实现。如下所示，就是采用TSO和不采用TSO的示意图： 在下图中，我们可以看到，TCP分片需要将现有的较大的TCP分片拆分成较小的TCP分片，在这个过程中，不需要提供特殊的信息，仅仅需要复制TCP的包头，更新头里面的长度相关的信息，重新计算校验和，显然这些功能非常适合硬件来实现。 在dpdk/testpmd中提供了两条TSO相关的命令行： 12tso set 14000：用于设置tso分片大小。tso show 0：用于查看tso分片的大小。 和csum硬件卸载功能类似，tso分片硬件卸载功能也需要对mbuf进行设置，同样从设置mbuf的角度，如下字段需要关注： 12345678/* fields to support TX offloads */ uint64_t tx_offload; /**&lt; combined for easy fetch */ struct &#123; uint64_t l2_len：7; /**&lt; L2 (MAC) Header Length．*/ uint64_t l3_len：9; /**&lt; L3 (IP) Header Length．*/ uint64_t l4_len：8; /**&lt; L4 (TCP/UDP） Header Length．*/ uint64_t tso_segsz：16; /**&lt; TCP TSO segment size */&#125; 同时tso使用了ol_flag中的PKT_TX_TCP_SEG来指示收发包处理流程中当前的包需要开启tso的硬件卸载功能。 RSC组包功能卸载：RSC（Receive Side Coalescing，接收方聚合）是TCP组包功能的硬件卸载。硬件组包功能针对TCP实现，是接收方向的功能，可以将拆分的TCP分片聚合成一个大的分片，从而减轻软件的处理。如下图所示，LRO是RCS的另一种表述，下图左边是通过硬件层面完成组包功能，下图右边是通过驱动层面完成组包。 当硬件接收到TCP分片后，硬件可以将多个TCP分片缓存起来，并且将其排序，这样多个TCP分片最终传递给软件时将会呈现为一个分片，软件将不再需要分析处理多个数据包的头，同时对TCP包的排序的负担也有所减轻。如下图所示： RSC是一种硬件能力，使用此功能时需要先明确硬件支持此能力。我们通过配置来开启RSC功能，需要关注下面的数据结构： 12345678910111213141516/** * A structure used to configure the RX features of an Ethernet port. */ struct rte_eth_rxmode &#123; /** The multi-queue packet distribution mode to be used, e.g．RSS．*/ enum rte_eth_rx_mq_mode mq_mode; uint32_t max_rx_pkt_len; /**&lt; Only used if jumbo_frame enabled．*/ uint16_t split_hdr_size; /**&lt; hdr buf size (header_split enabled).*/ uint16_t header_split：1, /**&lt; Header Split enable．*/ hw_ip_checksum ：1, /**&lt; IP/UDP/TCP checksum offload enable．*/ hw_vlan_filter ：1, /**&lt; VLAN filter enable．*/ hw_vlan_strip ：1, /**&lt; VLAN strip enable．*/ hw_vlan_extend ：1, /**&lt; Extended VLAN enable．*/ jumbo_frame ：1, /**&lt; Jumbo Frame Receipt enable．*/ hw_strip_crc ：1, /**&lt; Enable CRC stripping by hardware．*/ enable_scatter ：1, /**&lt; Enable scatter packets rx handler */ enable_lro ：1; /**&lt; Enable LRO */ &#125;; 当对接收处理进行初始化ixgbe_dev_rx_init时，会调用ixgbe_set_rsc，此函数中对enable_lro进行判断，如果其为真，则会对RSC进行相关设置，从而使用此功能。 DPDK在NFV中应用案例回顾ETSI NFV参考架构，NFV技术通过运行在通用x86架构硬件上的虚拟化网络功能，通过软硬件解耦及功能抽象来实现各类网络功能在x86标准服务器上的灵活部署和业务的快速迭代。不同于典型数据中心业务和企业网业务，电信广域网业务要求网元（如DPI、FW等）具有高吞吐、低时延、海量流表支持、用户级QoS控制的特点。考虑到现实环境中的NFV解决方案一般由NFV基础设施和VNF两类系统服务商提供。因此，相应的NFV端到端性能优化，也应划分为底层的NFV基础设施性能与上层的VNF性能两类，以方便明确各自的性能瓶颈，避免不同层次的性能调优工作带来相互干扰。 在NFV基础设施性能优化技术方案中，DPDK软件加速方案已成为一种普遍采用的基本方法，它以用户数据I/O通道优化为基础，结合了Intel VT技术、操作系统、虚拟化层与vSwitch等多种优化方案，已经形成了完善的性能加速整体架构，并提供了用户态API供高速转发类应用访问。 场景一：VNF在物理机上应用运营商现有网络大部分都是专用网络设备，物理设备与应用软件紧耦合，设备升级成本高、功能扩展困难。将这些专用的网络功能设备，以软件化的VNF形式直接运行在物理服务器上，可以实现网络设备形态的通用化，方便设备功能灵活扩展。 该方案将一部分原来由硬件实现的网络功能，以VNF软件的形式直接运行在x86服务器OS上，同时在物理服务器上加载DPDK组件。此时，DPDK接管了物理网卡的I/O驱动， VNF也不再使用传统Linux内核网络协议栈，而是通过调用DPDK的用户态API进行快速转发。同时，DPDK进程将使用少量的处理器核（如2个核）与内存以满足高速转发处理， VNF 可以直接使用剩余的全部硬件资源，适用于数据转发频繁等资源利用率长期较高的网络业务，比如C/U分离后的U面网元采用VNF部署。该方案下整台服务器仅支持单一VNF应用，但因设备形态统一，软件功能部署灵活，仍具有较高应用价值。具体实现架构如下图所示： 场景二：VNF + OVS应用当多个VNF分别运行在一台服务器的多个VM中时，为满足VNF之间可能的流量交换或者共享物理网卡的需要，可以在Host OS上安装OVS类虚拟交换机，用于连接各个VNF（VM）和服务器的物理网卡端口。此时，这多个VNF一般是不同类型的VNF应用，VNF之间可能产生交互流量或者业务链处理流量（东西+南北流量模型），而同类型多个VNF或者纯粹共享网卡的多个VNF，一般采用场景三的“VNF + SR-IOV”方案。 在“VNF + OVS”方案中，因主要的性能瓶颈存在于VNF的虚拟I/O通道和OVS交换机，DPDK需要分别安装在运行VNF的VM镜像内部和运行OVS的物理服务器OS上：前者用于优化VM内部的VNF数据平面转发性能，包括提供虚拟化网卡驱动、提供用户态转发API等，DPDK的各种配置方法与VNF运行在物理机中的机制类似，VNF并不能感知是运行在VM环境；后者用于优化OVS交换机性能，连接VM与各NUMA节点上的DPDK端口。该场景可以细分为两种： ​ A：VM到OVS仅使用单一连接。 ​ B：VM到OVS采用一进一出的双向连接，如各类业务链中的VNF应用，见下图所示。 该方案可以实现VNF的灵活扩容/缩容，以及在资源池中按需迁移，方案中的第三方VNF厂商可以屏蔽物理设备差异，提供各自的高性能业务产品。同时，使用经过DPDK优化后的OVS 交换机，可以灵活实现VNF间流量的灵活转发与互联互通，节省硬件交换机。 场景三：VNF + SR-IOV当多个VNF运行在VM中时，各VM的虚拟网卡可以直接连至HOST上支持SR-IOV功能的物理网卡（VF）进行数据收发，如同独占物理网卡一样。该方案适用于VNF间无需流量交互的场景，或者是基于硬件交换机进行VNF互连和流量控制的场景。由于旁路了HOST的虚拟化层实现直接转发，可以达到近似物理转发的性能，被业界普遍用于消除Hypervisor带来的数据转发性能影响。如下图所示： 尽管“VNF+SR-IOV”方案消除了从物理网卡到VM虚拟网卡的性能瓶颈，但VM内部仍然需要通过加载DPDK以进一步优化各VNF（VM内部）的转发性能。此时，DPDK可以采用与前两种场景中类似的方法进行加载，同时占用VM内部一定CPU核和内存资源。 综上，就是DPDK技术在电信云中最佳实践的全部内容，后续我们将开始讲述电信云中另一个重要领域内容–虚拟化。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-30-DPDK技术栈在电信云中的最佳实践（二）]]></title>
    <url>%2F2019%2F05%2F01%2F2019-04-30-DPDK%E6%8A%80%E6%9C%AF%E6%A0%88%E5%9C%A8%E7%94%B5%E4%BF%A1%E4%BA%91%E4%B8%AD%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[DPDK技术基础（2）原子操作虽然，DPDK提出理念之一就是“遵循资源局部化的原则，解耦数据的跨核共享，使得性能可以有很好的水平扩展”。这种跨核解耦数据并同步的的本质就是原子操作，所谓原子操作简单来说就是：多个线程执行一个操作时，其中任何一个线程要么可以完全执行完此操作，要么根本不执行。比如：在单处理器系统（UniProcessor）中，能够在单条指令中完成的操作都可以认为是“原子操作”，因为中断只能发生于指令之间。 在多核CPU的时代，架构中运行着多个独立的CPU，即使在单个指令中可以完成的操作也可能会被干扰。典型的例子就是decl指令（递减指令），它细分为三个过程：“读-&gt;改-&gt;写”，涉及两次内存操作。如果多个CPU运行的多个进程或线程在同时对同一块内存执行这个指令，那情况是无法预测的。 这里需要特别介绍一下CMPXCHG这条指令，它的语义是比较并交换操作数（CAS，Compare And Set）。而用XCHG类的指令做内存操作，CPU自动遵循LOCK语义，可见该指令是一条原子的CAS单指令操作，它是实现很多无锁数据结构的基础。CAS操作需要输入两个数值，一个旧值（期望操作前的值）和一个新值，在操作期间先比较下旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化，则不交换。比如：CMPXCHG r/m，r将累加器AL/AX/EAX/RAX中的值与首操作数（目的操作数）比较，如果相等，第2操作数（源操作数）的值装载到首操作数，zf置1。如果不等， 首操作数的值装载到AL/AX/EAX/RAX并将zf清0。该指令只能用于486及其后继机型。第2操作数（源操作数）只能用8位、16位或32位寄存器。第1操作数（目地操作数）则可用寄存器或任一种存储器寻址方式。我们通过下列代码可以对CMPXCHG指令（CAS)进行测试理解： 1）不相等条件测试 12345678910111213141516171819202122#includeusing namespace std;int main()&#123; int a=0,b=0,c=0; __asm&#123; mov eax,100; mov a,eax &#125; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; b = 99; c = 11; __asm&#123; mov ebx,b cmpxchg c,ebx mov a,eax &#125; cout &lt;&lt; "b := " &lt;&lt; b &lt;&lt; endl; cout &lt;&lt; "c := " &lt;&lt; c &lt;&lt; endl; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; return 0;&#125; 输出:(如果不等， “首操作数”(c)的值装载到AL/AX/EAX/RAX并将zf清0) 1234a := 100b := 99c := 11a := 11 2）相等条件测试 1234567891011121314151617181920212223#includeusing namespace std;int main()&#123; int a=0,b=0,c=0; __asm&#123; mov eax,100; mov a,eax &#125; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; b = 99; c = 99; __asm&#123; mov eax,99 mov ebx,777 cmpxchg c,ebx mov a,eax &#125; cout &lt;&lt; "b := " &lt;&lt; b &lt;&lt; endl; cout &lt;&lt; "c := " &lt;&lt; c &lt;&lt; endl; cout &lt;&lt; "a := " &lt;&lt; a &lt;&lt; endl; return 0;&#125; 输出:(如果相等，第2操作数（源操作数）的值装载到首操作数，zf置1) 1234a := 100b := 99c := 777a := 99 在x86平台上，CPU提供的硬件原子操作分为三种独立的原子锁机制：原子保证操作、加LOCK指令前缀和缓存一致性协议。我们这里只讨论软件原子锁机制，硬件原子锁有兴趣的详见英特尔的软件开发者手册《Volume 38.1LOCKED ATOMIC OPERATIO》章节。 软件级的原子操作实现依赖于硬件原子操作的支持。对于Linux而言，内核提供了两组原子操作接口：一组是针对整数进行操作；另一组是针对单独的位进行操作。针对整数的原子操作只能处理atomic_t类型的数据，这是一种抽象出来的整型数据类型结构，而没有使用C语言的int类型是因为：1）将原子操作涉及的数据与普通int类型数据进行区分；2）可以屏蔽不同硬件架构的差异性。而针对位的原子操作，在Linux内核中，原子位操作分别定义于include\linux\types.h和arch\x86\include\asm\bitops.h。 尽管Linux支持的所有机器上的整型数据都是32位，但是使用atomic_t的代码只能将该类型的数据当作24位来使用。这个限制完全是因为在SPARC体系结构上，原子操作的实现不同于其他体系结构：32位int类型的低8位嵌入了一个锁，因为SPARC体系结构对原子操作缺乏指令级的支持，所以只能利用该锁来避免对原子类型数据的并发访问。 原子操作在DPDK代码中的定义都在rte_atomic.h文件中，主要包含两部分：内存屏蔽和原16、32和64位的原子操作API。 rte_mb（）：内存屏障读写API rte_wmb（）：内存屏障写API rte_rmb（）：内存屏障读API 这三个API的实现在DPDK代码中没有什么区别，都是直接调用__sync_synchronize()。比如：在virtio_dev_rx()函数中，在读取avail-&gt;flags之前，加入内存屏障API以防止乱序的执行： 1234567*(volatile uint16_t *)&amp;vq-&gt;used-&gt;idx += count; vq-&gt;last_used_idx = res_end_idx; /* flush used-&gt;idx update before we read avail-&gt;flags．*/ rte_mb(); /* Kick the guest if necessary．*/ if (！(vq-&gt;avail-&gt;flags &amp; VRING_AVAIL_F_NO_INTERRUPT)) eventfd_write(vq-&gt;callfd, (eventfd_t)1); DPDK代码中提供了16、32和64位原子操作的API，以rte_atomic64_add()API源代码为例，讲解一下DPDK中原子操作的实现，其代码如下： 12345678910static inline void rte_atomic64_add(rte_atomic64_t *v, int64_t inc) &#123; int success = 0; uint64_t tmp; while (success == 0) &#123; tmp = v-&gt;cnt; success = rte_atomic64_cmpset((volatile uint64_t *)&amp;v-&gt;cnt, tmp, tmp + inc); &#125;&#125; 可以看到这个API中主要是使用了比较和交换的原子操作API函数rte_atomic64_cmpset()，里面通过嵌入汇编来实现，代码如下： 123456rte_atomic64_cmpset(volatile uint64_t *dst, uint64_t exp, uint64_t src) &#123; uint8_t res; asm volatile( MPLOCKED "cmpxchgq ％[src], ％[dst];" "sete ％[res];" ：[res] "=a" (res), [dst] "=m" (*dst) ：[src] "r" (src), "a" (exp), "m" (*dst) ："memory"); return res; &#125; 在现网VXLAN的数据包校验和错包统计中，就是通过加上原子操作的数据包统计才能实现多核场景下的准确性。 12345678910111213141516171819int vxlan_rx_pkts(struct virtio_net *dev, struct rte_mbuf **pkts_burst, uint32_t rx_count) &#123; uint32_t i = 0; uint32_t count = 0; int ret; struct rte_mbuf *pkts_valid[rx_count]; for (i = 0; i &lt; rx_count; i++) &#123; if (enable_stats) &#123; rte_atomic64_add( &amp;dev_statistics[dev- &gt;device_fh].rx_bad_ip_csum, (pkts_burst[i]-&gt;ol_flags &amp; PKT_RX_IP_CKSUM_BAD) ！= 0); rte_atomic64_add( &amp;dev_statistics[dev-&gt;device_fh].rx_bad_ip_csum, (pkts_burst[i]-&gt;ol_flags &amp; PKT_RX_L4_CKSUM_BAD) ！= 0); &#125; ret = vxlan_rx_process(pkts_burst[i]); if (unlikely(ret &lt; 0)) continue; pkts_valid[count] = pkts_burst[i]; count++; &#125; ret = rte_vhost_enqueue_burst(dev, VIRTIO_RXQ, pkts_valid, count); return ret; &#125; 自旋锁何谓自旋锁（spin lock）？它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，“自旋”一词就是因此而得名。 自旋锁必须基于CPU的数据总线锁定，它通过读取一个内存单元（spinlock_t）来判断这个自旋锁是否已经被别的CPU锁住。如果否，它写进一个特定值，表示锁定了总线，然后返回。如果是，它会重复以上操作直到成功，或者spin次数超过一个设定值。 锁定数据总线的指令只能保证一个指令操作期间CPU独占数据总线。（自旋锁在锁定的时侯，不会睡眠而是会持续地尝试）。其作用是为了解决某项资源的互斥使用。因为自旋锁不会引起调用者睡眠，所以自旋锁的效率远高于互斥锁，但是它也有些不足之处： 自旋锁一直占用CPU，它在未获得锁的情况下，一直运行——自旋，所以占用着CPU，如果不能在很短的时间内获得锁，这无疑会使CPU效率降低。 在用自旋锁时有可能造成死锁，当递归调用时有可能造成死锁，调用有些其他函数（如copy_to_user()、copy_from_user()、kmalloc()等）也可能造成死锁。 因此，要慎重使用自旋锁，自旋锁只有在内核可抢占式或SMP的情况下才真正需要，在单CPU且不可抢占式的内核下，自旋锁的操作为空操作。自旋锁适用于锁使用者保持锁时间比较短的情况。 Linux内核中的自旋锁API 在Linux kernel实现代码中，自旋锁的实现与体系结构有关，所以相应的头文件&lt;asm/spinlock.h&gt;位于相关体系结构的代码中。 在Linux内核中，自旋锁的基本使用方式如下： 先声明一个spinlock_t类型的自旋锁变量，并初始化为“未加锁”状态。在进入临界区之前，调用加锁函数获得锁，在退出临界区之前，调用解锁函数释放锁。比如： 1234spinlock_t lock = SPIN_LOCK_UNLOCKED; spin_lock(&amp;lock);/* 临界区 */ spin_unlock(&amp;lock); spin_lock函数用于获得自旋锁，如果能够立即获得锁，它就马上返回，否则，它将自旋在那里，直到该自旋锁的保持者释放。spin_unlock函数则用于释放自旋锁。此外，还有一个spin_trylock函数用于尽力获得自旋锁，如果能立即获得锁，它获得锁并返回真；若不能立即获得锁，立即返回假。它不会自旋等待自旋锁被释放。 自旋锁使用时有两点需要注意： 1）自旋锁是不可递归的，递归地请求同一个自旋锁会造成死锁。 2）线程获取自旋锁之前，要禁止当前处理器上的中断。 比如：当前线程获取自旋锁后，在临界区中被中断处理程序打断，中断处理程序正好也要获取这个锁，于是中断处理程序会等待当前线程释放锁，而当前线程也在等待中断执行完后再执行临界区和释放锁的代码。 Linux中自旋锁方法汇总如下： DPDK自旋锁实现和应用 DPDK中自旋锁API的定义在rte_spinlock.h文件中，其中下面三个API被广泛的应用在告警、日志、中断机制、内存共享和link bonding的代码中，用于临界资源的保护。 123rte_spinlock_init(rte_spinlock_t *sl)； // 初始化自旋锁rte_spinlock_lock(rte_spinlock_t *sl); // 获取自旋锁rte_spinlock_unlock (rte_spinlock_t *sl); // 释放自旋锁 rte_spinlock_t定义如下，简洁并且简单: 123456/** * The rte_spinlock_t type. */ typedef struct &#123; volatile int locked; /**&lt; lock status 0 = unlocked, 1 = locked */ &#125; rte_spinlock_t; 读写锁读写锁实际是一种特殊的自旋锁，它把对共享资源的访问操作划分成读操作和写操作。这种锁相对于自旋锁而言，能提高并发性，因为在多处理器系统中，它允许同时有多个读操作来访问共享资源，最大可能的读操作数为实际的逻辑CPU数。写操作是排他性的，一个读写锁同时只能有一个写操作或多个读操作（与CPU数相关），但不能同时既有读操作又有写操作。 读写锁除了和普通自旋锁一样有自旋特性以外，还有以下特点： 读锁之间资源是共享的：即一个线程持有了读锁之后，其他线程也可以以读的方式持有这个锁。 写锁之间是互斥的：即一个线程持有了写锁之后，其他线程不能以读或者写的方式持有这个锁。 读写锁之间是互斥的：即一个线程持有了读锁之后，其他线程不能以写的方式持有这个锁。 每个共享资源关联一个唯一的读写锁，线程只允许以下方式访问共享资源： 申请锁。 获得锁后，读写共享资源。 释放锁。 读写锁主要用于比较短小的代码片段，线程等待期间不能进入睡眠状态，因为睡眠/唤醒操作相当耗时，大大延长了获得锁的等待时间，所以读写锁适用的场景是要求忙等待。申请锁的线程必须不断地查询是否发生退出等待的事件，不能进入睡眠状态。 Linux中读写锁主要API函数 上述API函数的定义在各个Linux内核文件的&lt;asm/rwlock.h&gt;中。 DPDK读写锁实现和应用 DPDK读写锁的定义在rte_rwlock.h文件中， rte_rwlock_init（rte_rwlock_t*rwl）：初始化读写锁到unlocked状态。 rte_rwlock_read_lock（rte_rwlock_t*rwl）：尝试获取读锁直到锁被占用。 rte_rwlock_read_unlock（rte_rwlock_t*rwl）：释放读锁。 rte_rwlock_write_lock（rte_rwlock_t*rwl）：获取写锁。 rte_rwlock_write_unlock（rte_rwlock_t*rwl）：释放写锁。 读写锁在DPDK中主要应用在下面几个地方，主要用于对操作的对象进行保护。 在查找空闲的memory segment的时候，使用读写锁来保护memseg结构。LPM表创建、查找和释放。 Memory ring的创建、查找和释放。 ACL表的创建、查找和释放。 Memzone的创建、查找和释放等。 比如：查找空闲的memory segment的时候，使用读写锁来保护memseg结构的代码实例如下: 123456789101112/* Lookup for the memzone identified by the given name */ const struct rte_memzone * rte_memzone_lookup(const char *name) &#123; struct rte_mem_config *mcfg; const struct rte_memzone *memzone = NULL; mcfg = rte_eal_get_configuration()-&gt;mem_config; rte_rwlock_read_lock(&amp;mcfg-&gt;mlock); memzone = memzone_lookup_thread_unsafe(name); rte_rwlock_read_unlock(&amp;mcfg-&gt;mlock); return memzone; &#125; 无锁环形缓冲区高性能的服务器软件（例如，HTTP加速转发器）在大部分情况下是运行在多核服务器上的，当前的硬件可以提供32、64或者更多的CPU，在这种高并发的环境下，锁竞争机制有时会比数据拷贝、上下文切换等更伤害系统的性能。因此，在多核环境下，需要把重要的数据结构从锁的保护下移到无锁环境，以提高软件性能。现在无锁机制变得越来越流行，在特定的场合使用不同的无锁队列，可以节省锁开销，提高程序效率。Linux内核中有无锁队列的实现，可谓简洁而不简单。 Linux内核无锁环形缓冲 环形缓冲区通常有一个读指针和一个写指针。读指针指向环形缓冲区中可读的数据，写指针指向环形缓冲区中可写的数据。通过移动读指针和写指针就可以实现缓冲区的数据读取和写入。在通常情况下，环形缓冲区的读用户仅仅会影响读指针，而写用户仅仅会影响写指针。如果仅仅有一个读用户和一个写用户，那么不需要添加互斥保护机制就可以保证数据的正确性。但是，如果有多个读写用户访问环形缓冲区，那么必须添加互斥保护机制来确保多个用户互斥访问环形缓冲区。具体来讲，如果有多个写用户和一个读用户，那么只是需要给写用户加锁进行保护；反之，如果有一个写用户和多个读用户，那么只是需要对读用户进行加锁保护。 在Linux内核代码中，kfifo就是采用无锁环形缓冲的示例，kfifo是一种“First In First Out”数据结构，它采用了前面提到的环形缓冲区来实现，提供一个无边界的字节流服务。采用环形缓冲区的好处是，当一个数据元素被用掉后，其余数据元素不需要移动其存储位置，从而减少拷贝，提高效率。更重要的是，kfifo采用了并行无锁技术，kfifo实现的单生产/单消费模式的共享队列是不需要加锁同步的。详情可以参考Linux内核代码中的kififo的头文件（include/linux/kfifo.h）和源文件（kernel/kfifo.c）。 DPDK无锁环形缓冲 基于无锁环形缓冲的的原理，Intel DPDK提供了一套无锁环形缓冲区队列管理代码。支持单生产者或者多生产者入队列，单消费者或多消费者出队列。下面会记录dpdk中是如何管理所有使用的无锁环形缓冲区以及无锁环形缓冲区中支持的一些操作。 DPDK中的rte_ring的数据结构定义，可以清楚地理解rte_ring的设计基础。 123456789101112131415161718192021222324252627282930313233343536373839404142/* * An RTE ring structure. * * The producer and the consumer have a head and a tail index．The particularity * of these index is that they are not between 0 and size(ring)．These indexes * are between 0 and 2^32, and we mask their value when we access the ring[] * field．Thanks to this assumption, we can dosubtractions between 2 index * values in a modulo-32bit base：that's why the overflow of the indexes is not * a problem. */ struct rte_ring &#123; char name[RTE_RING_NAMESIZE]; /**&lt; Name of the ring．*/ int flags; /**&lt; Flags supplied at creation．*/ /** Ring producer status．*/ struct prod &#123; uint32_t watermark; /**&lt; Maximum items before EDQUOT．*/ uint32_t sp_enqueue; /**&lt; True, if single producer．*/ uint32_t size; /**&lt; Size of ring．*/ uint32_t mask; /**&lt; Mask (size-1) of ring．*/ volatile uint32_t head; /**&lt; Producer head．*/ volatile uint32_t tail; /**&lt; Producer tail．*/ &#125; prod __rte_cache_aligned; /** Ring consumer status．*/ struct cons &#123; uint32_t sc_dequeue; /**&lt; True, if single consumer．*/ uint32_t size; /**&lt; Size of the ring．*/ uint32_t mask; /**&lt; Mask (size-1) of ring．*/ volatile uint32_t head; /**&lt; Consumer head．*/ volatile uint32_t tail; /**&lt; Consumer tail．*/ #ifdef RTE_RING_SPLIT_PROD_CONS &#125; cons __rte_cache_aligned; #else &#125; cons;#endif#ifdef RTE_LIBRTE_RING_DEBUG struct rte_ring_debug_stats stats[RTE_MAX_LCORE];#endif void * ring[0] __rte_cache_aligned; /**&lt; Memory space of ring starts here. not volatile so need to be careful dering */&#125;; 从上面的定义可以看出，无锁环形缓冲区对象中定义了一个生产者对象和一个消费者对象，对应的也就是缓冲区的写对象和读对象。另外，也可以看出无锁环形缓冲区在内存中的组织形式是前面是无锁环形缓冲区对象本身，然后紧接着就是实际用于存储内容的环形队列，在某一时刻其内存布局如下图所示： 无锁环形缓冲区是一种通用的数据结构，所以可能会在多个地方使用，在dpdk中就会有多种情况会使用，比如内存池。所以，随之而来的一个问题就是dpdk如何管理其所使用的所有的无锁环形缓冲区？从它的源码实现(rte_ring.c/rte_ring.h)中我们可以找到答案，与此相关的部分源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647TAILQ_HEAD(rte_ring_list, rte_tailq_entry);static struct rte_tailq_elem rte_ring_tailq = &#123; .name = RTE_TAILQ_RING_NAME,&#125;;EAL_REGISTER_TAILQ(rte_ring_tailq)struct rte_ring *rte_ring_create(const char *name, unsigned count, int socket_id,unsigned flags)&#123; char mz_name[RTE_MEMZONE_NAMESIZE]; struct rte_ring *r; struct rte_tailq_entry *te; const struct rte_memzone *mz; ssize_t ring_size; int mz_flags = 0; struct rte_ring_list* ring_list = NULL; int ret; ring_list = RTE_TAILQ_CAST(rte_ring_tailq.head, rte_ring_list); /* get the size of memory occupied by ring */ ring_size = rte_ring_get_memsize(count); …… te = rte_zmalloc("RING_TAILQ_ENTRY", sizeof(*te), 0); if (te == NULL) &#123; RTE_LOG(ERR, RING, "Cannot reserve memory for tailq\n"); rte_errno = ENOMEM; return NULL; &#125; rte_rwlock_write_lock(RTE_EAL_TAILQ_RWLOCK); mz = rte_memzone_reserve(mz_name, ring_size, socket_id, mz_flags); if (mz != NULL) &#123; r = mz-&gt;addr; /* no need to check return value here, we already checked the arguments above */ rte_ring_init(r, name, count, flags); /* 低维尾队列的entry存放着rte_ring的管理对象地址 */ te-&gt;data = (void *) r; r-&gt;memzone = mz; /* 将存放着环形缓冲区对象的尾队列entry插入到低维尾队列的末端 */ TAILQ_INSERT_TAIL(ring_list, te, next); &#125; else &#123; r = NULL; RTE_LOG(ERR, RING, "Cannot reserve memory\n"); rte_free(te); &#125; rte_rwlock_write_unlock(RTE_EAL_TAILQ_RWLOCK); return r;&#125; 从上面的源码我们可以知道，dpdk是用尾队列来管理其所使用的所有无锁环形缓冲区的，也就是说一个尾队列中的元素就是一个无锁环形缓冲区对象。那用来管理所有无锁环形缓冲区的尾队列，dpdk又如何管理呢？在函数rte_ring_create()中可以看到管理着无锁环形缓冲区的尾队列头部是存放在一个类型为struct rte_tailq_elem的全局变量rte_ring_tailq的head成员中，其中struct rte_tailq_elem定义如下： 12345struct rte_tailq_elem &#123; struct rte_tailq_head *head; TAILQ_ENTRY(rte_tailq_elem) next; const char name[RTE_TAILQ_NAMESIZE];&#125;; 从struct rte_tailq_elem的定义可以看到，管理着无锁环形缓冲区尾队列的头部是另外一个尾队列的一个元素，类似的管理方式还用在了dpdk的内存池等数据结构中，所以在dpdk中采用了两级尾队列来管理所使用的数据结构。其实这样说也不完整，因为除了用二级尾队列来管理所使用的数据结构之外，dpdk还用了一个全局共享内存中的列表数组rte_config.mem_config-&gt;tailq_head[RTE_MAX_TAILQ]来分别存储这个二级尾队列中低维尾队列存放的元素，即管理某一种特定数据结构的尾队列头部。 接下来对dpdk中实现的无锁环形缓冲区所支持的操作做一个说明。从一开始说过无锁环形缓冲区支持单生产者或者多生产者入队列，单消费者或多消费者出队列等操作。其实从另外一个角度还可以说dpdk中的无锁环形缓冲区中支持两种出入队列的模式，即出入队列元素数目固定模式和尽力而为模式，出入队列元素数目固定模式就是说只有进入队列的元素数目达到指定数目才算操作成功，否则失败；而出入队列元素数目尽力而为模式就是说对于指定的数目，如果当时队列状态并不能满足，则以当时队列状态为准，尽可能满足指定的数目。比如：如果参数指定需要入队列3个元素，但队列中只剩下2个空闲空间，那么就将其中2个元素入队列，出队列情况同理。 下面以两个CPU核同时往队列各写入一个元素来介绍无锁环形缓冲区支持的多生产者入队列功能，其他的操作方式都可以从这里推演出来。在代码中多生产者入队列相关的函数为__rte_ring_mp_do_enqueue()，下面的流程也是根据这个函数整理出来的。 初始状态下生产者的头和尾指向了同一个位置。如下图所示： 1）在两个核上，将r-&gt;prod.head和r-&gt;prod.tail分别拷贝到本地的临时变量prod_head和prod_tail中，然后将本地临时变量prod_next指向队列的下一个空闲位置。检查队列中是否有足够的空间，如果没有，则返回失败（如果是写入多个元素，没有足够剩余空间的话，则需要看指定的模式，如果是尽力而为模式，则尽可能往队列中写入元素；否则返回失败）。如下图所示： 2）使用CAS操作指令将本地变量prod_next的值赋值给r-&gt;prod.head，即两者指向同一个位置。CAS操作指令有如下特性：如果r-&gt;prod.head不等于本地变量prod_head，则CAS操作失败，代码重新从第一步开始执行；如果r-&gt;prod.head等于本地变量prod_head，则CAS操作成功，代码继续往后执行。在这里我们假定在核1上操作成功，那么对于核2该操作就会失败，核2会从第一步重新执行。如下图所示： 3）核2上的CAS操作成功，核1上成功往环形缓冲区中写入了一个元素（obj4），接着核2也成功往环形缓冲区中写入了一个元素（obj5）。如下图所示： 4）在上一步中两个核都已经成功往环形缓冲区写入了一个元素，现在两个核都需要更新r-&gt;prod.tail。这里又有一个条件，就是只有r-&gt;prod.tail等于本地变量prod_head的核才能去更新r-&gt;prod.tail的值。从图中可以看到，目前只有在核1上才能满足这个条件，核2不满足，因此更新r-&gt;prod.tail的操作只在核1上进行，核2需要等待核1完成。如下图所示： 5）在上一步中，一旦核1完成了更新r-&gt;prod.tail的操作，那么核2也能满足更新r-&gt;prod.tail的条件，核2此时也会去更新r-&gt;prod.tail。如下图所示： DPDK中的无锁环形缓冲区还有另一个特性，那就是充分利用了unsigned类型的回绕特点，这样对于缓冲区中已用空间和剩余空间的计算就得到了极大的简化，也使得生产者头和尾、消费者头和尾的下标值不局限在0和size(ring) – 1之间，只要在0和2^32 - 1范围之内即可。这一点可以参考dpdk的开发者文档，其具体实现也包含在了上节介绍的操作流程当中，感兴趣的可以去看下。参考文献http://dpdk.org/doc/guides/prog_guide/ 以上部分是DPDK技术栈在电信云领域CPU密集性操作的优化技术介绍，后面第三篇部分将开始DPDK技术栈在电信云领域IO密集性操作的优化技术介绍和NFV的应用场景。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-28-未来网络如何重构]]></title>
    <url>%2F2019%2F04%2F29%2F2019-04-28-%E6%9C%AA%E6%9D%A5%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[写在前面的话虽然本人因为出差关系没有参加公司组织的现场培训，但是通过自学相关材料还是想谈一谈自己的一点浅见。 与其说是对5G认识，不如更广泛地将其定义为“未来网络”这个概念，来谈谈对未来网络的一点儿浅见；与其说是网络转型，不如更彻底说是网络重构；与其说是学习心得，不如说是我自己的思考，尤其是对现有网络如何重构的一点儿思考。 下面我将按照自己学习新东西的“3W”原则（即，What？Why？How？）做一具体阐述: 首先放张本文主体思路的架构图，让大家先有一点儿感性认识。接下来，开始进入正题。 What？—网络重构的目标和趋势是什么？什么是未来网络？广义上来说，未来网络应该是网络质量足够好、响应速度足够快、组网足够灵活、覆盖足够广、能能够连接人和万物、性价比足够好的庞大基础设。要满足上述复杂多变的要求，运营商必须先打破现在刚性网络架构的制约，构筑一个简洁、开放、敏捷和集约的新型网路架构。 狭义上来说，未来网络是以云计算、SDN和NFV为三大技术支柱的网络架构。云计算打破了网络与IT资源分离的局面，构筑了统一云化的虚拟资源池；SDN打破了控制与转发一体的封闭网路架构，实现网络软件可编程；NFV打破了软硬件一体的封闭网元架构，实现了网络资源虚拟化。 网络重构的背景是什么？现有的网络架构是在电话网时代建立的，是以本地电话网为基础、按照行政区域规划构建的省-市-县的分层分域管理体系。 随着云计算、大数据、“互联网+”、物理网、虚拟现实、人工智能等新技术与新业务的出现和加速发展，网络应用由消费型向生产型扩展，网络的连接也由人人互联向万物互联延伸，信息通信业面临新的机遇和挑战，开放、创新、融合成为重要趋势。而我们现有网络一直以来针对各类业务问题总是以出台具体解决方案的方式来解决，不仅各业务解决方案间不兼容，而且长期积累后形成一套十分复杂且僵化的业务流程，同时其烟囱式的网络架构在面对全新业务时变得越来越力不从心，这使得现有网络架构必须重新审视，重新定义，重新设计，重新架构。 运营商网络重构的终极目标是什么？国内三大运营商—中国移动、中国电信和中国联通分别提出了网络重构的战略目标—NovoNet2020、CTNet2025和CUBE-Net2.0，核心是构建能够主动适应新业务发展需求的智能化柔性网络，实现网络基础设施巨大价值的重新释放。 而个人认为网络重构的战略目标应该是致力于实现根本性的转变：一是从互联网+被动地适应网络转变为网络主动、快速、灵活地适应网络应用；二是从传统的“烟囱式”分省、分专业的网络转变为“水平整合”的扁平化网络；三是从分层次、分专业基于中心端局（CO）的组网模式转变为以数据中心（DC）为核心的组网模式。 未来网络的演进趋势是什么？网络作为我们的核心资源，也是我们实现战略转型的主要抓手。随着IT与CT融合，未来网络的演进趋势将有以下几个特征。 1) 网络以DC（即Data Center，数据中心）为中心 随着云计算、大数据等新技术的快速应用，网络的流量、流向已经发生了巨大变化。未来80%甚至更多的应用将部署在云上，DC正逐渐成为网络流量的中心。而现有的网络一般是以省-市-县的划分来组织，在这种逐级收敛的树状架构中，DC仅仅作为一种普通的接入点。因此，必须调整为以DC为中心的新型架构，以适应网络流量及流向变化的新趋势。 2) 网络和云深度融合 云计算的不断发展对网络带宽提出了越来越高的要求，用户需求的不断升级使得云网融合成为必然趋势。业务、IT和网络都可以基于云化技术实现和部署，但是当前云与网之间缺乏灵活互动的机制。比如：在现有的厂家给出的云网融合解决方案中，网络资源是随计算资源同步分配的，即NaaS服务打包在IaaS服务中一起提供给用户，相对于客户或业务的不同需求，两种资源调配存在一定的浪费。个人认为随着容器技术兴起，云化的网络资源池在提供计算、存储等虚拟化资源的同时，网络资源也可以随云资源池的需求而按需变动，即将NaaS从IaaS中拆分独立出来，通过SDN和NFV的跨域协同，真正实现计算、存储和网络资源的统一动态分配和调度，实现云与网的深度协同。 3) 网络功能的软件化 现有的网络以专有硬件设备为主，网络调整以及功能升级的周期很长，且成本较高，完全无法适应新兴业务的快速、灵活的特点。 随着SDN技术逐步成熟、北向接口标准化的制定，将进一步实现三层解耦，甚至通过定义东/西向接口的标准化来进一步解耦VNFM与VNF之间的对接，利用通用的硬件平台为功能网元提供统一的虚拟化运行环境，上层的网元功能按照应用层的模式统一通过软件来实现，软件的升级和更新不再与硬件绑定。一方面实现了网络容量的按需动态伸缩；另一方面利于业务的快速部署和升级，面对不同客户的不同业务需求也可灵活定制，也就是常说的切片网络概念。 4) 集中管控、灵活智能 由于现有网络以行政区域和地理位置划分，受此影响现有网络的管理的也是分段、分级进行。一个跨省链路的开通往往需在在十几个系统上制作几百条甚至上千条数据，同时还要历经多级流程的审批、确认，导致网络服务存在响应慢、资源利用率低下、端到端体验差等弊端。近期最典型的的例子就是江苏的全国虚假主叫拦截平台的升级，从2017年6月各省反馈误拦截VoLTE用户呼叫问题开始，一直到现在都因为跨省升级测试流程未完成而没法部署，各省目前只能采取临时规避措施进行规避。 网络功能软件化后，网络的控制与调度均可通过软件来实现，通过控制与转发分离等方式，使得网络的集中控制成为可能，从而可以更好地实现网络的敏捷部署及灵活调整，提供端到端的业务保障，满足客户全网一致性的体验要求。 Why？—我们为什么要进行网络重构？我们现有网络面临什么样的挑战？近年来，随着网络承载业务的富丰，尤其互联网IT企业推出的业务体验逐渐与运营商推出的业务体验同一化，迫使运营商在当前网络运营中面临一系列挑战。 1) 网络连接数和流量增长推动网络规模快速膨胀 近期随着不限量套餐的普及，物联网行业兴起和发展，集团大连接战略布局，未来将有海量的设备和用户接入网络，连接将变得无处不在。宽带从连接十几亿将增长到几百亿，同时宽带流量将有10倍以上的增长。家庭千兆以及个人百兆服务将成为普遍服务，而一些新业务（如4K/8K视频、虚拟现实游戏、无人驾驶等）对网络丢包率、时延等QoS要求更苛刻。 2) 业务云化和终端虚拟化将颠覆网络全局的流量模型 随着云计算的发展，私有云、公有云的逐步普及，必将推动大规模的移动网络建设，用户对宽带的需求必将从基于覆盖的连接，转向基于内容和社交体验的连接。现有网络业务流量主要服务于网络终端节点之间的通信，符合泊松分布模型。但是，现在随着内容和社交体验业务对流量和流向进行牵引，导致业务流量难以预测。因此必须以数据中心（DC）为主要的流量生产和分发中心，且要呈现无尺度分布的特征，而我们现有的网络部署架构是与之并不匹配的。 3) 专用网络和专用设备极大增加网络经营压力 随着固移融合业务发展，固定和移动网络覆盖范围的扩大，网络规模日趋庞大。比如中国移动现有2/3/4G/固网的融合是全球最庞大、最复杂的一张网络。这就导致网络服务需要由具有不同功能的多个专业网络组合提供，各专业网络彼此之间条块化分割，能力参差不齐，业务的端到端部署和优化困难，导致新业务的创新乏力以及响应滞后，无法满足互联网+时代应对业务的动态请求。比如最近为了解决苹果手表呼叫时无法回落的问题，全国所有省份核心设备、接入设备和网关设备均在大范围改造升级，已经历时3个多月还未全部完成。 4) 互联网+业务创新加快驱动网络智能化转型 以亚马逊、阿里和腾讯为首的互联网IT巨头借助运营商管道资源，加快部署自己的云化业务服务，不仅可以及时洞察用户需求，实时响应客户需求，而且可以提供更加智能、弹性的网络服务。比如我个人前期浏览过阿里的EC云服务器资源，只过了一天就有阿里的专有客户经理来电咨询我的业务需求，拿到我的答复后1小时内就给出让我满意的解决方案。这种灵活性、高效性和专业性以我们目前的网络能力，是难以胜任的。 IT行业发展经验给了我们哪些启示？既然提到IT行业，必然离不开摩尔定律。摩尔定律其实揭示了两个发展方向：高性能和低成本。作为运营商的我们更重视前者，代表性就是要求电信级的质量；而IT企业更重视后者，通过采用适度性能要求、宽松可靠性要求的通用IT设备来进行低成本解决方案部署。 其实，IT行业在20世纪70年代以前也是一个封闭的“烟囱式”架构群，但从80年代开始就打破了这种封闭的垂直架构（标志性事件就是小型机向X86转型），转向开放的水平架构。其基本思路就是我们今天谈到的“软硬件解耦”（标志性事件就是操作系统从设备中分离），双方分工明确，各自发展，从而也就诞生了今天的云计算（云计算其本质上就是一个分布式的、借助虚拟化技术实现、与硬件无关的linux操作系统）。 而对性能追求更高的我们，通常采用电信级网络设备，因此一直保持着专业化和高成本的特点。设备仍然是软硬件一体化的垂直架构，整个生态环境较为封闭（有限的电信设备厂商控制着从硬件、软件一直到业务实现的生产过程），不仅设计复杂、成本高、升级难、而且不同厂家之间兼容性不好。比如：爱立信EPC设备、卡特IMS设备、诺西HSS设备经现网验证均存在不同程度的兼容性问题。这也就直接造成我们的网络改造难、升级慢、维护成本高、开放性差、灵活性不足，同时也限制了我们在网络和业务上的创新。 随着SDN、NFV技术的引入，我们的网络设备封闭性有望打破（注意，我这里只用了有望一词，而不是必须，至于原因是因为VNFM与VNF之间的东西向接口仍是私有接口还未标准化，因此VNFM和VNF必然是同厂家组网，这其实也是一种设备封闭性问题），软硬件解耦，产业链生态走向开放，不仅有利于降低CAPEX和OPEX，更有利于实现网络开放和弹性，促进新型网络和业务的创新。 现有网络为何无法满足新兴业务的需求？在中国甚至全球，5G从提出就是被提到国家战略的地位，这将进一步推动云计算、大数据、移动互联、物联网等与现代制造业和服务业相结合，对运营商的基础设施提出了更高的要求。为了顺应新环境下业务的发展需求，我们的基础网络改造势在必行，特别是作为网络灵魂的网络架构必须重新定义、重新设计，构建新型的泛在、敏捷、按需的智能型网络，提升公共服务水平。 从顺应业务需求的角度来看，我们主要面临以下几个方面的挑战。 1) 全面“流量经营”时代，我们面临超大流量对网络挑战 流量经营已经获得广泛认可，随着不限量业务的普及以及物联网的兴起，直到现在流量对网络的挑战才刚刚开始。最近热门的4K、8K等富媒体技术将进一步激发流量的爆发增长，而AR/VR的发展普及将会把流量增长推向新的高潮。很多行业、部门都在预测流量趋势，包括中国移动自己的大数据平台也在做相关工作。从预测数据和历史经验看，流量的规模将更加刺激我们对宽带化的发展要求，但其背后对网络容量的压力和挑战也是巨大的。如果还是像现在这种“搭积木”方式进行简单扩容来解决，那以后势必难以为继，自动化、弹性化的重构势在必行。 2) 万物互联到万物智联既是新机遇也是新挑战 万物互联到万物智联是实现“工业互联网”的重要基础。一方面带来巨大的连接规模，为我们运营商提供广泛的业务增长；另一方面也带来了更大的连接广度和深度，需要我们提供面向无线、有线全连接的接入广度，更对我们的网络提出高密度、低时延、广覆盖的个性化高要求。所以，我们的网络能不能适应万物智联，能不能承担“工业互联网”的发展重任，将是决定我们转型成败的关键，而现有网络模式势必进行智能化重构。 How？—我们如何拥抱网络重构？我们期望重构后的网络特征和架构是什么样？1) 重构后的网络应该具备以下关键特征 结构简化：网络层级、种类、类型等尽量减少，降低运营和维护的复杂性和成本，也有助于业务和应用的保障能力提升。比如：通过层级简化，业务路由优化，在全国90%的地方实现不大于30ms的业务传输网时延，这也是5G时代“三朵云”中接入云设计的初衷。 灵活高效：网络通过软件定义的方式具备弹性可伸缩的能力，实现业务的快速部署和扩/缩n 容。比如：面向客户的VIP网络可以提供分钟级的配套开通和调整能力，使得客户按照需求来随时调整网络连接，这也是切片网络概念引入的初衷。 集中控制：通过软件定义的方式实现网络的集中控制，打破目前分级、分层、分段的管理模式，实现面向全局的最优化网络管理，为客户提供全网一致性体验。比如：一点受理全国性的跨域VPN业务，并能实现即时开通。 泛在安全：一方面满足客户无论何时何地的无缝接入，另一方面通过安全防护的配套建设对承载客户信息实现安全保障。 2) 重构后网络应该具备以下参考架构 要实现重构后网络的上述特征，我们希望未来网络的架构简化为三层组网架构，如下图所示。 基础设施层：分为3类资源。第一类是可虚拟化的通用基础设施，一般由云资源池提供，之上承载各类虚拟化的网元；第二类是可以将控制和转发进行分离的专用基础设施，其控制层可以抽象出来由上层SDN控制器直接进行管理；第三类是高性能专用设施，一般指无法升级改造的传统设备，依靠现有的传统网管进行管理。 网络功能层：主要面向软件化的网络功能实现，结合虚拟资源和物理资源的管理系统/平台，实现逻辑功能和网元实体的分离，便于资源的集约化调度管控。其中，云管理平台主要负责对虚拟化资源的的管理协同，包括计算、存储和网络的统一管控；VNFM主要负责对基于NFV实现的虚拟网络功能的管控；SDN控制器实现基础设施的管控。 协同编排层：主要提供对网络功能协同和对业务能力的编排，以及对上层应用的接口和能力开放。其中，网络协同和业务编排主要负责向上对业务需求的网络进行语言翻译和能力封装，向下对网络功能层的不同系统和网元进行协同，保障网络端到端打通。IT系统和业务平台则主要服务于网络资源的标准化封装，支持各类标准化API的调用。 重构后的网络与我们现有网络相比，主要改变在以下几点： 硬件通用化：绝大多数功能网元都是通过标准化的云资源池进行承载，除了少数采用专用设施的设备。 功能软件化：网元功能与底层硬件完全解耦，主要以软件的形式存在，充分发挥弹性、灵活和敏捷的特征。 管控集中化：各网元的控制部分进行剥离，由上层云管理平台、VNFM、SDN控制器、传统网管进行管理，并由上层协同编排器进行集中协调与控制，更加体现了全程全网以及端到端的概 能力标准化：标准与开源并存。标准是为了满足服务的规模化和普适化，开源是为了实现服务的创新性和开放性。因此，对网络能力封装不仅要制定完善相关技术的标准框架，更要借助开源社区新的技术能力来完善新的标准化推动。制定与之对应的、满足生产需求，提升生产效率的标准化接口和协议（这一点也是中国移动集团从开始推出NFV试点以来一直致力要做的事情），与上层业务及应用通过API进行互动，使得网络不再仅仅是哑管道，而是能够及时感知业务需求并能随之进行灵活调整的开放式能力平台。 网络重构过程中我们需要具备哪些关键技术？SDN：软件定义网络 SDN主要将网络的控制平面与数据转发平面进行分离，采用集中控制替代现有的分布式控制，并通过开放的可编程的接口实现“软件定义”的网络架构。SDN是IT化的网络，是“软件主导一切”的趋势从IT产业向网络领域延伸的标志性技术，其核心就是网络的“软化”。 SDN的标准架构就是俗称的“三层两接口”，其实这种架构并不是SDN所独有，在现有网络中如VoLTE网络的核心网IMS也是一个标准的“三层两接口”架构，其目的就是实现转控分离。别忘了，IMS可是转发、控制和业务三层完全分离的一个架构体系。：） SDN的核心特点是将实体设备作为基础资源，抽象出NOS（网络操作系统），隐藏底层物理细节并向上层提供统一的管理和编程接口。以NOS为平台，开发的应用程序可以实现通过软件定义的网络拓扑、资源分配和处理流程及机制等。 SDN的技术重点在南/北向接口标准化，南向接口已经实现了标准化定义，统一采用openflow协议，而北向接口虽然业内共识采用REST ful协议，大家都号称支持REST ful协议，但是对接起来仍然问题百出。因此，更需要一个组织或机构来明确这层接口实现的各种细节，同时形成标准化规范，大家共同遵循开发。通过下面的简图，大家可以对SDN有个初步感性认识。 NFV：网络功能虚拟化* NFV其本质就是实现硬件资源和软件功能的解耦，其最终目标是通过标准X86服务器、存储和交换设备来取代现有网络中的私有专用网元。主要包括：NFVI、VNF、MANO和OSS/BSS四个逻辑层面。关键特征包括上层的业务云化、底层硬件标准化、分层运营和加快业务的上线与创新。 其中，NFVI就是典型云计算平台，主要实现对底层物理设备资源进行计算、存储、网络虚拟化呈现。VNF是软件实现的虚拟网元功能，其利用NFVI创建的虚拟化资源，在其上通过软件编程的方式实现各类网元的功能。MANO是业务编排层，主要用于整体的编排和控制管理，将网络服务从上而下进行业务层到资源层逐步分解和调度（主要通过NFVO、VNFM和VIM三个子模块互相配合、调度NFVI层资源实现）。OSS/BSS与传统网络功能类似，主要实现业务的发放，计费、网络管理和营帐等功能。（其实现在随着K8s技术的兴起，这一层也完全可通过云调度的方式呈现。） 通过对比SDN技术我们可以发现，NFV技术其本质也是一个“三层两接口”的架构模型，可以说它是SDN技术一个衍生品（个人观点：））。但是，它与SDN技术主要区别还是技术呈现侧重点的区别，NFV更偏重网络功能的软件化，在控制转发层面也可以不依赖于SDN技术通过专用物理设备也能实现，这也是现阶段NFV网络建设的基本模式。而SDN技术更偏重控制和转发层面，不看重网元功能是否软件化实现。如一台具备openflow协议物理交换机也可在SDN网络中承担相应的转发功能，同样它也能抽象出对应网络抽象层由SDN控制器来实现调度。比如山东省内济南和青岛两地市的DC大二层互通就是通过大量这样的物理交换机来实现转发控制。个人判断随着SDN技术的逐步演进，网络重构过程初期这类大量物理交换机会逐渐被openflow软件交换机替换。国际惯例，同样下面放张图让大家对NFV的架构有个初步感性认识。 云计算：OpenStack开源云—王者归来* 在现阶段云计算技术在网络重构过程中主要作用是各种硬件资源的虚拟化呈现。对于未来网络的维护人员或者DevOps人员掌握和熟悉云计算是必不可少的一项基础技能。NFV技术中NFVI层使用的OpenStack就是一个典型的云计算架构，同样SDN中控制器OpenDayLight也是一种云计算的架构，现在OpenStack社区在Pike版本和Queens版本已经将这两种云计算架构进行融合，也就是说在OpenStack服务中中包含了OpenDayLight功能。（相比于OpenDayLight，OpenStack功能更强大，最重要是完全符合生产条件，华为云化设备底层也是采用OpenStack的H版本搭建，中兴同样）。而在我自己的实验环境也分别通过手工搭建和容器部署的方式实现这两种架构的融合。 OpenStack的原理主要通过各种服务的交互来实现业务的调用和流程分发。其核心思想就是对虚拟机VM进行各类操作控制来实现物理设备的功能。其基本服务包括Keystone（认证服务）、Nova（计算服务）、Glance（镜像服务）、Neutron（网络服务）、Cinder（虚拟块存储服务）、Swift（虚拟对象存储服务）、Heat（云编排服务）和Horizon（人机交互Web服务），对于我们运营商可能还会用到Ceilometer（流量监控、计费服务）。除了这些基础服务外，还有很多负载均衡、密钥共享存储以及开源NFV等高级服务，这里不再一一列举。OpenStack的各类服务之间通过一种叫消息队列（Queus）公共服务来进行异步通信，因此各类服务不仅可以部署同一台物理服务器上，也可采用高可用集群的方式在多个物理服务器上分布式部署。 我在外面学习时，很多外省的学员甚至华为的老师都在询问云计算怎么学习，我的答案是学习云计算的前提条件需要具备一定操作系统原理知识、虚拟化技术知识和linux操作系统基础知识。最好的学习方式就是自己搭建一套实验环境一遍学习理论、流程，一边动手实践。由于个人时间实在有限，特意在我个人微信公众号上写了一篇实验环境的搭建流程供各位初学者参考学习。下面还是放上一张OpenStack的原理图，让大家对云计算有个初步感性认识。 我们如何紧跟网络重构的步伐进行运营转型升级？网络重构对我们现有的网络架构将进行颠覆性的变革，将促进我们的网络布局由传统的电信机房（CO）向数据中心架构（DC）转变，网元部署形态由软硬一体化的专用设备向基础设施的通用化和虚拟化以及网元功能的软件化方向转变，实现对网络的集中、跨层、跨域控制，这与我们现有网络采用分专业、分层、分域的规划、建设和运营管理模式截然相反。未来网络的这种变化将对我们现有业务模式、运营模式、管理模式、人才培养模式都提出了巨大的挑战，将迫使我们改变现有观念，积极拥抱变革。 1) 业务模式的转型升级 业务需求是技术创新的原动力，同时技术创新也将激发新业务的增长。按照集团“大连接”战略，我们基础设施的建设布局、基础技术的演进，网络系统的革新都是以支撑数字化新服务的智能应用为核心，秉承“连接无限可能”的目标来逐步实施。 未来，我们面向的对象包括人、物、企业和信息，将提供他们自身及之间的沟通、连接服务。个人理解，可能主要包括十大类数字化业务模式： 人-人：未来通信业务。—通过VoLTE/VoNR技术，以通信服务提供商的角色将提供随时、随地、即时、高效、高感知、无障碍的人与人之间的通信服务。 人-信息：内容信息业务。—通过内容运营平台，以内容提供商的角色，将提供引人入胜、引领潮流、多方共赢、私人定制的数字娱乐服务。 企业-企业：企业信息化业务。—通过企业网布局，以移动ICT生态服务商的角色，将提供安全可靠、高效、低成本、可灵活定制的企业级移动ICT服务。（这一点华为已经走在了我们前面，未来我们和华为将是竞争对手关系） 企业-人：行业应用业务。—通过跨领域合作，主要针对教育、医疗、金融行业以行业产品服务提供商的角色，提供方便、实惠、开放的跨界民生服务。 企业-信息：能力开放业务。—通过大数据平台运营和信息处理的模式，以大数据信息处理专家的角色，提供丰富、精选、定制化、实时的数据能力开放服务。（这一点阿里等互联网IT厂商已经走在了我们前面） 物-物：智能物联网业务。—通过物联网基础服务平台，以物理网运营专家的角色，提供无处不在、无所不能、自动化、低成本的万物互联服务。 物-人：智能家居业务。—通过智能CPE、机顶盒、行车记录仪等智能终端产品，以解决方案提供商的角色，将提供亲切、便捷、个性化、智能化的人物交互服务。 物-信息：社交化物联网业务。—通过社交化物联网平台，以社交化服务提供商的角色，提供主动关注、动态建圈、自动沟通的社交化物联网服务。 物-企业：产业信息化业务。—通过与传统产业融合，助力产业升级，实现产业再造，以服务和解决方案提供商的角色，将提供安全、低成本、大规模、可灵活定制的产业信息化服务。 信息-信息：数据资源提供业务。—通过大数据、人工智能平台，以数据服务商的角色提供无所不知、实施精确的知识服务及数据资源运营服务。 2) 运营模式的转型升级 未来网络的架构需要能够灵活地适配各类应用，主动对网络资源进行弹性伸缩。对我们目前的网络运营以及市场、网络和IT的协同能力提出了更高的要求，需要我们构建快速响应、高效率、灵活服务的运营能力。 虽然，我们目前正在逐步强化大数据应用，聚焦产品运营、渠道销售、客户服务、网络的开放合作等领域，但是还远远不够。对于未来网络运营，更需要我们实现集约化、智能化。需要我们以流程优化/变革为基础，以智能IT系统为载体，构建面向客户和业务一体化智慧运营服务。主要思路如下： 将数据中心作为我们智慧运营的核心资源，通过数据汇聚、数据清洗、数据挖掘来实时驱动我们的营销、服务、运营等生产和管理流程。 加强服务产品的统筹和规划，重点布局战略级服务和市场化服务，同时要掌控服务产品的核心能力，推进DevOps模式，提升专业化自主开发和迭代优化能力。 突出市场需求化导向，加强大数据应用，强化渠道O2O协同和跨界合作，提升全业务布局和服务能力。 充分利用大数据手段，加强用户需求和行为洞察，做精服务品质，持续提升价值经营能力。 持续推进网络简化和现有网络的智能化升级，积极布局SDN/NFV/云计算能网络能力，结合大数据、人工智能等技术手段，变革创新生产流程和机制，加强对新兴业务的支撑，实现目前的网络运维向端到端集约、云网协同的智能网络运营转变。 3) 管理模式的转型升级 网络重构对我们目前的组织、生产、运营、人力等管理体制将产生重大影响，主要体现在组织架构调整、规划模式转变、运营组织整合、创新体系建设、团队人才培养等方面。 未来网络架构将采用水平分层、纵向解耦的技术路线。基于SDN/NFV的网络架构将打破专业界限，管控和调度也将突破传统省-市-县三级的限制。由于网络以DC为核心的组网新格局，现有分域分层的组织架构已成为端到端自动化运营的最大障碍，必须建立纵向的面向云化网络的运维管理团队新模式，按照我们预期重构后网络参考模型，从上而下主要包括：编排器维护管理团队、虚拟网元维护管理团队、云计算维护管理团队、SDN控制器维护管理团队、虚拟化资源维护管理团队和基础设施维护管理团队。必须提前完成各团队的带头人，积极做好迎接转型的准备。这里可以拿我们网管中心内部做一简单举例。（只是举例，不包含任何其他意思） 按照目前网管中心的科室划分主要分为：监控室、质量室、安全室、传输室、互联网室和核心业务室。那么，我们对比云化网络纵向组织架构做一简单规划。从下而上： 基础设施维护管理团队。该团队的主要职责是维护各IDC机房的物理服务器、交换机等设备，需要一定IT维护从业经验的人才能胜任，最主要的是现场维护模式。我们的自有人员可以通过学习培训来上岗，但从IDC机房的分布和维护成本来看，我的建议采用外包代维的方式来解决。 虚拟化资源维护管理团队。该团队的主要职责是维护各类虚拟化资源池，主要涉及资源和虚机维护管理，需要具备虚拟化技术基础的人才能胜任，属于远程维护模式。而我们目前监控NOC大厅各类终端其实就是一种虚拟化部署模式，所以我们的监控室完全可以胜任。 SDN控制器维护管理团队。该团队主要职责主要完成各类软件定义的网络拓扑、路由寻址、资源分布调用等进行维护管理，不仅需要SDN控制器的知识，更需要大量数通的基础知识人员才能胜任。因此我们的传输室通过培训SDN方面技能就可以完全胜任。 云计算维护管理团队。主要负责云计算中各类服务维护和管理，需要具备linux系统知识、操作系统知识、虚拟化技术知识和云计算知识以及相关维护经验的人员才能胜任，属于纯IT范畴。而我们的安全室目前工作也基本属于IT范畴，因此通过培训和学习应该可以胜任。 虚拟网元维护管理团队。主要负责各类虚拟网元的维护管理，现阶段主要包括vIMS、vEPC、vBRAS、虚拟化短信中心、虚拟化智能网和虚拟化彩铃平台等。这部分的维护人员主要涉及CT类的知识和技能储备，在此基础上再进行相关IT类培训（开发语言类和协议类）。因此目前互联网室与核心业务室相关专业的维护人员通过培训完全可以胜任。 编排器维护管理团队。主要负责向上各类业务设计、流程规划和API调用，向下进行各类网元功能封装和策略下发。因此该部分实际上属于网业协同层，既要懂业务，还要懂网络，同时具备协议解读、功能封装、流程制定、策略制定等技能的人员才能胜任，属于端到端专业的范畴。因此目前质量室的人员通过培训上岗再加上外聘其他业务部门人员应该可以满足要求。 4) 人才培养模式转型升级 在现有的网络运维模式下，厂家和我们之间是一种高度依赖的关系，更确切的说我们更依赖厂家，主要表现在网络维护、保障、演练和优化等方面的实操环节。 CT行业除了设备厂家自有人员外，运营商内部人员更看重人才在信令分析、流程分析、数据分析和优化措施制定方面的能力，而实际措施部署和网络调整完全是厂家人员执行。这一点也正是我入职中国移动以来最疑惑的一点。当然，这其中也存在厂家对运营商有所保留的原因，但更重要是长期以来厂家这种保姆式协维让我们对其有了更大的依赖（全国一样）。 而IT行业恰恰相反，更看重实操能力，IT的运维效率除了依靠自动化手段外，更多的依赖个人维护经验来提升。比如：阿里前段时间一个高级运维人员离职，曾开出双倍年薪也没挖到同级别的人才。 因此，随着网络重构逐步进行，除了集团要求的三级解耦要求外（这一点其实也是集团层面和厂家之间的博弈），我们的自有人员转型或者人才培养必须提升DevOps能力，构建自有研发能力体系。一方面，是因为未来ICT的网络需要集成和运维大量不同来源的新老软件、硬件、网管，我们的自有人员必须要能够在代码级层次上深度介入开发、测试、集成、维护直至业务提供的全过程。另一方面，我们现有的IT和CT是一种割裂分离的状态，随着IT和CT的深度融合，需要既懂网络又懂软件、适应开发运营一体化的人才，包括软件开发人才、IT系统人才和网络技术专家。 结束语以上就是我个人对未来网络的一点儿浅见，有不妥或错误之处还请各位领导和专家批评指正。 目前，传统网络已经越来越力不从心，这一点通过现在互联网企业不断推出各类业务模式已经让我深有感触，重新构建一个基于SDN/NFV的云化网络架构成为必然选择。目前全球的运营商均已推出了自己的网络重构计划，可以预见未来基于SDN/NFV的云化网络架构将愈发充满活力。 既然网络重构势必进行，重构之路已经开启，一个更加开放、灵活的新网络时代即将到来，我们与其消极抵触，不如以更加积极的姿态去迎接挑战，拥抱未来。 未来？已来！ 你，准备好了吗？！]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-27-Linux系统命令-第五篇《文件备份与压缩命令》]]></title>
    <url>%2F2019%2F04%2F28%2F2019-04-27-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%BA%94%E7%AF%87%E3%80%8A%E6%96%87%E4%BB%B6%E5%A4%87%E4%BB%BD%E4%B8%8E%E5%8E%8B%E7%BC%A9%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[tar：打包备份在Linux系统里，tar是将多个文件打包在一起，并且可以实现解压打包的文件的命令。是系统管理员最常用的命令之一，tar命令不但可以实现对多个文件进行打包，还可以对多个文件打包后进行压缩。 打包是指将一大堆文件或目录变成一个总的文件，压缩则是将一个大的文件通过一些压缩算法变成一个小文件。tar命令选项的使用有点特殊，对于CentOS、Linux来说，“tar-z”和“tar z”的效果相同，加或不加“-”这个符号都是可以的。而在FreeBSD系统下，必须加“-”符号。 语法格式：tar [option] [file] 重要参数选项 【使用示例】 1）备份站点目录html 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 创建实验环境# 创建测试目录/var/www/html/kkutysllb/test[root@C7-Server01 kkutysllb]# mkdir -p /var/www/html/kkutysllb/test# 在测试目录下创建测试文件[root@C7-Server01 kkutysllb]# touch /var/www/html/kkutysllb/test/html&#123;01..10&#125;# 查看刚创建的文件信息[root@C7-Server01 kkutysllb]# ls -l /var/www/html/kkutysllb/test/total 0-rw-r--r-- 1 root root 0 Apr 27 21:45 html01-rw-r--r-- 1 root root 0 Apr 27 21:45 html02-rw-r--r-- 1 root root 0 Apr 27 21:45 html03-rw-r--r-- 1 root root 0 Apr 27 21:45 html04-rw-r--r-- 1 root root 0 Apr 27 21:45 html05-rw-r--r-- 1 root root 0 Apr 27 21:45 html06-rw-r--r-- 1 root root 0 Apr 27 21:45 html07-rw-r--r-- 1 root root 0 Apr 27 21:45 html08-rw-r--r-- 1 root root 0 Apr 27 21:45 html09-rw-r--r-- 1 root root 0 Apr 27 21:45 html10# 进入到目标目录的上一级目录，并对目标目录打包[root@C7-Server01 kkutysllb]# cd /var/www/html/kkutysllb/[root@C7-Server01 kkutysllb]# tar zcvf www.kkutysllb.gz ./test./test/./test/html01./test/html02./test/html03./test/html04./test/html05./test/html06./test/html07./test/html08./test/html09./test/html10# 查看打包文件信息[root@C7-Server01 kkutysllb]# ls -lhi *.gz101098015 -rw-r--r-- 1 root root 233 Apr 27 21:49 www.kkutysllb.gz 2）查看压缩包内的内容 1234567891011121314# 通过t选项可以不解压就能查看压缩包内的内容，加选项v可以显示文件的属性[root@C7-Server01 kkutysllb]# tar ztvf www.kkutysllb.gz drwxr-xr-x root/root 0 2019-04-27 21:45 ./test/-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html01-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html02-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html03-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html04-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html05-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html06-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html07-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html08-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html09-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html10 3）对压缩包进行解压 1234567891011121314151617181920212223# 将www.kkutysllb.gz压缩包的内容解压到/home/kkutysllb/html目录中# 如果目标目录不存在，不会自动创建，需要手动先建立目标目录[root@C7-Server01 kkutysllb]# mkdir -p /home/kkutysllb/html[root@C7-Server01 kkutysllb]# tar zxvf www.kkutysllb.gz -C /home/kkutysllb/html/./test/./test/html01./test/html02./test/html03./test/html04./test/html05./test/html06./test/html07./test/html08./test/html09./test/html10# 查看目标目录的信息[root@C7-Server01 kkutysllb]# ll -h /home/kkutysllb/html/total 0drwxr-xr-x 2 root root 146 Apr 27 21:45 test 如果不想看到太多的输出，则可以去掉v选项，功能不受影响。同时z选项也可以省略，只要涉及解压的操作，tar命令都能自动识别压缩包的压缩类型，但是压缩时必须要加上z选项。 4）排除打包 12345678910111213# 在我们刚才打包的文件中包含了目录test，如果不想打包test目录，可以使用exclude选项排除[root@C7-Server01 kkutysllb]# cd ..[root@C7-Server01 html]# tar zcvf www.kkutysllb.new.gz ./kkutysllb/ --exclude=kkutysllb/test./kkutysllb/./kkutysllb/www.kkutysllb.gz# 查看刚才打包的文件内容[root@C7-Server01 kkutysllb]# cd ..[root@C7-Server01 html]# tar ztvf www.kkutysllb.new.gz drwxr-xr-x root/root 0 2019-04-27 22:21 ./kkutysllb/-rw-r--r-- root/root 233 2019-04-27 21:49 ./kkutysllb/www.kkutysllb.gz 如果要排除多个目录，可以在后面接多个–exclude选项。 使用exclude选项排除某个子目录时，需要注意以下几点，否则不会排除成功： 若需要打包的目录为相对路径，则–exclude后只能接相对路径。 若需要打包的目录为绝对路径，则–exclude后既能接绝对路径也能接相对路径。 为方便，统一起见，建议–exclude的后接路径和打包路径应保持形式一致，要么都是相对路径，要么都是绝对路径。 5）排除多个文件打包参数-X 123456789101112131415161718192021222324252627282930313233343536# 将要排除的文件名写入一个文件[root@C7-Server01 kkutysllb]# cat &gt;&gt; filtername &lt;&lt; EOF&gt; html03&gt; html06&gt; html02&gt; html08&gt; html10&gt; EOF&gt; [root@C7-Server01 kkutysllb]# cat -n filtername &gt; 1 html03&gt; 2 html06&gt; 3 html02&gt; 4 html08&gt; 5 html10# 使用参数X排除上述文件进行打包[root@C7-Server01 kkutysllb]# tar zcvfX afterfilter.gz ./filtername ./test/./test/./test/html01./test/html04./test/html05./test/html07./test/html09# 查看新打包的文件内容[root@C7-Server01 kkutysllb]# tar ztvf afterfilter.gz drwxr-xr-x root/root 0 2019-04-27 21:45 ./test/-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html01-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html04-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html05-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html07-rw-r--r-- root/root 0 2019-04-27 21:45 ./test/html09 6）打包链接文件 1234567891011121314151617181920# 首先使用常规选项zc完成打包[root@C7-Server01 kkutysllb]# cd /etc/[root@C7-Server01 etc]# tar zcf local.tar.gz ./rc.local # 查看刚打包的文件内容，发现这是一个符号链接（软链接文件）[root@C7-Server01 etc]# tar ztvf local.tar.gz lrwxrwxrwx root/root 0 2019-04-20 16:07 ./rc.local -&gt; rc.d/rc.local# 这里是个坑，如果不加特殊参数，那么打包之后的文件是个软链接文件，不是rc.local的实体内容# 采用-h参数打包链接文件[root@C7-Server01 etc]# tar zcfh local.new.tar.gz ./rc.local # 再次查看打包的文件内容，发现内容是软链接文件指向的真实文件[root@C7-Server01 etc]# tar ztvf local.new.tar.gz -rw-r--r-- root/root 473 2019-02-20 01:35 ./rc.local 用tar的通用选项zcf打包文件时，如果这个文件是链接文件如/etc/rc.local，那么tar只会对链接文件本身打包，而不是对链接文件指向的真实文件打包，因此需要额外使用-h选项将软链接文件对应的实体文件打包。 对文件打包时，除了上述命令选项参数要掌握外，还需要遵循一些好的操作习惯，避免实际运维中不可预知的错误： 1）在打包一个目录之前，先进入到这个目录的上一级目录，然后执行打包命令，这是大部分情况下打包文件的规范操作流程。 2）少数情况下打包需要完整的目录结构时，也可以使用绝对路径打包，但是需要注意的是解压tar包时压缩包内的文件是否会覆盖本地文件。 小练习：请对/etc/目录下所有普通文件进行打包。 gzip：压缩或解压文件gzip命令用于将一个大的文件通过压缩算法（Lempel-Ziv coding（LZ77））变成一个小的文件。gzip命令不能直接压缩目录，因此目录需要先用tar打包成一个文件，然后tar再调用gzip进行压缩。 语法格式：gzip [option] [file] 重要选项参数 【使用示例】 1）把目录下每个文件都压缩单独的.gz文件 123456789101112131415161718192021222324252627282930313233# 查看test目录下所有文件的信息[root@C7-Server01 kkutysllb]# ls -l test/total 0-rw-r--r-- 1 root root 0 Apr 27 21:45 html01-rw-r--r-- 1 root root 0 Apr 27 21:45 html02-rw-r--r-- 1 root root 0 Apr 27 21:45 html03-rw-r--r-- 1 root root 0 Apr 27 21:45 html04-rw-r--r-- 1 root root 0 Apr 27 21:45 html05-rw-r--r-- 1 root root 0 Apr 27 21:45 html06-rw-r--r-- 1 root root 0 Apr 27 21:45 html07-rw-r--r-- 1 root root 0 Apr 27 21:45 html08-rw-r--r-- 1 root root 0 Apr 27 21:45 html09-rw-r--r-- 1 root root 0 Apr 27 21:45 html10# 使用gzip对每个单独压缩[root@C7-Server01 kkutysllb]# gzip test/html*# 再次查看test目录下的所有文件信息[root@C7-Server01 kkutysllb]# ls -l test/total 40-rw-r--r-- 1 root root 27 Apr 27 21:45 html01.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html02.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html03.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html04.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html05.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html06.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html07.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html08.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html09.gz-rw-r--r-- 1 root root 27 Apr 27 21:45 html10.gz gzip命令的缺点是压缩后源文件不见了，它的特性是压缩、解压都会自动删除源文件。 2）不解压显示上一个例子中每个压缩文件的信息 1234567891011121314# 使用-l选项显示压缩文件信息[root@C7-Server01 kkutysllb]# gzip -l test/*.gzcompressed uncompressed ratio uncompressed_name27 0 0.0% test/html0127 0 0.0% test/html0227 0 0.0% test/html0327 0 0.0% test/html0427 0 0.0% test/html0527 0 0.0% test/html0627 0 0.0% test/html0727 0 0.0% test/html0827 0 0.0% test/html0927 0 0.0% test/html10 因为源文件都是空文件，所以压缩率都为0.0％。 3）解压文件，并显示解压过程 12345678910111213141516171819202122232425262728# 使用-d选项解压，-v显示解压过程[root@C7-Server01 kkutysllb]# gzip -dv test/*.gztest/html01.gz: 0.0% -- replaced with test/html01test/html02.gz: 0.0% -- replaced with test/html02test/html03.gz: 0.0% -- replaced with test/html03test/html04.gz: 0.0% -- replaced with test/html04test/html05.gz: 0.0% -- replaced with test/html05test/html06.gz: 0.0% -- replaced with test/html06test/html07.gz: 0.0% -- replaced with test/html07test/html08.gz: 0.0% -- replaced with test/html08test/html09.gz: 0.0% -- replaced with test/html09test/html10.gz: 0.0% -- replaced with test/html10# 查看test目录下文件信息，发现解压后gzip也会将原来压缩包自动删除[root@C7-Server01 kkutysllb]# ls -l test/total 0-rw-r--r-- 1 root root 0 Apr 27 21:45 html01-rw-r--r-- 1 root root 0 Apr 27 21:45 html02-rw-r--r-- 1 root root 0 Apr 27 21:45 html03-rw-r--r-- 1 root root 0 Apr 27 21:45 html04-rw-r--r-- 1 root root 0 Apr 27 21:45 html05-rw-r--r-- 1 root root 0 Apr 27 21:45 html06-rw-r--r-- 1 root root 0 Apr 27 21:45 html07-rw-r--r-- 1 root root 0 Apr 27 21:45 html08-rw-r--r-- 1 root root 0 Apr 27 21:45 html09-rw-r--r-- 1 root root 0 Apr 27 21:45 html10 4）压缩解压保留源文件 1234567891011121314151617181920212223# 使用-c选项与输出重定向结合完成操作[root@C7-Server01 kkutysllb]# gzip -c test/html* &gt; test.gz# 查看当前目录下生成的test.gz压缩文件信息[root@C7-Server01 kkutysllb]# gzip -lv test.gzmethod crc date time compressed uncompressed ratio uncompressed_namedefla 00000000 Apr 27 23:07 270 0 0.0% test# 查看源文件是否还存在[root@C7-Server01 kkutysllb]# ls -l test/html*-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html01-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html02-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html03-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html04-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html05-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html06-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html07-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html08-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html09-rw-r--r-- 1 root root 0 Apr 27 21:45 test/html10 解压操作类似，请大家自行练习。 虽然上面使用重定向符号解决了保留源文件的问题，但是使用起来还是不太方便。其实，gzip套件包含了许多可以“在原地”处理压缩文件的实用程序。zcat、zgrep、zless、zdiff等实用程序的作用分别与cat、grep、less和diff相同，但是它们操作的是压缩的文件。比如： 12345678910111213141516171819202122# zcat命令可以直接读取压缩文件内容[root@C7-Server01 etc]# zcat local.new.tar.gz | head./rc.local0000644000000000000000000000073113433037107011333 0ustar rootroot#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure 请大家思考下，zcat命令是否可以对gz文件完成解压缩？如果可以，该怎么操作？ zip：打包和压缩文件zip压缩格式是Windows与Linux等多平台通用的压缩格式。和gzip命令相比，zip命令压缩文件不仅不会删除源文件，而且还可以压缩目录。 语法格式：zip [option] [file] 重要选项参数 【使用示例】 1）压缩文件 123456789# 因为测试文件都是空文件，所以压缩率为0%[root@C7-Server01 kkutysllb]# zip test.zip ./test adding: test/ (stored 0%)# 查看新生成的zip压缩文件信息[root@C7-Server01 kkutysllb]# ll -h *.zip-rw-r--r-- 1 root root 160 Apr 27 23:21 test.zip 2）压缩目录 12345678910111213141516171819202122232425262728293031323334# 进入根目录/下，压缩tmp目录[root@C7-Server01 /]# zip tmp.zip ./tmp/ adding: tmp/ (stored 0%)# 上面操作只是压缩tmp这个目录，目录下的文件并没有压缩# 要想同时压缩目录下的文件，可以使用-r选项递归压缩[root@C7-Server01 /]# zip -r tmp.zip ./tmp/updating: tmp/ (stored 0%) adding: tmp/.XIM-unix/ (stored 0%) adding: tmp/.font-unix/ (stored 0%) adding: tmp/.X11-unix/ (stored 0%) adding: tmp/.Test-unix/ (stored 0%) adding: tmp/.ICE-unix/ (stored 0%) adding: tmp/vmware-root/ (stored 0%) adding: tmp/vmware-root_16212-827959067/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/ (stored 0%) adding: tmp/vmware-root_9443-3886520225/ (stored 0%) adding: tmp/vmware-root_9458-2857896559/ (stored 0%) adding: tmp/vmware-root_9420-2857896526/ (stored 0%) adding: tmp/vmware-root_9400-3101375875/ (stored 0%) adding: tmp/vmware-root_9456-2866351085/ (stored 0%) adding: tmp/vmware-root_9609-4121731445/ (stored 0%) adding: tmp/vmware-root_9573-4146439782/ (stored 0%) adding: tmp/vmware-root_9660-3101179142/ (stored 0%) adding: tmp/vmware-root_9604-3101310240/ (stored 0%) adding: tmp/vmware-root_9409-3878589979/ (stored 0%) adding: tmp/vmware-root_9328-3134798637/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/ (stored 0%) adding: tmp/vmware-root_9392-3134798733/ (stored 0%) 3）排除压缩 1234567891011121314151617181920212223242526272829303132# 排除tmp目录下vmware-root/目录后，进行压缩[root@C7-Server01 /]# zip -r tmp_new.zip /tmp/ -x /tmp/vmware-root/ adding: tmp/ (stored 0%) adding: tmp/.XIM-unix/ (stored 0%) adding: tmp/.font-unix/ (stored 0%) adding: tmp/.X11-unix/ (stored 0%) adding: tmp/.Test-unix/ (stored 0%) adding: tmp/.ICE-unix/ (stored 0%) adding: tmp/vmware-root_16212-827959067/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/ (stored 0%) adding: tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/ (stored 0%) adding: tmp/vmware-root_9443-3886520225/ (stored 0%) adding: tmp/vmware-root_9458-2857896559/ (stored 0%) adding: tmp/vmware-root_9420-2857896526/ (stored 0%) adding: tmp/vmware-root_9400-3101375875/ (stored 0%) adding: tmp/vmware-root_9456-2866351085/ (stored 0%) adding: tmp/vmware-root_9609-4121731445/ (stored 0%) adding: tmp/vmware-root_9573-4146439782/ (stored 0%) adding: tmp/vmware-root_9660-3101179142/ (stored 0%) adding: tmp/vmware-root_9604-3101310240/ (stored 0%) adding: tmp/vmware-root_9409-3878589979/ (stored 0%) adding: tmp/vmware-root_9328-3134798637/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/ (stored 0%) adding: tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/ (stored 0%) adding: tmp/vmware-root_9392-3134798733/ (stored 0%)# 对比tmp.zip和tmp_new.zip文件内容差别# 压缩文件时二进制文件，所以只能使用vimdiff命令进行对比[root@C7-Server01 /]# vimdiff tmp.zip zip_new.zip unzip：解压zip文件unzip命令可以解压zip命令或其他压缩软件压缩的zip格式的文件。 语法格式：unzip [option] [file] 重要选项参数 【使用示例】 1）不解压直接查看压缩文件内容 123456789101112131415161718192021222324252627282930313233343536# 使用-l选项[root@C7-Server01 /]# unzip -l tmp.zip Archive: tmp.zipLength Date Time Name------0 04-27-2019 22:29 tmp/0 04-07-2019 20:34 tmp/.XIM-unix/0 04-07-2019 20:34 tmp/.font-unix/0 04-07-2019 20:34 tmp/.X11-unix/0 04-07-2019 20:34 tmp/.Test-unix/0 04-07-2019 20:34 tmp/.ICE-unix/0 04-20-2019 11:06 tmp/vmware-root/0 04-20-2019 16:08 tmp/vmware-root_16212-827959067/0 04-21-2019 16:27 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/0 04-21-2019 16:27 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/0 04-21-2019 16:27 tmp/vmware-root_9443-3886520225/0 04-22-2019 22:23 tmp/vmware-root_9458-2857896559/0 04-22-2019 22:52 tmp/vmware-root_9420-2857896526/0 04-22-2019 22:53 tmp/vmware-root_9400-3101375875/0 04-23-2019 10:29 tmp/vmware-root_9456-2866351085/0 04-23-2019 12:41 tmp/vmware-root_9609-4121731445/0 04-23-2019 12:50 tmp/vmware-root_9573-4146439782/0 04-23-2019 18:24 tmp/vmware-root_9660-3101179142/0 04-26-2019 17:24 tmp/vmware-root_9604-3101310240/0 04-27-2019 12:00 tmp/vmware-root_9409-3878589979/0 04-27-2019 21:42 tmp/vmware-root_9328-3134798637/0 04-27-2019 21:43 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/0 04-27-2019 21:43 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/0 04-27-2019 21:43 tmp/vmware-root_9392-3134798733/------0 24 files 2）常规解压文件的例子 123456789101112131415161718192021222324252627282930313233343536# 使用-v选项显示解压过程[root@C7-Server01 /]# unzip -v tmp.zipArchive: tmp.zipLength Method Size Cmpr Date Time CRC-32 Name------0 Stored 0 0% 04-27-2019 22:29 00000000 tmp/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.XIM-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.font-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.X11-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.Test-unix/0 Stored 0 0% 04-07-2019 20:34 00000000 tmp/.ICE-unix/0 Stored 0 0% 04-20-2019 11:06 00000000 tmp/vmware-root/0 Stored 0 0% 04-20-2019 16:08 00000000 tmp/vmware-root_16212-827959067/0 Stored 0 0% 04-21-2019 16:27 00000000 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/0 Stored 0 0% 04-21-2019 16:27 00000000 tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/0 Stored 0 0% 04-21-2019 16:27 00000000 tmp/vmware-root_9443-3886520225/0 Stored 0 0% 04-22-2019 22:23 00000000 tmp/vmware-root_9458-2857896559/0 Stored 0 0% 04-22-2019 22:52 00000000 tmp/vmware-root_9420-2857896526/0 Stored 0 0% 04-22-2019 22:53 00000000 tmp/vmware-root_9400-3101375875/0 Stored 0 0% 04-23-2019 10:29 00000000 tmp/vmware-root_9456-2866351085/0 Stored 0 0% 04-23-2019 12:41 00000000 tmp/vmware-root_9609-4121731445/0 Stored 0 0% 04-23-2019 12:50 00000000 tmp/vmware-root_9573-4146439782/0 Stored 0 0% 04-23-2019 18:24 00000000 tmp/vmware-root_9660-3101179142/0 Stored 0 0% 04-26-2019 17:24 00000000 tmp/vmware-root_9604-3101310240/0 Stored 0 0% 04-27-2019 12:00 00000000 tmp/vmware-root_9409-3878589979/0 Stored 0 0% 04-27-2019 21:42 00000000 tmp/vmware-root_9328-3134798637/0 Stored 0 0% 04-27-2019 21:43 00000000 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/0 Stored 0 0% 04-27-2019 21:43 00000000 tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/0 Stored 0 0% 04-27-2019 21:43 00000000 tmp/vmware-root_9392-3134798733/------0 0 0% 24 files 3） 指定解压目录解压文件 123456789101112131415161718192021222324252627# 将tmp_new.zip文件加压到/home/kkutysllb/目录下[root@C7-Server01 /]# unzip -d /home/kkutysllb/ tmp_new.zip Archive: tmp_new.zipcreating: /home/kkutysllb/tmp/creating: /home/kkutysllb/tmp/.XIM-unix/creating: /home/kkutysllb/tmp/.font-unix/creating: /home/kkutysllb/tmp/.X11-unix/creating: /home/kkutysllb/tmp/.Test-unix/creating: /home/kkutysllb/tmp/.ICE-unix/creating: /home/kkutysllb/tmp/vmware-root_16212-827959067/creating: /home/kkutysllb/tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/creating: /home/kkutysllb/tmp/systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm/tmp/creating: /home/kkutysllb/tmp/vmware-root_9443-3886520225/creating: /home/kkutysllb/tmp/vmware-root_9458-2857896559/creating: /home/kkutysllb/tmp/vmware-root_9420-2857896526/creating: /home/kkutysllb/tmp/vmware-root_9400-3101375875/creating: /home/kkutysllb/tmp/vmware-root_9456-2866351085/creating: /home/kkutysllb/tmp/vmware-root_9609-4121731445/creating: /home/kkutysllb/tmp/vmware-root_9573-4146439782/creating: /home/kkutysllb/tmp/vmware-root_9660-3101179142/creating: /home/kkutysllb/tmp/vmware-root_9604-3101310240/creating: /home/kkutysllb/tmp/vmware-root_9409-3878589979/creating: /home/kkutysllb/tmp/vmware-root_9328-3134798637/creating: /home/kkutysllb/tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/creating: /home/kkutysllb/tmp/systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806/tmp/creating: /home/kkutysllb/tmp/vmware-root_9392-3134798733/ scp：远程文件复制scp命令用于在不同的主机之间复制文件，它采用SSH协议来保证复制的安全性。scp命令每次都是全量完整复制，因此效率不高，适合第一次复制时使用，增量复制建议使用rsync命令替代。 语法格式：scp [option] [[user@]host1:]file1 [[user@]host2:]file2 重要参数选项 【使用示例】 1）将文件或目录从服务器01复制推懂到服务器02 123456789101112131415# 将C7 Server01（192.168.101.81）根目录下tmp_new.zip文件推动C7 Server02（192.168.101.81）的/tmp目录下[root@C7-Server01 /]# scp /tmp_new.zip 192.168.101.82:/tmpThe authenticity of host '192.168.101.82 (192.168.101.82)' can't be established.ECDSA key fingerprint is SHA256:xX2VNfiU72qXldv5e8O0B4jJ6+AR4UdVH1AsOvRKeOw.ECDSA key fingerprint is MD5:37:bc:16:82:b1:55:0f:55:2e:ae:08:9c:55:db:4e:f5.Are you sure you want to continue connecting (yes/no)? yes # 这里输入连接确认信息Warning: Permanently added '192.168.101.82' (ECDSA) to the list of known hosts.root@192.168.101.82's password: # 这里输入C7 Server02服务器登录密码tmp_new.zip 100% 4574 4.2MB/s 00:00 # 查看C7 Server02的/tmp目录下文件信息[root@C7-Server02 ~]# ll -h /tmp/tmp*.zip-rw-r--r-- 1 root root 4.5K Apr 28 00:09 /tmp/tmp_new.zip 2）保持文件属性进行远程文件/目录推送 123456789101112131415# 查看当前服务器/home/kkutysllb/test.sh文件属性信息[root@C7-Server01 /]# ls -lhi /home/kkutysllb/test.sh1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 /home/kkutysllb/test.sh# 使用-p选项，保持文件属性推送[root@C7-Server01 /]# scp -p /home/kkutysllb/test.sh 192.168.101.82:/tmproot@192.168.101.82's password: test.sh 100% 114 1.4KB/s 00:00# 查看C7 Server02服务器上/tmp目录下test.sh文件属性[root@C7-Server02 ~]# ls -lhi /tmp/test.sh 34631592 -rw-r--r-- 1 root root 114 Apr 19 18:03 /tmp/test.sh 3）递归推送目录到远程服务器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 使用-r选项递归复制目录到远程服务器# 复制目录，最好加上-p选项保持源目录下所有文件和子目录的属性# 查看C7 Server02下/home目录下信息[root@C7-Server02 ~]# ls /home[root@C7-Server02 ~]# # 将Sever01下/home/kkutysllb/目录整体推送到Server02下[root@C7-Server01 /]# scp -rp /home/kkutysllb/ 192.168.101.82:/homeroot@192.168.101.82's password: .bash_logout 100% 18 17.3KB/s 00:00 .bash_profile 100% 193 322.5KB/s 00:00 .bashrc 100% 231 3.1KB/s 00:00 .bash_history 100% 41 21.9KB/s 00:00 image008 100% 0 0.0KB/s 00:00 image009 100% 0 0.0KB/s 00:00 202012312234.55 100% 0 0.0KB/s 00:00 image010 100% 0 0.0KB/s 00:00 hard_link 100% 158 114.9KB/s 00:00 soft_link 100% 187 259.1KB/s 00:00 data001 100% 54 106.1KB/s 00:00 data002 100% 73 137.8KB/s 00:00 test.sh 100% 114 150.9KB/s 00:00 data003 100% 121 176.4KB/s 00:00 data004 100% 66 131.4KB/s 00:00 test01.sh 100% 82 38.6KB/s 00:00 test01 100% 0 0.0KB/s 00:00 data005 100% 102 86.8KB/s 00:00 html01 100% 0 0.0KB/s 00:00 html02 100% 0 0.0KB/s 00:00 html03 100% 0 0.0KB/s 00:00 html04 100% 0 0.0KB/s 00:00 html05 100% 0 0.0KB/s 00:00 html06 100% 0 0.0KB/s 00:00 html07 100% 0 0.0KB/s 00:00 html08 100% 0 0.0KB/s 00:00 html09 100% 0 0.0KB/s 00:00 html10 100% 0 0.0KB/s 00:00 # 再次查看Server02下/home目录信息[root@C7-Server02 ~]# ls -l /home/total 4drwx------ 7 root root 4096 Apr 28 00:00 kkutysllb 4）从远程服务器上拉取文件到本地 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 与推送时选项参数位置对调[root@C7-Server01 /]# scp -rp 192.168.101.82:/home/ /tmp/root@192.168.101.82's password: .bash_logout 100% 18 14.3KB/s 00:00 .bash_profile 100% 193 270.0KB/s 00:00 .bashrc 100% 231 439.6KB/s 00:00 .bash_history 100% 41 83.3KB/s 00:00 image008 100% 0 0.0KB/s 00:00 image009 100% 0 0.0KB/s 00:00 202012312234.55 100% 0 0.0KB/s 00:00 image010 100% 0 0.0KB/s 00:00 hard_link 100% 158 266.8KB/s 00:00 soft_link 100% 187 142.2KB/s 00:00 data001 100% 54 64.2KB/s 00:00 data002 100% 73 111.3KB/s 00:00 test.sh 100% 114 179.0KB/s 00:00 data003 100% 121 49.8KB/s 00:00 data004 100% 66 92.3KB/s 00:00 test01.sh 100% 82 156.1KB/s 00:00 test01 100% 0 0.0KB/s 00:00 data005 100% 102 170.8KB/s 00:00 html01 100% 0 0.0KB/s 00:00 html02 100% 0 0.0KB/s 00:00 html03 100% 0 0.0KB/s 00:00 html04 100% 0 0.0KB/s 00:00 html05 100% 0 0.0KB/s 00:00 html06 100% 0 0.0KB/s 00:00 html07 100% 0 0.0KB/s 00:00 html08 100% 0 0.0KB/s 00:00 html09 100% 0 0.0KB/s 00:00 html10 100% 0 0.0KB/s 00:00 # 查看本地服务器/tmp目录下文件信息[root@C7-Server01 /]# ls -lhi /tmp/total 0101098026 drwxr-xr-x 3 root root 23 Apr 28 00:22 home # 刚拉取过来的目录 35134 drwx------ 3 root root 17 Apr 27 21:43 systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806 1237 drwx------ 3 root root 17 Apr 21 16:27 systemd-private-f8e761a727fe467cad9e29b0663bc653-chronyd.service-6TlcAm 1231048 drwx------ 2 root root 6 Apr 20 11:06 vmware-root101013160 drwx------ 2 root root 6 Apr 20 16:08 vmware-root_16212-827959067 689143 drwx------ 2 root root 6 Apr 27 21:42 vmware-root_9328-3134798637 689145 drwx------ 2 root root 6 Apr 27 21:43 vmware-root_9392-3134798733 266371 drwx------ 2 root root 6 Apr 22 22:53 vmware-root_9400-3101375875 470132 drwx------ 2 root root 6 Apr 27 12:00 vmware-root_9409-3878589979 266297 drwx------ 2 root root 6 Apr 22 22:52 vmware-root_9420-2857896526 1875 drwx------ 2 root root 6 Apr 21 16:27 vmware-root_9443-3886520225 266369 drwx------ 2 root root 6 Apr 23 10:29 vmware-root_9456-2866351085 266299 drwx------ 2 root root 6 Apr 22 22:23 vmware-root_9458-2857896559 689124 drwx------ 2 root root 6 Apr 23 12:50 vmware-root_9573-4146439782 689142 drwx------ 2 root root 6 Apr 26 17:24 vmware-root_9604-3101310240 266294 drwx------ 2 root root 6 Apr 23 12:41 vmware-root_9609-4121731445 689141 drwx------ 2 root root 6 Apr 23 18:24 vmware-root_9660-3101179142]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-27-DPDK技术栈在电信云中的最佳实践（一）]]></title>
    <url>%2F2019%2F04%2F27%2F2019-04-27-DPDK%E6%8A%80%E6%9C%AF%E6%A0%88%E5%9C%A8%E7%94%B5%E4%BF%A1%E4%BA%91%E4%B8%AD%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文篇幅有限，很难用短短几语就勾勒出DPDK的完整轮廓，概括来说，DPDK是一个技术栈，主要用于Intel架构的服务器领域，其主要目的就是提升x86标准服务器的转发性能。因此，本文只重点介绍DPDK平台部分技术在电信云中的最佳实践。 为什么需要DPDK？在IA上，网络数据包处理远早于DPDK而存在。从商业版的Windows到开源的Linux操作系统，所有跨主机通信几乎都会涉及网络协议栈以及底层网卡驱动对于数据包的处理。然而，低速网络数据转发与高速网络数据转发的处理对系统的要求完全不一样。以Linux为例，传统网络设备驱动包处理的动作可以概括如下： 数据包到达网卡设备。 网卡设备依据配置进行DMA操作。 网卡发送中断，唤醒处理器。 驱动软件填充读写缓冲区数据结构。 数据报文达到内核协议栈，进行高层处理。 如果最终应用在用户态，数据从内核搬移到用户态。 如果最终应用在内核态，在内核继续进行。 随着网络接口带宽从千兆向万兆迈进，原先每个报文就会触发一个中断，中断带来的开销变得突出，大量数据到来会触发频繁的中断开销，导致系统无法承受。 在网络包高性能转发技术领域，有两个著名的技术框架NAPI和Netmap。NAPI策略用于高吞吐的场景，其策略是系统被中断唤醒后，尽量使用轮询的方式一次处理多个数据包，直到网络再次空闲重新转入中断等待，其目的就是解决数据包在转发过程过程中频繁中断引入的大量系统开销。Netmap就是采用共享数据包池的方式，减少内核到用户空间的包复制，从而解决大多数场景下需要把包从内核的缓冲区复制到用户缓冲区引入大量系统开销问题。 NAPI与Netmap两方面的努力其实已经明显改善了传统Linux系统上的包处理能力，但是，Linux作为分时操作系统，要将CPU的执行时间合理地调度给需要运行的任务。相对于公平分时，不可避免的就是适时调度。早些年CPU核数比较少，为了每个任务都得到响应处理，进行充分分时，用效率换响应，是一个理想的策略。现今CPU核数越来越多，性能越来越强，为了追求极端的高性能高效率，分时就不一定总是上佳的策略。以Netmap为例，即便其减少了内核到用户空间的内存复制，但内核驱动的收发包处理和用户态线程依旧由操作系统调度执行，除去任务切换本身的开销，由切换导致的后续cache替换（不同任务内存热点不同），对性能也会产生负面的影响。为此，Intel针对IA架构的这些问题，就提出了DPDK技术栈的架构，其根本目的就是尽量采用用户态驱动能力来替代内核态驱动，从而减少内核态的开销，提升转发性能。 鸟瞰DPDK什么是DPDK？在《DPDK深入浅出》一书中，有以下一段描述： 针对不同的对象，其定义并不相同。对于普通用户来说，它可能是一个性能出色的包数据处理加速软件库；对于开发者来说，它可能是一个实践包处理新想法的创新工场；对于性能调优者来说，它可能又是一个绝佳的成果分享平台。当下火热的网络功能虚拟化，则将DPDK放在一个重要的基石位置。 DPDK最初的动机很简单，就是为了证明IA多核处理器能够支撑高性能数据包处理。随着早期目标的达成和更多通用处理器体系的加入，DPDK逐渐成为通用多核处理器高性能数据包处理的业界标杆。 目前，DPDK技术主要应用于计算领域的硬件加速器、通信领域的网络处理器和IT领域的多核处理器。随着软件（例如，DPDK）在I/O性能提升上的不断创新，将多核处理器的竞争力提升到一个前所未有的高度。在SDN/NFV领域，DPDK技术得到了空前应用，产生了不少最佳实践案例。 DPDK提出的目的就是为IA上的高速包处理。下图所示的DPDK主要模块分解展示了以基础软件库的形式，为上层应用的开发提供一个高性能的基础I/O开发包。主要利用了有助于包处理的软硬件特性，如大页、缓存行对齐、线程绑定、预取、NUMA、IA最新指令的利用、Intel DDIO、内存交叉访问等。 核心库Core Libs，提供系统抽象、大页内存、缓存池、定时器及无锁环等基础组件。 PMD库，提供全用户态的驱动，以便通过轮询和线程绑定得到极高的网络吞吐，支持各种本地和虚拟的网卡。 Classify库，支持精确匹配（Exact Match）、最长匹配（LPM）和通配符匹配（ACL），提供常用包处理的查表操作。 QoS库，提供网络服务质量相关组件，如限速（Meter）和调度（Sched）。 除了这些组件，DPDK还提供了几个平台特性，比如节能考虑的运行时频率调整（POWER），与Linux kernel stack建立快速通道的KNI（Kernel Network Interface）。而Packet Framework和DISTRIB为搭建更复杂的多核流水线处理模型提供了基础的组件。 DPDK软件包内有一个最基本的三层转发实例（l3fwd），可用于测试双路服务器整系统的吞吐能力，通过现场实验，可以达到220Gbit/s的数据报文吞吐能力。除了通过硬件或者软件提升性能之外，如今DPDK整系统报文吞吐能力上限已经不再受限于CPU的核数，当前瓶颈在于PCIe（IO总线）的LANE数。换句话说，系统性能的整体I/O天花板不再是CPU，而是系统所提供的所有PCIe LANE的带宽，也就是能插入多少个高速以太网接口卡。 在这样的性能基础上，网络节点的软化（NFV）就成为可能。对于网络节点上运转的不同形态的网络功能，通过软化并适配到一个通用的硬件平台，就是软硬件解耦。解耦正是NFV的一个核心思想，而硬件解耦的多个网络功能在单一通用节点上的隔离共生问题，就是另一个核心思想—虚拟化。 DPDK技术基础（1）cache的作用在当今服务器领域，一个处理器通常包含多个核心（Core），集成Cache子系统，内存子系统通过内部或外部总线与其通信。在经典计算机系统中一般都有两个标准化的部分：北桥（North Bridge）和南桥（SouthBridge）。它们是处理器和内存以及其他外设沟通的渠道。在这类系统中，北桥就是真个架构的瓶颈，一旦北桥处理不过来或故障，整个系统的处理效率就会变低或瘫痪。因此，后来计算机系统中只存在南桥芯片，而北桥部分就被全部移植到CPU的SoC中，其中最重要的部分就是内存控制器，并在此基础上进一步衍生出NUMA和MPP架构，这个放在后面会讲。 我们在本科学习计算机基础课程时，都知道计算机的内存分为SRAM、DRAM、SDRAM和DDR(1/2/3/4)等不同类型。在早期的PC系统中，主要使用DRAM和SDRAM来作为内存，相比SRAM在成本、功耗方面有不小的优势，而且速度也还可以。后来在现今的PC系统中，利用SDRAM在一个时钟周期的上下边沿进行数据读写，整体数据吞吐率带宽翻倍，也就是DDR RAM，DDR根据不同的主频，又分为DDR1/DDR2/DDR3/DDR4。而SRAM，由于其功耗高、成本高，速度很快，一般都作为CPU的cache使用，目前都被封装的CPU的SoC中。 一般来说，Cache由三级组成，之所以对Cache进行分级，也是从成本和生产工艺的角度考虑的。一级（L1）最快，但是容量最小；三级（LLC，Last Level Cache）最慢，但是容量最大。 一级Cache：一般分为数据Cache和指令Cache，数据Cache用来存储数据，而指令Cache用于存放指令。这种Cache速度最快，一般处理器只需要3～5个指令周期就能访问到数据，因此成本高，容量小，一般都只有几十KB。 二级Cache：和一级Cache分为数据Cache和指令Cache不同，数据和指令都无差别地存放在一起。速度相比一级Cache慢一些，处理器大约需要十几个处理器周期才能访问到数据，容量也相对来说大一些，一般有几百KB到几MB不等。 三级Cache：速度更慢，处理器需要几十个处理器周期才能访问到数据，容量更大，一般都有几MB到几十个MB。在多核处理器内部，三级Cache由所有的核心所共有。这样的共享方式，其实也带来一个问题，有的处理器可能会极大地占用三级Cache，导致其他处理器只能占用极小的容量，从而导致Cache不命中，性能下降。因此，Intel公司推出了Intel® CAT技术，确保有一个公平，或者说软件可配置的算法来控制每个核心可以用到的Cache大小，有兴趣的可参考https://software.intel.com/zh-cn/articles/introduction-to-cache-allocation-technology?_ga=2.54835683.913561365.1556263296-1903721401.1556263296。 为了将cache与内存进行关联，需要对cache和内存进行分块，并采用一定的映射算法进行关联。分块就是将Cache和内存以块为单位进行数据交换，块的大小通常以在内存的一个存储周期中能够访问到的数据长度为限。当今主流块的大小都是64字节，因此一个Cache line就是指64个字节大小的数据块。而映射算法是指把内存地址空间映射到Cache地址空间。具体来说，就是把存放在内存中的内容按照一定规则装入到Cache中，并建立内存地址与Cache地址之间的对应关系。当CPU需要访问这个数据块内容时，只需要把内存地址转换成Cache地址，从而在Cache中找到该数据块，最终返回给CPU。 根据Cache和内存之间的映射关系的不同，Cache可以分为三类：第一类是全关联型Cache（full associative cache），第二类是直接关联型Cache（direct mapped cache），第三类是组关联型Cache（N-ways associative cache）。 全关联型cache：需要在cache中建立一个目录表，目录表的每一项由内存地址、cache块号和一个有效位组成。当CPU需要访问某个内存地址时，首先查询该目录表判断该内容是否缓存在Cache中，如果在，就直接从cache中读取内容；如果不在，就去通过内存地址转换去内存冲读取。具体原理如下： 首先，用内存的块地址A在Cache的目录表中进行查询，如果找到等值的内存块地址，检查有效位是否有效，只有有效的情况下，才能通过Cache块号在Cache中找到缓存的内存，并且加上块内地址B，找到相应数据，这时则称为Cache命中，处理器拿到数据返回；否则称为不命中，CPU则需要在内存中读取相应的数据。使用全关联型Cache，块的冲突最小（没有冲突），Cache的利用率也高，但是需要一个访问速度很快的相联存储器。随着Cache容量的增加，其电路设计变得十分复杂，因此一般只有TLB cache才会设计成全关联型。 直接关联型Cache：是指将某一块内存映射到Cache的一个特定的块，即Cache line中。假设一个Cache中总共存在N个Cache line，那么内存就被分成N等分，其中每一等分对应一个Cache line。比如：Cache的大小是2K，而一个Cache line的大小是64B，那么就一共有2K/64B=32个Cache line，那么对应我们的内存，第1块（地址0～63），第33块（地址6432～6433-1），以及第（N32+1）块都被映射到Cache第一块中；同理，第2块，第34块，以及第（N32+2）块都被映射到Cache第二块中；可以依次类推其他内存块。直接关联型Cache的目录表只有两部分组成：区号和有效位。具体原理如下： 首先，内存地址被分成三部分：区号A、块号B和块内地址C。根据区号A在目录表中找到完全相等的区号，并且在有效位有效的情况下，说明该数据在Cache中，然后通过内存地址的块号B获得在Cache中的块地址，加上块内地址C，最终找到数据。如果在目录表中找不到相等的区号，或者有效位无效的情况下，则说明该内容不在Cache中，需要到内存中读取。可以看出，直接关联是一种很“死”的映射方法，当映射到同一个Cache块的多个内存块同时需要缓存在Cache中时，只有一个内存块能够缓存，其他块需要被“淘汰”掉。因此，直接关联型命中率是最低的，但是其实现方式最为简单，匹配速度也最快。 组关联型Cache：是目前Cache中用的比较广泛的一种方式，是前两种Cache的折中形式。在这种方式下，内存被分为很多组，一个组的大小为多个Cache line的大小，一个组映射到对应的多个连续的Cache line，也就是一个Cache组，并且该组内的任意一块可以映射到对应Cache组的任意一个。可以看出，在组外，其采用直接关联型Cache的映射方式，而在组内，则采用全关联型Cache的映射方式。比如：有一个4路组关联型Cache，其大小为1M，一个Cache line的大小为64B，那么总共有16K个Cache line，但是在4路组关联的情况下，就拥有了4K个组，每个组有4个Cache line。一个内存单元可以缓存到它所对应的组中的任意一个Cache line中去。具体原理如下： 目录表由三部分组成：“区号+块号”、Cache块号和有效位。一个内存地址被分成四部分：区号A、组号B、块号C和块内地址D。首先，根据组号B按地址查找到一组目录表项；然后，根据区号A和块号C在该组中进行关联查找（即并行查找，为了提高效率），如果匹配且有效位有效，则表明该数据块缓存在Cache中，得到Cache块号，加上块内地址D，可以得到该内存地址在Cache中映射的地址，得到数据；如果没有找到匹配项或者有效位无效，则表示该内存块不在Cache中，需要处理器到内存中读取。 Cache之所以能够提高系统性能，主要是因为程序执行存在局部性现象，即时间局部性（程序中指令和数据在时间上的关联性，比如：循环体中的变量和指令）和空间局部性（程序中指令和数据在空间上的关联性，比如：列表数据结构中的元素）。cache就可以根据程序的局部性特点，以及当前执行状态、历史执行过程、软件提示等信息，然后以一定的合理方法，在数据/指令被使用前取入Cache，也就是cache预取。 内存的数据被加载进cache后，最终还是需要写回到内存中，这个写回的过程存在两种策略： 直写（write-through）：在CPU对Cache写入的同时，将数据写入到内存中。这种策略保证了在任何时刻，内存的数据和Cache中的数据都是同步的，这种方式简单、可靠。但由于CPU每次对Cache更新时都要对内存进行写操作，总线工作繁忙，内存的带宽被大大占用，因此运行速度会受到影响。 回写（write-back）：回写相对于直写而言是一种高效的方法。回写系统通过将Cache line的标志位字段添加一个Dirty标志位，当处理器在改写了某个Cache line后，并不是马上把其写回内存，而是将该Cache line的Dirty标志设置为1。当处理器再次修改该Cache line并且写回到Cache中，查表发现该Dirty位已经为1，则先将Cache line内容写回到内存中相应的位置，再将新数据写到Cache中。 除了上述这两种写策略，还有WC（write-combining）和UC（uncacheable）。这两种策略都是针对特殊的地址空间来使用的，这里不做详细讨论，有兴趣的可以参考Intel官方社区。 在采用回写策略的架构中，如果多个CPU同时对一个cache line进行修改后的写回操作，就存在“脏”数据区域的问题，这就是cache一致性问题。其本质原因是存在多个处理器独占的Cache，而不是多个处理器。解决Cache一致性问题的机制有两种：基于目录的协议（Directory-based protocol）和总线窥探协议（Bus snooping protocol）。这里因为篇幅问题，不再展开讨论，有兴趣的可参见《深入浅出DPDK》一书相关内容。 事实上，Cache对于绝大多数程序员来说都是透明不可见的，cache完成数据缓存的所有操作都是硬件自动完成的。但是，硬件也不是完全智能的。因此，Intel体系架构引入了能够对Cache进行预取的指令，使一些对程序执行效率有很高要求的程序员能够一定程度上控制Cache，加快程序的执行。DPDK对cache进行预取操作如下： 1234567891011121314151617181920212223242526272829303132333435while (nb_rx &lt; nb_pkts) &#123; rxdp = &amp;rx_ring[rx_id]; //读取接收描述符 staterr = rxdp-&gt;wb.upper.status_error; //检查是否有报文收到 if (！(staterr &amp; rte_cpu_to_le_32(IXGBE_RXDADV_STAT_DD))) break; rxd = *rxdp; //分配数据缓冲区 nmb = rte_rxmbuf_alloc(rxq-&gt;mb_pool); nb_hold++; //读取控制结构体 rxe = &amp;sw_ring[rx_id]; …… rx_id++; if (rx_id == rxq-&gt;nb_rx_desc) &#123; rx_id = 0; //预取下一个控制结构体mbuf rte_ixgbe_prefetch(sw_ring[rx_id].mbuf); //预取接收描述符和控制结构体指针 &#125; /* 预取报文 */ if ((rx_id &amp; 0x3) == 0) &#123; rte_ixgbe_prefetch(&amp;rx_ring[rx_id]); rte_ixgbe_prefetch(&amp;sw_ring[rx_id]); &#125; /* 把接收描述符读取的信息存储在控制结构体mbuf中 */ rte_packet_prefetch((char *)rxm-&gt;buf_addr + rxm-&gt;data_off); rxm-&gt;nb_segs = 1; rxm-&gt;next = NULL; rxm-&gt;pkt_len = pkt_len; rxm-&gt;data_len = pkt_len; rxm-&gt;port = rxq-&gt;port_id; …… rx_pkts[nb_rx++] = rxm; &#125; 同时，DPDK在定义数据结构或者数据缓冲区时就申明cache line对齐，代码如下： 123456789101112131415#define RTE_CACHE_LINE_SIZE 64 #define __rte_cache_aligned __attribute__((__aligned__(RTE_CACHE_LINE_SIZE)))struct rte_ring_debug_stats &#123; uint64_t enq_success_bulk; uint64_t enq_success_objs; uint64_t enq_quota_bulk; uint64_t enq_quota_objs; uint64_t enq_fail_bulk; uint64_t enq_fail_objs; uint64_t deq_success_bulk; uint64_t deq_success_objs; uint64_t deq_fail_bulk; uint64_t deq_fail_objs; &#125; __rte_cache_aligned; 大页内存在前文《x86架构基础》一文中提到了TLB的概念，其主要用来缓存内存地址转换中的页表项，其本质上也是一个cache，称之为TLB cache。TLB和cache的区别是：TLB缓存内存地址转换用的页表项，而cache缓存程序用到的数据和指令。 TLB中保存着程序线性地址前20位[31：12]和页框号的对应关系，如果匹配到线性地址就可以迅速找到页框号，通过页框号与线性地址后12位的偏移组合得到最终的物理地址。TLB使用虚拟地址进行搜索，直接返回对应的物理地址，相对于内存中的多级页表需要多次访问才能得到最终的物理地址，TLB查找大大减少了CPU的开销。如果需要的地址在TLB Cache中，就会迅速返回结果，然后CPU用该物理地址访问内存，这样的查找操作也称为TLB命中；如果需要的地址不在TLB Cache中，也就是不命中，CPU就需要到内存中访问多级页表，才能最终得到物理地址。但是，TLB的大小是有限的，因此TLB不命中的概率很大，为了提高内存地址转换效率，减少CPU的开销，就提出了大页内存的概念。 在x86架构中，一般都分成以下四组TLB： 第一组：缓存一般页表（4KB页面）的指令页表缓存（Instruction-TLB）。 第二组：缓存一般页表（4KB页面）的数据页表缓存（Data-TLB）。 第三组：缓存大尺寸页表（2MB/4MB页面）的指令页表缓存（Instruction-TLB）。 第四组：缓存大尺寸页表（2MB/4MB页面）的数据页表缓存（Data-TLB） 如果采用常规页（4KB）并且使TLB总能命中，需要寻址的内容都在该内容页内，那么至少需要在TLB表中存放两个表项。如果一个程序使用了512个内容页也就是2MB大小，那么需要512个页表表项才能保证不会出现TLB不命中的情况。但是，如果采用2MB作为分页的基本单位，那么只需要一个表项就可以保证不出现TLB不命中的情况；对于消耗内存以GB为单位的大型程序，可以采用1GB为单位作为分页的基本单位，减少TLB不命中的情况。需要注意的是：系统能否支持大页，支持大页的大小为多少是由其使用的处理器决定的。 在Linux启动之后，如果想预留大页，则可以使用以下的方法来预留内存。在非NUMA系统中，可以使用以下方法预留2MB大小的大页。 123# 预留1024个大小为2MB的大页，也就是预留了2GB内存。echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages 系统未开启大页内存的状态 系统开启大页内存后的状态 如果是在NUMA系统中，假设有两个NODE的系统中，则可以用以下的命令： 1234# 在NODE0和NODE1上各预留1024个大小为2MB的大页，总共预留了4GB大小。echo 1024 &gt; /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages echo 1024 &gt; /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages 而对于大小为1GB的大页，则必须在Linux的GRUB配置文件中进行修改，并重启系统生效，不能动态预留。 DPDK中也是使用HUGETLBFS来使用大页。首先，它需要把大页mount到某个路径，比如/mnt/huge，以下是命令： 12mkdir /mnt/huge mount -t hugetlbfs nodev /mnt/huge 需要注意的是：在mount之前，要确保之前已经成功预留内存，否则会失败。该命令只是临时的mount了文件系统，如果想每次开机时省略该步骤，可以修改/etc/fstab文件，加上如下一行： 1nodev /mnt/huge hugetlbfs defaults 0 0 对于1GB大小的大页，则必须用如下的命令： 1nodev /mnt/huge_1GB hugetlbfs pagesize=1GB 0 0 然后，在DPDK运行的时候，会使用mmap（）系统调用把大页映射到用户态的虚拟地址空间，然后就可以正常使用了。 DDIO（Data Direct I/O）数据直连技术 如今，随着大数据和云计算的爆炸式增长，宽带的普及以及个人终端网络数据的日益提高，对运营商服务节点和数据中心的数据交换能力和网络带宽提出了更高的要求。并且，数据中心本身对虚拟化功能的需求也增加了更多的网络带宽需求。为此，英特尔公司提出了Intel® DDIO（Data Direct I/O）的技术。该技术的主要目的就是让服务器能更快处理网络接口的数据，提高系统整体的吞吐率，降低延迟，同时减少能源的消耗。 当一个网络报文送到服务器的网卡时，网卡通过外部总线（比如PCI总线）把数据和报文描述符送到内存。接着，CPU从内存读取数据到Cache进而到寄存器。进行处理之后，再写回到Cache，并最终送到内存中。最后，网卡读取内存数据，经过外部总线送到网卡内部，最终通过网络接口发送出去。可以看出，对于一个数据报文，CPU和网卡需要多次访问内存。而内存相对CPU来讲是一个非常慢速的部件。CPU需要等待数百个周期才能拿到数据，在这过程中，CPU什么也做不了。 DDIO技术思想就是使外部网卡和CPU通过LLC Cache直接交换数据，绕过了内存这个相对慢速的部件。这样，就增加了CPU处理网络报文的速度（减少了CPU和网卡等待内存的时间），减小了网络报文在服务器端的处理延迟。这样做也带来了一个问题，就是网络报文直接存储在LLC Cache中，对这一级cache的容量有很大需求。因此，在英特尔的E5处理器系列产品中，把LLC Cache的容量提高到了20MB。DDIO处理网络报文流程示意图如下： 为了发送一个数据报文到网络上去，首先是运行在CPU上的软件分配了一段内存，然后把这段内存读取到CPU内部，更新数据，并且填充相应的报文描述符（网卡会通过读取描述符了解报文的相应信息），然后写回到内存中，通知网卡，最终网卡把数据读回到内部，并且发送到网络上去。但是，没有DDIO技术和有DDIO技术条件的处理方式是不同的。 a) 没有DDIO时，如下图所示： 1）CPU更新报文和控制结构体。由于分配的缓冲区在内存中，因此会触发一次Cache不命中，CPU把内存读取到Cache中，然后更新控制结构体和报文信息。之后通知NIC来读取报文。 2）NIC收到有报文需要传递到网络上的通知后，读取控制结构体进而知道去内存中读取报文信息。 3）由于之前CPU刚把该缓冲区从内存读到Cache中并且做了更新，很有可能Cache还没有来得及把更新的内容写回到内存中(回写机制)。因此，当NIC发起一个对内存的读请求时，很有可能这个请求会发送到Cache系统中，Cache系统会把数据写回到内存中。 4）最后，内存控制器再把数据写到PCI总线上去，NIC从PCI总线上读取数据。 b) 有DDIO时，如下图所示： 1）CPU更新报文和控制结构体。这个步骤和没有DDIO的技术类似，但是由于DDIO的引入，处理器会开始就把内存中的缓冲区和控制结构体预取到Cache，因此减少了内存读的时间。 2）NIC收到有报文需要传递到网络上的通知后，通过PCI总线去读取控制结构体和报文。利用DDIO技术，I/O访问可以直接将Cache的内容送到PCI总线上。这样，就减少了Cache写回时等待的时间。 由此可以看出，由于DDIO技术的引入，网卡的读操作减少了访问内存的次数，因而提高了访问效率，减少了报文转发的延迟。在理想状况下，NIC和CPU无需访问内存，直接通过访问Cache就可以完成更新数据，把数据送到NIC内部，进而送到网络上的所有操作。 有网络报文需要送到系统内部进行处理，其过程一般是NIC从网络上收到报文后，通过PCI总线把报文和相应的控制结构体送到预先分配的内存，然后通知相应的驱动程序或者软件来处理。和之前网卡的读数据操作类似，有DDIO技术和没有DDIO技术的处理也是不一样的。 a) 没有DDIO时，如下图所示： 1）报文和控制结构体通过PCI总线送到指定的内存中。如果该内存恰好缓存在Cache中（有可能之前CPU有对该内存进行过读写操作），则需要等待Cache把内容先写回到内存中，然后才能把报文和控制结构体写到内存中。 2）运行在CPU上的驱动程序或者软件得到通知收到新报文，去内存中读取控制结构体和相应的报文，Cache不命中。之所以Cache一定不会命中，是因为即使该内存地址在Cache中，在步骤1中也被强制写回到内存中。因此，只能从内存中读取控制结构体和报文。 b) 有DDIO时，如下图所示： 1）这时，报文和控制结构体通过PCI总线直接送到Cache中。这时有两种情形：场景一就是如果该内存恰好缓存在Cache中（有可能之前处理器有对该内存进行过读写操作），则直接在Cache中更新内容，覆盖原有内容。场景二就是如果该内存没有缓存在Cache中，则在最后一级Cache中分配一块区域，并相应更新Cache表，表明该内容是对应于内存中的某个地址的。 2）运行在CPU上的驱动或者软件被通知到有报文到达，其产生一个内存读操作，由于该内容已经在Cache中，因此直接从Cache中读。 由此可以看出，DDIO技术在CPU和外设之间交换数据时，减少了CPU和外设访问内存的次数，也减少了Cache写回的等待，提高了系统的吞吐率和数据的交换延迟。 NUMA系统从系统架构来看，目前的商用服务器大体可以分为三类，即对称多处理器结构 (SMP ： Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP ： Massive Parallel Processing) 。它们的特征如下： SMP (Symmetric Multi Processing),对称多处理系统内有许多紧耦合多处理器，在这样的系统中，所有的CPU共享全部资源，如总线，内存和I/O系统等，操作系统或管理数据库的复本只有一个，这种系统有一个最大的特点就是共享所有资源。多个CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。SMP 服务器的主要特征是共享，系统中所有资源 (CPU 、内存、 I/O 等 ) 都是共享的。也正是由于这种特征，导致了 SMP 服务器的主要问题，那就是它的扩展能力非常有限。对于 SMP 服务器而言，每一个共享的环节都可能造成 SMP 服务器扩展时的瓶颈，而最受限制的则是内存。由于每个 CPU 必须通过相同的内存总线访问相同的内存资源，因此随着 CPU 数量的增加，内存访问冲突将迅速增加，最终会造成 CPU 资源的浪费，使 CPU 性能的有效性大大降低。实验证明， SMP 服务器 CPU 利用率最好的情况是 2 至 4 个 CPU 。 NUMA 服务器的基本特征是具有多个 CPU 模块，每个 CPU 模块由多个 CPU( 如 4 个 ) 组成，并且具有独立的本地内存、 I/O 槽口等。由于其节点之间可以通过互联模块 ( 如称为 Crossbar Switch) 进行连接和信息交互，因此每个 CPU 可以访问整个系统的内存 ( 这是 NUMA 系统与 MPP 系统的重要差别 ) 。显然，访问本地内存的速度将远远高于访问远地内存 ( 系统内其它节点的内存 ) 的速度，这也是非一致存储访问 NUMA 的由来。由于这个特点，为了更好地发挥系统性能，开发应用程序时需要尽量减少不同 CPU 模块之间的信息交互。利用 NUMA 技术，可以较好地解决原来 SMP 系统的扩展问题，在一个物理服务器内可以支持上百个 CPU 。NUMA 技术同样有一定缺陷，由于访问远地内存的延时远远超过本地内存，因此当 CPU 数量增加时，系统性能无法线性增加。 和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现 512 个节点互联，数千个 CPU 。MPP不是处理器内部节点互联，而是多个服务器通过外部互联。在 MPP 系统中，每个 SMP 节点也可以运行自己的操作系统、数据库等。但和 NUMA 不同的是，它不存在异地内存访问的问题。换言之，每个节点内的 CPU 不能访问另一个节点的内存。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为数据重分配 (Data Redistribution) 。MPP 服务器需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程。 NUMA系统是一种多处理器环境下设计的内存结构。在NUMA架构出现前，CPU欢快的朝着频率越来越高的方向发展。受到物理极限的挑战，又转为核数越来越多的方向发展。如果每个core的工作性质都是share-nothing（类似于map-reduce的node节点的作业属性），那么也许就不会有NUMA。由于所有CPU Core都是通过共享一个北桥来读取内存，无论核数如何的发展，北桥在响应时间上的性能瓶颈越来越明显。于是，聪明的硬件设计师们，想到了把内存控制器（原本北桥中读取内存的部分）也做个拆分，平分到了每个die上。于是NUMA就出现了！ NUMA中，虽然内存直接attach在CPU上，但是由于内存被平均分配在了各个die上。只有当CPU访问自身直接attach内存对应的物理地址时，才会有较短的响应时间（后称Local Access）。而如果需要访问其他CPU attach的内存的数据时，就需要通过inter-connect通道访问，响应时间就相比之前变慢了（后称Remote Access）。所以NUMA（Non-Uniform Memory Access）就此得名。 NUMA的几个概念（Node，socket，core，thread） socket：就是主板上的CPU插槽; Core：就是socket里独立的一组程序执行的硬件单元，比如寄存器，计算单元等; Thread：就是超线程hyperthread的概念，逻辑的执行单元，独立的执行上下文，但是共享core内的寄存器和计算单元。 Node：这个概念其实是用来解决core的分组的问题，具体参见下图来理解（图中的OS CPU可以理解thread，那么core就没有在图中画出），从图中可以看出共有4个socket，每个socket 2个node，每个node中有8个thread，总共4（Socket）× 2（Node）× 8 （4core × 2 Thread） = 64个thread。另外每个node有自己的内部CPU，总线和内存，同时还可以访问其他node内的内存，NUMA的最大的优势就是可以方便的增加CPU的数量，因为Node内有自己内部总线，所以增加CPU数量可以通过增加Node的数目来实现，如果单纯的增加CPU的数量，会对总线造成很大的压力，所以UMA结构不可能支持很多的核。下图出自：《NUMA Best Practices for Dell PowerEdge 12th Generation Servers》 由于每个node内部有自己的CPU总线和内存，所以如果一个虚拟机的vCPU跨不同的Node的话，就会导致一个node中的CPU去访问另外一个node中的内存的情况，这就导致内存访问延迟的增加。在NFV环境中，对性能有比较高的要求，就非常需要同一个虚拟机的vCPU尽量被分配到同一个Node中的pCPU上，所以在OpenStack的Kilo版本及后续版本均增加了基于NUMA感知的虚拟机调度的特性。详见www.openstack.org官方社区管理员手册文档。 查看服务器中NUMA拓扑架构常用以下命令： 1）比较常用的是lscpu 1234567891011121314151617181920212223242526[root@C7-Server01 ~]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 4On-line CPU(s) list: 0-3Thread(s) per core: 1Core(s) per socket: 2Socket(s): 2NUMA node(s): 2Vendor ID: GenuineIntelCPU family: 6Model: 158Model name: Intel(R) Core(TM) i9-8950HK CPU @ 2.90GHzStepping: 10CPU MHz: 2903.998BogoMIPS: 5807.99Virtualization: VT-xHypervisor vendor: VMwareVirtualization type: fullL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 12288KNUMA node0 CPU(s): 0-3Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec arat spec_ctrl intel_stibp flush_l1d arch_capabilities 从上面报文输出可以看出，当前机器有2个sockets，每个sockets包含1个numa node，每个numa node中有2个cores，每个cores包含1个thread，所以总的threads数量=2（sockets）×1（node）×2（cores）×1（threads）=4. 2）通过shell脚本打印出当前机器的socket，core和thread的数量 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/bin/bash# 简单打印系统CPU拓扑# Author: kkutysllbfunction get_nr_processor()&#123; grep '^processor' /proc/cpuinfo | wc -l&#125;function get_nr_socket()&#123; grep 'physical id' /proc/cpuinfo | awk -F: '&#123; print $2 | "sort -un"&#125;' | wc -l&#125;function get_nr_siblings()&#123; grep 'siblings' /proc/cpuinfo | awk -F: '&#123; print $2 | "sort -un"&#125;'&#125;function get_nr_cores_of_socket()&#123; grep 'cpu cores' /proc/cpuinfo | awk -F: '&#123; print $2 | "sort -un"&#125;'&#125;echo '===== CPU Topology Table ====='echoecho '+--------------+---------+-----------+'echo '| Processor ID | Core ID | Socket ID |'echo '+--------------+---------+-----------+'while read line; do if [ -z "$line" ]; then printf '| %-12s | %-7s | %-9s |\n' $p_id $c_id $s_id echo '+--------------+---------+-----------+' continue fi if echo "$line" | grep -q "^processor"; then p_id=`echo "$line" | awk -F: '&#123;print $2&#125;' | tr -d ' '` fi if echo "$line" | grep -q "^core id"; then c_id=`echo "$line" | awk -F: '&#123;print $2&#125;' | tr -d ' '` fi if echo "$line" | grep -q "^physical id"; then s_id=`echo "$line" | awk -F: '&#123;print $2&#125;' | tr -d ' '` fidone &lt; /proc/cpuinfoechoawk -F: '&#123; if ($1 ~ /processor/) &#123; gsub(/ /,"",$2); p_id=$2; &#125; else if ($1 ~ /physical id/)&#123; gsub(/ /,"",$2); s_id=$2; arr[s_id]=arr[s_id] " " p_id &#125;&#125; END&#123; for (i in arr) printf "Socket %s:%s\n", i, arr[i];&#125;' /proc/cpuinfoechoecho '===== CPU Info Summary ====='echonr_processor=`get_nr_processor`echo "Logical processors: $nr_processor"nr_socket=`get_nr_socket`echo "Physical socket: $nr_socket"nr_siblings=`get_nr_siblings`echo "Siblings in one socket: $nr_siblings"nr_cores=`get_nr_cores_of_socket`echo "Cores in one socket: $nr_cores"let nr_cores*=nr_socketecho "Cores in total: $nr_cores"if [ "$nr_cores" = "$nr_processor" ]; then echo "Hyper-Threading: off"else echo "Hyper-Threading: on"fiechoecho '===== END =====' 运行后输出结果如下： DPDK中有以下策略来适应NUMA系统： 1）Per-core memory：一个CPU上有多个核（core），per-core memory是指每个核都有属于自己的内存，即对于经常访问的数据结构，每个核都有自己的备份。这样做一方面是为了本地内存的需要，另外一方面也是前面提到的Cache一致性的需要，避免多个核访问同一个Cache Line。 2）本地设备本地处理：即用本地的处理器、本地的内存来处理本地的设备上产生的数据。如果有一个PCI设备在node0上，就用node0上的核来处理该设备，处理该设备用到的数据结构和数据缓冲区都从node0上分配。以下是一个分配本地内存的例子： 123/* allocate memory for the queue structure */ /* 该例分配一个结构体，通过传递socket_id，即node id获得本地内存，并且以Cache Line对齐 */q = rte_zmalloc_socket("fm10k", sizeof(*q), RTE_CACHE_LINE_SIZE, socket_id); CPU的亲和性调度当前，属于多核处理器时代，这类多核处理器自然会面对一个问题，按照什么策略将任务线程分配到各个处理器上执行。众所周知，这个分配工作一般由操作系统完成。负载均衡当然是比较理想的策略，按需指定的方式也是很自然的诉求，因为其具有确定性。简单地说，CPU亲和性（Core affinity）就是一个特定的任务要在某个给定的CPU上尽量长时间地运行而不被迁移到其他处理器上的倾向性。这意味着线程可以不在处理器之间频繁迁移，从而减少不必要的开销。 Linux内核包含了一种机制，它让开发人员可以编程实现CPU亲和性。也就是说可以将应用程序显式地指定线程在哪个（或哪些）CPU上运行。 在Linux内核中，所有的线程都有一个相关的数据结构，称为task_struct。这个结构非常重要，这里不展开讨论，只讨论其中与亲和性相关度最高的是cpus_allowed位掩码。这个位掩码由n位组成，与系统中的n个逻辑处理器一一对应。具有4个物理CPU的系统可以有4位。如果这些CPU都启用了超线程，那么这个系统就有一个8位的位掩码。 如果针对某个线程设置了指定的位，那么这个线程就可以在相关的CPU上运行。因此，如果一个线程可以在任何CPU上运行，并且能够根据需要在处理器之间进行迁移，那么位掩码就全是1。**实际上，在Linux中，这就是线程的默认状态。** Linux内核API提供了一些方法，让用户可以修改位掩码或查看当前的位掩码： sched_set_affinity（）（用来修改位掩码） sched_get_affinity（）（用来查看当前的位掩码） 注意，cpu_affinity会被传递给子线程，因此应该适当地调用sched_set_affinity。 将线程与CPU绑定，最直观的好处就是提高了CPU Cache的命中率，从而减少内存访问损耗，提高程序的速度。在多核体系CPU上，提高外设以及程序工作效率最直观的办法就是让各个物理核各自负责专门的事情。尤其在在NUMA架构下，这个操作对系统运行速度的提升有更大的意义，跨NUMA节点的任务切换，将导致大量三级Cache的丢失。从这个角度来看，NUMA使用CPU绑定时，每个核心可以更专注地处理一件事情，资源体系被充分使用，减少了同步的损耗。 通常Linux内核都可以很好地对线程进行调度，在应该运行的地方运行线程，也就是说在可用的处理器上运行并获得很好的整体性能。内核包含了一些用来检测CPU之间任务负载迁移的算法，可以启用线程迁移来降低繁忙的处理器的压力。只有在以下三个特殊场景会用到CPU亲和性绑定机制： 大量计算：在科学计算和理论计算中，如果不进行CPU亲和性绑定，会发现自己的应用程序要在多处理器的机器上花费大量时间进行迁移从而完成计算。 复杂程序测试：比如在线性可伸缩测试中，我们期望的理论模型是如果应用程序随着CPU的增加可以线性地伸缩，那么每秒事务数和CPU个数之间应该会是线性的关系。这样建模可以测试应用程序是否可以有效地使用底层硬件。如果一个给定的线程迁移到其他地方去了，那么它就失去了利用CPU缓存的优势。实际上，如果正在使用的CPU需要为自己缓存一些特殊的数据，那么其他所有CPU都会使这些数据在自己的缓存中失效。因此，如果有多个线程都需要相同的数据，那么将这些线程绑定到一个特定的CPU上，就可以确保它们访问相同的缓存数据或者至少可以提高缓存的命中率。 实时性线程：对于实时性线程经常会希望使用亲和性来指定一个8路主机上的某个CPU来处理，而同时允许其他7个CPU处理所有普通的系统调度。这种做法对长时间运行、对时间敏感的应用程序可以确保正常运行，同时可以允许其他应用程序独占其余的计算资源。 Linux内核提供了启动参数isolcpus。对于有4个CPU的服务器，在启动的时候加入启动参数isolcpus=2，3。那么系统启动后将不使用CPU3和CPU4。注意，这里说的不使用不是绝对地不使用，系统启动后仍然可以通过taskset命令指定哪些程序在这些核心中运行。 1）修改/etc/default/grub文件中内容，在CMDLINE中添加如下图所示设置 2）编译内核启动文件 123456789[root@C7-Server01 myshell]# grub2-mkconfig -o /boot/grub2/grub.cfgGenerating grub configuration file ...Found linux image: /boot/vmlinuz-3.10.0-957.10.1.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-957.10.1.el7.x86_64.imgFound linux image: /boot/vmlinuz-3.10.0-862.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-862.el7.x86_64.imgFound linux image: /boot/vmlinuz-0-rescue-e344b139f44946638783478bcb51f820Found initrd image: /boot/initramfs-0-rescue-e344b139f44946638783478bcb51f820.imgdone 3）重启系统后查看/proc/cmdline配置文件是否设置生效 12[root@C7-Server01 ~]# cat /proc/cmdline BOOT_IMAGE=/vmlinuz-3.10.0-957.10.1.el7.x86_64 root=UUID=0887567f-1df6-425f-ba3d-ce58584279e0 ro crashkernel=auto biosdevname=0 net.ifnames=0 rhgb quiet isolcpu=2,3 DPDK的线程基于pthread接口创建，属于抢占式线程模型，受内核调度支配。DPDK通过在多核设备上创建多个线程，每个线程绑定到单独的核上，减少线程调度的开销，以提高性能。DPDK的线程可以作为控制线程，也可以作为数据线程。控制线程一般绑定到MASTER核上，接受用户配置，并传递配置参数给数据线程等；数据线程分布在不同核上处理数据包。 DPDK的lcore指的是EAL线程，本质是基于pthread（Linux/FreeBSD）封装实现。Lcore（EAL pthread）由remote_launch函数指定的任务创建并管理。在每个EAL pthread中，有一个TLS（Thread Local Storage）称为_lcore_id。当使用DPDK的EAL‘-c’参数指定coremask时，EAL pthread生成相应个数lcore，并默认是1：1亲和到coremask对应的CPU逻辑核，_lcore_id和CPU ID是一致的。 123456789101112131415161718/* rte_eal_cpu_init（）函数中，通过读取/sys/devices/system/cpu/cpuX/下的相关信息，确定当前系统有哪些CPU核，以及每个核属于哪个CPU Socket。eal_parse_args（）函数，解析-c参数，确认哪些CPU核是可以使用的，以及设置第一个核为MASTER。为每一个SLAVE核创建线程，并调用eal_thread_set_affinity（）绑定CPU。线程的执行体是eal_thread_loop（）,函数内部的主体是一个while死循环，调用不同模块注册到lcore_config[lcore_id].f的回调函数 */RTE_LCORE_FOREACH_SLAVE(i) &#123; /* * create communication pipes between master thread * and children */ if (pipe(lcore_config[i].pipe_master2slave) &lt; 0) rte_panic("Cannot create pipe\n"); if (pipe(lcore_config[i].pipe_slave2master) &lt; 0) rte_panic("Cannot create pipe\n"); lcore_config[i].state = WAIT; /* create a thread for each lcore */ ret = pthread_create(&amp;lcore_config[i].thread_id, NULL, eal_thread_loop, NULL); if (ret！= 0) rte_panic("Cannot create thread\n"); &#125;/* 不同的模块需要调用rte_eal_mp_remote_launch（），将自己的回调处理函数注册到lcore_config[].f中。以l2fwd为例，注册的回调处理函数是l2fwd_launch_on_lcore（）*/rte_eal_mp_remote_launch(l2fwd_launch_one_lcore, NULL, CALL_MASTER); DPDK每个核上的线程最终会调用eal_thread_loop（）&gt;&gt;&gt; l2fwd_launch_on_lcore（），调用到自己实现的处理函数。默认情况下，lcore是与逻辑核一一亲和绑定的。带来性能提升的同时，也牺牲了一定的灵活性和能效。在现网中，往往有流量潮汐现象的发生，在网络流量空闲时，没有必要使用与流量繁忙时相同的核数。于是，EAL pthread和逻辑核之间进而允许打破1：1的绑定关系，使得_lcore_id本身和CPU ID可以不严格一致。EAL定义了长选项“——lcores”来指定lcore的CPU亲和性。对一个特定的lcore ID或者lcore ID组，这个长选项允许为EAL pthread设置CPU集。这个选项以及对应的一组API（rte_thread_set/get_affinity（））为lcore提供了亲和的灵活性。lcore可以亲和到一个CPU或者一个CPU集合，使得在运行时调整具体某个CPU承载lcore成为可能。同时，多个lcore也可能亲和到同一个核，但是这种情况下如果调度占用的内核库是非抢占式，就存在锁机制，DPDK技术栈在电信云中的最佳实践（2）中会专门针对不同锁进制进行讨论。 除了使用DPDK提供的逻辑核之外，用户也可以将DPDK的执行上下文运行在任何用户自己创建的pthread中。在普通用户自定义的pthread中，lcore id的值总是LCORE_ID_ANY，以此确定这个thread是一个有效的普通用户所创建的pthread。用户创建的pthread可以支持绝大多数DPDK库，没有任何影响。但少数DPDK库可能无法完全支持用户自创建的pthread，如timer和Mempool。详细请参见《DPDK开发者手册多线程章节》。 DPDK不仅可以通过绑核完成大量计算任务资源亲和性调度，同时在计算任务较小，一个核的资源绰绰有余的情况下，还可以通过Linux的cgroup对资源进行释放。因为，DPDK的线程其实就是普通的pthread，其本质就是使用cgroup能把CPU的配额灵活地配置在不同的线程上。因此，DPDK可以借助cgroup实现计算资源配额对于线程的灵活配置，可以有效改善I/O核的闲置利用率。 最后，用一张图来总结lcore的启动过程和执行任务分发的流程。]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-22-Linux系统命令-第四篇《系统信息显示命令》]]></title>
    <url>%2F2019%2F04%2F23%2F2019-04-22-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E5%9B%9B%E7%AF%87%E3%80%8A%E7%B3%BB%E7%BB%9F%E4%BF%A1%E6%81%AF%E6%98%BE%E7%A4%BA%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[uname：显示系统信息uname命令用于显示系统相关信息，比如内核版本号、硬件架构等。 语法格式：uname [option] 重要选项参数 【使用示例】 1）显示当前系统所有信息 12[root@C7-Server01 kkutysllb]# uname -aLinux C7-Server01 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux 2）显示当前服务器硬件架构 12[root@C7-Server01 kkutysllb]# uname -mx86_64 3）显示当前服务器主机名 12[root@C7-Server01 kkutysllb]# uname -nC7-Server01 4）显示内核发行版本 12[root@C7-Server01 kkutysllb]# uname -r3.10.0-957.10.1.el7.x86_64 5）显示当前服务器处理器类型 12[root@C7-Server01 kkutysllb]# uname -p x86_64 6）显示当前服务器硬件平台 12[root@C7-Server01 kkutysllb]# uname -ix86_64 hostname：显示或设置系统的主机名hostname命令用于显示或设置系统的主机名称。许多网络程序均用主机名来标识主机，若没有设置好主机名，则可能会导致网络服务不正常。 语法格式：hostname [option] 重要参数选项 【使用示例】 1）显示主机名 12[root@C7-Server01 kkutysllb]# hostnameC7-Server01 2）临时修改主机名 123[root@C7-Server01 kkutysllb]# hostname kkutysllb # 将主机名临时修改为kkutysllb[root@C7-Server01 kkutysllb]# hostnamekkutysllb 3）永久修改主机名 123[root@C7-Server01 kkutysllb]# hostnamectl set-hostname C7-Server01[root@C7-Server01 kkutysllb]# hostnamec7-server01 还有修改的方式就是修改/etc/hostname文件的内容： 4）配置主机名的DNS解析 1234567&gt; # 修改/etc/hosts文件，在其后追加如下配置&gt; &gt; [root@C7-Server01 kkutysllb]# echo "&gt; &gt; &gt; 192.168.101.81 C7-Server01&gt; &gt; " &gt;&gt; /etc/hosts&gt; stat：显示文件或文件系统状态stat命令用于详细显示文件或文件系统的状态信息。 语法格式：stat [option] [file] 重要选项参数 【使用示例】 1）查看文件的属性信息 123456789[root@C7-Server01 kkutysllb]# stat /etc/hostsFile: ‘/etc/hosts’Size: 187 Blocks: 8 IO Block: 4096 regular fileDevice: 803h/2051d Inode: 33589107 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2019-04-22 22:56:50.071492174 +0800Modify: 2019-04-22 22:56:43.218501936 +0800Change: 2019-04-22 22:56:43.218501936 +0800Birth: - 输出各项解释如下： size：文件大小 Blocks：占用block的数量 IO Blocks：Block总大小为4096（8*512） regular file：文件类型为普通文件 Device：设备编号的16进制和10进制 Inode：文件的inode值 Links：文件的硬链接数 Access：（0644/-rw-r–r–）：文件权限 Uid：文件归属用户 Gid：文件归属用户组 Access：文件的访问时间 Modify：文件的修改时间 Change：文件状态改变时间 2） 查看文件系统属性 12345678# 查看/etc/hosts文件所在分区的文件系统属性[root@C7-Server01 kkutysllb]# stat -f /etc/hostsFile: "/etc/hosts"ID: 80300000000 Namelen: 255 Type: xfsBlock size: 4096 Fundamental block size: 4096Blocks: Total: 10902067 Free: 10223852 Available: 10223852Inodes: Total: 21814784 Free: 21699625 显示/etc/hosts文件所在分区的文件系统类型为xfs，Blocks和Inodes的占用情况 3）使用指定格式输出文件内容 1234567891011121314[root@C7-Server01 kkutysllb]# stat -c %a /etc/hosts # %a显示文件的10进制格式权限644[root@C7-Server01 kkutysllb]# stat -c %A /etc/hosts # %A显示文件的可读格式权限-rw-r--r--[root@C7-Server01 kkutysllb]# stat -c %o /etc/hosts # %o显示文件IO块数量4096[root@C7-Server01 kkutysllb]# stat -c %n /etc/hosts # %n显示文件名/etc/hosts[root@C7-Server01 kkutysllb]# stat -c %i /etc/hosts # %i显示文件的inode值33589107[root@C7-Server01 kkutysllb]# stat -c %b /etc/hosts # %b显示文件占用的block块数量8[root@C7-Server01 kkutysllb]# stat -c %B /etc/hosts # %B显示文件占用的block块单位大小512 du：统计磁盘空间使用情du命令可以用于统计磁盘空间的使用情况，这个命令有助于我们找出哪个文件过多地占用了磁盘空间。 语法格式：du [option] [file] 重要选项参数 【使用示例】 1）通过参数-a显示所有当前目录或文件所占空间 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@C7-Server01 kkutysllb]# du -a4 ./.bash_logout4 ./.bash_profile4 ./.bashrc4 ./.bash_history0 ./mytest0 ./data/stu010 ./data/stu02/test010 ./data/stu02/test020 ./data/stu02/test030 ./data/stu02/test040 ./data/stu020 ./data/stu03/test010 ./data/stu03/test020 ./data/stu03/test030 ./data/stu03/test040 ./data/stu030 ./data0 ./image0080 ./image0090 ./202012312234.550 ./image0100 ./data_tmp/stu010 ./data_tmp/stu02/test010 ./data_tmp/stu02/test020 ./data_tmp/stu02/test030 ./data_tmp/stu02/test040 ./data_tmp/stu020 ./data_tmp/stu03/test010 ./data_tmp/stu03/test020 ./data_tmp/stu03/test030 ./data_tmp/stu03/test040 ./data_tmp/stu030 ./data_tmp4 ./hard_link0 ./soft_link4 ./data0014 ./data0024 ./test.sh4 ./data0034 ./data0044 ./test01.sh0 ./test014 ./data00552 . 上面占用大小单位的K字节。 2）显示当前目录的总大小 1234[root@C7-Server01 kkutysllb]# du -s52 .[root@C7-Server01 kkutysllb]# du -sh52K . 3）显示指定层次的目录大小 12345678910111213[root@C7-Server01 kkutysllb]# du -h --max-depth=1 /usr/local0 /usr/local/bin0 /usr/local/etc0 /usr/local/games0 /usr/local/include0 /usr/local/lib0 /usr/local/lib640 /usr/local/libexec0 /usr/local/sbin0 /usr/local/share0 /usr/local/src182M /usr/local/python3182M /usr/local 4）显示/usr/local下的第一和第二层子目录大小，但是不含python3 123456789101112131415[root@C7-Server01 kkutysllb]# du -h --max-depth=2 --exclude=/usr/local/python3 /usr/local0 /usr/local/bin0 /usr/local/etc0 /usr/local/games0 /usr/local/include0 /usr/local/lib0 /usr/local/lib640 /usr/local/libexec0 /usr/local/sbin0 /usr/local/share/applications0 /usr/local/share/info0 /usr/local/share/man0 /usr/local/share0 /usr/local/src0 /usr/local date：显示与设置系统时间date命令用于显示当前的系统时间或设置系统时间。 语法格式：date [option] [+FORMAT] 重要参数选项 【使用示例】 1）常用时间格式测试 123456789101112131415161718[root@C7-Server01 kkutysllb]# date +%y # 显示年份（短格式）19[root@C7-Server01 kkutysllb]# date +%Y # 显示年份（长格式）2019[root@C7-Server01 kkutysllb]# date +%m # 显示月份04[root@C7-Server01 kkutysllb]# date +%d # 显示日期22[root@C7-Server01 kkutysllb]# date +%H # 显示小时23[root@C7-Server01 kkutysllb]# date +%M # 显示分44[root@C7-Server01 kkutysllb]# date +%S # 显示秒52[root@C7-Server01 kkutysllb]# date +%F # 显示标准日期格式（年-月-日）2019-04-22[root@C7-Server01 kkutysllb]# date +%T # 显示标准时间格式（时:分:秒）23:45:02 2）通过参数-d显示指定字符串所描述的时间示例 12345678910[root@C7-Server01 kkutysllb]# date +%F -d '-1day' # 显示昨天2019-04-21[root@C7-Server01 kkutysllb]# date +%F -d '+1day' # 显示明天2019-04-23[root@C7-Server01 kkutysllb]# date +%F -d '1month' # 显示1个月后2019-05-22[root@C7-Server01 kkutysllb]# date +%F -d '1year' # 显示1年后2020-04-22[root@C7-Server01 kkutysllb]# date +%F -d '24hour' # 显示24小时后2019-04-23 3）时间格式转换 12[root@C7-Server01 kkutysllb]# date -d "Thu Jul 6 21:41:16 CST 2017" "+%Y-%m-%d %H:%M:%S"2017-07-06 21:41:16 -d选项后面接上需要转化的时间，最后再接上你想要输出的时间格式。 4）设置系统时间 12[root@C7-Server01 kkutysllb]# date -s "Mon Apr 22 23:56:53 CST 2019"Mon Apr 22 23:56:53 CST 2019 whereis：显示命令及其相关文件全路径whereis命令用于定位指定命令的可执行文件、源码文件及man帮助文件的路径。whereis命令用于在PATH环境变量里查找指定的命令。 语法格式：whereis [option] [filename] 重要选项参数 【使用范例】 1）将与python相关的文件都查找出来 12[root@C7-Server01 kkutysllb]# whereis pythonpython: /usr/bin/python /usr/bin/python2.7 /usr/bin/python2.7-config /usr/bin/python.bak /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 /usr/share/man/man1/python.1.gz 2）只查找python相关的可执行文件 12[root@C7-Server01 kkutysllb]# whereis -b pythonpython: /usr/bin/python /usr/bin/python2.7 /usr/bin/python2.7-config /usr/bin/python.bak /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 3）查找python的man帮助文件 12[root@C7-Server01 kkutysllb]# whereis -m pythonpython: /usr/share/man/man1/python.1.gz 4）查找python的源代码文件 12[root@C7-Server01 kkutysllb]# whereis -s pythonpython: # 为空表示没有找到 如果只是想查找命令的全路径，Linux中还有个which命令更常用，请大家自行研究。 who：显示已登录用户信息who命令能够显示已经登录系统的用户，以及系统的启动时间等信息。 语法格式：who [option] 重要选项参数 【使用示例】 1）显示已登录用户信息的不同参数实践例子 123456789[root@C7-Server01 kkutysllb]# whoroot pts/0 2019-04-22 22:53 (192.168.101.1)[root@C7-Server01 kkutysllb]# who -b system boot 2019-04-22 22:53[root@C7-Server01 kkutysllb]# who -lLOGIN tty1 2019-04-22 22:53 9468 id=tty1[root@C7-Server01 kkutysllb]# who -HNAME LINE TIME COMMENTroot pts/0 2019-04-22 22:53 (192.168.101.1) 2）显示最全的登录用户的信息 123456[root@C7-Server01 kkutysllb]# who -H -aNAME LINE TIME IDLE PID COMMENT EXITsystem boot 2019-04-22 22:53LOGIN tty1 2019-04-23 00:17 10515 id=tty1run-level 3 2019-04-22 22:53root + pts/0 2019-04-22 22:53 . 10337 (192.168.101.1) df：报告文件系统磁盘空间的使用情况显示文件系统磁盘空间的使用情况。如果不指定命令后面的文件参数，则会显示所有磁盘分区的使用情况，如果给定文件，则显示此文件所在磁盘分区的使用情况。 语法格式：df [option] [file] 重要选项参数 【使用示例】 1）显示磁盘的使用情况 123456789[root@C7-Server01 kkutysllb]# dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/sda3 43608268 2712880 40895388 7% /devtmpfs 3984384 0 3984384 0% /devtmpfs 3995140 0 3995140 0% /dev/shmtmpfs 3995140 11920 3983220 1% /runtmpfs 3995140 0 3995140 0% /sys/fs/cgroup/dev/sda1 406180 164980 241200 41% /boottmpfs 799032 0 799032 0% /run/user/0 2）以人类可读的方式显示磁盘的使用情况 123456789[root@C7-Server01 kkutysllb]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda3 42G 2.6G 40G 7% /devtmpfs 3.8G 0 3.8G 0% /devtmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 3.9G 12M 3.8G 1% /runtmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 397M 162M 236M 41% /boottmpfs 781M 0 781M 0% /run/user/0 3）显示磁盘inode的使用情况 123456789[root@C7-Server01 kkutysllb]# df -hiFilesystem Inodes IUsed IFree IUse% Mounted on/dev/sda3 21M 113K 21M 1% /devtmpfs 973K 399 973K 1% /devtmpfs 976K 1 976K 1% /dev/shmtmpfs 976K 1.3K 975K 1% /runtmpfs 976K 16 976K 1% /sys/fs/cgroup/dev/sda1 200K 335 200K 1% /boottmpfs 976K 1 976K 1% /run/user/0 4）显示指定文件系统类型的磁盘 1234[root@C7-Server01 kkutysllb]# df -t xfsFilesystem 1K-blocks Used Available Use% Mounted on/dev/sda3 43608268 2712880 40895388 7% //dev/sda1 406180 164980 241200 41% /boot 5）列出系统文件系统的类型 123456789[root@C7-Server01 kkutysllb]# df -TFilesystem Type 1K-blocks Used Available Use% Mounted on/dev/sda3 xfs 43608268 2712880 40895388 7% /devtmpfs devtmpfs 3984384 0 3984384 0% /devtmpfs tmpfs 3995140 0 3995140 0% /dev/shmtmpfs tmpfs 3995140 11920 3983220 1% /runtmpfs tmpfs 3995140 0 3995140 0% /sys/fs/cgroup/dev/sda1 xfs 406180 164980 241200 41% /boottmpfs tmpfs 799032 0 799032 0% /run/user/0 top：实时显示系统中各个进程的资源占用状况top命令用于实时地对系统处理器状态进行监控，它能够实时地显示系统中各个进程的资源占用状况。该命令可以按照CPU的使用、内存的使用和执行时间对系统任务进程进行排序显示，同时top命令还可以通过交互式命令进行设定显示。 语法格式：top [option] 重要选项参数 交互式命令 【使用示例】 1）显示进程信息 1[root@C7-Server01 kkutysllb]# top 2）显示多核不同核CPU的信息 在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况。 图中显示系统有4个逻辑CPU 3）将进程按照批处理方式排序 1[root@C7-Server01 kkutysllb]# top -b 4）显示进程的完整路径 1[root@C7-Server01 kkutysllb]# top -c 5）显示指定的进程 1[root@C7-Server01 kkutysllb]# top -p 10560 6）交互式例子 默认进入top命令模式，各进程是按照CPU的使用量来排序的 场景1：敲击键盘“b”和“x” 场景2：敲击键盘“z”和“x” 场景3：通过“&gt;”或“&lt;”可以向右或向左改变排序列 敲击”&gt;”变为按内存排序 一致敲击“&lt;”变为按PID排序]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-21-x86架构基础]]></title>
    <url>%2F2019%2F04%2F21%2F2019-04-21-x86%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[标准服务器技术是网络功能虚拟化（NFV）实现的一个关键因素，了解一些x86架构的基础知识对大家后续了解电信云关键技术，尤其是掌握虚拟化技术原理和关键优化方案是必须具备的。本文主要从x86架构的CPU指令集增强，内存管理、中断和异常、IO架构等部分进行阐述，以及包含一些基础IT的基本概念的讲解。 x86-64指令集的增强Intel 的x86体系结构是世界上最流行的处理器架构，从1978年8086/8088处理器问世到现在的Core i7和Core i9，以及Xeon系列处理器，Intel x86体系结构已经在CPU领域叱咤40多年。 x86-64是x86架构的延伸产品，是一种64位微处理器架构及其相应的指令集。在x86-64出现以前，Intel与惠普联合推出IA-64架构，此架构不与x86兼容，且市场反应冷淡。于是，与x86兼容的x86-64架构应运而生。1999年，AMD 首次公开64位指令集为IA-32 提供扩展，称为x86-64（后来改名为AMD64）。此架构后来也为Intel 所采用，也就是现在的Intel 64。 x86-64能有效地把x86架构移植到64位环境，并且兼容原有的x86应用程序，市场前景广阔。外界使用x84-64或者x64称呼这个64位架构，以保持中立，不偏袒任何一家厂商。 AMD 64指令集主要特点有：支持64 位通用寄存器、64 位整数及逻辑运算和64 位虚拟地址。 Intel 64架构加入了额外的寄存器和其他改良的指令集，可使处理器直接访问超过4GB的内存，允许运行更大的应用程序。通过64位的存储器地址上限，其理论存储器容量上限达16EB，目前大多数操作系统和应用程序已基本支持完整的64位地址。 x86的内存架构硬件架构中最复杂、最核心的部分就是其内存架构。此部分详细内容因为篇幅有限无法详细展开，面向的人员主要包括CPU架构设计、操作系统开发和内核底层优化等领域，至于运维方面后期如果不做内核优化的同事了解即可，如感兴趣可参考《手把手教你设计CPU-RISC-V处理器》、《嵌入式操作系统原理》和《处理器虚拟化技术》等书籍。 地址空间地址空间是所有可用资源的集合，我们姑且将它看做一个大大的数组，那么地址就是这个数组的索引。地址空间可以划分为物理地址空间和线性地址空间两大类。 （1）物理地址空间 硬件平台通常划分为CPU、内存和其他硬件设备三个部分。其中，CPU 是整个硬件平台的主导者，内存和其他硬件设备都是CPU 可以使用的资源。这些资源组合在一起，分布在CPU的物理地址空间内，CPU使用物理地址索引这些资源。物理地址空间的大小由CPU实现的物理地址位数所决定，物理地址位数由CPU经过MMU（Memory Management Unit，内存管理单元）转换后的外地址总线位数决定。外地址总线位数与CPU处理数据的能力（即CPU 位数）没有必然的联系，例如：16位的8086 CPU具有20位地址空间。 一个硬件平台只有一个物理地址空间，但每个程序都认为自己独享整个平台的硬件资源。为了让多个程序能够有效地相互隔离，也为了它们能够有效地使用物理地址空间的资源，引入了线性地址空间的概念。 （2）线性地址空间 线性地址空间的大小由CPU实现的线性地址位数决定，线性地址位数由CPU的内地址总线位数决定。由于CPU的内地址总线与CPU的执行单元直连，所以，内地址总线位数往往与CPU位数一致，如果是32 位处理器，那么它就实现了32 位线性地址，其线性地址空间为4GB，如果是64位处理器，那么它的线性地址空间的为2的64次方，即16384GB。需要注意的是，线性地址空间的大小与物理地址空间的大小没有必然联系，Intel的PAE平台具有4GB 的线性地址空间，而其物理地址空间为64GB。但是，线性地址空间会被映射到某一部分物理地址空间或整个物理地址空间。也就是说，线性地址空间小于等于物理地址空间。 CPU负责将线性地址空间转换成物理地址空间，保证程序能够正确访问到该线性地址空间所映射到的物理地址空间。在现代操作系统中，每个进程通常都拥有自己的私有线性地址空间。一个典型的线性地址空间构造如下图所示。 地址地址是访问地址空间的索引。根据访问地址空间的不同，索引可以分为物理地址和线性地址。但由于x86特殊的段机制，还存在一种额外的地址—逻辑地址。 （1）逻辑地址 逻辑地址是程序直接使用的地址（x86无法禁用段机制，逻辑地址一直存在）。逻辑地址由一个16位的段选择符和一个32位的偏移量（32位平台）构成。下面以具体程序为例进行解释。比如：我们写下面一段c语言代码： 12int a = 100; # 定义一个整型变量aint *p = &amp;a; # 定义一个整型指针p，指向变量a在内存中的地址 上述语句中的指针变量p存储的就是变量a的逻辑地址。实际上，p中存储的仅是逻辑地址的偏移部分，而偏移对应的段选择符位于段寄存器中，并没有在程序中显示。 （2）线性地址 线性地址又称虚拟地址。线性地址是逻辑地址转换后的结果，用于索引线性地址空间。当CPU使用分页机制时，还需要将线性地址转换成物理地址才能访问物理平台内存或其他硬件设备；当分页机制未启用时，线性地址与物理地址相同。 （3）物理地址 物理地址是物理地址空间的索引，是CPU提交到总线用于访问物理平台内存或其他硬件设备的最终地址，在x86下，物理地址有时也被称为总线地址。 根据上面的描述，我们可以总结如下： 分段机制启用，分页机制未启用：逻辑地址—&gt;线性地址=物理地址 分段机制、分页机制同时启用：逻辑地址—&gt;线性地址—&gt;物理地址 x86内存管理机制x86架构的内存管理机制分为两部分：分段机制和分页机制。分段机制为程序提供彼此隔离的代码区域、数据区域、栈区域，从而避免了同一个处理器上运行的多个程序互相影响。 分页机制实现了传统的按需分页、虚拟内存机制，可以将程序的执行环境按需映射到物理内存。此外，分页机制还可以用于提供多任务的隔离。 分段机制和分页机制都可以通过配置，支持简单的单任务系统、多任务系统或共享内存的多处理器系统。需要强调的一点是，处理器无论在何种运行模式下都不可以禁止分段机制，但是分页机制却是可选选项。 （1）分段机制 分段机制是x86架构下的朴素内存管理机制，不可以禁用。了解分段机制有利于对后续内存虚拟化原理和优化方案有更深的了解。 分段机制将内存划分成以基地址（Base）和长度（Limit）描述的块。段可以与程序最基本的元素联系起来，程序可以简单地划分为代码、数据和栈，段机制就有相应的代码段、数据段和栈段。 一个程序根据分段机制在内存中由逻辑地址、段选择符、段描述符和段描述符表4 个基本部分构成。 1）当程序使用逻辑地址访问内存的某个部分时，CPU通过逻辑地址中的段选择符索引段描述符表，进而得到该内存对应的段描述符（段描述符描述段的基地址、长度以及读/写、访问权限等属性信息） 2）根据段描述符中的段属性信息检测程序的访问是否合法，如果合法，再根据段描述符中的基地址将逻辑地址转换为线性地址。 这个流程可以用如下图示进行总结。 段选择符是逻辑地址的一个组成部分，用于索引段描述符表以获得该段对应的段描述符。段选择符作为逻辑地址的一部分，对应用程序是可见的。但是，正如前面在逻辑地址中介绍的，应用程序中只存储和使用逻辑地址的偏移部分，段选择符的修改和分配由连接器和加载器完成。 为了使CPU能够快速地获得段选择符，x86架构提供了6个段寄存器存放当前程序中各个段的段选择符。这6 个段寄存器分别如下： CS（Code-Segment，代码段）：存放代码段的段选择符。 DS（Data-Segment，数据段）：存放数据段的段选择符。 SS（Stack-Segment，栈段）：存放栈的段选择符。 ES、FS、GS：可以存放额外三个数据段的段选择符，由程序自由使用。 由于段选择符的存在最终是为了索引段描述符表中的段描述符，为了加速段描述符的访问，x86架构在不同的段寄存器后增加了一个程序不可见的段描述符寄存器。当相应段寄存器中加入一个新的段选择符后，CPU自动将该段选择符索引的段描述符加载到这个不可见的段描述符寄存器中。各个段寄存器的构造如下。 段描述符描述某个段的基地址、长度以及各种属性（例如，读/写属性、访问权限等）。这是分段机制的核心思想。当CPU通过一个逻辑地址的段选择符获得该段对应的段描述符后，会使用段描述符中各种属性字段对访问进行检查，一旦确认访问合法，CPU将段描述符中的基地址和程序中逻辑地址的偏移量相加就得到程序的线性地址。 正如前面讲到的，x86架构在每个段寄存器后增加了一个程序不可见的段描述符寄存器，每当段寄存器加入一个新的段选择符后，CPU自动将该段选择符索引的段描述符加载到这个段描述符寄存器中。后续只要不发生段寄存器的更新操作，CPU就不再查询段描述符表而是直接使用这个段描述符寄存器中的值，从而加快CPU的执行效率。 x86架构提供了两种段描述符表：GDT（全局段描述符表Global Descriptor Table）和LDT（本地段描述符表Local Descriptor Table）。具体选择哪个段描述符表，由段选择符中的TI字段决定，当TI=0时，索引GDT，当TI=1时索引LDT。系统中至少有一个GDT可以被所有的进程访问。与此同时，系统中可以有一个或多个LDT，可以被某个进程私有，也可以被多个进程共享。 GDT是内存中的一个数据结构。简单地讲，可以将GDT看成是一个数组，由基地址（Base）和长度（Limit）描述。 LDT是一个段，需要用一个段描述符来描述。LDT的段描述符存放在GDT中，当系统中有多个LDT时，GDT中必须有对应数量的段描述符。 为了加速对GDT和LDT的访问，x86架构提供了GDTR寄存器和LDTR寄存器。关于这两种寄存器的具体描述如下： GDTR：包括一个32位的基地址（Base）和一个16 位的长度（Limit）。 LDTR：结构与段寄存器相同（同样包含对程序不可见的段描述符寄存器）。 通过段选择符索引GDT/LDT的过程如下图所示： x86架构内存管理中分段机制总结： 1）在程序加载阶段，该进程LDT的段选择符首先索引GDT，获得LDT的段描述符并将其加载到LDTR寄存器中。此外，该进程的CS、DS、SS中加入相应的段选择符，CPU根据段选择符的TI字段索引相应的段描述符表，获得相应的段描述符，并加载入CS、DS、SS对应的程序不可见的段描述符寄存器。 2）程序执行到读/写内存中的数据时，把程序中相应变量的逻辑地址转换为线性地址： 进行必要的属性、访问权限检查； 从DS对应的段描述符寄存器获得该段的基地址； 将变量的32位偏移量和段描述符中的基地址相加，获得该变量的线性地址。 2）分页机制 分段机制的目的是将内存中的线性地址空间划分成以基地址和长度描述的多个段进行管理，程序对应的逻辑地址以基地址和偏移量来描述，实现逻辑地址到线性地址空间的映射。而分页机制是使用单位“页”来管理线性地址空间和物理地址空间的映射关系。同时，分页机制允许一个页面存放在物理内存中或磁盘的交换区域（如Linux下的Swap分区，Windows下的虚拟内存文件）中，程序可以使用比 机器物理内存更大的内存区域，从而实现现代操作系统中虚拟内存机制。（注意：操作系统的虚拟内存原理和映射关系和后面要讲的计算虚拟化技术中内存虚拟化技术基本一致，只是VMM实现时又多了一层嵌套）。 在x86架构下，页的典型大小为4KB，于是一个4GB的线性地址空间被划分成1024×1024个页面，参见本文线性地址空间示意图。物理地址空间的划分与此类似。x86架构允许大于4KB的页面大小（如2MB、4MB、1GB）等。 分页机制的核心思想是通过页表将线性地址转换为物理地址，并配合旁路转换缓冲区（Translation Lookaside Buffer，后面简称为TLB）来加速地址转换的过程。分页机制主要由页表、CR3 寄存器和TLB三个部件构成，如下图所示。 页表是用于将线性地址转换成物理地址的主要数据结构。一个地址对齐到页边界后的值称为页帧号（或者页框架），它实际上就是该地址所在页面的基地址。比如：一个页大小为4kB，那么第一个页帧号就是0，第二个页帧号就是4097，依次类推。线性地址对应的页帧号叫做虚拟页帧号（Virtual Frame Number，下面简称为VFN），物理地址对应的页帧号叫做物理页帧号（Physical Frame Number，下面简称为PFN）或机器页帧号。页表实际上是存储VFN到PFN映射的数据结构。 在传统的32位的保护模式中（未启用物理地址扩展PAE功能），x86处理器使用两级转换方案，在这种方案中，CR3寄存器指向一个4KB大小的页目录表，页目录中共有1024个记录，每一项记录大小4B空间，都指向一个4KB大小的页表，页表中也有1024项，每项大小4B空间，所以，最后整个线性地址空间大小就是1024 个长为4KB的页，即总共4GB大小的空间。未启用PAE 的4KB大小的页面如下图所示。 页目录项（Page Directory Entry，下面简称为PDE），包含页表的物理地地址，PDE存放在页目录表中。 页表项（Page Table Entry，下面简称为PTE）：包含该线性地址对应的物理页帧号PFN，PTE存在页表中，确定物理页帧号PFN 后，再将线性地址的0~11位偏移量与其相加，就可以确定该线性地址对应的物理地址。 虚拟内存实现的关键在于PDE和PTE都包含一个P（Present）字段： 当P=1时，物理页面存在于物理内存中，CPU完成地址转换后可以直接访问该页面。 当P=0时，物理页面不在物理内存中（在硬盘的交换分区中），当CPU访问该页面时，会产生一个缺页错误中断，由操作系统的缺页处理机制将存放在硬盘上的页面调入物理内存，使访问可以继续。同时，由于程序的局部性特点，操作系统会将该页面附近的页面一起调入物理内存，方便CPU的访问。所以，为了减少内存占用，要求程序开发人员尽量少的使用全局索引或递归调用等机制。 P=0时的PDE和PTE的1~31位都将为操作系统提供物理页面在硬盘上的信息，这些位存储着物理页面在硬盘上的位置。 启用物理地址扩展（之后简称为PAE）后，页表结构将发生相应的变化。页表和页目录的总大小仍是4KB，但页表和页目录中的表项都从32位扩为64位，以使用附加的地址位。这样，页表和页目录都只有512个表项，变成了原来方案的一半，所以又加入了一个级：CR3 指向页目录指针表，即一个包含4个页目录指针的表。启用PAE 的4KB大小的页面使用的三级页表如下图所示： CR3寄存器也称为页目录基地址寄存器（Page-Directory Base Register，PDBR），存放着页目录的物理地址。一个进程在运行前，必须将其页目录的基地址存入CR3，而且，页目录的基地址必须对齐到4KB页边界。启用PAE时，CR3指向页目录指针表，每一项都指向一个页目录表，共有4个页目录表。 为了提高地址转换的效率，x86架构使用TLB对最近用到的页面映射进行缓存。TLB中存放着VFN到PFN的转换记录，当CPU访问某个线性地址时，如果其所在页面的映射存在于TLB中，无须查找页表，即可得到该线性地址对应的PFN，CPU 再将它与线性地址的偏移相加，就能得到最后的物理地址。 x86架构内存管理中心分页机制总结： 1）CPU访问一个线性地址，在TLB中进行匹配，如果地址转换在TLB中，则跳到步骤6。否则，发生了一次TLB Miss（TLB 缺失），继续步骤2。 2）查找页表，如果页面在物理内存中，则跳到步骤4。 3）如果页面不在物理内存中，则产生缺页错误，由操作系统的缺页错误处理程序进行以下处理。 将页面从磁盘复制到物理内存中。 更改对应的PTE，设置P 位为1，并对其他字段进行相应的设置。 刷新TLB 中对应的PTE。 从缺页错误处理程序中返回。 4）此时，页面已经存在于物理内存中，并且页表也已经包含了这个映射。重新在TLB中进行匹配，如果地址转换在TLB中，则跳到步骤6。否则，发生了一次TLB Miss（TLB 缺失），继续步骤5。 5）CPU重新查页表，把对应的映射插入到TLB中。 6）此时，TLB已经包含了该线性地址对应的PFN。将PFN和线性地址中的偏移量相加，就得到了对应的物理地址。 中断与异常程序的执行往往不只是按顺序执行那么简单，一些异常和中断会打断顺序执行的程序流，转而进入一条完全不同的执行路径。中断提供给外部设备一种“打断CPU当前执行任务，并响应自身服务”的手段。中断(interrupt)是异步的事件，典型的比如由I/O设备触发；异常(exception)是同步的事件，典型的比如处理器执行某条指令时发现出错了等等，其实异常的本质就是同步中断。 中断通常被定义为一个打断CPU芯片指令执行的事件，该事件对应到CPU芯片内部或者外部的电路产生的电子信号。 中断信号可以被划分为同步中断和异步中断： 同步中断，该类型中断由CPU的控制单元在执行指令的时候产生，并且是在当前指令执行完毕下一个指令执行之前产生。 异步中断，该类型中断由其他硬件设备在任意的时间产生，并且遵循CPU的时钟信号传递给CPU。 对于Intel的CPU而言，它将同步中断称作异常，而将异步中断称作中断。 通常中断（即异步中断）由时钟定时器或者其他I/O设备产生，如键盘接收到敲击某个按键的信号后产生的中断信号。而异常（即同步中断）则通常由于编程错误或者由CPU检测到异常条件需要内核进行处理而产生，如上面讲到的Page Fault Exception（缺页异常），异常可以由程序通过int或者sysenter指令主动产生。 对于Intel x86 CPU而言，它将中断和异常进行了如下归类： 中断，即异步中断，中断信息随着CPU的时钟信号传递到CPU内部。中断分为可屏蔽中断和不可屏蔽中断两类。 可屏蔽中断，所有由I/O设备产生的IRQ请求都被归为可屏蔽中断。一个可屏蔽中断可以有两种状态，屏蔽或者不屏蔽，当一个中断被屏蔽时，该中断信号将被对应的控制单元所忽略。 不可屏蔽中断，即控制单元无法忽略该类型的中断信号，CPU肯定会接收到该类型的中断，一般对应到一些紧要的事件，比如硬件错误。 异常，即同步中断，中断信号在CPU执行完某个指令后产生并接收到。处理器检测到的异常，即当CPU执行指令的时候检测到硬件上存在一些异常条件的时候就会产生该信号。这种类型的异常根据产生时在内核堆栈中保存的EIP寄存器的值（即异常恢复后CPU重新执行的位置）进行细分： Faults，该异常可以被内核正确纠正，并且纠正后重新执行引起该异常的指令时不会造成程序的中断或者功能的异常。这时候保存到EIP寄存器的值是引起异常的指令的地址，故异常恢复的时候会重新执行该指令，如Page Fault Exception（缺页异常），当访问的内存地址没有被映射到物理内存时，产生异常，内核分配新的物理内存页并建立映射关系，然后异常处理完毕后，CPU重新访问该地址，即可访问到正确的物理内存。 Traps，该异常发生时，内核堆栈EIP寄存器保存的地址指向引起该异常的指令的下一条指令，即当该异常处理返回后会继续程序的执行，而不是重新执行引起异常的指令。x86 CPU的硬件虚拟化功能就是利用陷入（Traps）再模拟的方法，当CPU执行虚拟机指令的时候，如果执行的是敏感指令，就会触发Traps类型的异常，让VMM（Virtual Machine Monitor）对该敏感指令进行模拟，然后继续恢复虚拟机的运行。 Aborts，当发生严重的错误时，CPU已经无法保证内核堆栈中EIP寄存器存放的值是引起该异常的指令的地址。该异常用于汇报严重的错误，如硬件错误或者是内存的不一致性。该异常信号让CPU切换到相应的abort exception handler，该处理函数由于无法确认错误，只能结束当前进程。 我们在写程序时，经常会在容易产生错误的地方进行异常抛出，然后针对抛出的异常定义执行策略。这类编程产生的异常，由程序主动执行int或者int3之类的指令产生。CPU像处理Traps一样处理这些程序主动产生的异常，该类异常通常被称为软件中断（software interrupt）。这类异常主要有两种用途：实现系统调用和通知某个debugger特定的事件发生。 这些异常或中断由0~255的数字唯一标识，也就是经常说的中断信号量。对于不可屏蔽中断和异常来说，相应的中断信号量是固定的，而可屏蔽中断对应的中断信号量则可以通过设置中断控制器来更改。 x86系统的I/O架构计算机所处理的任务其实只有两种：CPU运算和I/O操作。这部分内容是后续学习计算虚拟化中I/O虚拟化的基础。I/O（输入/输出）是CPU访问外部设备的方法。设备通常通过寄存器和设备RAM将自身功能展现给CPU，CPU通过读/写这些寄存器和RAM完成对设备的访问及其他操作。按访问方式的不同，x86架构的I/O分为如下两类： 1）端口I/O（后文简称为Port I/O）：即通过I/O端口访问设备寄存器。x86有65536个8位的I/O端口，编号为0x0~0xFFFF。CPU将端口号作为设备端口的地址，进而对设备进行访问。这65536个端口构成了64KB的I/O端口地址空间。I/O端口地址空间是独立的，不是线性地址空间或物理地址空间的一 部分。需要使用特定的操作命令IN/OUT对端口进行访问，此时CPU通过一个特殊的芯片管脚标识这是一次I/O端口访问，于是芯片组知道地址线上的地址是I/O端口号并相应地完成操作。此外，2个或4个连续的8位I/O端口可以组成16位或32位的I/O端口。 2）内存映射I/O（Memory Map I/O，后文简称为MMIO）：即通过内存访问的形式访问设备寄存器或设备RAM。MMIO要占用CPU的物理地址空间，它将设备寄存器或设备RAM映射到物理地址空间的某段地址，然后使用MOV等访存指令访问此段地址，即可访问到映射的设备。MMIO方式访问设备也需 要进行线性地址到物理地址的转换，但是这个转换过程中的MMIO地址不可缓存到TLB中。MMIO是一种更普遍、更先进的I/O访问方式，很多CPU 架构都没有Port I/O，采用统一的MMIO方式。 DMA技术直接内存访问（Direct Memory Access，后文简称为DMA）是所有现代计算机的重要特色。DMA允许设备绕开CPU直接向内存中复制或读取数据。如果设备向内存复制数据都经过CPU，则CPU会有大量中断负载，中断过程中，CPU对其他任务来讲无法使用，不利于系统性能的提高。通过DMA，CPU只负责初始化这个传输动作，而传输动作本身由DMA 控制器（简称为DMAC）来实行和完成。在实现DMA传输时，由DMAC直接控制总线，在DMA传输前，CPU要把总线控制权交给DMAC，结束DMA传输后，DMAC立即把总线控制权交回给CPU。 一个完整的DMA 传输过程的基本流程如下： 1）DMA请求：CPU对DMAC进行初始化，并向I/O端口发出操作命令，I/O端口提出DMA请求。 2）DMA响应：DMAC对DMA请求进行优先级判别和屏蔽判别，然后向总线控制芯片提出总线请。CPU执行完当前总线周期后释放总线控制权。此时，总线控制芯片发出总线应答，表示DMA请求已被响应，并通过DMAC通知I/O端口开始DMA传输。 3）DMA传输：DMAC获得总线控制权后，CPU即可挂起或只执行内部操作，由DMAC发出读/写命令，直接控制RAM与I/O端口进行DMA传输。 4）DMA结束：当完成规定的成批数据传送后，DMAC释放总线控制权，并向I/O端口发出结束信号。当I/O端口接收到结束信号后，停止I/O设备的工作并向CPU提出中断请求，使CPU执行一段检查本次DMA传输操作正确性判断的代码，并从不介入的状态退出。 由此可见，DMA无须CPU直接控制传输，也没有中断处理方式那样保留现场和恢复现场的过程，通过硬件（DMAC）为RAM与I/O设备开辟了一条直接传送数据的通路，极大地提高了CPU效率。需要注意的是，DMA操作访问的必须是连续的物理内存。DMA 传输的过程如下图所示。 进程、线程和协程什么是进程和线程 进程是什么呢？大白话讲，进程就是应用程序的启动实例。比如我们运行一个游戏，打开一个软件，就是开启了一个进程。进程拥有代码和打开的文件资源、数据资源、独立的内存空间。 线程又是什么呢？线程从属于进程，是程序的实际执行者。一个进程至少包含一个主线程，也可以有更多的子线程。线程拥有自己的栈空间。 对操作系统来说，线程是最小的执行单元，进程是最小的资源管理单元。无论进程还是线程，都是由操作系统所管理的。线程一般具有五种状态：初始化&gt;&gt;&gt;可运行&gt;&gt;&gt;运行中&gt;&gt;&gt;阻塞&gt;&gt;&gt;销毁。线程不同状态之间的转化均需要CPU开销来完成。 什么是协程协程 携程英文Coroutines，是一种比线程更加轻量级的存在。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。最重要的是，协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行）。这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。 在Python语言中有个生成器的概念，里面有个关键字yield，当程序执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。但是，yield让程序暂停，和线程的阻塞是有本质区别的。通过yield关键字的暂停完全由程序控制，线程的阻塞状态是由操作系统内核来进行切换。大家可以在Python脚本中写入如下代码并执行体验下： 1234567891011def consume(): while True: # consume等待接收数据 number = yield print("我要执行啦。。。。开始计数：",number)consumer = consume()next(consumer)for num in range(0,100): print("开始执行：",num) consumer.send(num) 在《流畅的Python》一书中还有个例子，可以更好地来说明协程的特点。如下面示例是一个计算移动平均值的协程函数。实现的功能说明如下： 1）函数体中是一个无限循环，表明只要调用方不断把值发给这个协程，它就会一直接收值，然后生成结果。仅当调用方在协程上调用 .close() 方法，或者没有对协程的引用而被垃圾回收程序回收时，这个协程才会终止。 2）yield 表达式用于暂停执行协程，把结果发给调用方；还用于接收调用方后面发给协程的值，恢复无限循环。 使用协程的好处是，total 和 count 声明为局部变量即可，无需使用实例属性或闭包在多次调用之间保持上下文。 123456789101112131415161718192021def averager(): total = 0.0 count = 0 average = None while True: term = yield average total += term count += 1 average = total/count# 创建协程对象coro_avg = averager()# 调用next函数，激活协程next(coro_avg)# 计算移动平均值：多次调用 .send(...) 方法，产出当前的平均值print(coro_avg.send(10)) # 10.0print(coro_avg.send(20)) # 15.0print(coro_avg.send(30)) # 20.0print(coro_avg.send(50)) # 27.5]]></content>
      <categories>
        <category>NFV关键技术</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-20-Linux系统命令-第三篇《文件过滤及内容编辑处理》]]></title>
    <url>%2F2019%2F04%2F20%2F2019-04-20-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%B8%89%E7%AF%87%E3%80%8A%E6%96%87%E4%BB%B6%E8%BF%87%E6%BB%A4%E5%8F%8A%E5%86%85%E5%AE%B9%E7%BC%96%E8%BE%91%E5%A4%84%E7%90%86%E3%80%8B%2F</url>
    <content type="text"><![CDATA[cat：合并文件或查看文件内容cat命令可以理解为英文单词concatenate的缩写，其功能是连接多个文件并且打印到屏幕输出，或者重定向到指定的文件中。此命令常用来显示单个文件内容，或者将几个文件内容连接起来一起显示，还可以从标准输入中读取内容并显示，生产环境中它常与重定向或追加符号配合使用。 cat命令具备5大常用功能 语法格式：cat [option] [file] 重要选项参数 【使用示例】 1）在/home/kkutysllb目录下生成一个data001文件 123456789101112&gt; # EOF这里要按回车才能结束，另外，EOF必须成对出现，但也可以用别的成对标签来替换。&gt; &gt; [root@C7-Server01 kkutysllb]# cat &gt;&gt; data001 &lt;&lt; EOF&gt; &gt; &gt; day001...&gt; &gt; day002...&gt; &gt; day003...&gt; &gt; day004...&gt; &gt; ...&gt; &gt; day00n...&gt; &gt; EOF&gt; 123456789101112131415# 使用cat -n查看刚编辑生成的文件data001 [root@C7-Server01 kkutysllb]# cat -n data001 1 day001... 2 day002... 3 day003... 4 day004... 5 ... 6 day00n... 2）直接执行cat命令，不带任何选项 1234567# 查看当前目录下soft_link软链接文件的内容 [root@C7-Server01 kkutysllb]# cat soft_link 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 3）执行cat命令，分别带-n及-b选项，并对比区别 1234567891011121314&gt; # 新生成data002文件，输入以下内容，注意其中的空行要保留&gt; &gt; [root@C7-Server01 kkutysllb]# cat &gt;&gt; data002 &lt;&lt; EOF&gt; &gt; &gt; www.sina.com.cn&gt; &gt;&gt; &gt; www.baidu.com&gt; &gt;&gt; &gt; www.chinamobile.com&gt; &gt;&gt; &gt; sn.chinamobile.com&gt; &gt;&gt; &gt; EOF&gt; 1234567891011121314&gt; # 使用-n选项查看data002文件的内容&gt; &gt; # 文件内容中的空行也被标识行号&gt; &gt; [root@C7-Server01 kkutysllb]# cat -n data002&gt; 1 www.sina.com.cn&gt; 2 &gt; 3 www.baidu.com&gt; 4 &gt; 5 www.chinamobile.com&gt; 6 &gt; 7 sn.chinamobile.com&gt; 8 &gt; 12345678&gt; # 使用-b选项查看data002文件的内容，会忽略显示空白行行号&gt;&gt; [root@C7-Server01 kkutysllb]# cat -b data002&gt; 1 www.sina.com.cn&gt; 2 www.baidu.com&gt; 3 www.chinamobile.com&gt; 4 sn.chinamobile.com&gt; 3）执行cat命令，带-E选项 123456789101112131415# -E选项会在显示文件的每行行尾加上$符号 [root@C7-Server01 kkutysllb]# cat -E data001 day001...$ day002...$ day003...$ day004...$ ...$ day00n...$ 4）cat的一个特殊小用法（没基础的可以先大致了解，后续学了shell脚本后再看这部分） 1234567891011121314151617# 利用cat在shell脚本中显示帮助菜单 #!/bin/bash exportfs_usage() &#123; cat &lt;&lt; END USAGE: $0 &#123;start|stop|monitor|status|validate-all&#125; END&#125;exportfs_usage 执行上述脚本的输出如下： 为了实现自动化运维，有时运维人员不得不通过脚本把操作写好，然后让其他同事，通过傻瓜式的菜单选择，来完成相应的工作，进而提升工作效率。因此，cat命令在shell编程中常用来描写菜单输出操作。 小练习：其他选项用法请自行练习，将结果贴在讨论区。提示：与cat命令作用相反的命令为tac，其作用是方向显示文件的内容，请自行研究练习。 补充知识：LInux系统中的重定向 重定向简介 重定向是Linux中一个重要知识点，对于它的作用，直白点儿说，就是可以让数据从一个地方（文件或工具）无损地流到另一个地方。 标准输入/输出/错误输出 标准输入是一个名称，它表示数据的一个流入方向，通常表示数据从文件等流入到处理的工具或命令中，用代码0表示，使用&lt;或&lt;&lt;符号来指示数据朝箭头所指的方向流动。 标准输出也是一个名称，也表示数据的一个流入方向，通常用代码1表示，使用&gt;或&gt;&gt;符号来指示数据朝箭头的方向流动。和标准输入不同的是，1表示将命令等处理的一般信息输出到文件。 标准错误输出是另一个名称，也是表示数据的一个流入方向，通常用代码2表示，使用&gt;或&gt;&gt;符号来指示数据朝箭头的方向流动。和标准输出不同的是，标准错误输出2表示将错误的信息输出到文件等，不输出正确的普通信息（仅输出错误信息）。 more：分页显示文件内容more命令的功能类似于cat，但cat命令是将整个文件的内容一次性显示在屏幕上，而more则会一页一页地显示文件内容。但more的功能还是比较简单的，有一个增强版的命令是less，将在后面讲解。（其实，学会了less命令，大家就可以把more命令忘记了） 语法格式：more [option] [file] 重要选项参数 在交互模式下，使用more命令打开文本之后，会进入一个基于vi的交互界面，在这里可以使用部分vi编辑器的功能，如搜索功能，还可以切换到vi编辑器。常用操作方式如下： 【使用示例】 1）more命令后面不接任何参数 123# 显示/etc/services文件内容 [root@C7-Server01 kkutysllb]# more /etc/services 2）显示指定行数的内容 12345678910111213# 显示/etc/service文件的前5行内容[root@C7-Server01 kkutysllb]# more -5 /etc/services # /etc/services:# $Id: services,v 1.55 2013/04/14 ovasik Exp $## Network services, Internet style# IANA services version: last updated 2013-04-10 3）从指定的行数开始显示内容 12345678910111213141516171819202122232425262728293031323334353637383940414243# 从465行开始显示/etc/service文件的内容[root@C7-Server01 kkutysllb]# more -5 /etc/services # /etc/services:# $Id: services,v 1.55 2013/04/14 ovasik Exp $## Network services, Internet style# IANA services version: last updated 2013-04-10[root@C7-Server01 kkutysllb]# more +465 /etc/services bpjava-msvc 13722/tcp # BP Java MSVC Protocolbpjava-msvc 13722/udp # BP Java MSVC Protocolvnetd 13724/tcp # Veritas Network Utilityvnetd 13724/udp # Veritas Network Utilitybpcd 13782/tcp # VERITAS NetBackupbpcd 13782/udp # VERITAS NetBackupvopied 13783/tcp # VOPIED Protocolvopied 13783/udp # VOPIED Protocol# This port is registered as wnn6, but also used under the unregistered name# "wnn4" by the FreeWnn package.wnn6 22273/tcp wnn4wnn6 22273/udp wnn4quake 26000/tcpquake 26000/udpwnn6-ds 26208/tcpwnn6-ds 26208/udptraceroute 33434/tcptraceroute 33434/udp## Datagram Delivery Protocol services。。。 4）分页显示目录下的内容 12345678910111213141516171819202122232425# 分页显示/etc/目录下的内容，每页显示10行内容 [root@C7-Server01 kkutysllb]# ls -l /etc/|more -10 total 1108 -rw-r--r--. 1 root root 16 Apr 7 20:39 adjtime -rw-r--r--. 1 root root 1518 Jun 7 2013 aliases -rw-r--r--. 1 root root 12288 Apr 7 20:43 aliases.db drwxr-xr-x. 2 root root 236 Apr 7 20:35 alternatives -rw-------. 1 root root 541 Apr 11 2018 anacrontab -rw-r--r--. 1 root root 55 Apr 11 2018 asound.conf drwxr-x---. 3 root root 43 Apr 7 20:35 audisp drwxr-x---. 3 root root 83 Apr 7 20:43 audit drwxr-xr-x. 2 root root 33 Apr 7 20:35 bash_completion.d --More-- 。。。 小练习：more的其它用法请自行练习，可在讨论区进行讨论 less：分页显示文件内容如果使用man less查看less的帮助文档，会发现官方的解释是less为more的反义词（opposite of more）。但less命令的名称只是个文字游戏，它是more命令的高级版本（less这个名称来自俗语“越简单就越丰富”，即less is more）。 less命令的基本功能类似于more命令，可以分页显示文件内容，但比more的功能更强大。less命令在读取文件内容时，并不是像more、vi命令一样，要一次性将整个文件加载之后再显示，而是会根据需要来加载文件的内容，这样打开文件的速度会更快。而且less命令支持[page up]、[page down]等按键的功能，可以通过该功能向前或向后翻看文件，这样更容易查看一个文件的内容。 语法格式：less [option] [file] 重要选项参数 在交互模式下，less命令也是基于more命令和vi命令的，在这里可以使用vi编辑器的部分功能，如搜索功能，还可以切换到vi编辑器。表3-7给出了less命令的交互式子命令。 less交互式命令说明 【使用示例】 1）查看文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 使用less查看/etc/services文件内容[root@C7-Server01 kkutysllb]# less /etc/services # /etc/services:# $Id: services,v 1.55 2013/04/14 ovasik Exp $## Network services, Internet style# IANA services version: last updated 2013-04-10## Note that it is presently the policy of IANA to assign a single well-known# port number for both TCP and UDP; hence, most entries here have two entries# even if the protocol doesn't support UDP operations.# Updated from RFC 1700, ``Assigned Numbers'' (October 1994). Not all ports# are included, only the more common ones.## The latest IANA port assignments can be gotten from# http://www.iana.org/assignments/port-numbers# The Well Known Ports are those from 0 through 1023.# The Registered Ports are those from 1024 through 49151# The Dynamic and/or Private Ports are those from 49152 through 65535## Each line describes one service, and is of the form:## service-name port/protocol [aliases ...] [# comment]tcpmux 1/tcp # TCP port service multiplexertcpmux 1/udp # TCP port service multiplexerrje 5/tcp # Remote Job Entryrje 5/udp # Remote Job Entryecho 7/tcpecho 7/udpdiscard 9/tcp sink nulldiscard 9/udp sink nullsystat 11/tcp userssystat 11/udp usersdaytime 13/tcpdaytime 13/udp。。。 2）显示行号 123456789101112131415161718192021222324252627282930313233343536373839404142# 使用less -N查看文件/etc/services的内容[root@C7-Server01 kkutysllb]# less -N /etc/services 1 # /etc/services:2 # $Id: services,v 1.55 2013/04/14 ovasik Exp $3 #4 # Network services, Internet style5 # IANA services version: last updated 2013-04-106 #7 # Note that it is presently the policy of IANA to assign a single well-known8 # port number for both TCP and UDP; hence, most entries here have two entries9 # even if the protocol doesn't support UDP operations.10 # Updated from RFC 1700, ``Assigned Numbers'' (October 1994). Not all ports11 # are included, only the more common ones.12 #13 # The latest IANA port assignments can be gotten from14 # http://www.iana.org/assignments/port-numbers15 # The Well Known Ports are those from 0 through 1023.16 # The Registered Ports are those from 1024 through 4915117 # The Dynamic and/or Private Ports are those from 49152 through 6553518 #19 # Each line describes one service, and is of the form:20 #21 # service-name port/protocol [aliases ...] [# comment]22 23 tcpmux 1/tcp # TCP port service multiplexer24 tcpmux 1/udp # TCP port service multiplexer25 rje 5/tcp # Remote Job Entry26 rje 5/udp # Remote Job Entry27 echo 7/tcp28 echo 7/udp29 discard 9/tcp sink null30 discard 9/udp sink null31 systat 11/tcp users32 systat 11/udp users33 daytime 13/tcp34 daytime 13/udp35 qotd 17/tcp quote36 qotd 17/udp quote37 msp 18/tcp # message send protocol (historic)38 msp 18/udp # message send protocol (historic)39 chargen 19/tcp ttytst source 小练习：使用less分页显示/etc/目录下的内容。（提示：可参照more使用示例） head：显示文件内容头部head命令用于显示文件内容头部，它默认输出文件的开头10行。如果指定了多个文件，则在每一段输出前会给出文件名作为文件头。 语法格式：head [option] [file] 重要选项参数 【使用示例】 1）不带任何参数，默认显示文件的前10行内容 12345678910111213# 显示/etc/passwd文件的前10行内容[root@C7-Server01 kkutysllb]# head /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltmail:x:8:12:mail:/var/spool/mail:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologin 2）显示文件的前n行内容 12345678910# 显示/etc/passwd文件的前5行内容# 选项n可带也可不带，通常情况下，为简化指令一般不带选项n，直接输入数字即可[root@C7-Server01 kkutysllb]# head -5 /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 3） 读取文件的前10个字节 123456# 使用-c选项，读取/etc/hosts文件的前10个字节# 注意：空格符也统计在内[root@C7-Server01 kkutysllb]# head -c 10 /etc/hosts127.0.0.1 # 这行最后有一个空格 4） 显示多个文件 12345678910111213# 同时显示/etc/hosts和/etc/passwd文件的前5行内容[root@C7-Server01 kkutysllb]# head -c 10 /etc/hosts127.0.0.1 [root@C7-Server01 kkutysllb]# head -5 /etc/hosts /etc/passwd==&gt; /etc/hosts &lt;== # 每个文件名作为文件头127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6==&gt; /etc/passwd &lt;== # 每个文件名作为文件头root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 小练习：使用head命令只显示/etc/services文件的最后一行内容 tail：显示文件内容尾部tail命令用于显示文件内容的尾部，它默认输出文件的最后10行。如果指定了多于一个文件，则在每一段输出前会给出文件名作为文件头。 语法格式：tail [option] [file] 重要选项参数 【使用示例】 1）不带任何参数，默认显示文件的最后10行 12345678910111213# 显示/etc/passwd文件的最后10行内容[root@C7-Server01 kkutysllb]# tail /etc/passwdnobody:x:99:99:Nobody:/:/sbin/nologinsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologindbus:x:81:81:System message bus:/:/sbin/nologinpolkitd:x:999:998:User for polkitd:/:/sbin/nologintss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologinsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologinpostfix:x:89:89::/var/spool/postfix:/sbin/nologinchrony:x:998:996::/var/lib/chrony:/sbin/nologinntp:x:38:38::/etc/ntp:/sbin/nologinkkutysllb:x:1000:1000::/home/kkutysllb:/bin/bash 2）显示文件的末尾5行内容 12345678# 与head命令一样，可以省略-n选项，直接填写数字[root@C7-Server01 kkutysllb]# tail -5 /etc/services com-bardac-dw 48556/tcp # com-bardac-dwcom-bardac-dw 48556/udp # com-bardac-dwiqobject 48619/tcp # iqobjectiqobject 48619/udp # iqobjectmatahari 49000/tcp # Matahari Broker 3）实时监控文件的变化 123# 使用-f选项实时监控系统安全日志/var/log/secure[root@C7-Server01 kkutysllb]# tail -f /var/log/secure 初始状态如下 当我再次打开一个终端时，文件实时发生变化 需要说明的是：使用tail -f实时跟踪日志文件结束后，必须使用Ctrl+C退出。同时，在Linux系统还有一个专门跟踪系统日志文件的命令tailf，功能几乎等同于tail-f，与tail-f不同的是，如果文件不增长，那么它不会去访问磁盘文件，也不会更改文件的访问时间。 cut：从文本中提取一段文字并输出cut命令从文件的每一行剪切字节、字符或字段，并将这些字节、字符或字段输出至标准输出。 语法格式：cut [option] [file] 重要选项参数 【使用示例】 1）以字节为分隔符 123456789101112131415161718192021222324252627282930313233343536373839# 显示当前目录下data001文件的内容[root@C7-Server01 kkutysllb]# cat data001day001...day002...day003...day004......day00n...# 以-b选项，只显示data001文件每行的第3个字节内容[root@C7-Server01 kkutysllb]# cut -b 3 data001yyyy.y# 显示data001文件每行的第3到第5个字节内容[root@C7-Server01 kkutysllb]# cut -b 3-5 data001y00y00y00y00.y00# 显示data001文件每行从第1个字节到第3个字节的内容[root@C7-Server01 kkutysllb]# cut -b -3 data001daydaydayday...day 2）以字符为分隔符 123456789# 显示data001文件每行的第2到5个字符[root@C7-Server01 kkutysllb]# cut -c 2-5 data001ay00ay00ay00ay00..ay00 发现与-b选项的输出没有区别，这是英文是以单字母为字符的，如果换成中文就看出区别了，请大家自行练习。 3）自定义分隔符 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 使用：作为分隔符，显示/etc/passwd文件所有行的第一列的内容# 此方法和后续三剑客中awk使用场景类似[root@C7-Server01 kkutysllb]# cut -d : -f 1 /etc/passwdrootbindaemonadmlpsyncshutdownhaltmailoperatorgamesftpnobodysystemd-networkdbuspolkitdtsssshdpostfixchronyntpkkutysllb# 以：为分隔符，显示/etc/passwd文件每行的3-5列内容[root@C7-Server01 kkutysllb]# cut -d : -f 3-5 /etc/passwd0:0:root1:1:bin2:2:daemon3:4:adm4:7:lp5:0:sync6:0:shutdown7:0:halt8:12:mail11:0:operator12:100:games14:50:FTP User99:99:Nobody192:192:systemd Network Management81:81:System message bus999:998:User for polkitd59:59:Account used by the trousers package to sandbox the tcsd daemon74:74:Privilege-separated SSH89:89:998:996:38:38:1000:1000: sort：文本排序sort命令将输入的文件内容按照指定的规则进行排序，然后将排序结果输出。 语法格式：sort [option] [file] 重要选项参数 【使用示例】 1）默认以行为单位进行比较 默认比较的原则是从首字符向后，依次按ASCII码值进行比较，输出默认按升序进行排列。 123456789101112131415161718192021# 生成一个实验文件，写入如下内容[root@C7-Server01 kkutysllb]# cat &gt;&gt; data003 &lt;&lt; EOF&gt; 10.0.2.1&gt; 10.0.2.56&gt; 10.0.2.3&gt; 10.0.2.5&gt; 10.0.2.14&gt; 10.0.2.11&gt; EOF# 使用sort默认对data003文件进行升序排序输出[root@C7-Server01 kkutysllb]# sort data00310.0.2.110.0.2.1110.0.2.1410.0.2.310.0.2.510.0.2.56 2）使用-n选项使输出按数字从小到大的顺序进行排列 1234567[root@C7-Server01 kkutysllb]# sort -n data00310.0.2.110.0.2.1110.0.2.1410.0.2.310.0.2.510.0.2.56 3）结合-r选项进行反向排序 1234567[root@C7-Server01 kkutysllb]# sort -nr data00310.0.2.5610.0.2.510.0.2.310.0.2.1410.0.2.1110.0.2.1 4）使用-t和-k按照指定格式要求进行排序 1234567891011121314151617181920212223242526272829# 对/etc/passwd文件按要求进行排序# 使用-t选项指定分割符为：# 使用-k数字选项指定按照分割后的第3列进行排序[root@C7-Server01 kkutysllb]# sort -t : -k 3 /etc/passwdroot:x:0:0:root:/root:/bin/bashkkutysllb:x:1000:1000::/home/kkutysllb:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologinbin:x:1:1:bin:/bin:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologinftp:x:14:50:FTP User:/var/ftp:/sbin/nologinsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinntp:x:38:38::/etc/ntp:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/synctss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologinshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologinmail:x:8:12:mail:/var/spool/mail:/sbin/nologindbus:x:81:81:System message bus:/:/sbin/nologinpostfix:x:89:89::/var/spool/postfix:/sbin/nologinchrony:x:998:996::/var/lib/chrony:/sbin/nologinpolkitd:x:999:998:User for polkitd:/:/sbin/nologinnobody:x:99:99:Nobody:/:/sbin/nologin 5）使用-k选项的进阶用法 1234567891011# 使用-k选项指定字符范围进行排序# -k4.1,4,2表示按照第4列的第一个字符到第二个字符范围整体排序[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data00310.0.2.110.0.2.310.0.2.510.0.2.1110.0.2.1410.0.2.56 uniq：去除重复行uniq命令可以输出或忽略文件中的重复行。在工作中，我们常用的场景是使用sort命令对文件排序，然后使用uniq命令去重并计数。 语法格式：uniq [option] [INPUT] 重要选项参数 【使用示例】 1）去重测试 123456789101112131415161718192021222324252627# 创建测试文件[root@C7-Server01 kkutysllb]# cat data00310.0.2.110.0.2.5610.0.2.310.0.2.510.0.2.1410.0.2.1110.0.2.210.0.2.210.0.2.310.0.2.1110.0.2.110.0.2.110.0.2.1# 使用uniq指令去重，结合sort指令一起使用[root@C7-Server01 kkutysllb]# sort data003 | uniq10.0.2.110.0.2.1110.0.2.1410.0.2.210.0.2.310.0.2.510.0.2.56 2）显示重复行的个数 12345678[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data003 | uniq -c4 10.0.2.12 10.0.2.22 10.0.2.31 10.0.2.52 10.0.2.111 10.0.2.141 10.0.2.56 3）只显示重复行 12345[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data003 | uniq -cd4 10.0.2.12 10.0.2.22 10.0.2.32 10.0.2.11 4）只显示唯一行 1234[root@C7-Server01 kkutysllb]# sort -n -t . -k 4.1,4.2 data003 | uniq -u10.0.2.510.0.2.1410.0.2.56 wc：统计文件的行数、单词数或字节数wc命令用于统计文件的行数、单词数或字节数。 语法格式：wc [option] [file] 重要选项参数 【使用示例】 1）查看文件的字节数、字数、行数等 123456# 查看/etc/hosts的统计值（字节数、字数、行数等）# 什么参数都不加时，显示的三个数字从左到右分别是行数，单词数和字节数[root@C7-Server01 kkutysllb]# wc /etc/hosts2 10 158 /etc/hosts 1234# 显示文件的字节数[root@C7-Server01 kkutysllb]# wc -c /etc/hosts158 /etc/hosts 1234# 显示文件的单词数[root@C7-Server01 kkutysllb]# wc -w /etc/hosts10 /etc/hosts 1234# 显示文件的行数[root@C7-Server01 kkutysllb]# wc -l /etc/hosts2 /etc/hosts 1234# 显示文件最长行的长度[root@C7-Server01 kkutysllb]# wc -L /etc/hosts78 /etc/hosts 2）在shell脚本常用来判断某个软件包是否安装 diff：比较两个文件的不同diff命令可以逐行比较纯文本文件的内容，并输出文件的差异。只能同时比较2个文件。 语法格式：diff [option] [file1] [file2] 重要选项参数 【使用示例】 1）比较两个文本文件 123456789101112131415161718192021222324252627282930313233343536&gt; # 将data003文件去重后生成一个新文件data004&gt; &gt; [root@C7-Server01 kkutysllb]# sort data003 | uniq &gt; data004&gt; [root@C7-Server01 kkutysllb]# cat data004&gt; 10.0.2.1&gt; 10.0.2.11&gt; 10.0.2.14&gt; 10.0.2.2&gt; 10.0.2.3&gt; 10.0.2.5&gt; 10.0.2.56&gt; &gt; # 比较data003和data004两个文件&gt; &gt; [root@C7-Server01 kkutysllb]# diff data003 data004&gt; 2,5d1 # 以data003为基准，data003比data004多了2,3,4,5行，所以data003的2-5行需要删除&gt; &lt; 10.0.2.56&gt; &lt; 10.0.2.3&gt; &lt; 10.0.2.5&gt; &lt; 10.0.2.14&gt; 7c3 # 以data003为基准，其第7行对应data004的第3行，两者结果不一致，需要修改&gt; &gt; ## &lt; 10.0.2.2&gt; &gt; &gt; 10.0.2.14&gt; &gt; 10,13c6,7 # 以data003为基准，其第10,11行对应data004的第6,7行，两者结果不一致，需要修改。同时，data003多了12,13两行内容&gt; &gt; &lt; 10.0.2.11&gt; &gt; &lt; 10.0.2.1&gt; &gt; &lt; 10.0.2.1&gt; &gt; &lt; 10.0.2.1&gt; &gt; ------&gt; &gt; &gt; 10.0.2.5&gt; &gt; 10.0.2.56&gt; 2） 并排格式输出 12345678910111213141516# 使用-y选项就可以并排格式输出对比[root@C7-Server01 kkutysllb]# diff -y data003 data00410.0.2.1 10.0.2.110.0.2.56 &lt;10.0.2.3 &lt;10.0.2.5 &lt;10.0.2.14 &lt;10.0.2.11 10.0.2.1110.0.2.2 | 10.0.2.1410.0.2.2 10.0.2.210.0.2.3 10.0.2.310.0.2.11 | 10.0.2.510.0.2.1 | 10.0.2.5610.0.2.1 &lt;10.0.2.1 &lt; 3） 使用-c选项就可以上下文输出 1234567891011121314151617181920212223242526272829303132333435# -表示data004比data003少的行数# +表示data004比data003多的行数# !表示两者不一样的行数[root@C7-Server01 kkutysllb]# diff -c data003 data004*** data003 2019-04-20 15:44:51.488782009 +0800--- data004 2019-04-20 17:21:03.569053069 +0800------ *** 1,13 **** 10.0.2.1- 10.0.2.56 - 10.0.2.3 - 10.0.2.5 - 10.0.2.14 10.0.2.11 ! 10.0.2.2 10.0.2.2 10.0.2.3 ! 10.0.2.11 ! 10.0.2.1 ! 10.0.2.1 ! 10.0.2.1 --- 1,7 ---- 10.0.2.1 10.0.2.11 ! 10.0.2.14 10.0.2.2 10.0.2.3 ! 10.0.2.5 ! 10.0.2.56 ​ 小练习：除了diff指令可以比较两个文件外，vimdiff指令还可以可视化比较两个文件，大家自行研究。 tee：多重定向tee命令用于将数据重定向到文件，同时提供一份重定向数据的副本作为后续命令的标准输入。简单地说就是把数据重定向到给定文件和屏幕上。 语法格式：tee [option] [file] 重要选项参数 【使用示例】 1）tee命令允许标准输出同时把内容写入（覆盖）到文件中 12345678910111213141516171819202122232425262728293031323334353637383940# 显示当前目录下内容详细信息，并将结果写入到data005文件中（如果data005文件不存在，则直接创建，否则将内容覆盖原文件内容）[root@C7-Server01 kkutysllb]# ls -lhi | tee data005total 28K1231037 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.5534100198 drwxr-xr-x 5 root root 45 Apr 17 11:59 data1231031 -rw-r--r-- 1 root root 54 Apr 19 17:14 data0011231047 -rw-r--r-- 1 root root 73 Apr 19 17:31 data0021231030 -rw-r--r-- 1 root root 121 Apr 20 15:44 data0031231056 -rw-r--r-- 1 root root 66 Apr 20 17:21 data0041231024 -rw-r--r-- 1 root root 0 Apr 20 17:41 data0051231057 drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp33583813 -rw-r--r--. 1 root root 158 Jun 7 2013 hard_link1231054 -rw-r--r-- 1 root root 0 Apr 16 17:56 image0081231055 -rw-r--r-- 1 root root 0 Apr 16 17:56 image0091231032 -rw-r--r-- 1 root root 0 Apr 17 11:46 image0101231039 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts1231022 -rw-r--r-- 1 root root 0 Apr 20 17:10 test011231025 -rw-r--r-- 1 root root 82 Apr 20 17:08 test01.sh1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 test.sh[root@C7-Server01 kkutysllb]# cat -n data0051 total 28K2 1231037 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.553 34100198 drwxr-xr-x 5 root root 45 Apr 17 11:59 data4 1231031 -rw-r--r-- 1 root root 54 Apr 19 17:14 data0015 1231047 -rw-r--r-- 1 root root 73 Apr 19 17:31 data0026 1231030 -rw-r--r-- 1 root root 121 Apr 20 15:44 data0037 1231056 -rw-r--r-- 1 root root 66 Apr 20 17:21 data0048 1231024 -rw-r--r-- 1 root root 0 Apr 20 17:41 data0059 1231057 drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp10 33583813 -rw-r--r--. 1 root root 158 Jun 7 2013 hard_link11 1231054 -rw-r--r-- 1 root root 0 Apr 16 17:56 image00812 1231055 -rw-r--r-- 1 root root 0 Apr 16 17:56 image00913 1231032 -rw-r--r-- 1 root root 0 Apr 17 11:46 image01014 1231039 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest15 1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts16 1231022 -rw-r--r-- 1 root root 0 Apr 20 17:10 test0117 1231025 -rw-r--r-- 1 root root 82 Apr 20 17:08 test01.sh18 1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 test.sh 2）tee命令允许标准输出同时把内容追加到文件中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 使用-a选项，会将内容追加到目标文件最后[root@C7-Server01 kkutysllb]# ls -lhi /boot | tee -a data005total 123M 72 -rw-r--r--. 1 root root 145K Apr 21 2018 config-3.10.0-862.el7.x86_64 81 -rw-r--r-- 1 root root 149K Mar 18 23:10 config-3.10.0-957.10.1.el7.x86_64 67 drwxr-xr-x. 3 root root 17 Apr 7 20:33 efi 68 drwxr-xr-x. 2 root root 27 Apr 7 20:34 grub786496 drwx------. 5 root root 132 Apr 20 16:09 grub2 76 -rw-------. 1 root root 50M Apr 7 20:38 initramfs-0-rescue-e344b139f44946638783478bcb51f820.img 75 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-862.el7.x86_64.img 78 -rw------- 1 root root 11M Apr 20 16:08 initramfs-3.10.0-862.el7.x86_64kdump.img 84 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-957.10.1.el7.x86_64.img 73 -rw-r--r--. 1 root root 298K Apr 21 2018 symvers-3.10.0-862.el7.x86_64.gz 82 -rw-r--r-- 1 root root 307K Mar 18 23:10 symvers-3.10.0-957.10.1.el7.x86_64.gz 71 -rw-------. 1 root root 3.3M Apr 21 2018 System.map-3.10.0-862.el7.x86_64 80 -rw------- 1 root root 3.4M Mar 18 23:10 System.map-3.10.0-957.10.1.el7.x86_64 77 -rwxr-xr-x. 1 root root 6.0M Apr 7 20:38 vmlinuz-0-rescue-e344b139f44946638783478bcb51f820 74 -rwxr-xr-x. 1 root root 6.0M Apr 21 2018 vmlinuz-3.10.0-862.el7.x86_64 83 -rwxr-xr-x 1 root root 6.4M Mar 18 23:10 vmlinuz-3.10.0-957.10.1.el7.x86_64[root@C7-Server01 kkutysllb]# [root@C7-Server01 kkutysllb]# cat -n data005 1 total 28K 2 1231037 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.55 3 34100198 drwxr-xr-x 5 root root 45 Apr 17 11:59 data 4 1231031 -rw-r--r-- 1 root root 54 Apr 19 17:14 data001 5 1231047 -rw-r--r-- 1 root root 73 Apr 19 17:31 data002 6 1231030 -rw-r--r-- 1 root root 121 Apr 20 15:44 data003 7 1231056 -rw-r--r-- 1 root root 66 Apr 20 17:21 data004 8 1231024 -rw-r--r-- 1 root root 0 Apr 20 17:41 data005 9 1231057 drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp 10 33583813 -rw-r--r--. 1 root root 158 Jun 7 2013 hard_link 11 1231054 -rw-r--r-- 1 root root 0 Apr 16 17:56 image008 12 1231055 -rw-r--r-- 1 root root 0 Apr 16 17:56 image009 13 1231032 -rw-r--r-- 1 root root 0 Apr 17 11:46 image010 14 1231039 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest 15 1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts 16 1231022 -rw-r--r-- 1 root root 0 Apr 20 17:10 test01 17 1231025 -rw-r--r-- 1 root root 82 Apr 20 17:08 test01.sh 18 1231052 -rw-r--r-- 1 root root 114 Apr 19 18:03 test.sh 19 total 123M 20 72 -rw-r--r--. 1 root root 145K Apr 21 2018 config-3.10.0-862.el7.x86_64 21 81 -rw-r--r-- 1 root root 149K Mar 18 23:10 config-3.10.0-957.10.1.el7.x86_64 22 67 drwxr-xr-x. 3 root root 17 Apr 7 20:33 efi 23 68 drwxr-xr-x. 2 root root 27 Apr 7 20:34 grub 24 786496 drwx------. 5 root root 132 Apr 20 16:09 grub2 25 76 -rw-------. 1 root root 50M Apr 7 20:38 initramfs-0-rescue-e344b139f44946638783478bcb51f820.img 26 75 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-862.el7.x86_64.img 27 78 -rw------- 1 root root 11M Apr 20 16:08 initramfs-3.10.0-862.el7.x86_64kdump.img 28 84 -rw------- 1 root root 19M Apr 20 16:09 initramfs-3.10.0-957.10.1.el7.x86_64.img 29 73 -rw-r--r--. 1 root root 298K Apr 21 2018 symvers-3.10.0-862.el7.x86_64.gz 30 82 -rw-r--r-- 1 root root 307K Mar 18 23:10 symvers-3.10.0-957.10.1.el7.x86_64.gz 31 71 -rw-------. 1 root root 3.3M Apr 21 2018 System.map-3.10.0-862.el7.x86_64 32 80 -rw------- 1 root root 3.4M Mar 18 23:10 System.map-3.10.0-957.10.1.el7.x86_64 33 77 -rwxr-xr-x. 1 root root 6.0M Apr 7 20:38 vmlinuz-0-rescue-e344b139f44946638783478bcb51f820 34 74 -rwxr-xr-x. 1 root root 6.0M Apr 21 2018 vmlinuz-3.10.0-862.el7.x86_64 35 83 -rwxr-xr-x 1 root root 6.4M Mar 18 23:10 vmlinuz-3.10.0-957.10.1.el7.x86_64 3）tee还有个用法类似cat的创建/追加文件的用法，区别是不需要添加输出重定向符号 12345678910# 创建docker.service.d目录mkdir /etc/systemd/system/docker.service.d# 在docker.service.d/目录下创建一个kolla.conf文件，并完成相应配置tee /etc/systemd/system/docker.service.d/kolla.conf &lt;&lt; 'EOF'[Service]MountFlags=sharedEOF tr：替换或删除字符tr命令从标准输入中替换、缩减或删除字符，并将结果写到标准输出。 语法格式：tr [option] [SET1] [SET2] 重要选项参数 【使用示例】 1）将文件中出现的“www”替换为“xyz” 1234567891011121314151617181920212223242526272829# 生成测试文件[root@C7-Server01 kkutysllb]# tee data005 &lt;&lt; 'EOF'&gt; www.sina.com.cn&gt; www.aliyun.com&gt; www.chinamobile.com&gt; www.openstack.org&gt; www.tsinghua.edu.cn&gt; kkutysllb.cn&gt; EOF&gt; www.sina.com.cn&gt; www.aliyun.com&gt; www.chinamobile.com&gt; www.openstack.org&gt; www.tsinghua.edu.cn&gt; kkutysllb.cn# 不使用任何选项参数时，默认按照set1和set2的对应字符位置替换# 下面的例子w对应z，所有全部替换为z[root@C7-Server01 kkutysllb]# tr 'www' 'xyz' &lt; data005zzz.sina.com.cnzzz.aliyun.comzzz.chinamobile.comzzz.openstack.orgzzz.tsinghua.edu.cnkkutysllb.cn 2）使用tr命令“统一”字母大小写 123456789# 这个例子中使用了正则的匹配规则，后续正则表达式会专门讲解[root@C7-Server01 kkutysllb]# tr '[a-z]' '[A-Z]' &lt; data005WWW.SINA.COM.CNWWW.ALIYUN.COMWWW.CHINAMOBILE.COMWWW.OPENSTACK.ORGWWW.TSINGHUA.EDU.CNKKUTYSLLB.CN 3）删除所有的点”.” 1234567[root@C7-Server01 kkutysllb]# tr -d "." &lt; data005wwwsinacomcnwwwaliyuncomwwwchinamobilecomwwwopenstackorgwwwtsinghuaeducnkkutysllbcn 4）将所有的非w字符替换为&amp; 12345[root@C7-Server01 kkutysllb]# tr -c 'w' '&amp;' &lt; data005# 除w字符外，其它所有字符被替换为&amp;，包括换行符\n和制表符\twww&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;www&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp; 小练习：想想怎么使用br指令完成整单词替换？ vi/vim编辑器vi是Linux命令行界面下的文字编辑器，几乎所有的Linux系统都安装了vi，只要学会了vi这个编辑工具，就可以在任何Linux系统上使用它。而vim是vi命令的增强版（Vi IMproved），与vi编辑器完全兼容，此外还有很多增强功能，例如用不同颜色高亮显示代码。因此，如果系统有vim命令，那么建议大家就使用vim编辑文本。 语法格式：vim [option] [file] 【vim的三种模式】 一般来说，vim可分为三种模式：普通模式、编辑模式、命令模式。这三种模式的作用分别如下。 （1）普通模式 用vim命令打开一个文件，默认的状态就是普通模式。在这个模式中，不能进行编辑输入操作，但可以按“上下左右”键来移动光标，也可以执行一些操作命令进行如删除、复制、粘贴等之类的工作。 （2）编辑模式 在普通模式下不能进行编辑输入操作，只有按下“i，I，o，O，a，A，r，R，s，S”（其中“I”最常用）等字母进入编辑模式之后才可以执行录入文字等编辑操作。看文件是否处于编辑模式状态有一个重要的特征，那就是在窗口的左下角要有插入的标记“——INSERT——”或“——插入——”，如下图所示： （3）命令模式 在普通模式下，输入“：”或“/”或“?”时，光标会自动定位在那一行，在这个模式中，可以执行保存、退出、搜索、替换、显示行号等相关操作。如下图所示： 【vi/vim的操作示意图】 1）进入编辑模式指令 i：在当前光标所在处输入文字 a：在当前光标所在的下一个字符处插入文字 I：在当前所在行的行首第一个非空格字符处开始插入文字、和A相反 A：在当前所在行的行尾最后一个字符处开始插入文字，和I相反 O：在当前所在行的上一行处插入新的一行 o：在当前所在行的下一行处插入新的一行 Esc：退出编辑模式，回到命令模式中。 2）命令行模式下指令 :wq：退出并保存 :wq!：退出并强制保存。 :q!：强制退出，不保存 :n1,n2,w filename：n1、n2为数字，将n1行到n2行的内容保存成filename这个文件。 :n1,n2 co n3：n1、n2为数字，将n1行到n2行内容拷贝到n3位置下。 :n1,n2 m n3：n1、n2为数字，将n1行到n2行的内容移动到n3位置下。 :!command：暂时离开vi到命令行模式下执行command的显示结果 :set nu：显示行号 :set nonu：与set nu相反，取消行号 :vs filename：垂直分屏显示，同时显示当前文件和filename对应文件的内容 :sp filename：水平分屏显示，同时当前文件和filename对应文件的内容 I+#+ESC：在可视模式下（Ctrl+v），一次性注释所选的多行，取消注释可用n1,n2s/#//gc。这里操作是一个通用操作，#可以换成tab键，这样可以实现批量缩进 Del：在可视块模式下（Ctrl+v），一次性删除所选内容 r：在可视块模式下（Ctrl+v），一次性替换所选内容 【常用操作】 1）普通模式下移动光标的操作 G：将光标自动文件的最后一行 gg：将光标移动文件的第一行 0：数字0，将光标从所在位置移动到当前行的开头 $：从光标所在位置移动到当前行的结尾 n：n为数字，为回车键，将光标从当前位置向下移动n行 ngg：n为数字，移动光标到文件的第n行 H：光标移动当前窗口最上方那一行 M：光标移动到当前窗口中间的那一行 L：光标移动到当前窗口最下方的那一行 h或向左箭头：光标向左移动一个字符 j或向下箭头：光标向下移动一个字符 k或向上箭头：光标向上移动一个字符 l或向右箭头：光标向右移动一个字符 2）普通模式下搜索与替换操作： /关键字：从光标位置开始向下搜索关键字行 ?关键字：从光标位置开始向上搜索关键字 n：从光标位置开始，向下重复前一个搜索的动作 N：从光标位置开始，向上重复前一个搜索的动作 :g/A/s/B/g：把符合A的内容全部替换为B，左斜线为分隔符，可以用@、#等替换 :%s/A/B/g：把符合A的内容全部替换为B，左斜线为分隔符，可以用@、#等替换。 :n1,n2s/A/B/gc：n1、n2为数字，在第n1行和n2行间寻找A，用B替换 3）普通模式下复制、粘贴、删除等操作 yy：复制光标所在行的全部内容 nyy：n为数字，复制光标开始向下共n行的内容 p/P：p将已复制的数据粘贴到光标的下一行，P则为粘贴到光标的上一行 dd：删除所在的当前行 ndd：n为数字，删除从光标开始向下的共n行。 u：恢复（回滚）前一个执行过的操作 .：点号。重复前一个执行过的动作 %：向后删除字符 X：向前删除字符 d1G：删除当前行至第一行 dG：删除当前行至最后一行 d0：删除当前光标文本至行首 d$：删除当前光标文本至行尾 vi/vim编辑器是Linux运维、开发、测试中最常用的工具，需要大家重点掌握。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-19-中国移动NovoNet2020]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-19-%E4%B8%AD%E5%9B%BD%E7%A7%BB%E5%8A%A8NovoNet2020%2F</url>
    <content type="text"><![CDATA[2015年7月，中国移动发布了NovoNet 2020愿景，推出下一代革新网络NovoNet，2016—2020年将成为中国移动SDN/NFV技术的关键部署期。中国移动希望利用SDN、NFV等新技术，构建一张资源可全局调度、能力可全面开放、容量可弹性伸缩、架构可灵活调整的新一代网络，满足“互联网+”、物联网等创新型业务对通信网络的需求。 中国移动网络重构战略中国移动NovoNet的核心发展理念是实现网络的“三化”和“三可”，即网络功能部署的软件化、虚拟资源的共享化、硬件基础设施的通用化，同时达到网络开放可编程、控制转发可解耦、功能编排可调度的能力，以便在移动网络、IP承载网络、传送网络、数据中心等领域实现网络和业务的虚拟化、软件化。 中国移动NovoNet的愿景是实现网络可编程、部署更灵活、调度更高效、网络更智能、服务更开放、成本更低廉的总体效果，通过SDN和NFV两大基础技术，在网络架构、运营管理和网络开放等层面持续优化网络和服务。 网络架构 1、 网络功能软件化 将软件和硬件解耦，网元功能以软件部署在通用硬件平台上，实现业务的快速部署和升级，有利于运营商快速满足客户的业务需求。 2、资源共享化 借助NFV技术，实现硬件资源的通用化和虚拟资源的共享化，可以降低硬件件成本，实现资源的灵活配置和调度，有利于运营商开发创新业务并灵活部署。 3、 网络可编程 应用SDN技术，将网络控制与物理网络拓扑分离，实现网络可编程，有利于快速响应和满足用户的业务需求，最大化利用运营商的网络资源。 网络运营管理 在2017全球未来网络发展峰会网络重构与转型论坛上，中国移动通信研究院副院长杨志强明确指出，5G和固定宽带业务将会成为运营商未来网络的核心应用，在开源软件和硬件的基础上进行自主研发和运营将成为新的主流运营模式。 1、集中控制 网络功能的控制和调度通过软件完成，提升网络的调度优化能力，可以逐步实现面向全局最优的网络管理和简化网络运维。 2、灵活调度 实现业务部署、业务资源的动态调度，可快速、灵活调度网络资源，应对网络故障、突发事件等。 3、绿色节能 根据业务量需求动态调度网络资源，并可实现高效率集中控制，大幅提升网络资源的利用效率，构建绿色节能的通信网络。 网络服务 1、全面开放的服务 提供全面的开放能力，构建与第三方业务开发者合作共赢的生态环境，全面服务“互联网+”。 2、高效敏捷的服务 具备完善的业务部署和调度能力，能够有效疏导网络流量、对业务提供完善的生命周期管理能力，支持业务的快速迭代、灵活部署、高效使用。 3、按需调度的服务 计算、存储、网络资源可全局调度，网络可动态编程，可根据用户、业务的需求动态调配资源。 中国移动网络重构目标中国移动将SDN/NFV作为网络重构的技术基础，实现基础设施云化、网元功能软件化以及运营管理智能化。未来网络的核心是通过SDN/NFV的引入，实现网络的软件化、资源池化、集中控制、灵活的编排与调度，构建云化的部署形态、智能化的网络调度、全局化的网络编排管理。 （1）引入网络功能虚拟化（NFV）技术，采用IT通用服务器构建资源池，电信设备软硬解耦，以软件形成电信云，并支撑内容分发、边缘计算等。 （2）引入软件定义网络（SDN）技术，采用控制转发分离和路由集中计算，实现网络灵活、智能调度和网络能力的开放。 （3）引入协同编排技术，实现跨领域、端到端的全网资源、网元和流量流向的管理编排与调度。 中国移动网络重构主要包括节点重构、架构重构、网元功能重构以及网络管理与业务运营重构四个方面。 节点重构 构建云化数据中心，替代传统的核心网机房。云化数据中心采用标准化和微模块方式构建，易于快速复制部署，是电信云的基本组件和满足电信网络要求的关键基础设施，可以承载各类虚拟化的电信类软件应用。 ① 标准化的组网：以电信标准为基准的更为严格的网段隔离和网络平面划分原则，业务、管理、基础设施平面独立。 ② 标准化的基础设施：硬件采用通用的X86硬件架构，增强性能要求和电信级管理要求；以统一的云操作系统支持统一的虚拟层指标要求；以电信级增强的OpenStack/VIM实现云资源的管理和分配。 ③ 统一的管理编排体系：以整合的NFVO和SDN编排器和控制器作为统一的管理编排体系。 架构重构 ① 构建基础设施资源池：分布式部署云化数据中心，形成基础设施资源池，承载不同的网络功能。 ② 控制功能集中化：网络控制功能集中在核心云数据中心。 ③ 媒体面下沉：靠近接入点设置边缘云数据中心，将大流量的媒体内容调度到网络边缘，实现快速疏导，提升用户体验。 网元功能重构 通过基于服务的网络架构、切片、控制转发分离等，结合云化技术，实现网络的定制化、开放化、服务化，支持大流量、大连接和低时延的万物互联需求。 ① 用户面功能可实现灵活部署和独立扩缩容，采用集中式部署支持广域移动性和业务连续性，采用网络边缘部署，实现流量本地卸载，支持端到端毫秒级时延，降低回传网络和集中式用户面的处理压力。 ② 网络切片提供端到端资源隔离的逻辑专网，可基于行业用户的需求，实现专网专用，基于业务场景灵活组合网络功能，灵活扩缩容，功能扩展敏捷。 ③ 固网宽带业务控制设备通过转发控制分离，应对固网发展中面临的运维复杂、资源利用率低及业务开通慢等挑战。 网络管理与业务运营重构 构建下一代网络编排器，实施对网络资源池的管理，完成网络和网元部署以及生命周期管理，协调SDN控制器完成网络的统一调度，实现业务的统一编排和对外开放。 中国移动网络重构研发与实践为了验证NovoNet的理念，中国移动已开展了相关研发和测试，包括研发SDN应用和控制器、SDN交换机、多厂家vIMS、vEPC软硬件解耦测试及MANO实验室验证、基于SDN网元功能重构的EPC原型验证、三层解耦的Nanocell网关测试等实验室验证。同时，中国移动也在数据中心、RCS、IMS、SPTN等领域开展了现网的试点及试商用的探索。NovoNet试验网由陕西、安徽、山东、河北、浙江、广东6个省组成，试验网的每个节点均构建两层云数据中心，用全局的编排器实现全网资源和网络的编排调度，以统一的资源池支持数据中心SDN、广域网SDN、SPTN、固网宽带、物联网、VoLTE、vCDN等多业务环境。 基于NovoNet的发展理念，中国移动加快推动SDN/NFV开源开放和相关产业发展，积极参与相关标准组织的项目推进。从2013年开始，中国移动牵头推动成立了Carrier Grade SDN工作组并任主席，参与无线和移动、光传送网、北向接口工作组标准化工作。2014年5月，中国移动与华为联合发起3GPP第一个NFV虚拟网络管理研究项目，并得到23家单位的支持。中国移动在SA1牵头发起Service Chaining立项，研究Gi-LAN场景业务链的场景和需求。2014年9月，中国移动联合国际运营商和厂商发起成立开源组织OPNFV，并出任董事会要职。2016年9月，中国移动加入领先的开源可编程、软件定义网络平台OpenDaylight项目，并利用该平台和OpenStack在其NovoDC项目中部署企业私有云服务产品。 2014年，中国移动完成7家厂商的OpenStack+SDN数据中心解决方案测试，以及2家厂商的广域网方案测试。2015年，中国移动成立Open NFV实验室，引入了15个合作厂商，进行三层解耦测试，搭建云数据中心平台，就NFV部署展开了相关实践。目前，实验室和外场均已按计划开展三层解耦的集成和测试、OPNFV开源平台测试，已搭建了多套基于多厂家NFV组件互操作的云数据中心平台，可模拟核心云数据中心和边缘云数据中心。 2015年，中国移动完成业界首次SDN+NFV的面向商用外场测试，在中国移动自有数据中心采用该架构进行了国内第一次比较全面的组合测试。 ① 在VoLTE和vIMS方面，中国移动进行了NFV实践，2015年-2019年连续5年在陕西、安徽、河北、广东、浙江和山东共6个城市验证了3个系统厂商以及自研产品的核心网云化的快速上线、弹性伸缩、网络快速更新等能力。 ② 在虚拟化RCS（Rich Comunication Suite，富媒体通信套件）方面，中国移动融合通信业务平台（新消息、VoWiFi、业务管理功能）基于NFV架构进行部署，采用软硬件解耦方案，硬件由惠普和思科提供，中兴提供虚拟层和虚拟网元功能并完成系统集成，目前已在中国南北基地进行部署商用。 ③ 在Nanocell商用方面，2016年中国移动基于NFV的4G一体化小基站（Nanocell）开始部署商用。 中国移动网络重构小结NovoNet是中国移动面向2020年的网络愿景和行动计划，NovoNet的目标形态是通过NFV和SDN技术的结合，打造以DC为部署核心、以MANO/SDN控制器体系为管理控制核心、以虚拟化和软件化实现网元功能、以SDN技术实现网络灵活调度的电信云。NovoNet是中国移动下一代网络发展的愿景，也是中国移动下一代网络发展的重大项目和平台，是中国移动对未来网络的重新定义。 标准化和开源系统是SDN和NFV发展的两个重要基石，国际合作和产业推进是NovoNet发展的助推器。中国移动正面向未来网络发展的方向，围绕着NovoNet的引入策略、技术攻关、试验验证、产业推进、测试认证五个方面全面展开工作，以开放共赢的理念从顶层设计、标准推动、开源开放等多个层面全面推动产业发展，积极构建NovoNet试验网，推动NovoNet网络技术和管理成熟。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-19-电信云落地过程若干问题]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-19-%E7%94%B5%E4%BF%A1%E4%BA%91%E8%90%BD%E5%9C%B0%E8%BF%87%E7%A8%8B%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[NFV技术从诞生起，从根本上来说就是为了解决运营商网络演进中部署成本高，迭代更新慢，架构僵化等痛点问题。同时，在引入NFV技术前，旧有产业链相对单一，核心成员主要包括设备制造商、芯片制造商等，而NFV引入后拉长了整体通信产业链条，传统设备制造商面临严峻的挑战，原本软硬件一体化设备销售模式被拆解为通用硬件、虚拟化平台和网元功能三部分销售模式。这也直接决定了运营商期望的多层解耦部署模式推行困难。同时，在NFV的转发性能提升、MANO管理模式选型、VNFM选型和NFVO部署等方面也多多少少存在影响电信云落地的问题。 NFV部署模式选型NFV通过软硬件解耦，使得网络设备开放化，软硬件可以独立演进，避免厂家锁定。基于NFV分层解耦的特性，根据软硬件解耦的开放性不同，可将集成策略分为单厂家、共享资源池、硬件独立和三层全解耦4种方案，如下图所示。 方案1：单厂家方案，优点就是可以实现快速部署，整体系统的性能、稳定性与可靠性都比较理想，不需要进行异构厂商的互通测试与集成。缺点是与传统网络设备一样，存在软硬件一体化和封闭性问题，难以实现灵活的架构部署，不利于实现共享；与厂商存在捆绑关系，不利于竞争，会再次形成烟囱式部署，总体成本较高，也不利于自主创新以及灵活的迭代式部署升级。目前，中国电信的4G/VoLTE/IMS网络就是采用这种方式，在短期内对中国移动的业务发展形成较大压力。 方案2：倾向于IT化思路，选择最好的硬件平台和虚拟机产品，要求上层应用向底层平台靠拢。只对VNF与NFVI层解耦，VNF能够部署于统一管理的虚拟资源之上，并确保功能可用、性能良好、运行情况可监控、故障可定位；不同供应商的VNF可灵活配置、可互通、可混用、可集约管理。其中，VNFM与VNF通常为同一厂商（即“专用VNFM”），这种情况下VNF与VNFM之间的接口不需标准化；特殊场景下采用跨厂商的“VNFM”（即“通用VNFM”）。VMware的解决方案就是典型的方案二厂商A的定位，考虑到中国移动苏州研发中心与VMware的战略合作情况，可以预期不远的将来中国移动的NFV网络架构中会出现类似部署方案。 方案3：倾向于电信思路，通用硬件与虚拟化层软件解耦，基础设施全部采用通用硬件，实现多供应商设备混用；虚拟化层采用商用/开源软件进行虚拟资源的统一管理。可以由电信设备制造商提供所有软件，只是适配在IT平台上。目前，中国移动大区集中化网络建设就是采用此部署方案。 方案4：全解耦的好处是可以实现通用化、标准化、模块化、分布式部署，架构灵活，而且部分核心模块可选择进行定制与自主研发，也有利于形成竞争，降低成本，实现规模化部署；不利的地方是需要规范和标准化，周期很长，也需要大量的多厂商互通测试，需要很强的集成开发能力，部署就绪时间长，效率较低，后续的运营复杂度高，故障定位和排除较为困难，对运营商的运营能力要求较高。该模式是中国移动一直不遗余力推广的模式，目前在陕西移动已初步完成苏研VIM+分布式存储、华为VNFM和研究院NFVO+的标准三层部署模式验证，并打通了标准三层组网下FirstCall。 另外，以上各方案都涉及MANO的解耦，涉及运营商自主开发或者第三方的NFVO与不同厂商的VNFM、VIM之间的对接和打通，屏蔽了供应商间的差异，统一实现网络功能的协同、面向业务的编排与虚拟资源的管理。但是，NFVO+的解耦目前还停留在实验验证阶段，在中国移动的电信云一阶段还是采用NFVO与VNFM同厂商捆绑的模式。 NFV转发性能的提升NFV设计的初衷是针对部分低转发流量类业务功能，x86服务器在配备高速网卡（10Gbit/s）后，业务应用不经特殊优化，基本也可以满足大多数低速率转发业务的处理要求（即使后续随着SDN技术的推动，引入了40Gbit/s的高速转发能力，但目前也只是实验验证阶段，并未实际部署）。 传统硬件网元能够通过专用芯片实现高转发性能，而x86环境下的虚拟化网元尚不具备万兆以上端口的小包线速转发能力，在同等业务量的情况下，虚拟化网元和传统设备相比存在一定的性能差距。x86服务器采用软件转发和交换技术，报文在服务器各层面间传递，会受到CPU开销等多方面因素的影响，因此服务器的内部转发性能是NFV系统的主要瓶颈。 NFV中的网络业务应用运行于服务器的虚拟化环境中，单个应用业务流量的收发要经过虚拟化层、服务器I/O通道、内核协议栈等多个处理流程，而多个应用业务之间又可以用复杂的物理或虚拟网络相连接。因此，NFV系统的整体性能取决于单服务器转发性能与业务组链转发性能两个方面。如下所示： 业务应用流量的收发I/O通道依次包括物理网卡、虚拟交换机、虚拟网卡3个环节（见上图左半部分）；从软件结构上看，报文的收发需要经过物理网卡驱动、宿主机内核网络协议栈、内核态虚拟交换层、虚拟机网卡驱动、虚拟机内核态网络协议栈、虚拟机用户态应用等多个转发通道（见上图右半部分），存在着海量系统中断、内核上下文切换、内存复制、虚拟化封装/解封等大量CPU开销操作过程。 影响NFV转发性能的主要因素整理如下： 1. 网卡硬件中断 目前大量流行的PCI/PCIe（Peripheral Component Interconnect，外设部件互连标准/PCI-Express）网卡在收到报文后，一般采用DMA（Direct Memory Access，直接存储器存取）方式直接写入内存并产生CPU硬件中断，在低速转发应用中此方法十分有效。 但是，当网络流量激增时，CPU的大部分时间阻塞于中断响应。在多核系统中，可能存在多块网卡绑定同一个CPU核的情况，使其占用率达到100%。中断处理方式在低速网络I/O场景下非常有效。然而，随着高速网络接口等技术的迅速发展，10Gbit/s、40Gbit/s甚至100Gbit/s的网络端口已经出现。随着网络I/O速率的不断提高，网卡面对大量高速数据分组引发频繁的中断，中断引起的上下文切换开销将变得不可忽视，造成较高的时延，并引起吞吐量下降。因此，网卡性能改进一般采用减少或关闭中断（如轮询取代中断、零复制技术、大页内存技术等）、多核CPU负载均衡等优化措施。 2. 内核网络协议栈 在Linux或FreeBSD系统中，用户态程序调用系统套接字进行数据收发时，会使用内核网络协议栈。这将产生两方面的性能问题：一是系统调用导致的内核上下文切换，会频繁占用CPU周期；二是协议栈与用户进程间的报文复制是一种费时的操作。 NFV系统中，业务应用报文处理从物理网卡到业务应用需要完成收发操作各1次，至少经过4次上下文切换（宿主机2次以及VM内2次）和4次报文复制。将网络协议栈移植到用户态是一种可行的思路，但这种方法违反了GNU协议。GNU是GNU GPL（GNU General Public License，通用公共许可证）的简称，Linux内核受GNU GPL保护，内核代码不能用于Linux内核外。因此，弃用网络协议栈以换取转发性能，是唯一可行的办法，但需要付出大量修改业务应用代码的代价。 3. 虚拟化层的封装效率 业务应用中存在两类封装：服务器内部的I/O封装和网络层对流量的虚拟化封装。前者是由于NFV的业务应用运行于VM中，流量需要经历多次封装/解封装过程，包括：宿主机虚拟化软件对VM的I/O封装、虚拟交换机对端口的封装、云管理平台对虚拟网络端口的封装；后者是为实现NFV用户隔离，在流量中添加的用户标识，如VLAN、VxLAN（Virtual Extensible Local Area Network，可扩展虚拟局域网）等。这两类封装/解封均要消耗CPU周期，会降低NFV系统的转发效率。 4. 业务链网络的转发效率 NFV的业务链存在星形和串行两种组网方式，如下图所示。 星形连接依赖于物理网络设备的硬件转发能力，整体转发性能较优，但当应用的数量较大时，会消耗大量网络设备端口。因此，在业务链组网范围不大时，如在IDC内部，为简化组网和节约端口，更多地采用串行连接。 当串行连接时，NFV控制器需要在多个业务应用中选择合适位置的应用进程或进程组来处理流量，以均衡各应用负荷并兼顾业务链网络性能。不合适的负载均衡算法会造成流量在不同进程组的上下行链路之间反复穿越，严重降低业务链网络的带宽利用率。 5. 其他开销 （1）缓存未命中开销：缓存是一种能够有效提高系统性能的方式，然而，由于设计的不合理造成频繁的缓存未命中，则会严重削弱NFV数据平面的性能。 （2）锁开销：当多个线程或进程需要对某一共享资源进行操作时，往往需要通过锁机制来保证数据的一致性和同步性，而加锁带来的开销会显著降低数据处理的性能。 （3）上下文切换开销：NFV的扩展需要多核并行化的支持，然而在该场景下，数据平面需要进行资源的分配调度，调度过程中涉及多种类型的上下文切换。在网卡中断、系统调用、进程调度与跨核资源访问等上下文切换过程中，操作系统均需要保存当前状态，而这一类的切换开销往往相当昂贵，严重影响系统性能。 以上3种开销对于NFV转发性能的影响较大，在实际的转发过程中，开销不止这3种。 针对以上影响转发性能的挑战，NFV在落地过程引入不同开源技术进行应对，具体的实现原理会在第二部分《NFV关键技术》中详细阐述，这里只是做一个简单的介绍，使初学者有个概念性的了解。 1. 轮询取代中断 作为I/O通信的另一种方式，轮询不存在中断所固有的开销。以网卡接收分组为例，在轮询模式下，系统会在初始化时屏蔽收发分组中断，并使用一个线程或进程来不断检测收取分组描述符中的收取分组成功标志是否被网卡置位，以此来判断是否有数据分组。整个收取过程没有发生上下文切换，因此也就避免了相应的开销。 当I/O速率接近CPU速率时，中断的开销变得不可忽略，轮询模式的优势明显；相反，如果数据吞吐率很低，中断能有更好的CPU利用率，此时不宜采用轮询模式。基于以上分析，针对网络流量抖动较大的场景，可以选用中断与轮询的混合模式，即在流量小时使用中断模式，当遇到大流量时切换为轮询模式。目前Linux内核与DPDK都支持这种混合中断轮询模式。 2. 零复制技术 零复制技术主要用以避免CPU将数据从一个内存区域复制到另一个内存区域带来的开销。在NFV数据平面操作的场景下，零复制指的是除网卡将数据DMA复制进内存外（非CPU参与），从数据分组接收到应用程序处理数据分组，整个过程中不存在数据复制。零复制技术对于高速网络而言是十分必要的。 DPDK、Netmap、PF-ring等高性能数据分组处理框架都运用了零复制技术，可以实现在通用平台下高效的网络处理，大幅提升单服务器内的报文转发性能。进一步地，DPDK不仅实现了网卡缓冲区到用户空间的零复制，还提供虚拟环境下的虚拟接口、兼容OpenvSwitch虚拟交换机、专为短小报文提供的hugepage访问机制等实用技术。 上述开源方案能很好地满足NFV中DPI（Deep Packet Inspection，深度数据包检测）、防火墙、CGN（Carrier-Grade NAT ，运营商级网络地址转换）等无需协议栈的网络业务功能，但存在着大量改写原有业务应用套接字的问题，应用中需要在性能提升与代码改动之间进行取舍。 3. 高效虚拟化技术 目前在NFV领域常用的高效虚拟化技术大致可以归为以下两类。 （1）基于硬件的虚拟化技术 I/O透传与SR-IOV是两种经典的虚拟化技术。I/O透传指的是将物理网卡直接分配给客户机使用，这种由硬件支持的技术可以达到接近宿主机的性能。不过，由于PCIe设备有限，PCI研究组织提出并制定了一套虚拟化规范——SR-IOV，即单根I/O虚拟化，也就是一个标准化的多虚机共享物理设备的机制。完整的带有SR-IOV能力的PCIe设备，能像普通物理PCIe设备那样被发现、管理和配置。 SR-IOV主要的应用还是在网卡上，通过SR-IOV，每张虚拟网卡都有独立的中断、收发队列、QoS等机制，可以使一块物理网卡提供多个虚拟功能（VF），而每个VF都可以直接分配给客户机使用。 SR-IOV使虚拟机可以直通式访问物理网卡，并且同一块网卡可被多个虚拟机共享，保证了高I/O性能，但SR-IOV技术也存在一些问题。由于VF、虚端口和虚拟机之间存在映射关系，对映射关系的修改存在复杂性，因此除华为外，大部分厂商目前还无法支持SR-IOV场景下的虚拟机迁移功能。另外，SR-IOV特性需要物理网卡的硬件支持，并非所有物理网卡都提供支持。 （2）半虚拟化技术 半虚拟化无需对硬件做完全的模拟，而是通过客户机的前端驱动与宿主机的后端驱动一同配合完成通信，客户机操作系统能够感知自己处在虚拟化环境中，故称为半虚拟化。由于半虚拟化拥有前后端驱动，不会造成VM-exit，所以半虚拟化拥有更高的性能。主流虚拟化平台Xen就使用了半虚拟化的驱动，半虚拟化比起SR-IOV的优势在于支持热迁移，并且可以与主流虚拟交换机对接。但是，在大流量转发场景下，前后端驱动中Domain0也是最大的瓶颈。 4. 硬件分流CPU能力 CPU具有通用性，需要理解多种指令，具备中断机制协调不同设备的请求，因此CPU拥有非常复杂的逻辑控制单元和指令翻译结构，这使得CPU在获得通用性的同时，损失了计算效率，在高速转发场景下降低了NFV的转发性能。 业界普遍采用硬件分流方法来解决此问题，CPU仅用于对服务器进行控制和管理，其他事务被卸载到硬件进行协同处理，降低CPU消耗，提升转发性能。 网卡分流技术是将部分CPU事务卸载到硬件网卡进行处理，目前大多数网卡设备已经能够支持卸载特性。网卡卸载的主要功能有：数据加解密、数据包分类、报文校验、有状态流量分析、Overlay报文封装和解封装、流量负载均衡，以及根据通信协议最大传输单元限制，将数据包进行拆分或整合。 除此之外，CPU+专用加速芯片的异构计算方案也是一种硬件分流思路。异构计算主要是指使用不同类型指令集（X86、ARM、MIPS、POWER等）和体系架构的计算单元（CPU、GPU、NP、ASIC、FPGA等）组成系统的计算方式。在NFV转发性能方面，使用可编程的硬件加速芯片（NP、GPU和FPGA）协同CPU进行数据处理，可显著提高数据处理速度，从而提升转发性能。 5．整体优化方案DPDK PCI直通、SR-IOV方案消除了物理网卡到虚拟网卡的性能瓶颈，但在NFV场景下，仍然有其他I/O环节需要进行优化，如网卡硬件中断、内核协议栈等。开源项目DPDK作为一套综合解决方案，对上述问题进行了优化与提升，可以应用于虚拟交换机和VNF。DPDK是Intel提供的数据平面开发工具集，为Intel处理器架构下用户空间高效的数据包处理提供库函数和驱动的支持。它不同于Linux系统以通用性设计为目的，而是专注于网络应用中数据包的高性能处理。有关DPDK的详细介绍，大家可参见《深入浅出DPDK》这本书。 一般来说，服务器上的每个CPU核会被多个进程/线程分时使用，进程/线程切换时，会引入系统开销。DPDK支持CPU亲和性技术，优化多核CPU任务执行，将某进程/线程绑定到特定的CPU核，消除切换带来的额外开销，从而保证处理性能。 同时，DPDK支持巨页内存技术。一般情况下，页表大小为4KB，巨页技术将页表尺寸增大为2MB或1GB，使一次性缓存内容更多，有效缩短查表消耗时间。同时，DPDK提供内存池和无锁环形缓存管理机制，加快了内存访问效率。 报文通过网卡写入服务器内存的过程中，会产生CPU硬件中断，在数据流较大的情况下，硬件中断会占用大量时间。DPDK采用轮询机制，跳过网卡中断处理过程，释放了CPU处理时间。服务器对报文进行收发时，会使用内核网络协议栈，由此产生内核上下文频繁切换和报文拷贝问题，占用了CPU周期，消耗了处理时间。DPDK使用户态进程可直接读写网卡缓冲区，旁路了内核协议栈处理。 DPDK以用户数据I/O通道优化为基础，结合Intel虚拟化技术（主要是VT-d技术）、操作系统、虚拟化层与虚拟交换机等多种优化方案，形成了完善的转发性能加速架构，并开放了用户态API供用户应用程序访问。DPDK已逐渐演变为业界普遍认可的完整NFV转发性能优化技术方案。但目前DPDK还无法达到小包线速转发，仍需进行性能提升研究和测试验证工作。 运营商如何推动三层解耦落地？在NFV方面，解耦是首当其冲的问题，目前业界有不解耦、软硬件解耦和三层解耦这3种思路，其中软硬件解耦又分为共享虚拟资源池和硬件独立两种方案，不同方案的对比介绍在本文的NFV部署模式部分已有介绍，这里不再赘述。 不解耦无法实现硬件共享，运营商依赖厂商，网络开放能力弱，不支持自动化部署，显然不符合NFV技术的初衷；而仅硬件解耦不支持多厂商VNF在同一云平台部署，运营商仍旧依赖厂商；三层解耦可以解决上述问题，但其涉及多厂商垂直互通，系统集成和维护难度大，部署周期长。NFV三层解耦要求在部署NFV时不同组件由不同的厂商提供，需要比传统电信网络更复杂的测试验证、集成和规划部署工作。 NFV分层解耦的方式由于缺乏主集成商（苏研努力的目标，陕西目前试点的主要目的）和完整验证，距离开放的全解耦目标还有相当距离，运营商会面临一定的运维风险和技术挑战。NFV分层解耦的技术挑战主要有以下几点： （1）不同厂商的硬件设备之间存在管理和配置的差异，如存储设备管理配置、安全证书、驱动、硬件配置等方面的问题，会导致统一资源管理困难、自动化配置失效；另一方面，各类VNF和虚拟化软件部署于不同的硬件设备上，在缺乏预先测试验证的情况下，硬件板卡或外设之间，如PCIe网卡、RAID卡硬件、BIOS，存在兼容性不一致问题。因此，NFV三层解耦规模商用前，需要运营商细化服务器安全证书、硬件选型方面的规范要求，重点关注硬件可靠性和兼容性问题，在商用前进行软硬件兼容性和可靠性验证。以上问题需要通过大量的适配、验证和调优来解决。 （2）不同基础软件之间存在兼容性问题，如操作系统与驱动层之间、虚拟交换机与操作系统之间、虚拟化软件与VNF之间，不同的模块和不同的版本，以及不同的配置参数、优化方法，都会造成性能、稳定性、兼容性的较大差异，有待进一步测试与验证。为此，需要尽量减少虚拟化层类型，适时引入自主研发虚拟化层软件，减少持续不断的三层解耦测试工作量。采用集中的云管平台（统一VIM），降低NFVO与VIM集成的复杂度。 （3）分层之后，从NFV各层之间的接口定义与数据类型，到层内功能的实现机制，乃至层间的协同处理均需要运营商去推动和完善。如VNF在发生故障时，涉及VM迁移与业务倒换机制以及NFVI、NFVO和VIM的处理流程；又如VNF对配置文件管理和存储设备使用不当，同样会导致VM实例化失效。因而，在VNF多厂家集成过程中，集成方或者运营商需要需要有角色对问题定界、定位进行裁决，在集成和运维的过程中，对技术问题进行端到端的管理，对各层的功能进行详细定义或者详细规范。 （4）NFV系统集成涉及多厂商、多软硬组件的高度集成，由于虚拟化环境的存在，在初期的测试验证、中期的系统部署、后期的运维过程中，进行系统评测与管理部署都较为困难。这就要求运营商在提升DevOps能力的基础上，依托持续集成与持续部署和运维自动化技术，形成NFV系统的持续集成、测试和部署能力，大白话就是要求运营商亟待需要提升自主开发、自主集成和自主测试能力。同时，MANO架构需要全网统一。由于目前VNFM通常与VNF是绑定的厂商组件，而实际上真正的VIM也是厂商提供的，因此VNFM、VIM仍然是与VNF、NFVI就近部署。所以需要尽早明确NFVO的架构（例如，采用集团NFVO+区域NFVO两层架构），明确VNFM和VIM的跨专业、跨地域部署能力和部署位置，明确已部署的云管平台与VIM架构的关系，以及已有的EMS、NMS与VNFM架构的关系。 对于运营商来说，三层解耦会是一个较长的过程，与厂商的博弈也需要时间，再加上自主能力（研发、测试、集成）也需要时间，在实现最终目标之前可以先选择过渡方案，例如厂商一体化方案（不适合作为商业化规模部署方案）、部分解耦方案（硬件与软件解耦、MANO中的NFVO解耦出来）等，在试点和小规模部署过程中培育能力，逐渐实现最终的解耦目标，并在解耦基础上逐步提升自主研发比例，增强对网络NFV化的掌控力。 MANO管理模式利弊分析EISI NFV对MANO的资源管理提出直接模式和间接模式两种方案。NFV-MANO允许NFVO和VNFM两者都能管理VNF生命周期所需的虚拟化资源，直接和间接是相对VNFM而言的。 （1）直接（Direct）模式：VNFM直接通过VIM分配VNF生命周期管理所需的虚拟化资源。VNFM向NFVO提出对VNF的生命周期管理操作进行资源授权，NFVO根据操作请求及整体资源情况返回授权结果；VNFM根据授权结果直接与VIM交互完成资源的调度（分配、修改、释放等）；VNFM向NFVO反馈资源变更情况。如下图所示： （2）间接（Indirect）模式：VNFM向NFVO提出对VNF的生命周期管理操作进行资源授权，NFVO根据操作请求及整体资源情况返回授权结果；VNFM根据授权结果向NFVO提出资源调度（分配、修改、释放等）请求，NFVO与VIM交互完成实际的资源调度工作；NFVO向VNFM反馈资源变更情况。如下图所示： 总体而言，两者都由VNFM提供VNF生命周期管理。在执行VNF生命周期管理操作之前，无论该操作新增资源，还是修改或者释放已分配的资源，VNFM都需要向NFVO请求资源授权；资源容量和状态等信息由NVFO统一维护管理。两种模式的不同主要体现在：直接模式下，VNFM和NFVO都需要与VIM交互，将VIM的虚拟资源管理接口暴露给VNFM使用；间接模式下，VFNM不需要和VIM进行交互，NFVO需要提供VIM代理能力。 两种模式在架构、业务成效、性能、集成复杂度以及安全性方面的对比分析如下所示： 综合以上分析，从功能、落地部署、安全性、未来演进角度考虑，间接模式较好；性能方面，直接模式占优；系统集成复杂度两者相当。考虑网络的未来发展，从运营商对网络的自主掌控能力出发，要求厂商必须支持间接模式，以推进分层解耦、实现对虚拟资源的统一管控。 VNFM如何选型？通用VNFM和专用VNFM是ETSI定义的两种架构选项。 （1）通用VNFM：通用VNFM可以服务于不同类型或不同厂商提供的VNF，对它所管理的多种类型、多厂商VNF的操作没有依赖性，但它必须能够在VNF包中定义的不同VNF的特定脚本。按照管理要求，可能有多个通用VNFM，每个VNFM管理一定VNF的子集。在这种情况下，NFVO需要同时处理多个通用VNFM。下图展示了通用VNFM的架构。 （2）专用VNFM：专用VNFM与它所管理的VNF之间具有依赖性，一般管理由同一厂商提供的VNF。NFV架构框架同时也允许一个或多个专用VNFM连接到单个NFVO。在VNF生命周期管理过程复杂，且一些管理特性与这些VNF紧耦合的场景下，就需要使用专用VNFM。下图展示了专用VNFM的架构。 两种架构选项具有相同的VNFM功能要求，如VNFD解析，获得部署VNF所需资源要求及所需部署的业务软件；NFVI告警与VNF告警关联、VNF弹性策略执行；VNF生命周期管理，包括实例化、查询、扩/缩容、终止等。但是，两种架构在技术实现难度、运维复杂度等方面却存在着差异。 NFVO如何部署？目前，ETSI NFV进一步细化了NFVO功能模块的具体功能要求。按照MANO规范，NFVO可以分解为网络服务编排（Network Service Orchestrator，NSO）和资源编排（Resource Orchestrator，RO）。网络服务生命周期的管理功能，即NSO功能；跨VIM的NFVI资源编排功能，即RO功能。NFVO作为MANO的一个功能实体，在部署时，可以有如下两种部署形态。 1. NFVO功能不分解部署 NFVO作为一个独立的实体部署，可采用级联的方式来部署。如下图所示，每个管理域可以被当作一个或多个数据中心，在该管理域中部署一套独立的NFVO，以及VNFMs、VIMs，用来管理该域中的网络服务。另外，再部署一套顶层NFVO，用来管理域间的网络服务，它并不管理下层管理域中的网络服务，不过它可以接收下层管理域中网络服务实例化、弹性伸缩，以及终止操作的请求，并将此请求直接传递给下层管理域中的NFVO，由下层管理域的NFVO来完成实际的操作。 2. NFVO功能分解部署 NFVO可以分为两个独立的实体来部署，NSO主要完成NS的生命周期管理，包括NS模板以及VNF包的管理，如下图所示。NSO不再关注资源的状态以及资源所在的管理域，仅关注资源的额度。RO主要完成管理域内资源的管理和编排，如资源的预留、分配、删除等操作，以及支持资源的使用率和状态的监控。 NFVO功能不分解部署时，资源申请效率高；集成难度相对较低；若NFVO故障，则只会影响该NFVO管理域的业务和资源。NFVO分解后，VNFM访问或申请资源的效率会降低；如果RO出现故障，则只会影响该RO管理的资源；但是，一旦NSO出现故障，则将影响所有整个NFV的业务功能；NFVO分解为NSO、RO之后，或增加NSO-RO之间的接口，增加系统集成难度。 根据分析比较，在一定的业务规模下，将NFVO分解为NSO、RO难以带来明显的优势或收益，反而会导致性能降低、集成复杂。因此，建议NFVO采用不分解架构。另外，考虑后续的演进和发展，在技术架构上可将NSO和RO进行内部功能解耦，并实现微服务化，以增强未来NFVO部署的灵活性。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-19-NFV网络参考架构]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-19-NFV%E7%BD%91%E7%BB%9C%E5%8F%82%E8%80%83%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[ETSI定义的NFV网络参考架构ETSI作为NFV的发起标准组织，于2015年年初发布了NFV参考架构等系列文稿，具体包括：用例文档、架构框架、虚拟化需求、NFV基础设施、NFV MANO、VNF、服务质量、接口、安全、PoC框架、最佳实践等内容。虽然ETSI NFV阶段成果不是强制执行的标准，但是得到了业界的普遍认可，已经成为了业界的事实标准。 目前，NFV标准框架已基本稳定，如下图所示。NFV标准框架主要包括NFV 基础设施、虚拟网络功能和NFV管理与编排（NFV Management and Orchestration，NFV MANO）。ETSI定义的NFV技术架构同当前网络架构（独立的业务网络+OSS系统）相比，NFV从纵向和横向上进行了解耦，纵向分为3层：基础设施层、虚拟网络层和运营支撑层。横向分为两个域：业务网络域和管理编排域。 ​ VIM + NFVI 基本等同于OpenStack管理下的IaaS NFVI包括各种计算、存储、网络等硬件设备，以及相关的虚拟化控制软件，将硬件相关的计算、存储和网络资源全面虚拟化，实现资源池化。NFVI物理基础设施可以是多个地理上分散的数据中心，通过高速通信网连接起来，实现资源池统一管理。 VNF运行在NFVI之上。VNF旨在实现各个电信网络的业务功能，将物理网元映射为虚拟网元 VNF，VNF所需资源需要分解为虚拟的计算、存储、交换资源。VNF作为一种软件功能，部署在一个或多个虚机上，并由NFVI来承载。VNF之间可以采用传统网络定义的信令接口进行信息交互。VNF的性能和可靠性可通过负载均衡和HA等软件措施以及底层基础设施的动态资源调度来解决。 EMS（Element Management System，网元管系统）可以管理VNF，厂商通常对原网管系统进行扩展，统一管理虚拟化和非虚拟化的网元。 运营支撑层OSS/BSS，就是目前的OSS/BSS系统，需要为虚拟化进行必要的修改和调整。为了适应 NFV 发展趋势，未来的业务支撑系统（BSS)与运营支撑系统（OSS）将进行升级（中国移动目前正推出OSS4.0系统），实现与VNF Manager和网元编排管理的互通。 NFV MANO（NFV Management and Orchestration，NFV管理与编排）负责对整个NFVI资源的管理和编排，业务网络和NFVI资源的映射和关联，OSS业务资源流程的实施等。MANO内部包括编排管理（Orchestrator）、虚拟化的网络功能管理（VNF Manager，VNFM）和虚拟化的基础设施管理（Virtualised Infrastructure Manager，VIM）3个实体，分别完成对NFVI、VNF和NS（Network Service，业务网络提供的网络服务）3个层次的管理。其中，Orchestrator编排管理NFV基础设施和软件资源，在NFVI上实现网络服务的业务流程和管理。VNFM实现VNF生命周期管理，如实例化、更新、查询和弹性等。VIM控制和管理VNF所需要的与算、存储和网络资源的管理和调度，即所谓的Cloud OS。 NFVI到底是个什么鬼？NFVI能够同时为一个或多个VNF实例提供基础设施资源，并实现不同VNF资源的动态配置。ETSI对NFVI功能架构和接口进行了定义，将NFVI细分为计算域、管理域和网络域3个功能域。每个域自主管控，通过标准接口进行信息交互，协作实现对VNF的具体承载。 计算域包括通用高容量服务器和存储设备。 管理域包括各种Hypervisor，对硬件进行抽象以支撑软件应用在不同服务器之间的可移植性；为虚拟机VM分配计算域资源；为编排和管理系统提供管理界面，允许虚拟机VM的加载和监控。 网络域包括所有的通用高容量交换机，这些交换机互联形成一个可配置网络，提供基础设施网络服务。 NFVI内部3个域之间接口示意如下图所示： 计算域包括各种服务器与存储设备，其作用是结合Hypervisor域的一个管理程序，负责VNF各个组件的需求，提供COTS计算和存储资源。概括地说，计算域提供与网络基础设施领域的接口，但其自身不支持网络连接。根据给定计算域的元素：服务器、存储和网络结构，各种接口可以被归类为：物理网络接口、内部和外部域接口、管理和编排接口。 物理网络接口：有几种不同类型的设备接口可能被利用，这些接口包括以太网、光纤通道、无线接入（这几个接口是比较常用的，但不仅限于这3类）。 内部和外部域接口：该部分主要是两个接口，一个是NFVI与VIM的内部接口，另一个是与VNF应用的外部接口。NFVI与VIM（Nf-Vi）的接口主要负责VIM与NFVI相连，被用作管理接口，是一个内部接口。而VIM被作为一个单独的外部服务，这样做既考虑了逻辑上的隔离，同时也顾及到安全方面的因素。Nf-Vi接口是唯一授权的NFV管理界面和交互接口，其管理组件就是VIM。同时，在NFVI内，Vl-Ha/CSr连接到外部计算域，包括所有的基础设施，是计算域和Hypervisor域之间的接口，该接口被Hypervisor/OS用于监控计算域内可用的物理资源。 管理和编排接口：NFVI的编排和管理是严格通过Nf-Vi接口的。为了正常工作，应当有某种类型的接口，物理接口的类型并不重要，重要的是需要有一个专门的安全接口，该接口将被用来建立基础设施的鉴权和授权。除此接口外，每个底层域组件（如计算、存储、网络）都应当有相应的代理。 Hypervisor域本身是抽象硬件和实现服务的软件环境，如启动/终止虚拟机，作用于策略、缩放、实时迁移和高可用性。仅当VIM被告知或者指示Hypervisor域的主要接口时，服务才能被启用。Hypervisor域的主要接口包括NF-Vi、Vi-Ha和Vn-NF接口，其内部结构如下图示。 NF-Vi接口：对接VIM的接口，虚拟机监控程序服务请求需通过该接口。只有VIM或NFVO才能通过这些接口与Hypervisor交互。Hypervisor不得自主执行业务，除非是在VIM应用策略的范围内。 Vi-Ha接口：通过该接口Hypervisor获取硬件信息，并创建虚拟机需要使用的虚拟硬件组件。 Vn-NF接口：该接口是VM到VNF的逻辑接口。一个VNF由一个或多个虚拟机创建。虚拟机在本质上是软件运行某种函数、算法、应用，而忽略其类型、模型、实际物理单元的数量。 网络域的主要功能包括：分布式VNF的虚拟网络功能组件之间进行通信的通道、不同VNF之间通信的通道、VNF编排和管理模块之间的通信通道、NFVI编排和管理功能模块之间的通信通道、VNFC远程部署的方案和现有运营商网络之间交互通信的方案。 网络域的网络架构应该提前设置好VNF的网络结构，同时提供足够的网络连接。为了实现上述功能，网络架构需要包含足够多的组件来提供网络域的功能。包含的组件如下： 1）网络架构的寻址机制，可以有多于一种的寻址机制，同时该机制还包括地址定位和管理功能； 2）路由机制，该机制用来生成网络拓扑，并进行路由转发； 3）带宽定位机制； 4）OAM集合，用来检测可靠性、可用性、连接服务的完整性。 网络域中的虚拟网络由NFVI中网络组件提供，通常在数据中心内部或数据中心之间使用使用L2或L3的overlay网络来实现。所谓的overlay网络，就是重叠在数据中心内部TOR/EOR交换机组成的物理网络之上的网络，通过在多个虚拟网络功能实体之间建立隧道来完成互联。 网络域的内部接口可以南北接口和东西接口，按类型分为管理协议接口、管理模块/信息接口、数据模型和流监控协议接口三类。南北向接口可以参见传统网络SNMP接口模型，通过MIB模块中的数据模块提供管理协议接口。新的南北向接口是NetConf接口，使用YANG数据模型，定义简单，支持多种协议，典型应用就是SDN控制器的管理。东西向接口主要分为前向关联和后向反射两种模式。前向关联模式主要用来探测服务故障，后向反射模式主要用来进行故障状态的上报和恢复检测。 网络域的外部接口主要有：不同网络资源之间的接口[Vi-Ha]/Nr，例如网络资源、端口资源之间就是采用这类接口；[Vn-Nf]/N接口用于VNF（虚拟网元）接入到虚拟网络的接口；Ha/CSr-Ha/Nr计算域、网络域与物理资源的接口；Ex-Nf是NFVI与现有网络之间的永久接口。 虚拟化场景下的逻辑组网如下 虚拟层 DVS：虚拟分布式交换机，与物理交换机一样，构建起虚拟机之间的网络，并提供与外部网络互通的能力. 接入层：划分为存储网络，业务网络和管理网络，各个网络层面间，通过划分不同的vlan将管理、业务、存储三个平面逻辑隔离。为简化组网提高组网可靠性，建议接入交换机采用堆叠方式 汇聚层：接入交换机上行到汇聚层交换机，采用ETH-TRUNK上行至汇聚交换机，汇聚交换机堆叠之后，无需启用VRRP功能，如果需要汇聚交换机提供网关功能，则直接将VLANIF接口作为用户网关地址。建议采用集群方式。. 核心层：汇聚交换机上行接入核心层交换机，核心交换机采用OSPF或者静态路由的方式同上层设备进行对接.建议采用集群方式。 虚拟网元层VNF虚拟网元功能VNF旨在通过虚拟化技术实现各种电信业务网络功能，将物理网元映射为虚拟网元VNF，VNF所需资源可以分解为虚拟的计算、存储和交换资源，由NFVI来承载。如下图所示： ETSI NFV架构中，VNF是NFV总体架构的一个功能单元，通过接口参考点Vn-Nf和Ve-Vnfm，分别与NFVI模块和MANO 的VNFM模块进行信息交互。VNF以软件模块形式部署在NFVI提供的资源上，从而实现网络功能虚拟化。 VNF之间的接口指两个实体之间的交互点，这些实体可以是软硬件服务或者资源。软件接口将各个软件功能实体分开，它们被创建并用于软件实体之间的相互作用与交互，同时限制各软件实体之间的通信。ETSI和3GPP为VNF定义了以下5种接口： SWA-1接口：这是各种网络功能之间明确定义的通信接口，表示网络功能上信令或媒体的接口。SWA-1接口是在2个VNF，或者一个VNF和PNF，或者一个VNF和一个节点之间。一个VNF可以支持多个SWA-1接口。 SWA-2接口：属于VNF内部接口，如VNFC到VNFC的通信。这些接口由VNF厂商定义，因此是私有接口。这些接口对底层虚拟化基础设施有明确要求，但对一个 VNF 而言用户是不可见的。SWA-2 接口利用底层的通信机制连接到SWA-5接口上网络连接模块。 SWA-3接口：VNF与NFV管理和编排器间的接口，特别是与VNFM间的接口。管理接口用来为一个或多个 VNF 执行生命周期管理，SWA-3 接口可以使用IP/ L2连接。 SWA-4接口：EM使用SWA-4接口与VNF通信，用于VNF的运行期的管理。 SWA-5接口：对应VNF-NFVI接口，存在于每个VNF和底层NFVI之间的接口。每个不同的VNF依赖于一组不同的SWA-5接口提供的NFVI资源，如网络、计算、存储等。SWA-5接口描述了一种用于VNF的可部署实例的执行环境，是NFVI和VNF本身之间的所有子接口的抽象，SWA-5子接口为：通用计算功能、专用功能、存储、网络I/O。 NFV管理与编排层在传统的网络中，网络功能实现常常与它们运行的架构、资源等紧紧联系在一起。NFV引入了一个新的虚拟化层，让云化后的网络管理格局变得更加复杂，需要更多编排工作，以管理 VNF 和网络服务（Network Service, NS）的全生命周期。VNF能够被其他的VNF或者物理网络关联来实现一个网络服务。VNF软件和NFVI之间的解耦，NS可以包括相关虚拟链路、物理网络功能、虚拟网络功能、网络功能虚拟化基础设置以及相互之间的关系等等这些，它们的协作处理需要一组新的管理和业务流程功能。为此，在NFV网络架构引入了MANO这个逻辑实体，实现NFV网络架构在横向的业务网络管理和编排管理。 在ETSI的定义中，对引入NFV-MANO架构框架，要求遵循一些原则： 1、编排由多个功能块提供，功能块不进行优先级区分，即没有哪个功能块是优先于其他功能块。 2、在架构功能的分类上要求遵循总体平等的架构原则。 3、NFV-MANO功能可能以不同的方式实现，如作为整体的单一实例、作为一个带有多负载均衡的可扩展系统、作为一个分布式协作系统、一个功能上分解的分布式的系统。它也可以实现为云网一体化管理的扩展，或者作为一个实现 NFV功能的与云网管理系统交互的单独系统。 基于上述原则，NFV-MANO既可以通过虚拟化以纯软件的方式实现，也可以异构管理跨地域的NFVI，实现基于策略的分布式统一纳管和自动化响应能力。 参考ETSI NFV文稿，MANO内部包括虚拟基础设施管理、虚拟网络功能管理和网元/服务编排器3个实体，分别实现对NFVI、VNF和NS这3个层次的管理。MANO的架构如下图右边所示 NFVO：编排器，具有两个主要职责，一是执行资源编排功能，对多VIM之间的NFVI资源进行编排；二是完成网络服务生命周期管理，执行网络服务编排功能。 VNFM：虚拟网元管理单元，负责VNF实例的生命周期管理，每个VNF均有一个与之关联的 VNF Manager，一个VNF Manager可以管理一个或多个VNF。 VIM：虚拟化基础设施管理单元，主要实现对NFVI纳管的计算、存储和网络资源的控制与管理，通常在一个运营商的基础设施域内。 NS Catalogue：网络服务目录，代表所有加载的网络服务资源，通过 NFVO所公开的接口支持创建和管理NS部署模板，包括网络服务描述（Network Service Descriptor，NSD）、虚拟链路描述（Virtual Link Descriptor，VLD）和VNF转发图描述（VNF Forwarding Graph Descriptor，VNFFGD）。 VNF Catalogue：虚拟网络功能服务目录，代表了所有加载的VNF网元资源，通过NFVO公开的接口来创建和管理VNF网元，包括虚拟网元功能描述（VNF Descriptor，VNFD）、软件镜像、清单文件和策略文件等。NFVO和VNFM均可通过查询VNF Catalogue来发现和检索VNFD以支持不同的操作，如鉴权授权或检查实例的可行性。 NFV Instances Repository：NFV实例库，记录所有VNF实例和网络服务NS实例的信息。每个VNF实例由一条VNF记录来表示，每个NS实例由一条NS记录来表示。这些记录在每个实例的生命周期中被反复刷新，用于反映执行NS生命周期管理操作或VNF生命周期管理操作的变化。 NFVI Resources Repository：NFVI资源库，记录可用的、保留的及可分配的NFVI资源信息，支持资源预留、分配和用于监控的有用信息。NFVI资源库主要用于NFVO的资源编排和管理，允许根据与NS和VNF实例相关联的资源来追踪NFVI保留和分配资源，如某个VNF实例在其生命周期内的任意时刻所使用VM的数量、配额、端口和弹性IP等信息。 NFV-MANO与多个模块或系统具有接口关系，包括资源管理、虚拟化网络功能、运营系统支撑、业务系统支撑功能和NFV基础设施等，例如：Os-Ma-nfvo为OSS/BSS和NFVO之间的接口；Ve-Vnfm-em是EM和VNFM之间的接口等等，通过这些定义良好的接口，实现MANO和外围模块或系统之间良好协作，华为IES，中国移动OSS4.0等都是基于MANO的这些接口实现。 参考上述ETSI NFV架构，结合电信业务特点，给出一种NFV-MANONFV-MANO系统实现方案。如下图所示，该系统的主要功能模块包括：网络服务编排管理模块（NFVO）、虚拟化网络功能管理模块（VNFM）、虚拟资源管理模块（VIM）、视图管理、接口管理和Rest API。 结合上图实现虚拟化编排的主要系统流程。 1）用户上传VNF以及NS模板； 2）选取需要实例化NS模板，如IMS； 3）系统解析模板，根据策略配置决定如何初始化VNF； 4）根据VNF模板配置初始化需要的虚拟网络环境，如子网、路由、ACL、NAT等； 5）创建VNF需要的云主机； 6）实例化NS所需的VNF； 7）监控脚本监控VNF服务是否启动，并发送监控数据； 8）通过监控脚本判断IMS是否启动成功。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-18-NFV基础概念]]></title>
    <url>%2F2019%2F04%2F19%2F2019-04-18-NFV%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[NFV技术的起源和概念在移动互联网时代，运营商面临内外困局。就自身而言，采用的流量增长—网络扩容—收入增长的商业模型正在失效，庞大、僵化的电信基础网络，不能够满足用户的丰富需求；就竞争对手而言，互联网企业以天为计的业务迭代时间，能够很好地贴合用户需求，飞速发展的OTT业务，使运营商越来越趋向于管道工的角色。 随着云计算普及和x86服务器性能提高，各大运营商为了避免进一步成为哑管道的尴尬，由全球各大运营商牵头提出网络功能虚拟化技术（Network Function Virtualization，NFV）。NFV的思路是通过虚拟化技术降低成本，实现业务的灵活配置。对运营商来说，NFV是一次改变困局、实现跨越发展的难得机遇，一方面可以降低CAPEX和OPEX成本，降低整体的TCO；另一方面也可以加速新产品推出和业务创新。 所谓的网络功能虚拟化就是利用IT虚拟化技术将现有网络设备功能整合进标准x86服务器、存储和网络数通设备，以软件的形式实现网络功能，以此取代目前网络中私有、专有和封闭的网元设备。NFV网络功能示意图如下所示： 维基百科对NFV的定义是：NFV是一种网络架构概念，给予IT虚拟化技术将网络功能节点虚拟化为可以链接在一起提供通信服务的功能模块。 OpenStack基金会对NFV的定义是：通过软件和自动化替代专用网络设备来定义、创建和管理网络功能的新方式。 ETSI NFV标准化组织对NFV的定义是：NFV致力于改变网络运营商构建网络的方式，通过IT虚拟化技术将各种网元变成独立的应用，可以灵活部署在基于标准服务器、存储、交换机构建的统一平台上，实现在数据中心、网络节点和用户端等各个位置的部署与配置。 NFV并非只是简单地在设备中部署虚拟机，其重要特征在于引入虚拟化层之后，虚拟功能网元与硬件完全解耦，改变了电信领域软件、硬件绑定的设备提供模式。打破了传统电信设备的竖井式体系，其核心是网元的分层解耦和引入新的MANO管理体系实现全生命周期管理。 NFV的组织NFV的工作开展涉及标准化和开源两条线，需要多组织的协作，促进NFV技术架构的成熟。其中，标准化线中3GPP SA5、TMF等组织负责NFV流程与接口的设计，ETSI NFV ISG以及ITU-T等组织负责NFV的需求和框架，并为流程和接口提供管理功能。而开源线中包括开源集成软件NFV开放平台项目（OPNFV,Open Platform for NFV)以及相应的开源组件（OpenStack，KVM，OVS等）。 标准化线条 ETSI：在2012年，由全球13家网络运营商（AT&amp;T、BT、Century Link、中国移动、Colt、Deutsche、Telecom、KDDI、NTT、Orange、Telecom Italia、Telefonica、Telstra、Verizon）提出NFV的目标与行动计划，并主要负责NFV接口参考点定义、流程、需求和信元定义。 3GPP：在2013年Rel-13中也开始关注NFV移动网络虚拟化的研究，其主要负责NFV技术在5G业务和市场的应用。 DMTF：2015年开始将自身研究与NFV工作结合，开始支持NFVD需求，支持VNF的管理和网络管理。 CCSA：国内主导NFV技术的组织，主要负责承载网、核心网、接入网等网络功能虚拟化技术研究，编排和接口功能需求以及虚拟化管理技术研究。 开源线条 OPNFV：2014年，由AT&amp;T、NTT、中国移动、RedHat、爱立信等厂商发起OPNFV开源社区，目的是为NFV提供一个统一的开源基础平台，集成OpenStack、OpenDaylight、OVS、ceph等上游社区的成果，推动上游社区加速接纳NFV架构。 OPEN-O：2016年，由华为与Linux基金会、中国移动共同举办OPEN-O新闻发布会，携手中国电信、韩国电信、爱立信、因特尔、RedHat、F5等15家产业领导者发起全球首个统一SDN和NFV开源协同器OPEN-O，主要负责NFV的管理和编排方面的研究。 NFV的技术基础NFV的目标是降低运营商成本的同事提供服务的灵活性和资源的利用率。近几年，标准服务器技术、虚拟化技术、云计算、SDN、开源项目的发展都推动了NFV技术产生和应用。 1、标准服务器 移动通信网络是一个“标准先行”的网络。在传统网络下，由于所有的网络流程、协议信元都经过深入讨论形成标准之后，各设备厂商才设计产品、实现功能。因此，不同的硬件根据实现的功能不同其设计标准、布局和元器件选择等均不相同，不同硬件之间无法兼容替换。同时，由于移动通信协议是固定的，厂商产品所处理的内容不会超过协议定义的范围，一般采用专有芯片来处理完成。因此，这类硬件无法承担较复杂的计算任务，虽然性能优越，但部署成本较高。如下图所示两种专有硬件设计： 在网络功能虚拟化技术中，行业标准服务器的使用是一个关键，不仅软硬件兼容可替换，且供应商市场充分竞争。近几年，x86、ARM架构的快速发展，使得服务器平台多CPU、多核、多线程技术非常成熟。同时，随着SR-IOV网卡的广泛应用，DPDK技术的开源化，使得通用芯片的计算能力，通用网卡的转发能力也越来越强（已经达到40Gbit/s等）。 2、虚拟化技术 在计算机科学中，虚拟化技术是一种资源管理技术，将物理资源抽象，提供给用户使用，打破物理资源的实现方式、地理位置、封装等限制。不仅提高了资源的最优化利用，而且还可实现资源的负载均衡、节能减排和自愈等功能。主要涉及计算虚拟化、存储虚拟化和网络虚拟化三大领域虚拟化技术。 在虚拟化技术实现中，Hypervisor是所有虚拟化技术实现的核心，它是运行基础服务器和操作系统之间的中间层软件，如VMware的ESX。通过它，多个操作系统和应用程序可以共享硬件资源，协调和管理服务器上所有物理设备和虚拟机，因此，也称为虚拟机监视器VMM（Virttual Machine Monitor）。主流的Hypervisor有VMware vSphere、Hyper-V、XenServer、PowerVM、KVM、FusionCompute等。 3、云计算 目前，云计算没有一个统一的定义。维基百科的定义是：云计算是一种基于互联网的计算方式，通过这种方式，共享软硬件资源和信息可以按需提供为计算机和其它设备。云计算的核心思想是将大量用网络连接的计算资源统一管理和调度，构成一个计算资源池向用户按需服务。提供资源的网络就称为“云”。 云计算提供三种服务模式，分为基础设施即服务（IaaS）、平台即服务（PaaS）和软件即服务（SaaS）。网络功能虚拟化NFV主要用于电信网络领域，因此也被称为电信网络云化实现，简称电信云。既然是“云”，那就必然离不开云计算的三种服务模式。在NFV的网路架构中，VIM/NFVI是真正的与“云”有关联的部分，其对应云计算三种服务模式的IaaS。而NFV的VNFM和NFVO部分其本质只是“云”上的APP应用。 云计算部分详见《云计算的技术架构》。 提到云计算必然要提及OpenStack，这一个开源云操作系统从诞生起，社区活跃度就非常高。目前，除了Amazon、微软外，几乎所有的私有云和公有云解决方案都是基于开源OpenStack的改造和增强版。那么NFV，也就是电信云的VIM+NFVI是否也可以采用OpenStack？从NFV的技术特点和应用场景来看，其对OpenStack有三点基本诉求： 1、业务面的高性能要求：涉及虚拟机性能和网络转发性能两个方面。 2、高可用和高可靠性要求：涉及控制平面、业务平面和存储平面三个方面。 3、易运维要求：涉及云化电信网络统一管理、统一调度、自动化运维等方面。 为此，为了适用NFV，OpenStack需要在业务平面进行相应增强，具体如下图所示： 上述增强技术主要涉及VIM层面和OpenStack+KVM的场景，主要改变在Nova和Neutron中，本质上是对虚拟化性能优化技术的充分利用。 4、SDN 网络设备一般由控制平面和数据平面组成。控制平面为数据平面制定转发策略，规划转发路径，如路由协议、网关协议等。数据平面则是执行控制平面策略的实体，包括数据的封装/解封装、查找转发表等。目前，设备的控制面和转发面都是由设备厂商自行设计和开发，不同厂家实现的方式不尽相同。并且，软件化的网络控制面功能被固化在设备中，使设备使用者没有任何控制网络的能力。这种控制平面和数据平面紧耦合的方式带来了网络管理复杂、网络测试繁杂、网络功能上线周期漫长等问题。因而，软件定义网络应运而生。 SDN技术是软件定义网络，本质是把网络软件化，提高网络可编程能力和易修改性。SDN没有改变网络的功能，而是重构了网络的架构。SDN的价值在于：网络业务自动化和网络自治，更快部署网络业务实例。更快在网络中增加新业务，大量需求仅需要升级控制器软件就可以实现。同时，简化了网络协议，大量网络业务协议逐渐消失，用户的策略处理集中在控制器实现。通过集中控制，对网络资源进行统筹调度和深度挖掘，提高网络资源利用率，接入更多业务，从垂直整合走向水平整合，使得芯片、设备、控制器各层可以独立分层充分竞争。 NFV没有改变设备的功能，而是改变了设备的形态。NFV的本质是把专用硬件设备变成一个通用软件设备，共享硬件基础设施。NFV的价值在于：软件设备的发行安装速度远比硬件设备快，容量伸缩更快，避免硬件采购安装的长周期，可按需实时扩容。实现新需求新业务更快，避免了硬件的冗长开发周期。同时，简化了设备形态，统一了底层硬件资源，都是服务器和交换机。采用通用服务器作为和交换机作为基础设施，大大降低设备成本。水平整合改变了原来的竞争格局，各个层次可以分层竞争。 SDN与NFV有什么关系？NFV的软件设备（统称VNF）快速部署以及VNF之间网络快速建立，需要支持网络自动化和虚拟化能力，这需要SDN网络提供支持。在SDN网络情况下的一些网络诉求，比如能够快速提供虚拟网络，快速部署增值业务处理设备和网络设备等这些快速业务上线需求，需要NFV的软件网络设备（FW、vRouter）才能达成目的。 所以，SDN是面向网络的，SDN没有改变网络的功能，而是重构了网络的架构。NFV是面向设备的，NFV没有改变设备的功能，而是改变设备的形态。]]></content>
      <categories>
        <category>NFV基础</category>
      </categories>
      <tags>
        <tag>电信云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-16-Linux系统命令-第二篇《文件或目录操作命令》]]></title>
    <url>%2F2019%2F04%2F17%2F2019-04-16-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%BA%8C%E7%AF%87%E3%80%8A%E6%96%87%E4%BB%B6%E6%88%96%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[pwd：显示当前所在的位置pwd命令是“print working directory”中每个单词的首字母缩写，其功能是显示当前工作目录的绝对路径。在实际工作中，我们在命令行操作命令时，经常会在各个目录路径之间进行切换，此时可使用pwd命令快速查看当前我们所在的目录路径。 语法格式：pwd [option] 通常情况下，执行pwd命令不需要带任何参数。 重要选项参数 【使用举例】1）不带任何选项执行pwd命令。 1[root@C7-Server01 ~]# pwd #显示当前目录为root用户的家目录 2）对比使用-L和-P参数 1[root@C7-Server01 ~]# ls -l /etc/rc0.d # 显示/etc/目录下rc0.d的目录信息 显示结果为/etc/rc0.d目录为/etc/rc.d/rc0.d目录的软链接 1[root@C7-Server01 ~]# cd /etc/rc0.d/ # 进入/etc/rc0.d目录 在该目录下分别执行pwd -L和pwd -P，对比执行结果。 当前系统命令提示符的显示格式是系统默认的格式，受环境变量PS1限制，查看环境变量PS1的方法为： 1[root@C7-Server01 ~]# echo $PS1 如果要修改系统提示符的显示格式，可以根据下表自行调整/etc/profile文件中PS1环境变量的设置，修改完成保存退出，通过source /etc/profile加载新设置的环境变量使配置生效。 cd：切换目录cd命令是“change directory”中每个单词的首字母缩写，其功能是从当前工作目录切换到指定的工作目录。 语法格式：cd [option] [dir] cd命令后面的选项和目录等参数都可以省略。默认情况下，单独执行cd命令，可切换到当前登录用户的家目录（由系统环境变量HOME定义）。cd是bash shell的内置命令，查看该命令对应的系统帮助需要使用help cd。在使用cd命令时，如果使用键盘上“Tab”键的自动补齐功能，可以提高输入速度和准确度。这个“Tab”键的自动补齐功能同样也适用于其他命令。 要了解路径的概念，比如，相对路径是不从“/”（斜线）开始的路径，而是从当前目录或指定的目录开始，如：data/、mnt/oldboy；绝对路径是从“/”（斜线）根开始的路径，如：/root/mydata/、/mnt/VMware。 重要选项参数 需熟练掌握带*选项的用法：当需要切换到当前用户上一次所在的目录时，请使用“cd -”（注意空格）；当需要切换到当前用户的家目录时，请使用“cd～”（注意空格）；当需要切换到当前目录的上一级目录所在的路径时，请使用“cd ..”（注意空格）。 【使用举例】 1）进入系统/etc目录 1[root@C7-Server01 ~]# cd /etc/ 2）切换到/usr/local目录下 1[root@C7-Server01 etc]# cd /usr/local/ 3）切换到当前目录的上一级目录 1[root@C7-Server01 local]# cd .. # ..表示当前目录的父目录，.表示当前目录 4）进入当前目录的父目录的父目录 1[root@C7-Server01 local]# cd ../.. # 当前目录为/usr/local，其父目录就是/usr，其爷爷目录就是根目录/ 只要目录有足够多的层次，可以一直这样继续下去“cd../../../..”，直到退到“/”为止。 5）返回当前用户上一次所在的目录 1[root@C7-Server01 /]# cd - # -就是表示返回到进入当前目录前的那个目录 6）进入当前用户的家目录 1[root@C7-Server01 local]# cd ~ tree：以树形结构显示目录下的内容tree命令的中文意思为“树”，功能是以树形结构列出指定目录下的所有内容，包括所有文件、子目录及子目录里的目录和文件。 语法格式：tree [option] [directory] tree命令后若不接选项和目录就会默认显示当前所在路径目录的目录结构。tree命令可能需要单独安装，首先检查系统是否安装了tree命令，如果采用的是最小化安装Linux系统的方式，那么tree命令有可能没有安装，此时可用yum命令安装tree命令。 1、检查系统是否安装tree命令，如果没有输出，就表示没有安装，此时通过第二步命令进行安装。 1[root@C7-Server01 ~]# rpm -aq tree 2、安装tree命令 1[root@C7-Server01 ~]# yum install -y tree 重要选项参数 【使用示例】 1）不带任何参数执行tree命令 1[root@C7-Server01 ~]# tree 2）以树形结构显示目录下的所有内容（-a的功能） 在Linux系统中，以“.”点号开头的文件为隐藏文件，默认不显示。 1[root@C7-Server01 bin]# tree -a # 在/usr/bin目录下执行 3）只列出根目录下第一层目录的结构（-L功能） 1[root@C7-Server01 /]# tree -L 1 # -L参数后接数字，表示查看目录的层数，不带-L选项默认显示所有层 4）只显示所有的目录（但不显示文件） 1[root@C7-Server01 ~]# tree -d 1[root@C7-Server01 ~]# tree -dL 1 # -d参数只显示目录，-L参数显示层数，组合起来就是只显示当前目录下第一层目录。 5） -f选项和-i选项的使用 1[root@C7-Server01 ~]# tree -L 1 -f /boot # 只显示/boot目录下第一层的所有文件的全路径 1[root@C7-Server01 ~]# tree -L 1 -fi /boot # 加上-i选项，显示结果不带树枝，便于复制粘贴 6）使用tree命令区分目录和文件的方法（常用） 1[root@C7-Server01 ~]# tree -L 1 -F /boot # 使用-F参数会在目录后面添加“/”，方便区分目录。 123[root@C7-Server01 ~]# tree -L 1 -F /etc/ | grep /$ # 结合管道符|和grep，可以方便将目录过滤出来#/$是Linux的通配符，表示以/结尾的字符#管道符的作用在于指令拼接，上面指令的意思就是将tree指令的执行结果交给grep指令处理。 小练习：通过tree指令将/usr/local/下的第一级文件全部过滤出来，结果只能包含文件，不能包含目录（答案贴在本文的讨论区） mkdir：创建目录mkdir命令是“make directories”中每个单词的粗体字母组合而成，其功能是创建目录，默认情况下，如果要创建的目录已存在，则会提示此文件已存在；而不会继续创建目录。 语法格式：mkdir [option] [directory] mkdir命令可以同时创建多个目录，格式为mkdir dir1 dir2…，也可以使用{}来完成指定序列的批量目录创建。使用mkdir创建多级目录时，建议直接使用-p参数，可以避免出现“No such file or directory”这样没有文件或目录的报错了，且不会影响已存在的目录。 重要选项参数： 【使用示例】 1）不使用任何命令参数创建目录用法示例 1[root@C7-Server01 kkutysllb]# mkdir mytest 1[root@C7-Server01 kkutysllb]# mkdir mytest # 当再次创建时，会提示目标目录已存在 2）使用-p参数递归创建目 当我们创建多级目录时，如果第一级目录（data）不存在，那么创建结果会报错，导致无法创建成功，操作如下： 1[root@C7-Server01 kkutysllb]# mkdir data/stu01 此时，可以指定-p参数递归创建多级目录： 1[root@C7-Server01 kkutysllb]# mkdir -p data/stu01 3）加-v参数显示创建目录的过程 12# 在data目录下同时创建2个子目录stu02和stu03，并且同时在两个子目录下再分别创建四个子目录test01-test04 [root@C7-Server01 kkutysllb]# mkdir -pv data/stu&#123;02,03&#125;/test&#123;01..04&#125; 其实这个-v没有什么实际用途，它只是用来显示创建目录的过程。 大括号（{}）的特殊用法，见如下图示： 4）创建目录时可使用-m参数设置目录的默认权限 如果创建目录是不设置目录的权限，所创建目录的默认权限是755，如下所示：（目录的权限问题在讲解ls指令会讲到） 1[root@C7-Server01 kkutysllb]# mkdir -m 333 dir01 这个指令在创建特殊安全要求的目录时会用到，平时一般不用。因为，目录的权限还决定了目录中子目录及其文件的权限，所以在创建目录时要综合考虑。 touch：创建空文件或改变文件的时间戳属性touch命令有两个功能：一是创建新的空文件；二是改变已有文件的时间戳属性。 语法格式：touch [option] [file] 注意区分touch和mkdir命令的功能，mkdir命令是创建空目录，而touch是创建空文件。在Linux中，一切皆文件。虽然touch命令不能创建目录，但是可以修改目录的时间戳。 重要选项参数 【使用示例】 1）创建文件示例（文件事先不存在的情况） 1[root@C7-Server01 kkutysllb]# touch image001 12#可以使用&#123;&#125;同时创建多个文件[root@C7-Server01 kkutysllb]# touch image&#123;002..009&#125; 2）更改文件的时间戳属性 12#查看文件时间戳属性，可以使用stat指令[root@C7-Server01 kkutysllb]# stat image001 文件的时间戳属性分为访问时间、修改时间、状态改变时间。 1[root@C7-Server01 kkutysllb]# touch -a image001 # -a选项更改文件的访问时间为当前时间 1[root@C7-Server01 kkutysllb]# touch -m image001 # -m选项更改文件的修改时间为当前时间 无论是-a选项修改文件的访问时间，还是-m选项修改文件的修改时间，文件的状态改变时间ctime也同步进行改变。 3）指定时间属性创建/修改文件 12# 将文件image001的访问时间和修改时间统一修改为2020年10月21日 [root@C7-Server01 kkutysllb]# touch -d 20201021 image001 这个功能经常用于黑客入侵后的操作。 12#使用image002文件的时间戳为模板改为image001文件的时间戳[root@C7-Server01 kkutysllb]# touch -r image002 image001 12#还可以利用-t选项将image001的时间修改为202012312234.55的格式[root@C7-Server01 kkutysllb]# touch -t 202012312234.55 image001 知识扩展： 这里扩展一点有关时间戳属性的知识。GNU/Linux的文件有3种类型的时间戳： 12345Access：2015-07-30 17:48:20.502156890 +0800 #&lt;==最后访问文件的时间。 Modify：2015-07-30 17:48:45.006106223 +0800 #&lt;==最后修改文件的时间。 Change：2015-07-30 17:48:45.006106223 +0800 #&lt;==最后改变文件状态的时间。 ls：显示目录下的内容及相关属性信息ls命令可以理解为英文单词list的缩写，其功能是列出目录的内容及其内容属性信息（list directorycontents）。该命令有点类似于DOS系统下的dir命令，有趣的是，Linux下其实也有dir命令，但我们更习惯于使用ls。 语法格式：ls [option] [file] ls命令后面的选项和目录文件可以省略，表示查看当前路径的文件信息。 重要选项参数 【使用示例】 1）直接执行ls命令，不带任何参数 12# 列出/home/kkutysllb目录下的文件信息 [root@C7-Server01 kkutysllb]# ls 2）使用-a参数显示所有文件，特别是隐藏文件 12# 加了-a参数，就会把以“.”（点号）开头的内容显示出来了。这里显示的第一个点号，表示当前目录，即/home/kkutysllb/目录本身，而两个点号则表示当前目录的上级目录，此处就代表/home/目录了 [root@C7-Server01 kkutysllb]# ls -a 3）使用-l参数显示详细信息 1[root@C7-Server01 kkutysllb]# ls -l 4）显示完整时间属性的参数 –time-style=long-iso 1[root@C7-Server01 kkutysllb]# ls -l --time-style=long-iso 小练习：关于–time-style=long-iso的其它参数大家自行练习，将答案贴在本文的讨论区。 5）执行ls命令，带显示内容的访问时间属性的参数 12# 首先通过stat指令查看image001文件的访问时间信息 [root@C7-Server01 kkutysllb]# stat image001 12# 通过选项--time=atime显示文件的访问时间[root@C7-Server01 kkutysllb]# ls -l --time-style=long-iso --time=atime image001 同理，通过选项–time=mtime可以查看文件的修改时间，选项–time=ctime可以查看文件状态的改变时间，大家可以自行练习。（国际惯例，答案贴在本文的讨论区） 6）执行ls命令，带-F参数（这一点与tree命令的-F很类似） 12#加了-F,我们可以清晰地看到所有目录的结尾都被加上了斜线/[root@C7-Server01 kkutysllb]# ls -lF 12# 过滤出当前目录下所有子目录[root@C7-Server01 kkutysllb]# ls -lF | grep /$ 123# 过滤出当前目录下所有普通文件# grep命令的-v选项是反向过滤的意思[root@C7-Server01 kkutysllb]# ls -lF | grep -v /$ 7）使用-d参数只显示目录本身的信息 12# 有时候我们想查看目录本身的信息，但是若使用“ls目录”命令，就会显示目录里面的内容，比如： [root@C7-Server01 kkutysllb]# ls -l data/ 12# 通过-d选项可以查看目录的详细信息[root@C7-Server01 kkutysllb]# ls -ld data/ 8）使用-R参数递归查看目录 12# tree指令没有递归方式查看多级目录下详细信息的选项，但是ls指令就有，通过-R选项实现 [root@C7-Server01 kkutysllb]# ls -lR data/ 小练习：通过ls命令迅速查找/boot目录下最近更新过的文件（不知文件名的情况下） 提示： 1、利用-t选项对目录下文件按照修改时间排序。 2、利用-r选项对时间排序后的文件进行倒序排序。 9）ls命令列出文件的属性解读 123456# 利用-h和-i选项可以列出某个目录下所有文件和子目录的详细信息# -h选项是将文件的大小通过人类可读的方式显示出来，不加-h选项系统默认是字节单位# -i选项是文件的inode值，在linux系统中一个文件分为inode和block两部分，无论哪一部分占用满，都会显示磁盘空间不足的提示。# 因此，有时你会发现磁盘还有很大空间，但是提示磁盘占用满，这就是因为inode占用满的缘故。[root@C7-Server01 kkutysllb]# ls -lhi /boot 第一列：inode索引节点编号。第二列：文件类型及权限（第一个字符为类型，后9个字符为文件权限符号）。第三列：硬链接个数（详细请参看ln命令的讲解）。第四列：文件或目录所属的用户（属主）。第五列：文件或目录所属的组。第六列：文件或目录的大小。第七、八、九列：文件或目录的修改时间。第十列：实际的文件名或目录名。 cp：复制文件或目录cp命令可以理解为英文单词copy的缩写，其功能为复制文件或目录。 语法格式：cp [option] [source] [dest] 重要参数选项 【使用示例】 1）无参数和带参数-a的比较 12# 查看当前目录下的文件信息 [root@C7-Server01 kkutysllb]# ls -l 12# 通过cp指令复制image001文件为image010文件[root@C7-Server01 kkutysllb]# cp image001 image010 12# 使用-a选项复制image001文件为image011[root@C7-Server01 kkutysllb]# cp -a image001 image011 2）-i参数的例子 123# 使用-i选项覆盖已存在文件时，会有提示确认 [root@C7-Server01 kkutysllb]# cp -i image001 image011 请大家思考下：为什么不使用-i选项覆盖已存在文件时，也会有提示确认？（提示：利用Google搜索下Linux系统的别名功能） 3）使用-r参数复制目录 12345# 如果要复制一个目录，可以用-r参数递归复制**# 在linux系统中，没有单独复制目录的命令 [root@C7-Server01 kkutysllb]# cp -r data/ data_tmp/ 通过diff指令或vimdiff指令对比复制后的两个文件（diff和vimdiff指令后面会讲） mv：移动或重命名文件mv命令可以理解为英文单词move的缩写，其功能是移动或重命名文件（move/rename files）。 语法格式：mv [option] [source] [dest] 重要参数选项 【使用示例】 1）给文件改名的例子 1234567891011121314151617181920212223242526272829303132333435363738394041# 将image001文件修改为image_t001 [root@C7-Server01 kkutysllb]# ls -l total 0 -rw-r--r-- 1 root root 0 Apr 16 18:35 202012312234.55 drwxr-xr-x 5 root root 45 Apr 17 11:59 data drwxr-xr-x 5 root root 45 Apr 17 11:59 data_tmp d-wx-wx-wx 2 root root 6 Apr 16 15:09 dir01 -rw-r--r-- 1 root root 0 Dec 31 2020 image001 -rw-r--r-- 1 root root 0 Apr 16 17:56 image002 -rw-r--r-- 1 root root 0 Apr 16 17:56 image003 -rw-r--r-- 1 root root 0 Apr 16 17:56 image004 -rw-r--r-- 1 root root 0 Apr 16 17:56 image005 -rw-r--r-- 1 root root 0 Apr 16 17:56 image006 -rw-r--r-- 1 root root 0 Apr 16 17:56 image007 -rw-r--r-- 1 root root 0 Apr 16 17:56 image008 -rw-r--r-- 1 root root 0 Apr 16 17:56 image009 -rw-r--r-- 1 root root 0 Apr 17 11:46 image010 -rw-r--r-- 1 root root 0 Dec 31 2020 image011 drwxr-xr-x 2 root root 6 Apr 16 14:30 mytest [root@C7-Server01 kkutysllb]# mv image001 image_t001 [root@C7-Server01 kkutysllb]# ls -l 2）移动文件的例子 12345# 将image_t001文件移动到dir01目录下 # 源目录下已不存在刚移动过的文件 [root@C7-Server01 kkutysllb]# mv image_t001 dir01/ 在dir01目录下存在刚移动进来的文件image_t001 12345# 移动多个文件# 第一种方式，源文件在前，目标目录在后[root@C7-Server01 kkutysllb]# mv image002 image003 image004 dir01/ 123# 第二种方式，使用-t选项，目标目录在前，源文件在后[root@C7-Server01 kkutysllb]# mv -t dir01/ image005 image006 3）关于mv命令的使用小结 rm：删除文件或目录rm命令可以理解为英文单词remove的缩写，其功能是删除一个或多个文件或目录（remove files or directories）。这是Linux系统里最危险的命令之一，请慎重使用。 语法格式：rm [option] [file] 重要选项参数 【使用示例】 1）不带参数删除例子实践 1234567#删除/home/kkutysllb/image011这个文件 #不带参数时，删除前默认要求确认，原因与cp命令一致 [root@C7-Server01 kkutysllb]# rm image011 rm: remove regular empty file ‘image011’? y # 回复y删除，回复n不删除 2）强制删除例子实践 1234567# 使用-f选项强制删除image007文件 # 直接删除，不提示确认 [root@C7-Server01 kkutysllb]# rm -f image007 [root@C7-Server01 kkutysllb]# 3） 使用-r选项递归删除目录 1234567891011121314151617181920212223# 使用-r选项可以递归删除目录，原理是先删除目录中文件和子目录，然后再删除目录本身 # 不带-f参数时，每删除一个文件都需要确认 # linux还有个删除目录的命令rmdir，但是只能删除空目录 [root@C7-Server01 kkutysllb]# rm -r dir01/ rm: descend into directory ‘dir01/’? y rm: remove regular empty file ‘dir01/image_t001’? y rm: remove regular empty file ‘dir01/image002’? yrm: remove regular empty file ‘dir01/image003’? y rm: remove regular empty file ‘dir01/image004’? y rm: remove regular empty file ‘dir01/image005’? y rm: remove regular empty file ‘dir01/image006’? y rm: remove directory ‘dir01/’? y 4）关于删除的实践经验 常在河边走，哪有不湿鞋！但是如果能遵守下面的要领就可以少湿鞋甚至不湿鞋！ 1）用mv替代rm，不要急着删除，而是先移动到回收站/tmp。 2）删除前务必备份，最好是异机备份，若出现问题随时可以还原。 3）如果非要删除，那么请用find完成要删除文件查找，然后通过管道符接rm指令进行精准删除，最大限度避免误删除，包括通过系统定时任务等清理文件方法。比如下面的指令： 1find ．-type f -name "*.txt" -mtime +7|xargs rm –f 上面指令的意思是查找当前目录下7天以前的所有以.txt结尾的文件（注意，linux没有文件扩展名的概念，在linux中一切皆是文件），然后将结果通过管道符“|”传递给xargs rm -f指令组合进行删除（xargs是一个标准输入指令，其后面接rm指令就是模拟用户在终端侧直接输入rm指令的用法） ln：硬链接与软链接ln命令可以理解为英文单词link的缩写，其功能是创建文件间的链接（make links between files），链接类型包括硬链接（hard link）和软链接（符号链接，symbolic link）。 硬链接（Hard Link）：创建语法为“ln源文件目标文件”，硬链接生成的是普通文件（-字符）。 软链接或符号链接（Symbolic Link or Soft Link）：创建语法为“ln-s源文件目标文件（目标文件不能事先存在）”，软链接生成的是符号链接文件（l类型）。 硬链接是指通过索引节点（Inode）来进行链接。在Linux（ext2、ext3、ext4）文件系统中，所有文件都有一个独有的inode编号。在Linux文件系统中，多个文件名指向同一个索引节点（inode）是正常且允许的。这种情况下的文件就称为硬链接。硬链接文件相当于文件的另外一个入口。它的作用之一就是允许一个文件拥有多个有效路径名（多个入口），这样用户就可以建立硬链接到重要文件，以防止误删源数据，只有删除了源文件以及源文件所有对应的硬链接文件，文件实体才会被删除。 软链接或符号链接（Symbolic Link or Soft Link）有点像Windows里的快捷方式。软链接类似于一个文本文件，里面存放的是源文件的路径，指向源文件实体。即使删除了源文件，软链接文件也还是依然存在，但是无法访问指向的源文件路径内容了。软链接和源文件是不同类型的文件，也是不同的文件，inode号也不相同。 语法格式：ln [option] [source] [target] 重要选项参数 【使用示例】1）创建一个硬链接文件 1234567891011121314151617181920212223242526[root@C7-Server01 kkutysllb]# ln /etc/hosts hard_link# 通过ls -i查看源文件和硬链接文件的inode值[root@C7-Server01 kkutysllb]# ls -li /etc/hosts hard_link 33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 /etc/hosts33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 hard_link# 可以看见硬链接文件与原文件inode值一样# 删除源文件/etc/hosts[root@C7-Server01 kkutysllb]# rm -f /etc/hosts[root@C7-Server01 kkutysllb]# cat /etc/hostscat: /etc/hosts: No such file or directory # 提示文件不存在# 可以通过刚才的硬链接文件进行误删除文件的找回，即在做一个硬链接[root@C7-Server01 kkutysllb]# ln hard_link /etc/hosts[root@C7-Server01 kkutysllb]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@C7-Server01 kkutysllb]# ls -li hard_link /etc/hosts33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 /etc/hosts33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 hard_link#找回后的文件inode值与删除前一致，就好像误删除从没发生过。 2）创建一个软连接文件 12345678910111213141516#使用-s选项创建一个软连接文件[root@C7-Server01 kkutysllb]# ln -s /etc/hosts soft_link# 通过ls -li指令可以看出软连接文件和源文件的inode值不一致，说明这是两个不同的文件[root@C7-Server01 kkutysllb]# ls -li /etc/hosts soft_link 33583813 -rw-r--r--. 2 root root 158 Jun 7 2013 /etc/hosts1231043 lrwxrwxrwx 1 root root 10 Apr 17 14:04 soft_link -&gt; /etc/hosts#查看软连接文件的内容就是查看原文件的内容[root@C7-Server01 kkutysllb]# cat soft_link 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6#通过readlink命令可以查看软链接中的存放的具体信息（非原文件内容）[root@C7-Server01 kkutysllb]# readlink soft_link /etc/hosts#可以看出软链接中存放的就是源文件的绝对路径 软链接和源文件的关系图示： *注意一点：**目录不可以创建硬链接，但是可以创建软链接。因为，前面讲过Linux系统的磁盘需要挂载到一个目录才可以使用，但是多个磁盘可能对应分区格式和文件系统不一样，因此如果两个目录之间如果是硬链接关系，但是对应的磁盘分区格式又不一致就会出现问题。*** find：查找目录下的文件find命令用于查找目录下的文件，同时也可以调用其他命令执行相应的操作。 语法格式：find [-H] [-L] [-P] [-D debugopts] [-Olevel] [pathname] [expression] 注意子模块的先后顺序。 find命令用法的使用说明： 重要选项参数 【使用示例】 1）查找指定时间内修改过的文件 123# 查找当前目录下2天内访问过的文件 [root@C7-Server01 kkutysllb]# find . -atime -2 2）用-name指定关键字查找 123#查找/var/log下5天以前修改过的以.log结尾的所有文件 [root@C7-Server01 kkutysllb]# find /var/log -mtime +5 -name "*.log" 3）利用“！”反向查找 123#查找当前目录下非目录文件 [root@C7-Server01 kkutysllb]# find . ! -type d 小练习： 1、按照文件大小来查找文件 2、按照文件或目录的权限来查找文件 3、查找文件时希望忽略一个或多个目录的方法 4、按照文件的更新时间来差找文件 5、按照文件或目录的归属用户和用户组来查找文件 至此，Linux系统文件和目录的操作命令部分已经写完了。但是，这并不是所有命令（Linux系统核心命令使用频率较高的越有150条），而是和运维工作息息相关的最最最常用的几个命令，作为初学者必须首先掌握上述这些命令，再结合实际工作中的需要去掌握其它命令。其它命令章节也是遵循此原则来写，后续不再赘述。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-10-Linux系统命令-第一篇《关机、重启命令》]]></title>
    <url>%2F2019%2F04%2F15%2F2019-04-10-Linux%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4-%E7%AC%AC%E4%B8%80%E7%AF%87%E3%80%8A%E5%85%B3%E6%9C%BA%E3%80%81%E9%87%8D%E5%90%AF%E5%91%BD%E4%BB%A4%E3%80%8B%2F</url>
    <content type="text"><![CDATA[Linux命令行的概述众所周知，Linux是一个主要通过命令行来进行管理的操作系统，即通过键盘输入指令来管理系统的相关操作，包括但不限于编辑文件、启动停止服务等。这和初学者曾经使用的Windows系统使用鼠标点击的可视化管理大不相同。 使用鼠标可视化管理的优势是简单、容易上手，但缺点是不便于快速、批量、自动化管理系统，而且感觉系统很臃肿，这个时候Linux系统的命令行管理优势就凸显了。使用Linux命令行管理，不但可以批量、自动化管理，而且还可以实现智能化、可视化管理；当然，后者需要开发人员配合开发管理界面来完成。但是无论如何，Linux系统的优势基因还是快速、批量、自动化、智能化管理系统及处理业务。 Linux命令行介绍安装Linux系统时，无论是使用文本模式（命令行）安装，还是使用图形模式安装，最终管理系统的任务都会落到命令行之上。 大多数互联网企业在安装系统时甚至不会安装图形管理软件包，而是直接使用文本模式安装，因此登录后直接面对的就是命令行的界面，如下图所示： Linux命令行结尾的提示符有“#”和“$”两种不同的符号，代码如下所示： 1[root@C7-Server01 ～]# # 这是超级管理员root用户对应的命令行。 1[kkutysllb@C7-Server01 ～]$ # 这是普通用户kkutysllb对应的命令行。 Linux命令提示符由PS1环境变量控制。示例代码如下： 1[root@C7-Server01 ~]# set | grep PS1 这里的PS1=[\u@\h\W]\$，可以通过全局配置文件/etc/bashrc或/etc/profile进行按需配置和调整。 Linux命令行常用快捷键这里需要特别说明一下的是，在企业工作中，管理Linux时一般不会直接采用键盘、显示器登录系统，而是会通过网络在远程进行管理，因此，需要通过远程连接工具连接到Linux系统中。目前最常用的Linux远程连接工具为：SecureCRT和Xshell客户端软件，因此，本节涉及的常用命令快捷键也是基于这两款客户端软件的，其他软件的快捷键使用情况与此基本类似。提高Linux运维效率的30个命令行常用快捷键如下： Linux重启、关机、注销命令1、shutdown 命令常用操作shutdown是一个用来安全关闭或重启Linux系统的命令，系统在关闭之前会通知所有的登录用户，系统即将关闭，此时所有的新用户都不可以登录，与shutdown类似功能的命令还有init、halt、poweroff、reboot。 语法格式为：shutdown [OPTION]…… TIME [MESSAGE] 通常情况下，我们执行的shutdown命令为shutdown-h now或shutdown-r now shutdown命令常用的参数选项如下： 【使用举例】 1）10分钟后关闭或重启系统 12#关机[root@C7-Server01 ~]# shutdown -h +10 12#重启[root@C7-Server01 ~]# shutdown -r +10 2）11点整定时关闭或重启系统 12#11点整重启 [root@C7-Server01 ~]# shutdown -r 11:00 12#11点整关闭[root@C7-Server01 ~]# shutdown -h 11:00 3）立即关闭或重启Linux系统的命令如下： 12#立即关机 [root@C7-Server01 ~]# shutdown -h now 12#立即重启[root@C7-Server01 ~]# shutdown -r now 2、halt/poweroff/reboot命令的常用操作从RedHat或CentOS 6开始，你会发现halt、poweroff、reboot这三个命令对应的都是同一个man帮助文档。在CentOS 6时代，而halt、poweroff是reboot命令的链接文件，而在CentOS 7时代，这三个命令都是systemctl的连接文件。 语法格式为：halt/poweroff/reboot [OPTIONS] 通常情况下，我们执行这三个命令时都不带任何参数。因此，这三个命令的选项参数也就没什么好研究的。 【使用举例】 1）使用halt命令完成关机 1[root@C7-Server01 ~]# halt 2）使用poweroff命令完成关机 1[root@C7-Server01 ~]# poweroff 3）使用reboot命令重启系统 1[root@C7-Server01 ~]# reboot 除此之外，还可以使用init指令完成关机或重启指令，执行init 0为关机，执行init 6为重启，这是因为0和6是系统的两个运行级别，分别对应关机和重启。你们可以在自己的实验环境尝试执行看看。。。 3、常见不常见关机、重启和注销的命令列表 Linux命令正是组成Linux系统最核心、重要的基础之一，因此，大家只要牢牢掌握基础命令，在日后linux运维、shell编程、云计算/大数据、甚至Python自动化运维都能如鱼得水。]]></content>
      <categories>
        <category>Linux核心命令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-04-05-CentOS操作系统6.x版本与7.x版本的区别]]></title>
    <url>%2F2019%2F04%2F15%2F2019-04-05-CentOS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F6-x%E7%89%88%E6%9C%AC%E4%B8%8E7-x%E7%89%88%E6%9C%AC%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Centos7与6之间最大的差别就是初始化技术的不同，7采用的初始化技术是Systemd,并行的运行方式，除了这一点之外，服务启动、开机启动文件、网络命令方面等等也存在不同。 1.系统初始化技术在Linux系统常用系统初始化技术主要有三种：Sysvinit技术、Upstart技术和Systemd技术。在CentOS 6时代，主要使用sysvinit技术完成系统的初始化，其实现原理通过调用各种服务的shell脚本完成初始化工作。而在CentOS 7时代，开始采用Systemd技术来完成系统的初始化工作，其最大的特点的各类服务可以并发完成。同时，在CentOS 7时代也保留Sysvinit技术。三种不同的初始化技术的优缺点如下： Sysvinit技术特点： 1.系统第1个进程为init; 2.init进程是所有进程的父进程，不可kill； 3.大多数Linux发行版的init系统是和SystemV相兼容的，被称为sysvinti 4.代表系统：CentOS5 CentOS6 优点： sysvinit运行非常良好，概念简单清晰，它主要依赖于shell脚。所以，如果大家开始学习shell编程时，建议安装一个CentOS6系统。 缺点： 1.按照一定顺序执行——&gt;启动太慢。 2.很容易hang住，fstab与nfs挂载问题 Upstart技术CentOS6采用了upstart技术代替sysVinit进行引导，Upstart对rc.sysinit脚本做了大量的优化，缩短了系统初始化的启动时间。但是CentOS6为了简便管理员的操作，upstart的很多特性并没有凸显或直接不支持。代表系统：CentOS6, Ubuntu14, 从CentOS7, Ubuntu15开始使用systemd。 Systemd技术新系统都会采用的技术（RedHat7,CentOS7,Ubuntu15等），设计目标是克服sysvinit固有的缺点，提高系统的启动速度。和Sysvinit兼容，降低迁移成本，最主要优点：并行启动，Pid为1的进程。 2.在yum源上的优化在centos6的时候，默认是从官方源下载rpm包的，由于是国外的yum源很慢不能用，CentOS7在这里做了优化，当我们使用yum安装软件的时候，默认不会再从官方下载，而是自动寻找离自己地理位置最近的yum源开始下载。 3.网络命令如果在安装系统的时候选择minimal，会比之前6的时候以更小的包来安装，比如：vim、ifconfig、route、setup、netstat等等都不会安装。因此，使用CentOS7通过最小化安装后，还需挂在ISO完成上述常用工具的安装。如果采用脚本自动化安装，可以将上述的工具的安装命令写入到安装到脚本中。 4.字符集修改 1234567891011/etc/locale.conf #字符集配置文件 localectl set-locale LANG=zh_CN.UTF-8 # 命令行一步到位 [root@CentOS7 ~]# localectl set-locale LANG=zh_CN.UTF-8 [root@CentOS7 ~]# localectl status System Locale: LANG=zh_CN.UTF-8 VC Keymap: us X11 Layout: us 5.开机启动管理/etc/rc.local 这个文件还是存在，不过如果我们还想继续使用这种方式需要给它加执行权限chmod +x /etc/rc.d/rc.local system一统天下 snapshot(支持快照) 12345678systemctl status cron.service #查看定时任务状态systemctl stop cron.service#关闭定时任务systemctl status cron.service#查看操作情况systemctl list-unit-files|grep enable #查看当前正在运行的服务systemctl disable postfix.service #关闭邮件服务systemctl list-unit-files|grep postfix #查看邮件服务是否开启systemctl stop firewalld.service #关闭防火墙systemctl is-enable #开启的服务 systemctl disable#关闭的服务 6.运行级别runlevel/etc/inittab 是无效的，由system target 替代。12345678910#永久生效下次登录生效 systemctl get-default graphical.target # 切换到5 systemctl get-default multi-user.target # 切换到3 ##临时生效的话 init3 七种种运行级别如下1[root@centos7 ~]# ls -lh /usr/lib/systemd/system/runlevel*.target 7.网卡名称CentOS7的网卡名称太长，这不符合我们的使用习惯，增加了管理难度，最简单粗暴的方法是在安装系统的时候就把网卡名改了。当然，安装好的系统也是可以修改的。下面分别介绍两种方法。 方法一：(推荐）在进入安装界面的时候把光标移动到Install CentOS7,按下tab键，在后面输入“net ifnames=0 biosdevname=0”回车即可。 方法二：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#进入网卡配置文件目录[root@centos7 ~]# cd /etc/sysconfig/network-scripts/#重命名网卡[root@centos7 network-scripts]# mv ifcfg-eno16777736 ifcfg-eth0#修改配置文件NAME、DEVICE[root@centos7 network-scripts]# vim ifcfg-eth0TYPE=EthernetBOOTPROTO=staticTYPE=EthernetBOOTPROTO=staticDEFROUTE=yesTYPE=EthernetBOOTPROTO=staticDEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noNAME=eth0UUID=552c01f7-fd9d-4f19-913e-379a2bf5a467DEVICE=eth0ONBOOT=yesIPADDR=10.0.0.111"ifcfg-eth0" 14L, 239C written#修改grubsed -i.bak 's#crashkernel=auto rhgb quiet#crashkernel=auto rhgb net.ifnames=0 biosdevname=0 quiet#g' /etc/sysconfig/grub[root@centos7 network-scripts]# vim /etc/sysconfig/grub GRUB_TIMEOUT=5GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT="console"GRUB_CMDLINE_LINUX="crashkernel=auto rhgb net.ifnames=0 biosdevname=0 quiet""/etc/sysconfig/grub" 7L, 263C written#生成启动菜单[root@centos7 network-scripts]# grub2-mkconfig -o /boot/grub2/grub.cfg Generating grub configuration file ...Found linux image: /boot/vmlinuz-3.10.0-327.el7.x86_64Found initrd image: /boot/initramfs-3.10.0-327.el7.x86_64.imgFound linux image: /boot/vmlinuz-0-rescue-7ed5d4eebe4c43e3aadbda68cd0ef311Found initrd image: /boot/initramfs-0-rescue-7ed5d4eebe4c43e3aadbda68cd0ef311.imgdone#重启系统生效[root@centos7 network-scripts]# reboot 8.桌面系统1234567[CentOS6] GNOME 2.x[CentOS7] GNOME 3.x（GNOME Shell） 9.文件系统1234567[CentOS6] ext4 [CentOS7] xfs 10.内核版本1234567[CentOS6] 2.6.x-x [CentOS7] 3.10.x-x 11.启动加载器1234567[CentOS6] GRUB Legacy (+efibootmgr)[CentOS7] GRUB2 12.防火墙1234567[CentOS6] iptables[CentOS7] firewalld 13.默认数据库1234567[CentOS6] MySQL [CentOS7] MariaDB 14.文件结构1234567[CentOS6] /bin, /sbin, /lib, and /lib64在/下 [CentOS7] /bin, /sbin, /lib, and /lib64移到/usr下 15.主机名123[CentOS7] /etc/hostname 16.时间同步1234567891011[CentOS6] $ ntp $ ntpq -p [CentOS7] $ chrony $ chronyc sources 17.修改时间1234567891011[CentOS6] $ vim /etc/sysconfig/clock ZONE="Asia/Shanghai" UTC=fales $ sudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime [CentOS7] $ timedatectl set-timezone Asia/Shanghai $ timedatectl status 18.修改地区1234567891011[CentOS6] $ vim /etc/sysconfig/i18n LANG="zh_CN.utf8" $ /etc/sysconfig/i18n $ locale [CentOS7] $ localectl set-locale LANG=zh_CN.utf8 $ localectl status 19.服务相关1）启动停止123456789101112131415[CentOS6] $ service service_name start $ service service_name stop $ service sshd restart/status/reload [CentOS7] $ systemctl start service_name $ systemctl stop service_name $ systemctl restart/status/reload sshd 2) 自启动123456789[CentOS6] $ chkconfig service_name on/off [CentOS7] $ systemctl enable service_name $ systemctl disable service_name 3) 服务一览123456789[CentOS6] $ chkconfig --list [CentOS7] $ systemctl list-unit-files $ systemctl --type service 4) 强制停止1234567[CentOS6] $ kill -9 &lt;PID&gt; [CentOS7] $ systemctl kill --signal=9 sshd 20.网络1）网络信息123456789101112131415[CentOS6] $ netstat $ netstat -I $ netstat -n [CentOS7] $ ip n $ ip -s l $ ss 2）IP地址MAC地址1234567[CentOS6] $ ifconfig -a [CentOS7] $ ip address show 3）路由1234567891011[CentOS6] $ route -n $ route -A inet6 -n [CentOS7] $ ip route show $ ip -6 route show 21.重启关闭1）关闭123456789[CentOS6] $ shutdown -h now [CentOS7] $ poweroff $ systemctl poweroff 2）重启1234567891011[CentOS6] $ reboot $ shutdown -r now [CentOS7] $ reboot $ systemctl reboot 3）单用户模式1234567[CentOS6] $ init S [CentOS7] $ systemctl rescue 4）启动模式123456789101112131415161718192021[CentOS6] $ vim /etc/inittab id:3:initdefault: $ startx [CentOS7] $ systemctl isolate multi-user.target $systemctl isolate graphical.target #默认 $ systemctl set-default graphical.target $ systemctl set-default multi-user.target #当前 $ systemctl get-default]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-03-30-Linux的文件系统]]></title>
    <url>%2F2019%2F04%2F15%2F2019-03-30-Linux%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[Linux目录结构的特点Windows系统目录和磁盘是强对应的关系，联系很紧密，例如c盘下的目录和文件不可能和D盘的目录有关联或交集。而Linux系统目录与之不同，Linux系统的目录和磁盘等设备是不直接关联的，每个目录都可以关联（官称：挂载）在不同的设备（例如磁盘）上，例如：看似有包含关系的几个目录/、/etc、/boot、/var很可能是在不同的分区或磁盘上。 在Linux中，一切皆是文件。Linux是以文件的方式来管理系统中各个组件，所有的文件存储都以根（”/“）开始，整个文件系统结构就像一颗倒挂的”树“。Linux文件系统的目录结构如图所示： 在Linux中，一切从“根”开始，“/”是所有目录的起点（顶点），Linux根下面的目录是一个有层次的树状结构。我们熟悉的Windows系统，目录和磁盘是强对应的关系，联系很紧密，例如c盘下的目录和文件不可能和D盘的目录有关联或交集。而Linux系统目录与之不同，Linux系统的目录和磁盘等设备是不直接关联的，每个目录都可以关联（官称：挂载）在不同的设备（例如磁盘）上，例如：看似有包含关系的几个目录/、/etc、/boot、/var很可能是在不同的分区或磁盘上。 在逻辑上，所有的目录（包括目录下的子目录）都在最高级别的目录“/”下，根（“/”）目录是所有目录的起始点（顶点），而实际上访问目录/、/etc、/boot、/var时，可能是在访问完全不同的分区和磁盘。 Linux下面的设备（磁盘），如果不挂载是看不到入口的，就像没窗没门的房间，是不能被正常使用的。如果要访问设备，就必须为设备开一个入口，这个入口就是挂载点。挂载点实质就是一个目录，开入口的过程就是将挂载点和磁盘设备关联，即挂载。 Linux的根目录特点 1、根目录“/“是所有目录的顶点。 2、目录结构像一颗倒挂的“树”。 3、目录和分区没有关联，因此不同目录可以映射不同的分区。 4、分区或设备要想访问，必须将其与目录挂载，此时挂载的目录相当于分区的入口。 5、挂载目录的指令为mount。 6、系统开机自动挂载的配置文件为/etc/fstab。 Linux目录的层次标准FHSFHS全称为Filesystem Hierarchy Standard，中文意思是目录层次标准，是Linux的目录规范标准。详细见http://www.pathname.com/fhs/。FHS定义了两层规范： 第一层是“/”目录下的各个目录应该放什么文件数据。 第二层是针对/usr（Unix software resource）和/var(Variable data)这两个目录的子目录在定义的。 参考资料： http://www.pathname.com/fhs/ http://www.ibm.com/developerworks/linux/library/l-proc/index.html 所有目录的命名和结构是有规范，在Linux的目录结构遵照FHS规范。常用目录和子目录的作用如下： /bin ：存放普通用户或管理员使用所有二进制命令文件。 /sbin：存放管理员使用的所有二进制命令文件。 /boot : 存放Linux系统启动引导的安装文件存放目录。 /dev ： 存放特殊存储文件或设备文件存放的目录，比如：cpu、内存、硬盘、光驱、鼠标、键盘等。 /etc ： 存放应用程序配置文件，比如：yum或rpm安装软件的配置文件存放路径，很多服务的启动程序存放的路径。 /home ：非关键性目录，是可选目录，是普通用户的家目录。一般每个用户的家目录，默认为此目录下与用户名同名的目录。 /lib：存放基本的共享库文件或Linux内核模块文件,为系统启动或根文件系统上的应用程序（/bin,/sbin等）提供共享库，以及为内核提供内核模块。 ​ libc.so.：*动态链接的C库，在64位系统下，有可能位于/lib64目录下。 ​ ld：*运行时链接器/加载器，在64位系统下，有可能位于/lib64目录下。 ​ modules：用于存储内核模块的目录，在64系统下，也位于/lib目录下。 /lib64：存放64位操作系统基本的共享库文件或Linux内核模块文件。 /media：便携式设备挂载点。比如：U盘，光盘，移动硬盘等。 /mnt：其他文件系统的临时挂载点。 /opt：附件应用程序的安装位置，比如第三方应用程序devstack。 /root ：管理员root用户的家目录，也是可选目录，并非必选。因为，管理员root在生产环境下一班不允许登录。 /srv：存放此系统专门提供给运行此系统应用程序上的数据。 /tmp：为那些会产生临时文件的程序用于存储临时文件的目录，可供所有用户执行写入操作，有有特殊权限。 /usr：是一个层级结构目录，存放全局共享只读数据。 ​ /usr/bin：普通用户命令。 ​ /usr/sbin：管理员命令，并非系统必须，用于扩展系统功能的命令。 ​ /usr/lib64：存放系统的扩展共享库文件。 ​ /usr/include：存放c程序的头文件位置。 ​ /usr/share：命令手册页和自带文档等架构特有的文件存放位置。 ​ /usr/local：又一个层级目录，让系统管理员安装本地应用程序或者第三方的应用程序。 ​ /usr/X11R6：存放X window的程序文件。 ​ /usr/src：程序源码文件的存位置。 /var：层级目录，存储常发生变化的数据的目录。 ​ /var/cache：应用程序缓存数据。 ​ /var/lib：应用程序的状态信息。 ​ /var/local：为/usr/local提供经常变化的数据。 ​ /var/lock：锁文件存放位置。 ​ /var/log：应用程序的日志文件。 ​ /var/opt：为/opt目录下应用程序提供变化的数据。 ​ /var/run：为运行中的进程提供相关数据。 ​ /var/spool：为应用程序提供的管道数据。 ​ /var/tmp：系统重启后依然需要留存的数据。 /proc：存放内核和进程的基于内存的虚拟文件系统。说白了，就是为内核和进程存储运行的相关数据，多为内核参数。例如：net.ipv4.ip_forward，虚拟化为net/ipv4/ip_forward，存储于/proc/sys/下，因此其完整路径为/proc/sys/net/ipv4/ip_foward /sys：挂载sysfs虚拟文件系统的挂载点，比proc更为理想的访问内核数据的途径，也是一个基于内存的虚拟文件系统。为管理LInux设备提供一种统一文件系统。在Linux内核2.6版本开始才出现此目录，以前并没有。 Linux系统的文件类型： -：表示普通文件，在其他命令中用f表示普通文件 d：表示目录文件，完成路径映射，与windows功能不同。 b：表示块设备文件，完成块设备映射的文件，支持以block为单位随机访问设备 c：表示字符设备文件，完成字符设备映射的文件，支持以字符为单位线性访问设备 ​ major number：主设备号，用于表示设备类型，进而确定要加载的驱动程序 ​ minor number：次设备号，用于表示同一种类型设备下不同的设备，进而确定要驱动的嗯对象 l：表示符号链接文件，类似windows上快捷方式，也成为软链接文件。 p：表示管道文件 s：表示套接字文件，用于两个进程之间进行通信时套接的数据。 Linux系统重要的配置文件/etc/sysconfig/network-scripts/ifcfg-eth : 网卡配置文件 /etc/resolv.conf : DNS配置文件 /etc/hosts : 主机名与IP的映射关系配置文件 /etc/sysconfig/network : 配置主机名的目录 /etc/fstab : 实现开机要挂载的文件系统的一个文件 /etc/rc.local : 实现开机启动的配置或软件 /etc/inittab : 实现开机后系统运行的级别，加载相关的启动文件 /etc/issue : 用户登录时的系统提示 /etc/motd : 用户登录后的系统提示 /etc/redhat-release：系统发行版本信息 /usr/local : 通过源码编译的文件 /var/log/message：系统日志文件]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-03-18-Linux系统的内存管理]]></title>
    <url>%2F2019%2F04%2F15%2F2019-03-18-Linux%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[内存管理中的基本概念内存管理一向是所有操作系统书籍不惜笔墨重点讨论的内容，无论市面上或是网上都充斥着大量涉及内存管理的教材和资料。因此，我们这里所要写的Linux内存管理采取避重就轻的策略，从理论层面就不去班门弄斧，贻笑大方了。 遵循“理论来源于实践”的“教条”，我们先不必一下子就钻入内核里去看系统内存到底是如何管理，那样往往会让你陷入似懂非懂的窘境（我当年就犯了这个错误！）。所以最好的方式是先从外部（用户编程范畴）来观察进程如何使用内存，等到大家对内存的使用有了较直观的认识后，再深入到内核中去学习内存如何被管理等理论知识。 每个程序在操作系统中都对应一个进程（例如：QQ，微信等），所有进程都必须占用一定数量的内存，它或是用来存放从磁盘载入的程序代码，或是存放取自用户输入的数据等等。不过进程对这些内存的管理方式因内存用途不一而不尽相同，有些内存是事先静态分配和统一回收的，而有些却是按需要动态分配和回收的。 对任何一个普通进程来讲，它都会涉及到5种不同的数据段。稍有编程知识的朋友都能想到这几个数据段中包含有“程序代码段”、“程序数据段”、“程序堆栈段”等。不错，这几种数据段都在其中，但除了以上几种数据段之外，进程还另外包含两种数据段。下面我们来简单归纳一下进程对应的内存空间中所包含的5种不同的数据区。 代码段：代码段是用来存放可执行文件的操作指令，也就是说它是可执行程序在内存中的镜像。代码段需要防止在运行时被非法修改，所以只准许读取操作，而不允许写入（修改）操作——它是不可写的。 数据段：数据段用来存放可执行文件中已初始化全局变量，换句话说就是存放程序静态分配的变量和全局变量。 BSS段：BSS段包含了程序中未初始化的全局变量，在内存中 bss段全部置零。 堆（heap）：堆是用于存放进程运行中被动态分配的内存段，每一个程序当开始执行，就会在内存中划出一片空间作为程序运行时代码和数据存放地方。它的大小并不固定，可动态扩张或缩减。当进程调用malloc等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用free等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）。 栈：栈是用户存放程序临时创建的局部变量，也就是说我们函数括弧“{}”中定义的变量（但不包括static声明的变量，static意味着在数据段中存放变量）。除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中，并且待到调用结束后，函数的返回值也会被存放回栈中。由于栈的先进先出特点，所以栈特别方便用来保存/恢复调用现场。从这个意义上讲，我们可以把堆栈看成一个寄存、交换临时数据的内存区。 机器语言指令中出现的内存地址，都是逻辑地址，需要转换成线性地址，再经过MMU(CPU中的内存管理单元)转换成物理地址才能够被访问到。我们写个最简单的hello world程序，用gccs编译，再反编译后会看到以下指令： 1mov 0x8747ab0, %eax 这里的内存地址0x8747ab0 就是一个逻辑地址，必须加上隐含的DS 数据段的基地址，才能构成线性地址。也就是说 0x8747ab0 是当前任务的DS数据段内的偏移量offset。 在x86保护模式下，段的信息（段基线性地址、长度、权限等）即段描述符占8个字节，段信息无法直接存放在段寄存器中（段寄存器只有2字节）。Intel的设计是段描述符集中存放在GDT或LDT中，而段寄存器存放的是段描述符在GDT或LDT内的索引值(index)。 Linux系统逻辑地址=线性地址为什么这么说呢？因为Linux所有的段（用户代码段、用户数据段、内核代码段、内核数据段）的线性地址都是从 0x00000000 开始，长度4G，这样，线性地址=逻辑地址+ 0x00000000，也就是说逻辑地址等于线性地址了。 这种情况下Linux只用到了GDT，不论是用户任务还是内核任务，都没有用到LDT。GDT的第12和13项段描述符是 KERNEL_CS 和KERNEL_DS，第14和15项段描述符是 USER_CS 和USER_DS。内核任务使用KERNEL_CS 和KERNEL_DS，所有的用户任务共用USER_CS 和USER_DS，也就是说不需要给每个任务再单独分配段描述符。内核段描述符和用户段描述符虽然起始线性地址和长度都一样，但DPL(描述符特权级)是不一样的。KERNEL_CS 和KERNEL_DS 的DPL值为0（最高特权），USER_CS 和USER_DS的DPL值为3。 用gdb调试程序的时候，用info reg 显示当前寄存器的值： 1234cs 0x73 115ss 0x7b 123ds 0x7b 123es 0x7b 123 可以看到ds值为0x7b, 转换成二进制为 00000000 01111011，TI字段值为0,表示使用GDT，GDT索引值为 01111，即十进制15，对应的就是GDT内的__USER_DATA 用户数据段描述符。 从上面可以看到，Linux在x86的分段机制上运行，却通过一个巧妙的方式绕开了分段。 Linux主要以分页的方式实现内存管理。如下图所示： 前面说了Linux中逻辑地址等于线性地址，那么线性地址怎么对应到物理地址呢？这个大家都知道，那就是通过分页机制，具体的说，就是通过页表查找来对应物理地址。 准确的说分页是CPU提供的一种机制，Linux只是根据这种机制的规则，利用它实现了内存管理。 在保护模式下，控制寄存器CR0的最高位PG位控制着分页管理机制是否生效，如果PG=1，分页机制生效，需通过页表查找才能把线性地址转换物理地址。如果PG=0，则分页机制无效，线性地址就直接做为物理地址。 分页的基本原理是把内存划分成大小固定的若干单元，每个单元称为一页（page），每页包含4k字节的地址空间（为简化分析，我们不考虑扩展分页的情况）。这样每一页的起始地址都是4k字节对齐的。为了能转换成物理地址，我们需要给CPU提供当前任务的线性地址转物理地址的查找表，即页表(page table)。注意，为了实现每个任务的平坦的虚拟内存，每个任务都有自己的页目录表和页表。 为了节约页表占用的内存空间，x86将线性地址通过页目录表和页表两级查找转换成物理地址。32位的线性地址被分成3个部分：最高10位 Directory 页目录表偏移量，中间10位 Table是页表偏移量，最低12位Offset是物理页内的字节偏移量。 页目录表的大小为4k（刚好是一个页的大小），包含1024项，每个项4字节（32位），项目里存储的内容就是页表的物理地址。如果页目录表中的页表尚未分配，则物理地址填0。页表的大小也是4k，同样包含1024项，每个项4字节，内容为最终物理页的物理内存起始地址。 每个活动的任务，必须要先分配给它一个页目录表，并把页目录表的物理地址存入cr3寄存器。页表可以提前分配好，也可以在用到的时候再分配。 还是以mov 0x8747ab0, %eax中的地址为例分析一下线性地址转物理地址的过程。 前面说到Linux中逻辑地址等于线性地址，那么我们要转换的线性地址就是 0x8747ab0。转换的过程是由CPU自动完成的，Linux所要做的就是准备好转换所需的页目录表和页表（假设已经准备好，给页目录表和页表分配物理内存的过程很复杂，这里不做展开讨论，喜欢内核优化的可自行研究）。 线性地址 0x8747ab0 转换成二进制后是 1000 0111 0100 0111 1010 1011 0000 0000，最高10位1000 0111 01的十进制是541，CPU查看页目录表第541项，里面存放的是页表的物理地址。线性地址中间10位00 0111 1010 的十进制是122，页表的第122项存储的是最终物理页的物理起始地址。物理页基地址加上线性地址中最低12位的偏移量，CPU就找到了线性地址最终对应的物理内存单元。 我们知道Linux中用户进程线性地址能寻址的范围是0 － 3G，那么是不是需要提前先把这3G虚拟内存的页表都建立好呢？一般情况下，32位机器的物理内存是小于3G的，加上同时有很多进程都在运行，根本无法给每个进程提前建立3G的线性地址页表。Linux利用CPU的一个缺页机制解决了这个问题。进程创建后我们可以给页目录表的表项值都填0，CPU在查找页表时，如果表项的内容为0,则会引发一个缺页异常，进程暂停执行，Linux内核这时候可以通过一系列复杂的算法给分配一个物理页，并把物理页的地址填入表项中，进程再恢复执行。当然进程在这个过程中是被蒙蔽的，它自己的感觉还是正常访问到了物理内存。]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-03-02-Linux系统基础介绍]]></title>
    <url>%2F2019%2F04%2F15%2F2019-03-02-Linux%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文作为Linux操作系统开篇文章，主要介绍一些操作系统的基础概念和原理，然后简要介绍Linux的发展历史，以及市面上常见的Linux系统版本。 操作系统基本概念和原理操作系统可以说是目前所有现代人都了解的一个名词，大家平时的日常工作和生活都离不开操作系统。目前，世界上主要流行的操作系统有三类：Linux、Windows和Mac OS。其中，后两个操作系统是大家熟悉的，大家的日常工作和生活大部分都会跟这两种操作系统打交道，对于Linux操作系统可能普通只是处在听说过的阶段，很少有人去详细了解和使用。 那么如果有人问你什么是操作系统？虽然大家平时都在用，但是估计很多人都会一脸懵逼。其实，操作系统就是处在计算机硬件和人之间的一个重要的中间部件，它存在意义有2个：一是通过在其上部署应用软件，满足人们操作计算机硬件的需求。二是将上层应用软件与底层硬件进行解耦，满足人们随时随地、随心所欲的操作计算机硬件的要求。 作系统的官方定义是：英文名Operating System，简称OS。是计算机系统中必不可少的基础系统软件，它是应用程序运行以及用户操作必备的基础环境支撑，是计算机系统的核心。**从官方定义不难看出，操作系统（后统称为OS）首先是一个软件，是一个支撑软件，支撑应用软件和人们的操作。所以，它是一种特殊软件，主要由内核+库两部分实现（如下图所示）。 内核存在的目的就是将底层的硬件进行软件化封装，方便上层调用来操作硬件。而库的作用就是将内核的软件封装再构造成一个个标准函数，供上层应用去调用从而避免应用直接操作内核的风险。其次，它还是计算机系统的核心。我们现在讲的计算机系统都是冯诺依曼架构（如下图所示），由控制器、运算器、存储设备和输入输出设备四部分构成，OS就是这些部件协调运作赖以支撑的基础，所以它也是计算机系统的核心。 综上，用一句话概括操作系统的概念就是：操作系统就是位于用户与计算机系统硬件之间用于传递信息系统程序软件。 什么是Linux Linux系统组成如上图 内核：是系统的心脏，是运行程序和管理像磁盘和打印机等硬件设备的核心程序。 Shell：是系统的用户界面，提供了用户和内核进行交互操作的一种接口。它接收用户输入的命令并把它送入内核去执行，是一个命令解释器。但它不仅是命令解释器，而且还是高级编程语言，shell编程。 文件系统：文件系统是文件存放在磁盘等存储设备上的组织方法，Linux支持多种文件系统，如ext3,ext2,NFS,SMB,iso9660等。 应用程序：标准的Linux操作系统都会有一套应用程序例如X-Window,Open Office等。 和windows一样，Linux也是一种操作系统。与windows的商业不同的是，Linux是一套完全开源的操作系统，其实现代码全部呈现给使用者。就像你亲自下厨做饭一样，所有的原材料和工序你都清楚，因此做出饭菜自然比外卖要安全可靠。所以，国内一些企业自研操作系统均是由Linux源码封装改造而来，其宣称”自主可控“，也无可厚非。 Linux在设计之初，是基于Intel x86 PC架构的，是一套多任务、多用户并支持多线程和多CPU的操作系统。其设计的本意就是打破商业软件版权的限制（因为当初unix系统被AT&amp;T回收版权，禁止向学生群体开放源码，从而引起版权纠纷），全世界都能使用的类unix系统兼容产品。从其1991年诞生到现在约30年的时间，Linux操作系统主要用于服务器领域、嵌入式开发，其在个人PC桌面领域应用较少，这也是其不被大众熟悉的原因。（目前，在个人PC桌面领域做的较好由国外的ubuntu、fedora，国内的深度deepin，中兴的新起点等操作系统）。其实，现在windows在个人桌面操作系统的优势除了软件生态以外，也不再剩下什么。如果Linux的软件生态得到广大发展，那么可以预期个人桌面系统将发生颠覆性的变化。这也是微软为什么在即将发布的windows 10的1903版本推出同时兼容读写Linux系统文件的原因，微软现在已经很着急啦：）。 全球超算99.9%都是使用Linux，全球和国内排名前1000的互联网公司90%的服务器也使用的是Linux，就是因为Linux系统优越性使其在服务器领域一举奠定了霸主地位，无论是windows还是mac os都无法与其竞争。Linux的优越性主要都继承自unix系统，主要由以下几点： 具备开放源代码的程序软件，可自由修改。 Unix系统兼容，具备几乎所有Unix系统的优点。 可自由传播，无任何商业化版权制约。 适合Intel x86CPU系列架构的计算机。 严格来说，Linux这个词本身只表示Linux内核，但是人们已经习惯用Linux来形容整个基于Linux内核的操作系统，并且是一种使用GNU通用公共许可证（GUN general public，GPL）工程的，包括各种工具和数据库的操作系统。 Linux内核除系统调用外，由五个主要的子系统组成：进程调度、内存管理、虚拟文件系统、网络和进程间通信(IPC)。 各个子系统的主要功能为： 1、进程调度：它控制着进程对CPU的访问，当需要选择一个进程开始运行时，由调度程序选择最应该运行的进程； 2、内存管理：它允许多个进程安全地共享主内存区域，支持虚拟内存；从逻辑上可以分为硬件无关的部分和硬件相关的部分； 3、虚拟文件系统(VFS)：它隐藏了各种不同硬件的具体细节，为所有设备提供统一的接口，支持多达数十种不同的文件系统，分为逻辑文件系统和设备驱动程序； 4、网络：它提供了对各种网络标准协议的存取和各种网络硬件的支持，分为网络协议和网络驱动程序两部分； 5、进程间通信：支持进程间各种通信机制，包括共享内存、消息队列和管道等。 GNU的全称为GNU’s not Unix，意思是“GNU不是Unix”，GNU计划，又称革奴计划，是由Richard Stallman在1984年公开发起的，是FSF（自由软件基金会）的主要项目，这个项目成立的本意就是建立一套完全自由的可移植的类unix操作系统。在Linux内核发布的时候，GUN项目已经完成了除系统内核之外的各种必备软件的开发。在Linus Torvalds和其他开发人员的努力下，GNU项目的部分组件又运行到了Linux内核之上，例如：GNU项目里的Emacs、gcc、bash、gawk等，至今都是Linux系统中很重要的基础软件。因此，如今我们说的Linux操作系统实际上是GNU/Linux操作系统。 而GPL是一个最著名的开源许可协议，其核心是保证任何人有共享和修改自由软件的自由，任何人有权取得、修改和重新发布自由软件的源代码权利，但都必须同时给出具体更改的源代码。正是因为该协议的存在，才使得开源软件有如今如火如荼的发展局面。 Linux系统启动顺序和基本概念开机自检（BIOS)—&gt;MBR引导（512字节，其中前446字节是Grub菜单，后64字节是分区表）—&gt;GRUB菜单（选择启动系统）—-&gt;加载内核Kernel—&gt;运行INIT进程。其中，INIT进程在Linux系统的用PID编号为1来表示，意思为所有进程的“大佬”。 BIOS：基本输入输出系统（basic input output system，BIOS），是一组固话到计算机主板ROM上的程序，保存着计算计算最重要的基本输入输出程序、系统设置信息、开机自检程序和系统自启动程序，为计算机提供最底层的、最直接的硬件设置和控制。 MBR：一种硬盘分区格式。目前，硬盘分区格式主要有两种，分别是MBR和GTP。MBR，即主引导记录扇区（master boot record），位于整块硬盘的0磁道0煮面1扇区，用于操作系统对硬盘读写时，判断分区的合法性以及分区引导信息的定位。总共512字节，前446字节用于主引导记录，后64字节用于存储硬盘分区表（DPT），每个分区表大小16字节，共4个分区表，所以采用MBR分区格式的硬盘最多只能分出4个分区（主分区+扩展分区），最后两个字节”55，AA“是分区的结束标志。MBR分区表的格式如下： GPT：全局唯一分区格式，正在逐渐取代MBR成为新标准。它和统一的可扩展固件接口（unified extensible firnware interface，UEFI）相辅相成。UEFI用于取代BIOS，而GPT用于取代MBR。在GPT硬盘中，分区表中的位置信息存储在GPT头中，第一个扇区同样有一个与MBR类似的标记，叫做受保护的主引导记录（protected main boot record，PMBR）。其作用是当使用不支持GPT的分区工具时，整个硬盘将显示为一个受保护的分区，防止数据被破坏，其中存储的内容与MBR一样，之后才是GPT头部信息。与MBR相比，支持2TB以上的磁盘，如果使用fdisk分区，最大只能建立2TB大小的分区，创建大于2TB的分区时，需使用parted工具，同时必须使用64位操作系统。以下是GPT分区表的数据格式： GRUB：多操作系统启动程序（GRand unified bootloader）。支持多操作系统引导，当系统中装载多操作系统时，在系统启动时便于用户选择。GRUB还可用于选择操作系统分区上的不同内核，也可用于向这些内核传递启动参数。Linux常见的引导程序包括：LILO、GRUB、GRUB2。CentOS 6.x系统和Ubuntu系统默认采用GRUB引导程序，所以当我们有需要编译GRUB菜单时，执行如下命令： 1grub-mkconfig -o /boot/grub/grub.cfg CentOS 7.x系统默认采用GRUB2引导程序，所以当我们有需要编译GRUB菜单时，执行如下命令： 1grub2-mkconfig -o /boot/grub2/grub.cfg GRUB加载引导程序的流程如下： GRUB2是基于GRUB开发的更加安全强大的多系统引导程序，同时采用模块化设计，使得GRUB2核心更加尽量，使用更加灵活，也不需要像GRUB那样分为stage1、stage1.5和stage2三个阶段。 init：就是系统的不同运行级别对应加载的启动文件。在Linux跟目录下的/etc/目录下，有与系统运行级别对应的rc开头的目录，里面存在对应系统运行级别的脚本文件。Linux内核加载完成后，通过加载这些启动文件完成系统的初始化。]]></content>
      <categories>
        <category>Linux基础</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-10-10-云计算的技术架构]]></title>
    <url>%2F2018%2F10%2F10%2F2018-10-10-%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[云计算总体技术架构下图是一张从云计算上下文参考架构引申出的云计算总体技术架构图。其实，当我们面临一个复杂系统的认知学习，首先需要从全局对其做鸟瞰式概览，将其关键部分抽象为几个关键模块。然后，分析每个关键模块之间的关联，也就是上下文对接关系。最后，再由上下文参考架构拓展出具体关键流程和服务模块。做到上述“收放自如”，才算真正入门，才具备继续深钻的基础条件。：） 上图左边是云计算的参考架构，主要由4个关键子模块和1个关键服务组成。4个关键子模块包括：CloudDevice（云设备）、User（用户&amp;开发者）、ServiceCenter（运营服务中心）和OperatorCenter（运维中心）。而将4个关键子模块进行衔接的关键服务就是我们常说的IaaS、PaaS和SaaS三层平台的逻辑抽象，由IaaS层的CloudOS统一完成纳管和呈现。 对上下文架构参考图进一步拓展，就是右边的云计算解决方案的整体技术架构图。这张整体技术架构图向上可以支撑公有云、私有云、电信云和混合云的各种方案部署。虽然，其涉及的技术方案很多，但其本质上还是底层四个关键技术领域。即，计算、网络、存储和安全。说白了，云计算要想彻底精通，必须同时精通计算、网络、存储和安全四个领域。 云计算的核心技术识别 虚拟化及资源调度平台 虚拟化软件：高性能、高可靠性、智能调度算法 数据中心的一体化自动管控 分布式计算/存储框架 虚拟化的硬件加速 计算与存储平台 定制化的服务器与存储：讲话涉及大内存，高网络/存储IOPS 数据中心安全性：可信赖、完整性、可用性 网络平台 高密度、低成本的10GE互联 网络的集群与虚拟化 基础设施平台 E2E的集成交付能力 绿色节能的工程设计 从物理设备（服务器、存储和网络设备）、虚拟化软件平台、分布式计算和存储资源调度、一体化自动化管控软件、虚拟化数据中心的安全和E2E的集成交付能力，都是构建高效绿色云数据中心的关键技术。 简化设计的大内存、高网络和存储IOPS的服务器，可以为云数据中心提供强大的计算能力。 高IOPS，支持链接克隆、精简置备、快照等功能的存储设备，可以为数据中心提供强大的存储能力。 高密度、低成本，支持大二层网络技术的交换设备为数据在二层网络流动提供交换能力。 虚拟化软件平台，可以抽象物理资源为资源池，给云用户配置不同规格虚拟机提供底层支撑。 灵活、高效的分布式计算或存储框架，为云计算的资源调度和调整提供支撑。 从门禁监控、网络接入、虚拟化平台软件安全、经过安全加固的OS和DB到用户的分权分域管理，保证数据中心的放心使用。 一体化自动化的管控软件，提升维护人员的效率，降低企业成本。 云计算的关键技术云计算的单点技术都是“老”技术，组合起来却有无与伦比的的价值。马云有句话说的好，从技术层面来讲，云计算的就是新瓶装旧酒。 计算架构：支持scale out模式，整体性能最优，基于软件可靠性和可扩展性。 云计算硬件 服务器：高可靠性、高性能 网络：高密度以太网交换机 存储：低成本、多备份 云计算软件 并行计算技术 分布式存储 分布式文件管理 虚拟化技术 智能化云计算系统管理技术 通过对多项核心技术进行归类汇总，可归结为三个方面：整体的计算架构、承载的硬件设备和软件系统。 整体的计算架构：需要涵盖高性能、高可靠和可扩展。 云计算硬件包括：高可靠和高性能的计算服务器提供计算资源；低成本、数据安全的存储设备提供数据存储空间；支持大二层网络的高密度交换机进行数据的通信和交流。 云计算软件包括：用于大数据的并行分析计算技术；整合存储资源提供动态可伸缩资源池的分布式存储技术；用于数据管理的分布式文件管理；计算、存储等资源池化的虚拟化技术；简化运维人员工作，方便高效智能运维的系统管理技术。 云计算的硬件技术：计算架构 早起许多IT系统开始很简单，但当需要进行系统扩展时就会变得复杂。升级系统最常见的原因是需要更多的容量，以支持更多的用户、文件、应用程序或连接的服务器。常见的系统扩展方式有Scale up和Scale out两种。 Scale up 纵向扩展架构主要是利用现有的系统，通过不断增加存储容量来满足数据增长的需求。但是这种方式只增加了容量，而带宽和计算能力并没有相应的增加。所以，整个系统很快就会达到性能瓶颈，需要继续扩展。 Scale out 横向扩展架构的升级通常是以节点为单位，每个节点往往将包含容量、处理能力和I/O带宽。一个节点被添加到系统，系统中的三种资源将同时升级。而且，Scale out架构的系统在扩展之后，从用户的视角看起来仍然是一个单一的系统。所以Scale out方式使得系统升级工作大大简化，用户能够真正实现按需购买，降低TCO。 云计算的设计思想是以最低成本构建出整体的性能最优，与传统电信设备和IT设备（服务器、大型机、企业存储等）追求设备可靠性和性能的思路完全不同。 云计算的硬件技术：存储系统 企业存储一般采用专用的存储设备，成本高。 分布式存储系统把使用便宜IDE/SATA硬盘的服务器本地存储构建存储资源池，既降低了服务器的成本，也降低了存储成本，构建最低成本的计算和存储。 通过“分布式存储和多副本备份”来解决海量信息的存储和系统可靠性，数据存储可以配置多份副本，保证数据的安全性。 云计算的硬件技术：数据中心的联网 东西向流量增长 并行计算业务(如：搜索)需要服务器集群协同运算，产生大量横向交互流量 虚拟机的自由部署和动态迁移，虚机间需要实时同步大量的数据 随着云计算的发展，越来越多业务承载在数据中心的虚拟机上，业务数据的流动从南北向转变为东西向，对数据中心网络的需求和冲击提出了很大挑战。 数据中心内部虚拟机的迁移促进了大二层网络虚拟交换技术的发展，支持大容量数据的通信和超高的端口密度，可以连接更多的服务器提升数据中心的处理能力。 云计算的软件技术：集群管理 云计算虚拟化平台软件，支持分布式的集群管理。可以针对业务模型，对物理服务器创建不同的业务集群，并在集群内实现资源调度和负载均衡，在业务负载均衡的基础上实现资源的动态调度，弹性调整。 云计算虚拟化平台需要支持各种不同的存储设备，包括本地存储、SAN存储、NAS存储和分布式本地存储，保证业务的广适配性。 同时，提供链接克隆、资源复用、精简置备和快照功能，降低企业成本并提供高效率、高可靠性的资源池。 结束语截止目前，云计算基础入门部分已更新完毕，此部分主要是针对打算入坑的新人，给其一个总体上概括认知。后续本打算重写虚拟化技术。但是，考虑到OpenStack以及Docker容器涉及很多Linux基础知识，故临时调整更新内容为重写Linux部分。主要涉及：Linux系统组成、常用命令总结（这部分是我自己总结，大家可下载留存参考）、三剑客基本使用教程和shell编程基础。 完成Linux部分更新后，再继续重写虚拟化技术入门，涉及计算、存储和网络虚拟化三部分。以上与目前更新云计算基础统一构成基础概念部分。所谓“基础不牢，地动山摇”，因此这部分虽然是入门，其实还是很重要的部分。]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-25-云计算带来哪些变化]]></title>
    <url>%2F2018%2F09%2F25%2F2018-09-25-%E4%BA%91%E8%AE%A1%E7%AE%97%E5%B8%A6%E6%9D%A5%E5%93%AA%E4%BA%9B%E5%8F%98%E5%8C%96%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[传统的IT架构与云计算架构投资决策的差异 传统的IT架构俗称三层架构，即，表示层、业务逻辑层和数据访问层。虽然三层架构将系统在逻辑上分层了三层，但它并不是物理意义上的分层。也就是说对于不同层的代码而言，经历编译、打包、部署之后，所有的代码最终还是运行在同一个进程之中。 对于这种功能集中、代码中心化、一个发布包、部署后运行在同一个进程中应用程序，我们称之为单块架构应用。企业采用这种架构开发某个业务时，对计算、网络、存储、服务器资源的需求都是独占的，不同业务之间是一种烟囱式的条块化分割。随着业务的增加，需求功能的迭代，单块架构只能通过增加自己独占资源组内的资源来实现，即使其他应用资源占用很低，也无法共享其它应用的空闲资源。因此，这种架构模式已经很难满足业务快速变化的需要。 一方面，代码的可维护、扩展性、灵活性在降低； 另一方面，系统的修改成本、构建以及维护成本在增加，因此单块架构的改造与重构势在必行。 随着云计算的出现，在技术上面其实有四点最关键的技术： 第一个技术就是服务器虚拟化，前面讲到了，就是把一台物理的服务器，当成很多逻辑的服务器来用，这种分割的目的就是那提升资源共享利用率，达到业务可以快速的部署，代码重构维护简单的目的。 第二个技术就是分布式的存储，就是把一些原来的专用大存储服务器，比如存储9000那种大的设备大的柜子。把它里面设计成一台一台小的服务器，通过软件方式融合在一起进行管理，当成是一个“存储池”来管理这个资源，这种就是分布式的存储。分布式的存储的好处是可以提高存储的速度，同时以按照客户的需要去扩容。比如：可以像虚拟CPU和虚拟内存那样，实现存储资源的弹性部署。同时，随着Server SAN概念的提出，分布式存储的硬件基于x86通用服务器，远比专用存储服务器的硬件成本低。 第三个技术就是软件定义的网络，也就是现在炒的火热的SDN。我们都知道网络这一块管理起来很复杂，需要提前做好规划，提前分配IP地址。那么，如何通过软件来实现呢？就是通过云计算实现的云资源池，利用里面基础虚拟网络资源，分析这类资源的随时变化，通过编程的方式将这种变化通过协议流去控制实现，通过软件去管理，自动化完成。这就是软件定义网络概念的本质。 第四个技术就是REST，它本质上是一个协议，一种资源状态转移协议。前面说过传统IT的三层架构，主要分为表示层、业务逻辑层和数据访问层。由于在云计算架构里，它的资源是池化的，资源之间是共享的（隔离的本质其实是最大的共享！详见后面OpenStack Neutron部分论述），交互很快，用传统的三层架构软件去实现这种调度根本不现实。因此，就出现了REST协议，它其实是将传统三层架构的功能做了整合，通过它应用可以直面底下的虚拟资源池。这相当于是一个资源调度程序，由应用程序调用，直接去使用这种云的资源。当这种弹性的资源调度模式与上层应用关联起来之后，应用不够或者坏掉了，都没关系，直接再起一个新应用，让应用扩展起来，底层的虚拟机资源也随之增加。这里面有几个特点，第一个就是这种协议是一种API接口协议，所有做开发的人都必须遵循的一种接口协议，为分布式开发和CI/CD(持续集成，持续交付）创造了条件。第二个是云计算采用轻量级的云OS，其主要职责是纳管，并不是具体实现。其上应用恢复与扩展的速度比传统的三层架构更快，为敏捷开发创造了条件。但是，它唯一的遗憾还是未能有一个组织出来进行标准化定义，因此兼容性方面还有缺陷。直到现在容器技术的兴起，才使上述两个场景（CI/CD和敏捷开发）真正落地。 云时代商业模式的变化：从自建变为租用 企业由纯粹的自建IT系统逐步转向混合云的模式。 部分核心应用私有云，一般应用租用公有云服务。 云服务又分为：IaaS、PaaS、SaaS三种方式。 以前，IT企业提供业务需要自己建设机房，买了一堆服务器、交换机部署一个数据中心，然后在上面再部署自己的软件来提供服务。往往这都是一笔不小的花费，从而导致好多创业公司的启动经费过高。 随着IDC机房的出现，云计算技术的普及，这种模式发生了变换。好多公司完全可以通过租借云DC的方式来部署自己的业务软件。所谓的云DC，其实就是通过云计算技术将IDC机房中基础设施通过虚拟化资源的方式来统一提供，租用的费用远远比自建少得多，同时可以节省一笔维护开销，从而导致创业的门槛大大降低。 往往这些企业只需要购买很少一部分服务器和交换机来部署自己的核心业务，存储自己的核心数据，同样也使用云计算来提高资源利用率，降低成本，这部分往往我们称为企业私有云。非核心的企业业务软件和数据，往往通过公有云提供商提供云DC/云主机部署，这部分往往我们称为公有云。两者的结合，就诞生了混合云的概念。因此，混合云是从企业业务的全局部署角度提出的，并不是云计算的新技术分支，现在IT企业提供业务大部分都是通过混合云的方式提供。与混合云类似的还有行业云，两者之间的区别非常小，可以合并为一种。 那么租用云DC到底租用的是什么？这个从需求者不同需求角度来说，分为IaaS（基础设施即服务），PaaS（平台即服务），SaaS（软件即服务）。有些企业，软件开发能力很强，具备平台级的开发能力，但是缺少运行软件平台的服务器、存储和交换机。为了节省成本，他们往往都是向云服务提供租用云主机，将自己的软件部署在云主机上，这种租用的就是IaaS。另一些企业，具备应用层软件的开发能力，但不具备数据之间逻辑处理的能力（即，算法），同时为了节省成本，他们往往都是云服务提供上租用开发平台（如大数据处理），也连带底层基础设施（云主机）一起租用，这种租用的就是IaaS+PaaS。目前还没有哪家企业只租用PaaS，自建IaaS的方式来部署自己的业务。至于SaaS的租用，目前只涉及个人办公（如web版office）或开发领域（如Mob等应用），很少有面向企业的服务应用。 云时代建设模式的变化：从烟囱变为水平 芯片、新介质取得突破，以及CPU、硬盘、网络性能大幅提升为IT架构的水平化演进提供了技术支撑。云计算经历了从虚拟化—云资源池—平台&amp;应用的逐步发展阶段。 云服务提供商为了提供各种云服务，必然要建设相应的数据中心，这种建设是去IOE化的。去IOE化的概念，最早是阿里巴巴提出的，其本意是，在阿里巴巴的IT架构中，去掉IBM的小型机、Oracle数据库、EMC存储设备，代之以自己在开源软件基础上开发的系统。说白了就是结合开源软件技术，将IOE的各层级功能通过通用硬件平台，分布式架构，软硬件解耦和软件定义存储的方式实现，通过Cloud OS进行纳管，并统一对外提供服务。 前面也提到了，云计算的出现不是突然出现的，详见博文 云计算的前世今生。随着软硬件技术的发展，他主要经历了三个发展阶段。第一个阶段就是虚拟化阶段，代表技术就是VMware和Xen，主要通过虚拟化技术来提升企业资源利用率，从而降低成本。第二个阶段是云DC阶段，代表技术就是OpenStack，但是OpenStack最成熟的部分还是其IaaS服务提供，这个阶段主要通过提供云主机和云DC来降低企业运营成本。第三个阶段就是现在火热的容器化应用阶段，其实容器也是一种虚拟化技术，属于轻量级操作系统级别的虚拟化。它是以软件进程的方式来封装各个应用模块，通过共享底层操作系统内核，来提供各种平台服务和应用服务（PaaS+SaaS）。通过这种技术，使得前几年DevOps概念完美落地并真正意义上实现敏捷开发，这是一种企业应用现有管理模式的变革。 云计算给企业带来哪些变化 要说云计算技术给企业带来了哪些变化，最明显的还要数银行。 银行采用基于云计算的新架构以后，它的第一个变换就是IT资源池化了，即使有一些资源损坏了，它还可以继续工作，并不影响它的业务。所以这样提高了就是业务的质量，减少了业务中断，保证这个业务的连续性，全年的业务停机时间就大大减少。 第二个变化就是业务提供的速度，当业务需要很多IT资源时，可以通过资源弹性扩缩容的方式，来满足业务需求。不像以前，一个业务办理的好慢好烦，甚至死机，影响客户的体验，客户就可能去选择其他家更快业务。所以通过把资源管理好，也让IT人员有更多的创新机会，可以开发包括手机端的应用，让更多的人通过手机随时随地去办理业务。不知你注意到没有，国内这么多银行，招商银行在这方面一直走在其他银行前面。 还有更重要的一点对于企业来说，它的业务成本的降低。银行对散户，对个人的业务，实际上都是亏钱的，个人取钱的交易或者是存钱的交易，银行都要投入人力和场地等费用，而现今，通过手机银行和互联网银行，可以把这个散户交易成本降下来，通过云计算等互联网技术，免除了场地网点的费用而且业务处理速度非常的快，用户体验也提升了。]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-12-云计算的前世今生]]></title>
    <url>%2F2018%2F09%2F12%2F2018-09-12-%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[云计算的发展历程 云计算的历史也是蛮好玩的… 一开始就是在2003年之前，大家都在在提升CPU的计算能力，提升存储的空间，使得CPU性能越来越高、存储变得更大。 到了2003年以后呢，企业的数据中心里面发现资源已经很空闲，因为架构设计的原因，让这些服务器很多时间都处在平均不到5%的使用率，大部分99%都是空闲的。相当云投资一千块，其实只用了五块钱，实际上产生了很大的浪费。同时，机房空间、动力等基础设施资源被占用过多，导致数据中心内部无法放置更多机柜、机架和服务器，需要买地建设新的机房（按照国内的地价/房价，这基本是不可能的）。因此，在2003年VMware这个厂商提供了服务器虚拟化的技术，就是把一台服务器一台设备，切成很多虚拟的小块服务器，也就是俗称的“虚拟机”。而让业务应用在虚拟机上面去工作，这就叫服务器虚拟化。 这个技术解决了数据中心里面机房空间和动力资源不够的问题，原来需要十台服务器的，现在在一台高级的服务器里面切了十份，分别给10个业务应用使用，即使这样也就才用了40%的CPU，电力节省了接近9/10，而且一个机房里面容量就可以做二十倍数的的提升。原来部署一百台物理的服务器，现在可以部署两千台虚拟服务器，大大的提升了资源的利用率。而虚拟化的另一个好处就是还大幅提升了客户业务的创新的积极性，因为以前他们服务器都摆不下，好多东西不能做，现在摆下了，所以他要做什么东西都可以试试。但是这个虚拟化技术只解决了资源的共享问题，还没解决运营效率问题。 到了2006年的时候，亚马逊提出了公有云的服务，那时还不叫云计算，叫Amazon Web Service, 简称AWS（现在亚马逊的云计算项目还是沿用这个叫法）。亚马逊大家都知道，它原本是一个做超市的，超市的概念就是你需要你去拿，拿了就到门口结帐，结帐你就拿走。然后亚马逊它把这个计算资源也当成一种，把服务器也当成资源去卖，但是它卖的不是物理的，不是让你搬一台服务器走。而是建好一个数据中心，当你用的时候，你要多少台我们就给你多少台，你只要刷信用卡在网上做支付，你就可以使用这些逻辑上提供给你的资源，这也是虚拟化资源第一次被当成商品出售的案例。随后，Amazon在此基础上提出了云计算的概念，并开发出了公有云业务。亚马逊的这种公有云的这种服务对企业而言，想做的时候，只要付钱就可以做了。而且一般企业业务部门都很有钱，所以业务部门的愿意投钱去做这个事情。 到了2010年的时候，出现了另外一种云计算架构，叫OpenStack。它是NASA就是美国太空总署那个NASA和RackSpace联合做的。RackSpace也是做公有云服务的运营商，就像我们的阿里云一样，它也是想学习亚马逊。但是它的人没有亚马逊的团队这么大，所以它把这个架构给开源出来，形成一个OpenStack开源社区，大家都在OpenStack上面去开发完善，然后让这个框架变得更好，目前OpenStack会聚了全球的研发力量，并吸引了几乎全球所有主流IT厂商的支持，帮它完善这个架构，形成了云计算领域一个全新的强大生态系统。OpenStack社区把这个架构公开出来以后，各个企业都看到了一个新的技术，可以运用到他们自己企业里面，所以很多企业都开始基于OpenStack做自己的私有云，让自己家的IT，变得效率更高，资源更有效。 然后，到了2014年以后，很多企业经过这么多年的建设以后，基本都走在了公有云和私有云这两条路上，私有云就是在家里，自建自用，不对外。公有云就是公开对外出售虚拟资源（如服务器、负载均衡器、防火墙等）在亚马逊、阿里或者华为都有这类服务，只要花钱买就可以。很多企业，尤其是中小企业，无力承担基础设施建设费用，就是通过购买EC2（云计算服务中资源服务器）来部署它的业务。还有一些企业，将一部分业务部署在公有云上，另一部分私密性业务部署在私有云上，这样就合成了一个混合云的状态。 云计算技术的演进历程 随着云计算技术走向成熟，在混合云时代，企业对于云计算的相关技术成熟度的问题已经不太关注了。企业新的追求是，让IT的人员怎么在一个团队的模式下面，对公有云和私有云的混合云资源进行统一的管理。这是一个管理的概念，也是混合云构建的核心。 云计算（Cloud Computing）是分布式计算（Distributed Computing）、并行计算（Parallel Computing）和网格计算（Grid Computing）的发展或者说是这些计算机科学概念的商业实现。 并行计算（Paralled Computing）：同时使用多种计算资源解决计算问题的过程，主要目的是快速解决大型且复杂的问题。 特点：把计算任务分派给系统内多个计算单元。 分布式计算（Distributed Computing）：把一个需要巨大计算能力才能解决的问题分成多个小部分，把这些小部分分配给多个计算进行处理，最后综合这些计算结果得到最终的结果。 特点：分配计算任务到网络中多台独立的机器。 网格计算（Grid Computing）：利用互联网把地理上广泛分布的各种资源连成一个逻辑的整体，就像一台超级计算机。 特点：分布式计算的一种，为用户提供一体化的信息和应用服务。 云计算的部署模式 私有云（Private Cloud）： 企业利用自有或租用的基础设施资源自建的云。 社区云/行业云（Community cloud）：为特定社区或行业所构建的共享基础设施的云。 公有云（Public cloud）：出租给公众的大型的基础设施的云。 混合云（Hybrid cloud）：由两种或两种以上部署模式组成的云。 从部署模式上看，云计算又分为私有云、社区云也就是行业云、公有云、和混合云，这几种形态，这里也给出了具体的定义。简单理解： 私有云就是企业建在自家院子里的，只给自己用的云，有些公司特别是公有云厂商喜欢把“私有云”称作“专有云”，名字不同，但含义基本相同，虽然这些厂商不愿意承认。 社区云既行业云，应用范围要比私有云广泛，更像是由家庭和家族的关系，往往应用在某个特定的区域或特定行业，主要给特殊血缘关系的行业或社区家族成员使用。 公有云的使用范围则更加宽泛，基本上只要给钱谁都可以用，因此公有云的规模往往也更大。 但公有云的用户鱼龙混杂，像火车站一样，而且目标明显，不仅使用的好人多，惦记着甚至使用的坏人也不少，所以经常成为黑客及非法集团的攻击或信息偷窃目标。但瑕不掩瑜，人们不能因为火车上有小偷，就不坐火车了。但贵重的东西，比如大额现金最好就别随身带了，放家里由家人看着最保险，也就是在使用公有云的同时还使用私有云。而且希望公有云与私有云能很好的协同。这就是混合云了。 云计算当前的商业模式 云基础设施即服务（IaaS）— 出租处理能力、存储空间、网络容量等基本计算资源。 云平台即服务（PaaS）— 为客户开发的应用程序提供可部署的云环境。 云软件即服务（SaaS）— 在网络上提供可直接使的应用程序。 现在不止这几种，还有存储即服务，桌面即服务，数据中心即服务等等。 在服务模式上，云计算分为三种，包括：IaaS 基础设施即服务、PaaS 开发平台即服务、SaaS 软件即服务这三种模式。 IaaS，就是以前买IBM、HP、DELL、华为服务器存储跑企业业务（比如：生产管理业务、财务、市场客户管理、邮箱等），现在企业把业务放到Amazon的网站或华为企业云服务（公有云）网站就全搞定了。简单讲就是由“买”变“租”。大型企业的IT部门也可以以IaaS服务的形式，对内部业务部门提供服务，然后进行内部结算。 PaaS，就是以前企业需要业务软件的时候，软件开发商在自己公司开发完在卖给企业客户，还要派人去安装软件。现在开发、安装都在企业客户那里全做了。当然给的钱也不再是软件费而是人头费了。 SaaS，就是以前写汇报胶片用微软PPT软件，现在登录www.prezi.com，在那个网站上就可以写胶片了，很酷。 云计算的8个通用特点 大规模（Massive scale） 同质化（Homogeneity） 虚拟化（Virtualization） 弹性计算（Resilient computing） 低成本软件（Low cost software） 地理分布（Geographic distribution） 服务定位（Service orientation） 先进安全技术（Advanced security technologies） 这是美国国家标准与技术研究院给云计算列出的8个通用特点： 大规模（Massive scale），因为云计算服务把IT的资源供应集中化了，自然规模很大。也正因为如此，量变导致质变，使得云计算与传统IT有了众多的区别。 同质化（Homogeneity），也可以理解成标准化，这点倒是和用电很类似，大家要保持相同电压、插座接口，这样人们的电器和各种设备才能被广泛使用。 虚拟化（Virtualization），有两层含义，一个是计算单元的精细化，一块蛋糕太大，一个人吃不了，那最好切成小块，大家分着吃，也就是让每个计算单元更小，这样可以充分利用IT资源；另外一层含义是软硬件的分离，虚拟化之前软件和指定硬件是绑在一起的，虚拟化之后软件在所有硬件上可以自由的迁移，这跟人们由买房变成租房是一样的，既然北上广深的房价太高，很多人便租房住了，拎个箱子想住哪就住哪。 弹性计算（Resilient computing），在前面已经说过，指的是IT资源供给可弹性伸缩。 低成本软件（Low cost software），是从竞争与市场需求发展的角度说的。云计算降低了人们使用IT的门槛，不仅仅在个人技术能力上，而且在资金能力上，很多小微的初创企业，本身就没啥钱，希望能够用最少的钱使用最多的IT服务，要想打开这部分市场，自然需要低成本的软件，通过薄利多销的形式赚到更多的钱。 地理分布（Geographic distribution）：前文我们提到了泛在接入，也就是能够在任意时间任意地点提供IT服务。从使用者的角度看，就是地理分散的，由于各地网络带宽的优劣差异，那么IT提供者，也就是云计算数据中心的部署，自然也是呈现出地理分布式特征的。大的公有云厂商都有几十个甚至数百个数据中心或服务节点，面向全球提供云计算服务。 服务定位（Service orientation）：因为云计算是一种服务模式，它的整个体系的设计也就是面向服务的。 先进安全技术（Advanced security technologies），林子大了，什么鸟都有，公有云大了，什么用户也都有，包括好的坏的，自然先进的安全技术保障是一个云计算必须的条件了。 到此为止，对云计算的总结就是：4部署-3服务-5特性-8个通用点，简称4358。 云计算的流派 从实现方式来看，云计算有两个典型的流派：大分小模式和小聚大模式。（现在这个概念已经很少有人提了，因为从资源的使用看，两种流派的本质是相同的） 大分小模式：也称为Amazon流派，不同的应用在使用资源时，通过时分复用算法来调用。 关键技术点包括：计算、存储和网络虚拟化以及虚拟机监控、调度和迁移。 典型代表：Amazon，alibaba，华为的EC2等。 小聚大模式：也称为Google流派，资源在多个应用间贡献，通过将应用划分多个子任务，结合调度算法来实现某个子引用在资源上的独占。 关键技术点包括任务分解、调度、分布式通信总线和全局一致性。 典型代表：Google，我国的天河2号等]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-09-03-什么是云计算?]]></title>
    <url>%2F2018%2F09%2F03%2F2018-09-03-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%91%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[云计算概念的定义 云计算基础知识入门 云计算是一种按使用量付费的模式，这种模式提供可用的、便捷的、按需的网络服务（即随时随地接入可接入），进入可配置的计算资源共享池（包括：网络、服务器、存储、应用软件、服务），这些资源能够被快速提供，只需投入很少的管理工作，或与服务供应商进行很少的交互。（美国国家标准与技术研究院） 云计算是一种基于互联网的计算方式，通过这种方式，共享软硬件资源和信息可以按需求提供给计算机和其他设备。云计算依赖资源的共享以达成规模经济，类似基础设施。（维基百科） 云计算概念诞生之初，市场上对其概念有很多种理解，经过一段时间的争论，现在大家一般来说都认可的就是美国标准与技术研究院的它给出的一个最标准的定义。它把云计算定义为一种模式，而不是一种技术。这种模式既可以是商业模式，也可以是服务模式。 云计算的关键特征 按需自助服务（On-demand Self-service） 无处不在的网络接入（Broad Network Access） 与位置无关的资源池（Locations independent resources pooling） 快速弹性（Rapid Elastic） 按使用付费（Pay per User） 我们从云计算的基本特质上进一步理解一下云计算的概念和内涵。 从管理层面上说，云计算实现了IT资源的按需自助服务。“按需”是从量的角度来说，这是一种“量体裁衣”的资源使用方式，避免粗放管理带来成本损耗。“自助”是从人的角度来说，减少了资源使用者与资源管理者之间的频繁交互，进一步减少人工成本的损耗。同时，从历史上看，IT的使用难度每降低一个层次，IT产业就会获得一次质飞跃，因为客户数量会因此有指数级的提升。而“自助”这种简单化操作意味着更多的人都可以使用IT产品和服务。 云计算实现了广泛网络接入。这意味着用户可以在全球各地7x24小时的使用IT服务，也就是随时、随地、随心、随意的使用。这极大的提升了用户工作的灵活性和经营工作效率。 云计算实现了资源池化。“池化”就意味着资源的同质化、归一化。无论是网络、服务器、存储、应用还是服务，都是这些同质化、归一化资源的组合、协同实现。使用者和管理者只需考虑需求的资源“量”，无需考虑资源提供商之间的差异性。 云计算资源弹性伸缩。是指资源能够快速的供应和释放，就是说当你要的时候，我能快速的很快就给你，不是说你申请了以后，十天半个月我才给你资源，而是你要我马上就给你，当你不用的时候，我马上就回收，资源释放。 从经营层面上说，云计算实现了可计量的服务。“技术免费、服务收费”是开源社区的一个宗旨。前面也提到云计算的本质就是一种服务，为了实现这类服务收费，就必须要求服务可计量，而计量的依据就是资源使用的可计量。比如：按使用小时为时间单位，以服务器CPU个数、占用存储的空间、网络的带宽等综合计费，当然也可以包时、包天、包月那种套餐模式进行计量。而且，计量的越精细，运营效率越高。 以上，就是美国标准与技术研究院给云计算标准的定义时所诠释的特质。 从技术视角来看：云计算=计算/存储的网络 商业视角：云计算=信息电厂计算和存储：由PC时代的局域网向云时代的互联网迁移。软件：由PC时代的终端向云端迁移。 用户消费模式变化 通过互联网提供软硬件与服务； 用户通过浏览器或轻量级终端获取、使用服务。 商业模式发生变化 从“购买软硬件产品”向“购买信息服务”转变，如同100年前用电的转变。 云计算产生的背景 云计算=需求推动+技术进步+商业模式转变 云计算的产生是需求推动、技术进步、商业模式转变共同促进的结果。 需求推动： 政企客户低成本且高性能的信息化需求。 个人用户的互联网、移动互联网应用需求强烈，追求更好用户体验。 技术进步 虚拟化技术、分布与并行计算、互联网技术的发展与成熟，使得基于互联网提供包括IT基础设施、开发平台、软件应用成为可能。 宽带技术及用户发展，使得基于互联网的服务使用模式逐渐成为主流。 商业模式转变 少数云计算的先行者（例如Amazon的IaaS、PaaS)的云计算服务已开始运营。 市场对云计算商业模式已认可，越来越多的用户接受并使用云计算服务。 生态系统正在形成，产业链开始发展和整合。 几年之内，云计算已从新兴技术发展成为当今的热点技术。从Google公开发布的核心文件到Amazon EC2（亚马逊弹性计算云）的商业化应用，再到美国电信巨头AT&amp;T（美国电话电报公司）推出的Synaptic Hosting（动态托管）服务，云计算从节约成本的工具到盈利的推动器，从ISP（网络服务提供商）到电信企业，已然成功地从内置的IT系统演变成公共的服务。]]></content>
      <categories>
        <category>云计算基础</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-31-为什么开始写博客？]]></title>
    <url>%2F2018%2F08%2F31%2F2018-08-31-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%80%E5%A7%8B%E5%86%99%E5%8D%9A%E5%AE%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言2018年3月—7月期间，有幸在外培训，时间相对宽松。当时，主要一直在自学SDN部分知识，鉴于同班同学一直询问询问云计算怎么学习？为此，我特意开通微信公众号，主要讲述一点儿OpenStack的基础知识。（如下） 当时，还特意列出一个大纲，从最基础的Linux开始，一直到后续高级服务和应用。比如：Tacker、Workflow、批量热迁移等。 但是，微信公众号的排版实在是太浪费时间，再加上后续回到工作岗位，个人时间实在有限，因此公众号的内容一直处在长期停更，偶尔写写的状态。实在有愧大家的期待-_-!!! 在2018年7月期间，一个偶然的机会让我接触到hexo和markdown两个小伙伴，让我燃起了写技术博客的动力。hexo主要用来生成静态页面，而markdown是一种极简的文本编写工具，其语法格式只有十来种，对编写网页博客有很大助力：-）利用这两个工具，可以提升网文的写作效率。至于怎么使用，网上的教程很多，大家可以自信搜索。 言归正传，后续公众号中已完成的内容我会逐渐搬到个人博客中来，并对前期挖的坑做个填补-_-!!!。新增的内容主要在个人博客中完成，微信公众号只能有空闲时间后，再进行补充，还请大家以后多关注博客内容。 另外，大家有什么疑惑，以后都可以在博客的留言区进行留言。]]></content>
      <categories>
        <category>随笔杂感</category>
      </categories>
      <tags>
        <tag>篇首语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-08-31-写在前面的话]]></title>
    <url>%2F2018%2F08%2F31%2F2018-08-31-%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2%E7%9A%84%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[学习大纲 鉴于好多人询问云计算怎么学习？我的建议是实践和理论相结合！IT不像CT那么多理论和流程，IT诞生和发展的源动力就是提升工作效率。因此，IT的理论并没有CT那么难，但是要想用好，用巧必须具备相当丰富的实操经验。对于新手，最好的学习方式就是边实践边消化相关理论流程。 OpenStack是目前最成熟并且符合生产部署的一套开源云操作系统，全球各大IT，CT厂商，甚至运营商都为此投入了大量的人力、物力和精力。由于其开源的特性，全世界的云计算爱好者也为此贡献了很多。从openstack的特点来看，它是由python语音开发，是一个提供多种服务，相当灵活和稳定的云操作系统。以前的定义主要集中IaaS层（基础设施层），但是现在随着docker的出现，IaaS和PaaS（平台层）的界限已经非常模糊，同时随着K8s的兴起，甚至与SaaS的界限也不再清晰。从操作系统的组成来看，openstack+docker+k8s的组合更像一个更广泛的云化操作系统，openstack是内核，dokcer是用户空间，K8s是调用各项基础进程的API。 转入正题，由于openstack非常灵活，同时也就引入另一个问题，学习起来非常困难。因此，在学习openstack的同时，如果自己手头有一套实验环境那是相当的完美。我当初学习openstack是从devstack开始学起（openstack开发者版本），同时我也建议初学者从devstack开始学起。接下来我们就开始学习OpenStack的第一步，基础准备工作。 工具准备 学习云计算的硬件和软件条件 1、一台不少于16G内存，2CPU+内核笔记本或二手服务器，且硬件要开启支持虚拟化功能。BIOS中的开启方法如下图所示： 2、下载CentOS 7的光盘安装镜像。下载地址在CentOS官网链接，下载DVD版本即可。https://www.centos.org/download/ 3、去VMWare官网下载VMWare虚拟机软件，需要新注册个账号。下载链接如下：（注意：如果你你的宿主机是win10系统，不要下载VirtualBox，因为win10的无线网络在VirtualBox中设置桥接网络有BUG）https://www.vmware.com/cn/support/workstation.html 以上工具就是我们环境搭建需要的基本工具。还有一些辅助工具如：XShell、XFTP可自行百度下载（个人版在官网申请是免费，记得千万不要升级） 下载地址如下： http://www.netsarang.com/products/xsh_overview.html]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
</search>
