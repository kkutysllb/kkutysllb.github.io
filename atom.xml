<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一花一菩提，一云一世界</title>
  
  <subtitle>佛系ICT人士技术博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://kkutysllb.cn/"/>
  <updated>2020-03-01T07:00:33.960Z</updated>
  <id>https://kkutysllb.cn/</id>
  
  <author>
    <name>kkutysllb</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-03-01-KVM实践初步</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E5%AE%9E%E8%B7%B5%E5%88%9D%E6%AD%A5/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM实践初步/</id>
    <published>2020-03-01T05:17:00.000Z</published>
    <updated>2020-03-01T07:00:33.960Z</updated>
    
    <content type="html"><![CDATA[<p>在x86-64架构的处理器中，KVM需要的硬件辅助虚拟化分别为Intel的虚拟化技术（Intel VT）和AMD的AMD-V技术。在前面我们介绍过，CPU不仅要在硬件上支持VT技术，还要在BIOS中将其功能打开，KVM才能使用到。目前，大多数服务器和PC的BIOS都默认已经打开VT。至于如何在VMware Workstations虚拟机和物理服务器上打开，详见本站<a href="[https://kkutysllb.cn/2019/06/22/2019-06-21-KVM%E5%88%B0%E5%BA%95%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F/](https://kkutysllb.cn/2019/06/22/2019-06-21-KVM到底是个啥？/">《KVM到底是个啥？》</a>)一文。<a id="more"></a></p><h2 id="部署和安装KVM"><a href="#部署和安装KVM" class="headerlink" title="部署和安装KVM"></a><strong>部署和安装KVM</strong></h2><p><strong>KVM的部署和安装主要有两种方式：一种源码编译安装，另一种就是通过CentOS的YUM工具安装。</strong>作为学习实验途径，我们这里主要介绍YUM工具安装方式。至于源码编译安装一般属于生产研发人员的操作，这里我们只给一些关键提示，有兴趣的同学可自行研究。</p><p><strong>1）源码编译安装方式。</strong>KVM作为Linux内核的一个module，是从Linux内核版本2.6.20开始的，所以要编译KVM，你的Linux操作系统内核必须在2.6.20版本以上，也就是CentOS 6.x和CentOS 7.x都天生支持，但是CentOS 5.x以下版本需要先升级内核。</p><p><strong>下载KVM源代码，主要通过以下三种方式：</strong></p><ol><li>下载KVM项目开发中的代码仓库kvm.git。</li><li>下载Linux内核的代码仓库linux.git。</li><li>使用git clone命令从github托管网站上下载KVM的源代码。</li></ol><p>根据上面三种途径下载源代码后就可以通过make install的方式编译安装了，编译安装完成后还需要根据KVM官方指导手册进行相关的配置。。。我们这里不展开，大家可自行搜索源码方式安装。<strong>需要注意一点儿的是，除了下载KVM的源码外，还需要同时下载QEMU的源码进行编译安装，因为它俩是一辈子的好基友嘛。。。</strong></p><p><strong>2）YUM工具安装。</strong>首先，需要查看 CPU是否支持VT技术，就可以判断是否支持KVM。然后，再进行相关工具包的安装。最后，加载模块，启动libvirtd守护进程即可。具体步骤如下：</p><p><strong>Step1：</strong>确认服务器是否支持硬件虚拟化技术，如果有返回结果，也就是结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的，否则就是不支持。如下图所示，表示服务器支持Intel的VT-x技术，且有4个CPU。</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/clipboard.png" alt></p><p><strong>Step2：</strong>确认系统关闭SELinux。如下图所示，表示系统已经关闭Linux。</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301142932.png" alt></p><p>如果没有关闭，可以使用如下命令永久关闭：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># setenforce 0   # 临时关闭</span></span><br><span class="line"><span class="comment"># 永久关闭</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># sed -i 's/^SELINUXTYPE=.*/SELINUXTYPE=disabled/g' /etc/selinux/config</span></span><br></pre></td></tr></table></figure><p><strong>Step3：</strong>安装rpm软件包。</p><p>由于KVM是嵌入到Linux内核中的一个模块，我们这里首先安装用户安装用户空间QEMU与内核空间KMV交互的软件协议栈QEMU-KVM，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum -y install qemu-kvm</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144136.png" alt></p><p>如上图，不仅完成qemu-kvm（红色框部分）安装，还同步安装qemu-img、qemu-kvm-comm两个依赖包的安装（黄色框部分）。注意：由于我已经安装过，上面提示是updated，没有安装的话，应该是installed。</p><p>完成QEMU-KVM软件协议栈的安装后，我们安装libvirt<em>、virt-</em>等管理工具，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y libvirt* virt-*</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144259.png" alt></p><p>如上图，将KVM的管理工具libvirt系列软件包和virt-系列软件包（红色框部分）及相关依赖软件包（黄色框部分）进行了安装。</p><p>最后，需要安装一个二层分布式虚拟交换机，用于KVM虚拟机的虚拟接入，可以选择OVS，也可以选择Linux bridge或是其他商用的分布式交换机如VMware、华为FC等。我们这里只出于学习的目的，选择安装Linux bridge即可。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y bridge-utils</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144434.png" alt></p><p>如上图，提示我们系统已经安装该软件包且是最新版本（红色框部分），如果你没有安装过，系统会直接安装。</p><p><strong>3）加载KVM模块，使得Linux Kernel变成一个Hypervisor。</strong>首先，加载KVM内核模块，然后查看KVM内核模块的加载情况即可。具体步骤如下：</p><p>首先，通过modprobe指令加载kvm内核模块，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># modprobe kvm</span></span><br></pre></td></tr></table></figure><p>然后，通过lsmod指令查看kvm模块的加载情况，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># lsmod | grep kvm</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144605.png" alt></p><p><strong>4）启动libvirtd守护进程，并设置开机启动。</strong>因为libvirt管理工具，是用于KVM虚机全生命周期的统一管理工具，它由外部统一API接口、守护进程libvirtd和CLI工具virsh三部分组成，其中守护进程libvirtd用于所有虚机的全面管理。同时，其他管理工具virt-*、openstack等都是调用libvirt的外部统一API接口完成KVM的虚机的管理。所以，我们需要启动libvirtd守护进程，并设置开机启动。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl enable libvirtd &amp;&amp; systemctl start libvirtd &amp;&amp; systemctl status libvirtd</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144720.png" alt></p><p>这里注意一下，一般情况我们单机玩KVM虚拟机时，libvirtd守护进程默认监听的是UNIX domain socket套接字，并没有监听TCP socket套接字。为了同时让libvirtd监听TCP socket套接字，需要修改/etc/libvirt/libvirtd.conf文件中，将tls和tcp，以及tcp监听端口前面的注释取消。然后重新通过libvirtd的原生命令加载配置文件，使其生效。而系统默认的systemctl命令由于不支持–listen选项，所以不能使用。参考代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl stop libvirtd </span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># libvirtd -d --listen</span></span><br></pre></td></tr></table></figure><p>通过netstat命令查看libvirtd监听的tcp端口为16509，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144827.png" alt></p><p>最后，进行tcp链接验证，如果连接成功，表示服务启动正常且tcp监听正常。如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144908.png" alt></p><h2 id="安装第一个KVM虚拟机"><a href="#安装第一个KVM虚拟机" class="headerlink" title="安装第一个KVM虚拟机"></a><strong>安装第一个KVM虚拟机</strong></h2><p>安装虚拟机之前，我们需要创建一个镜像文件或者磁盘分区等，来存储虚拟机中的系统和文件。首先，我们利用<strong>qemu-img工具</strong>创建一个镜像文件。<strong>这个工具不仅能创建虚拟磁盘，还能用于后续虚拟机镜像管理。</strong>比如，我们要创建一个raw格式的磁盘，具体代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># qemu-img create -f raw ubuntu1204.img 20G </span></span><br><span class="line"></span><br><span class="line">Formatting <span class="string">'ubuntu1204.img'</span>, fmt=raw size=21474836480</span><br></pre></td></tr></table></figure><p>如上，表示在当前目录下(/root）创建了一个20G大小的，raw格式的虚拟磁盘，名称为：ubuntu1204.img。虽然，我们看它的大小时20G，实际上并不占用任何存储空间，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145048.png" alt></p><p>这是因为qemu-img聪明地为你按实际需求分配文件的实际大小，它将随着image实际的使用而增大。如果想一开始就分配实际大小为20G的空间，不仅要使用raw格式磁盘，还需加上-o preallocation=full参数选项，这样创建速度会很慢，但是创建的磁盘实实在在占用20G空间。我们这里用创建一个5G磁盘来演示，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145129.png" alt></p><p>除raw格式以外，qemu-img还支持创建其他格式的image文件，比如qcow2，甚至是其他虚拟机用到的文件格式，比如VMware的vmdk、vdi、Hyper-v的vhd等，不同的文件格式会有不同的“-o”选项。为了演示我们的第一个虚拟机，我们现在创建一个qcow2格式的虚拟磁盘用作虚拟机的系统磁盘，大小规划40G，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145211.png" alt></p><p>上面创建的虚拟磁盘实际就是KVM虚拟机后续的系统盘，在创建完虚拟机磁盘后，我们将要安装的系统镜像盘上传到当前目录下或者通过光驱挂载也可。我们这里为了安装速度问题，采用上传到宿主机本地的方式，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145248.png" alt></p><p><strong>注意：这里的宿主机指的是我们的VMware虚拟机，这里的虚拟机指的是我们在VMware虚拟机中创建的KVM虚拟机，千万别搞混了。</strong></p><p>然后，就是我们的关键一步，通过virt-install命令安装虚拟机，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># virt-install --name ubt1204d \</span></span><br><span class="line">--virt-type kvm \</span><br><span class="line">--ram 2048 --vcpus=2 \</span><br><span class="line">--disk path=/root/ubt1204d.img,format=qcow2,size=40 \</span><br><span class="line">--network network=default,model=virtio \</span><br><span class="line">--graphics vnc,listen=192.168.101.251 --noautoconsole \</span><br><span class="line">--cdrom /root/ubuntu-12.04.5-desktop-amd64.iso </span><br><span class="line"></span><br><span class="line">Starting install...</span><br><span class="line">Domain installation still <span class="keyword">in</span> progress. You can reconnect to </span><br><span class="line">the console to complete the installation process.</span><br></pre></td></tr></table></figure><p>如上，上面的指令表达的意思为：–name指定虚拟机的名称，–virt-type指定虚拟机的类型为kvm，–ram指定给虚拟机分配的虚拟内存大小为2GB，–vcpus指定给虚拟机分配的虚拟cpu为2个，–disk指定虚拟机的系统盘，就是我们刚才创建的虚拟磁盘，–network指定虚拟机使用的虚拟交换机，这里使用系统的默认配置，默认配置文件为/etc/libvirt/qemu/networks/default.xml，其详细信息如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145417.png" alt></p><p>上图中网络模式采用nat方式就表示虚拟机可以通过网桥访问internet或者外部网络。–graphics指定虚拟机的模拟显示器界面，这里采用vnc方式，并监听宿主机地址192.168.101.251。–cdrom选项指定虚拟机的安装镜像配置，也就是系统安装盘iso的位置。</p><p>上面虚拟机正式启动后，可以通过本地机器的vnc客户端连接到KVM虚拟机，作为虚拟机的模拟显示器。为此，首先需要查看VNC连接的端口，代码如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145502.png" alt></p><p>如上图黄色框部分，通过本地机器VNC客户端连接目标机器192.168.101.251的0端口就能模拟虚拟机的显示器，进一步完成图形化安装配置，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145535.png" alt></p><p>连接成功后，与真实物理机装系统的操作一致，可以使用键盘、鼠标完成各类安装配置操作。虚拟机模拟显示器的界面如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145608.png" alt></p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145737.png" alt></p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145805.png" alt></p><p>由于我们安装的是ubuntu12.04的桌面版系统，所以完成虚拟机的安装后，后续每次使用虚拟机都需要使用VNC客户端进行连接操作。如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145843.png" alt></p><p>同时，我们可以通过virsh命令工具查看本机上所有虚拟机的状态，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145921.png" alt></p><p>最后，提醒一下：我们在自动化运维中介绍过通过kicstart或cobbler批量安装宿主机操作系统，这种方式也是可以在KVM虚拟机安装系统中使用，通过在virt-install命令增加–extra-args选项就可实现。但是，一般我们不这么玩，在云计算和虚拟化场景下，均是通过手动安装一台模板虚拟机，将其系统盘转换为模板镜像格式文件，然后通过批量分发虚拟机的方式（就是我们在存储虚拟化中讲的链接克隆和快照方式）完成虚拟机批量部署操作，电信云中通过VNFD描述的多台虚拟机资源部署也是同样的方式完成，只不过里面借助了OpenStack的编排引擎。这种方式我们在后续介绍KVM虚拟镜像格式实操一文中会详细介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在x86-64架构的处理器中，KVM需要的硬件辅助虚拟化分别为Intel的虚拟化技术（Intel VT）和AMD的AMD-V技术。在前面我们介绍过，CPU不仅要在硬件上支持VT技术，还要在BIOS中将其功能打开，KVM才能使用到。目前，大多数服务器和PC的BIOS都默认已经打开VT。至于如何在VMware Workstations虚拟机和物理服务器上打开，详见本站&lt;a href=&quot;[https://kkutysllb.cn/2019/06/22/2019-06-21-KVM%E5%88%B0%E5%BA%95%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F/](https://kkutysllb.cn/2019/06/22/2019-06-21-KVM到底是个啥？/&quot;&gt;《KVM到底是个啥？》&lt;/a&gt;)一文。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-27-打造文件实时同步架构之sersync篇</title>
    <link href="https://kkutysllb.cn/2019/06/27/2019-06-27-%E6%89%93%E9%80%A0%E6%96%87%E4%BB%B6%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E4%B9%8Bsersync%E7%AF%87/"/>
    <id>https://kkutysllb.cn/2019/06/27/2019-06-27-打造文件实时同步架构之sersync篇/</id>
    <published>2019-06-27T09:30:01.000Z</published>
    <updated>2019-06-27T10:15:51.408Z</updated>
    
    <content type="html"><![CDATA[<p>前文讲述的rsync主要用于较大量数据同步，一般简单的服务器数据传输会使用ftp/sftp等方式，但是这样的方式效率不高，不支持差异化增量同步也不支持实时传输。针对数据实时同步需求大多数人会选择rsync+inotify-tools的解决方案，但是这样的方案也存在一些缺陷，（下文会具体指出），sersync是国人基于前两者开发的工具，不仅保留了优点同时还强化了实时监控，文件过滤，简化配置等功能，帮助用户提高运行效率，节省时间和网络资源。<a id="more"></a></p><p><img src="https://i.loli.net/2019/06/27/5d148d0bbe06c11169.jpg" alt></p><h2 id="inotify-rsync模式"><a href="#inotify-rsync模式" class="headerlink" title="inotify+rsync模式"></a><strong>inotify+rsync模式</strong></h2><p>如果要实现定时同步数据，可以在客户端将rsync加入定时任务，但是定时任务的同步时间粒度并不能达到实时同步的要求。在Linux kernel 2.6.13后提供了inotify文件系统监控机制。通过rsync+inotify组合可以实现实时同步。</p><p>inotify实现工具有几款：inotify本身、sersync、lsyncd。其中sersync是金山的周洋开发的工具，克服了inotify的缺陷，且提供了几个插件作为可选工具。此处先介绍inotify的用法以及它的缺陷，通过其缺陷引出sersync，并介绍其用法。</p><h3 id="安装inotify-tools"><a href="#安装inotify-tools" class="headerlink" title="安装inotify-tools"></a><strong>安装inotify-tools</strong></h3><p>inotify由inotify-tools包提供。在安装inotify-tools之前，请<strong>确保内核版本高于2.6.13</strong>，且在/proc/sys/fs/inotify目录下有以下三项，这表示系统支持inotify监控，关于这3项的意义，下文会简单解释。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l /proc/sys/fs/inotify/</span></span><br><span class="line">total 0</span><br><span class="line">-rw-r--r-- 1 root root 0 Jun  3 18:30 max_queued_events</span><br><span class="line">-rw-r--r-- 1 root root 0 Jun  3 18:30 max_user_instances</span><br><span class="line">-rw-r--r-- 1 root root 0 Jun  3 18:30 max_user_watches</span><br></pre></td></tr></table></figure><p>epel源上提供了inotify-tools工具，可以先安装epel-release源，然后通过yum方式安装，也可以通过下载源码编译方式安装。由于我们部署了本地yum源，所以选择第一种方式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y epel-release</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y inotify-tools</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/27/5d14922a46f2577518.jpg" alt></p><p><strong>inotify-tools工具只提供了两个命令</strong>：<strong>inotifywait和inotifywatch</strong>。如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rpm -ql inotify-tools</span></span><br><span class="line">/usr/bin/inotifywait</span><br><span class="line">/usr/bin/inotifywatch</span><br></pre></td></tr></table></figure><p>其中，<strong>inotifywait命令用于等待文件发生变化，所以可以实现监控(watch)的功能，该命令是inotify的核心命令。</strong>inotifywatch用于收集文件系统的统计数据，例如发生了多少次inotify事件，某文件被访问了多少次等等，一般用不上。inotify相关的内核参数如下：</p><p><strong>1）/proc/sys/fs/inotify/max_queued_events：</strong>调用inotify_init时分配到inotify instance中可排队的event数的最大值，超出的事件被丢弃，但会触发队列溢出Q_OVERFLOW事件。</p><p><strong>2）/proc/sys/fs/inotify/max_user_instances：</strong>每一个real user可创建的inotify instances数量的上限。</p><p><strong>3）/proc/sys/fs/inotify/max_user_watches：</strong>每个inotify实例相关联的watches的上限，即每个inotify实例可监控的最大目录、文件数量。如果监控的文件数目巨大，需要根据情况适当增加此值。</p><h3 id="inotify的不足"><a href="#inotify的不足" class="headerlink" title="inotify的不足"></a><strong>inotify的不足</strong></h3><p>inotify是监控工具，监控目录或文件的变化，然后触发一系列的操作。假如有一台站点发布服务器A，还有3台web服务器B/C/D，目的是让服务器A上存放站点的目录中有文件变化时，自动触发同步将它们推到web服务器上，这样能够让web服务器最快的获取到最新的文件。需要搞清楚的是，监控的是A上的目录，推送到的是B/C/D服务器，所以在站点发布服务器A上装好inotify工具。除此之外，一般还在web服务器BCD上将rsync配置为daemon运行模式，让其在873端口上处于监听状态(并非必须，即使是sersync也非必须如此)。也就是说，<strong>对于rsync来说，监控端是rsync的客户端，其他的是rsync的服务端。</strong></p><p>以上描述的只是最可能的使用情况，并非一定需要如此。况且，inotify是独立的工具，它和rsync无关，它只是为rsync提供一种比较好的实时同步方式而已。</p><p>虽然inotify已经整合到了内核中，在应用层面上也常拿来辅助rsync实现实时同步功能，但是inotify因其设计太过细致从而使得它配合rsync并不完美，所以需要尽可能地改进inotify+rsync脚本或者使用sersync工具。</p><p>另外，<strong>inotify存在bug—</strong>当向监控目录下拷贝复杂层次目录(多层次目录中包含文件)，或者向其中拷贝大量文件时，inotify经常会随机性地遗漏某些文件。这些遗漏掉的文件由于未被监控到，所有监控的后续操作都不会执行，例如不会被rsync同步。实际上，上面描述的问题<strong>不是inotify的缺陷，而是inotify-tools包中inotifywait工具的缺陷。</strong></p><h2 id="Sersync模式"><a href="#Sersync模式" class="headerlink" title="Sersync模式"></a><strong>Sersync模式</strong></h2><p>sersync主要用于服务器同步，web镜像等功能。目前使用的比较多的同步解决方案是inotify-tools+rsync ，另外一个是google开源项目Openduckbill（依赖于inotify- tools），这两个都是基于脚本语言编写的。相比较上面两个项目，sersync优点是：</p><p>1）sersync是使用c++编写，而且对linux系统文件系统产生的临时文件和重复的文件操作进行过滤，所以在结合rsync同步的时候，<strong>节省了运行时耗和网络资源</strong>。</p><p>2）sersync<strong>配置起来很简单</strong>，其中bin目录下已经有基本上静态编译的二进制文件，配合bin目录下的xml配置文件直接使用即可。</p><p>3）相比较其他脚本开源项目，<strong>使用多线程进行同步</strong>，尤其在同步较大文件时，能够保证多个服务器实时保持同步状态。</p><p>4）<strong>有出错处理机制</strong>，通过失败队列对出错的文件重新同步，如果仍旧失败，则按设定时长对同步失败的文件重新同步。</p><p>5）<strong>自带crontab功能</strong>，只需在xml配置文件中开启，即可按您的要求，隔一段时间整体同步一次。无需再额外配置crontab功能。</p><p>6）<strong>具备socket与http插件扩展</strong>，满足您二次开发的需要。</p><p><strong>sersync架构图如下所示，其架构说明如下：</strong></p><p><img src="https://i.loli.net/2019/06/27/5d149291a3ebd68291.jpg" alt></p><p>1 ) 线程组线程是等待线程队列的守护线程，当事件队列中有事件产生的时候，线程组守护线程就会逐个唤醒同步线程。当队列中 Inotify 事件较多的时候，同步线程就会被全部唤醒一起工作。这样设计的目的是为了能够同时处理多个 Inotify 事件，从而提升服务器的并发同步能力。同步线程的最佳数量=核数 x 2 + 2。</p><p>2 ) 之所以称之为线程组线程，是因为每个线程在工作的时候，会根据服务器上新写入文件的数量去建立子线程，子线程可以保证所有的文件与各个服务器同时同步。当要同步的文件较大的时候，这样的设计可以保证每个远程服务器都可以同时获得需要同步的文件。</p><p>3 ) 服务线程的作用有三个：</p><ul><li>处理同步失败的文件，将这些文件再次同步，对于再次同步失败的文件会生成 rsync_fail_log.sh 脚本，记录失败的事件。</li><li>每隔10个小时执行 rsync_fail_log.sh 脚本一次，同时清空脚本。</li><li>crontab功能，可以每隔一定时间，将所有路径整体同步一次。</li></ul><p>4 ) 过滤队列的建立是为了过滤短时间内产生的重复的inotify信息，例如：在删除文件夹的时候，inotify就会同时产生删除文件夹里的文件与删除文件夹的事件，通过过滤队列，当删除文件夹事件产生的时候，会将之前加入队列的删除文件的事件全部过滤掉，这样只产生一条删除文件夹的事件，从而减轻了同步的负担。同时对于修改文件的操作的时候，会产生临时文件的重复操作。</p><h3 id="Sersync安装"><a href="#Sersync安装" class="headerlink" title="Sersync安装"></a><strong>Sersync安装</strong></h3><p><strong>安装规划如下：</strong></p><ul><li>服务器A（主服务器）：192.168.101.11</li><li>服务器B（从服务器/备份服务器）:192.168.101.251</li><li>rsync默认TCP端口为873</li></ul><p><strong>Step1：服务器B上安装rsync</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]# yum install -y rsync</span><br></pre></td></tr></table></figure><p><strong>Step2：配置/etc/rsyncd.conf</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># cat /etc/rsyncd.conf </span></span><br><span class="line"><span class="comment">#服务器B上的rsyncd.conf文件内容</span></span><br><span class="line">uid=root</span><br><span class="line">gid=root</span><br><span class="line"><span class="comment">##最大连接数</span></span><br><span class="line">max connections=36000</span><br><span class="line"><span class="comment">##默认为true，修改为no，增加对目录文件软连接的备份 </span></span><br><span class="line">use chroot=no</span><br><span class="line"><span class="comment">##定义日志存放位置</span></span><br><span class="line"><span class="built_in">log</span> file=/var/<span class="built_in">log</span>/rsyncd.log</span><br><span class="line"><span class="comment">##忽略无关错误</span></span><br><span class="line">ignore errors = yes</span><br><span class="line"><span class="comment">##设置rsync服务端文件为读写权限</span></span><br><span class="line"><span class="built_in">read</span> only = no </span><br><span class="line"><span class="comment">##认证的用户名与系统帐户无关在认证文件做配置，如果没有这行则表明是匿名</span></span><br><span class="line">auth users = rsync</span><br><span class="line"><span class="comment">##密码认证文件，格式(虚拟用户名:密码）</span></span><br><span class="line">secrets file = /etc/rsync.pass</span><br><span class="line"><span class="comment">##这里是认证的模块名，在client端需要指定，可以设置多个模块和路径</span></span><br><span class="line">[rsync]</span><br><span class="line"><span class="comment">##自定义注释</span></span><br><span class="line">comment  = rsync</span><br><span class="line"><span class="comment">##同步到B服务器的文件存放的路径</span></span><br><span class="line">path=/app/data/site/</span><br><span class="line">[img]</span><br><span class="line">comment  = img</span><br><span class="line">path=/app/data/site/img</span><br></pre></td></tr></table></figure><p><strong>Step3：创建rsync认证文件，可以设置多个，每行一个用户名:密码，注意中间以“:”分割</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># echo "rsync:rsync" &gt; /etc/rsync.pass</span></span><br></pre></td></tr></table></figure><p><strong>Step4：设置文件所有者和读写权限</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># chmod 600 /etc/rsyncd.conf </span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># chmod 600 /etc/rsync.pass</span></span><br></pre></td></tr></table></figure><p><strong>Step5：启动服务器B上rsync服务，并查询监听端口</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl enable rsyncd &amp;&amp; systemctl start rsyncd</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/rsyncd.service to /usr/lib/systemd/system/rsyncd.service.</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># netstat -an | grep 873</span></span><br><span class="line">tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN     </span><br><span class="line">tcp6       0      0 :::873                  :::*                    LISTEN     </span><br><span class="line">[root@c7-test01 ~]<span class="comment"># lsof -i tcp:873</span></span><br><span class="line">COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</span><br><span class="line">rsync   1960 root    4u  IPv4  25279      0t0  TCP *:rsync (LISTEN)</span><br><span class="line">rsync   1960 root    5u  IPv6  25280      0t0  TCP *:rsync (LISTEN)</span><br></pre></td></tr></table></figure><p><strong>以下安装步骤是在服务器A上执行！！！</strong></p><p><strong>Step6：在服务器A上安装rsync服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># yum install -y rsync</span></span><br></pre></td></tr></table></figure><p><strong>Step7：安装inotify-tools工具</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># yum install -y inotify-tools</span></span><br></pre></td></tr></table></figure><p><strong>Step8：安装sersync软件包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/sersync/sersync2.5.4_64bit_binary_stable_final.tar.gz</span><br><span class="line">[root@c7-server01 <span class="built_in">local</span>]<span class="comment"># tar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz </span></span><br><span class="line">GNU-Linux-x86/</span><br><span class="line">GNU-Linux-x86/sersync2</span><br><span class="line">GNU-Linux-x86/confxml.xml</span><br><span class="line">[root@c7-server01 <span class="built_in">local</span>]<span class="comment"># mv GNU-Linux-x86 sersync</span></span><br><span class="line">[root@c7-server01 <span class="built_in">local</span>]<span class="comment"># cd sersync/</span></span><br></pre></td></tr></table></figure><p><strong>Step9：配置密码文件并修改权限为600。这个密码用于访问服务器B，需要的密码和上面服务器B的密码必须一致：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 sersync]<span class="comment"># echo "rsync" &gt; /app/local/sersync/user.pass</span></span><br><span class="line">[root@c7-server01 sersync]<span class="comment"># chmod 600 /app/local/sersync/user.pass</span></span><br></pre></td></tr></table></figure><p><strong>Step10：配置confxml.xml文件</strong></p><p><img src="https://i.loli.net/2019/06/27/5d14966e1becf95654.jpg" alt></p><p><strong>Step11：在服务器A上运行sersync服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 sersync]<span class="comment"># cd /app/local/sersync/</span></span><br><span class="line">[root@c7-server01 sersync]<span class="comment"># ./sersync2 -r -d</span></span><br><span class="line">-d:启用守护进程模式</span><br><span class="line">-r:在监控前，将监控目录与远程主机用rsync命令推送一遍</span><br><span class="line">-n: 指定开启守护线程的数量，默认为10个</span><br><span class="line">-o:指定配置文件，默认使用confxml.xml文件</span><br></pre></td></tr></table></figure><h2 id="效果验证"><a href="#效果验证" class="headerlink" title="效果验证"></a><strong>效果验证</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在服务器A（192.168.101.11）的/tmp目录创建10个文件，比如：tets01..test10</span></span><br><span class="line">[root@c7-server01 sersync]<span class="comment"># touch /tmp/test&#123;01..10&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在服务器B（192.168.101.251）的/app/data/site/目录下查看效果</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/27/5d1496e9acb8536499.jpg" alt></p><p>最后，在服务器A上设置sersync开机启动，整体文件实时同步的架构部署完毕。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"/app/local/sersync/sersync2 -r -d -o /opt/sersync/confxml.xml &gt;/app/local/sersync/rsync.log 2&gt;&amp;1 &amp;"</span> &gt;&gt; /etc/rc.local</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前文讲述的rsync主要用于较大量数据同步，一般简单的服务器数据传输会使用ftp/sftp等方式，但是这样的方式效率不高，不支持差异化增量同步也不支持实时传输。针对数据实时同步需求大多数人会选择rsync+inotify-tools的解决方案，但是这样的方案也存在一些缺陷，（下文会具体指出），sersync是国人基于前两者开发的工具，不仅保留了优点同时还强化了实时监控，文件过滤，简化配置等功能，帮助用户提高运行效率，节省时间和网络资源。
    
    </summary>
    
      <category term="Linux常用运维工具" scheme="https://kkutysllb.cn/categories/Linux%E5%B8%B8%E7%94%A8%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="https://kkutysllb.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-27-基础正则的五朵金花</title>
    <link href="https://kkutysllb.cn/2019/06/27/2019-06-27-%E5%9F%BA%E7%A1%80%E6%AD%A3%E5%88%99%E7%9A%84%E4%BA%94%E6%9C%B5%E9%87%91%E8%8A%B1/"/>
    <id>https://kkutysllb.cn/2019/06/27/2019-06-27-基础正则的五朵金花/</id>
    <published>2019-06-27T05:28:03.000Z</published>
    <updated>2019-06-27T06:33:57.372Z</updated>
    
    <content type="html"><![CDATA[<p>在前一篇《正则表达式基础入门》中，我们总结了跟<strong>“位置匹配”</strong>的相关的正则，它其实也是基础正则五朵金花之一，我们只是在入门介绍中单独拿出来进行科普。这篇文章我们就来掰扯掰扯基础正则的其它四朵金花。<a id="more"></a></p><h2 id="基础正则的连续次数匹配"><a href="#基础正则的连续次数匹配" class="headerlink" title="基础正则的连续次数匹配"></a><strong>基础正则的连续次数匹配</strong></h2><p>连续次数匹配是什么意思？空口白话不容易说清楚，我们还是举个例子来说明。为了说明连续次数匹配的，我们首先得重新创建一个测试文件，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1454b93712376409.jpg" alt></p><p>现在，我们想查找测试文件中哪些行包含的连续的两个字母a。聪明如你，肯定想到了如下的查找办法：</p><p><img src="https://i.loli.net/2019/06/27/5d1454e06833880580.jpg" alt></p><p>如上图，我们通过匹配“aa”就能将包含连续两个a的行搜索出来（注意：上面的条件是指至少包含两个连续的a）。那么，我们现在需要找出连续10个a的行呢？简单。。。查找“aaaaaaaaaa”不就行了。但是，如果要找包含连续100个a的行呢？上面的笨办法显然不科学，<strong>这时候就要用了基础正则中连续次数匹配条件—”{n}“了，里面n代表具体的数字，也就是连续多少个的意思。</strong>上面的示例，我们使用基础正则中的连续匹配条件可以改写成下面的格式：</p><p><img src="https://i.loli.net/2019/06/27/5d14551e6bbfa73556.jpg" alt></p><p>如上图，结果符合我们的预期。“{2}”表示连续出现2次的意思，a\{2\}就表示连续出现两次a的意思。现在，你肯定能举一反三。比如：如果我们要匹配连续100个a，可以写成如下匹配条件“a\{100\}”，<strong>也就是我们只需要修改其中的数字n即可。</strong></p><p>上面的用法刚才也提到了，它实际的意思的是：<strong>“包含至少2个连续a的行”。</strong>如果，我们只想找到<strong>“只包含连续两个a的行”</strong>时怎么办？这时候，就要用到我们上一篇提到的“位置规则”匹配条件。我们可以配合使用单词界定符”\&lt;,\&gt;”或者”\b”，具体命令如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1455b05f2b135997.jpg" alt></p><p>如上图，我们单词界定符“\b”（前后的\b表示单词界定符，中间的字母b表示要查找的内容，后面花括号中的数字2表示要匹配的次数），将测试文件test01中的<strong>“只包含连续两个b的行找了出来”。</strong>那么，我们现在再继续延伸一下，首先来验证下<strong>d\{2,3\}的匹配条件</strong>表示什么意思，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1456002280099045.jpg" alt></p><p>上图中的匹配条件将<strong>“包含至少2个连续的d，至多3个连续的d的行”</strong>查找了出来。也就是说<strong>“\{x,y\}”这类匹配条件，表示的是“至少连续出现x次，至多连续出现y次”的意思。</strong>举一反三，“\{x</p><p>,\}”表示“至少连续出现x次，至多不限次数”的意思，”\{,y\}”表示“至少出现0次，至多连续出现y次”的意思。</p><p><strong>我们现在用中学数学的方法小结一下：\{x,y\}表示“至少连续出现x次，至多连续出现y次”。如果y=0，表示“至少连续出现x次，至多次数不限”，且“\{x\}”和“\{x,\}”的意思是一样的。如果x=0，表示“至多出现y次，至少出现0次”的意思。</strong></p><p>现在，我们再来认识另一个匹配次数的<strong>正则符号“*”。其实，在通配符中也有符号“*”，表示匹配任意长度的任意字符。但是，在正则表达式中，符号“*”表示之前的字符连续出现任意次的意思。注意：千万不要和通配符中的“*”混淆了。</strong>具体使用方法如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1456e50c72830042.jpg" alt></p><p>如上图，表示“搜索字母a前面出现任意个d的行”，任意个d当然也包含0个d。那么，如何在正则表达式中查找任意长度的任意字符呢？我们可以使用<strong>“.*”</strong>来实现，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d14571d813d021983.jpg" alt></p><p>上图中，表示匹配字母a后面接任意多个任意字符的行。其实，<strong>在正则表达式中“.”符号表示匹配任意单个字符。而在通配符中“?”表示单个字符，这一定同样要注意不要混淆。如下：</strong></p><p><img src="https://i.loli.net/2019/06/27/5d1463005ce9084092.jpg" alt></p><p>上图中，匹配条件“aa.”表示aa后面跟任意单个字符的行都会被匹配到，”aa..”表示aa后面跟任意两个字符的行都会被匹配到（空格也算字符）。理解了上述规则，回过头来我们再看“.<em>”就非常容易理解了，**”.\</em>“就表示任意单个字符连续出现多次的意思。**</p><p>接下来，我们再认识两个新的匹配条件：<strong>“\?”</strong>和<strong>“\+”</strong>。<strong>“\?”表示匹配其前面的字符出现0次或1次，也就说要么没有，要么最多出现1次。“\+”表示匹配前面的字符出现至少1次，也就是说前面的字符必须至少出现1次。</strong>我们再通过示例验证下：</p><p><img src="https://i.loli.net/2019/06/27/5d145810956e615136.jpg" alt></p><p>如上图，“a\?”表示a这个字符出现0次或1次的行，不仅匹配到了空行，也将包含连续多个a的也匹配到了。我们再看下面的例子：</p><p><img src="https://i.loli.net/2019/06/27/5d145836a590472950.jpg" alt></p><p>上图中，将a字符至少出现1次的行都匹配输出，那么空行也就自然不输出了。</p><p>到此，基础正则连续次数匹配规则就全部介绍完了，国际惯例，我们还是总结一下，便于后续用到了查询：</p><ul><li><strong>*符号：</strong>表示前面的字符连续出现任意次，包括0次。</li><li><strong>.符号：</strong>表示单个任意字符。</li><li><strong>.*组合：</strong>表示任意长度的任意字符。</li><li><strong>\?组合：</strong>表示匹配前面的的字符0次或1次。</li><li><strong>\+组合：</strong>表示匹配其前面的字符至少1次，或连续多次，连续次数上不封顶。</li><li><strong>\{n\}符号：</strong>表示前面的字符连续出现n次，将会被匹配到</li><li><strong>\{x,y\}组合：</strong>表示前面的字符至少连续出现x次，最多连续出现y次，都能被匹配到，换句话说就是之前的字符连续出现的次数在x和y之间，即可被匹配。</li><li><strong>\{,n\}组合：</strong>表示之前的字符连续出现至多n次，最少0次，即可被匹配。</li><li><strong>\{n,\}组合：</strong>表示之前的字符连续出现至少n次，才会被匹配到。</li></ul><h2 id="基础正则中的常用符号"><a href="#基础正则中的常用符号" class="headerlink" title="基础正则中的常用符号"></a><strong>基础正则中的常用符号</strong></h2><p>之前介绍的使用正则表达式去“匹配连续次数”中，我们使用过特殊符号“.”和”*”，分别用于匹配单个任意字符和连续出现任意次数。如果我们需要更精确的匹配呢？比如我们想从测试文本中找出a字母后接任意3个的字母的字串所在行，我们可以使用如下规则：</p><p><img src="https://i.loli.net/2019/06/27/5d1458c87910788625.jpg" alt></p><p>如上图，“[[:alpha:]]”就是我们现在需要认识的新符号。<strong>在正则表达式中，“[[:alpha:]]”表示“任意字母（不区分大小写）”。</strong>“[[:alpha:]]”这个符号看上去有些复杂，吐啊。。。吐啊。。。习惯就好了。其实，这个符号可以拆分成两部分去理解，所以也没那么可怕。<strong>“[[:alpha:]]”的第一部分是最外层的[ ]，表示指定范围内的任意单个字符，第二部分是最内层的[:alpha:]，表示不区分大小写的任意字母。所以，当两部分合起来就是表示任意单个字母（不区分大小写）。</strong></p><p>上面的[[:alpha:]]表示不区分大小写的任意字母，如果我们现在需要区分大小写呢？比如，上面的示例我们再细化一下，需要匹配a后面跟随的3个字母必须是小写字母，可以使用如下规则：</p><p><img src="https://i.loli.net/2019/06/27/5d14595a9f34d13289.jpg" alt></p><p>上图中，<strong>[[:lower:]]就是我们现在认识的第二个符号，它在正则表达式中表示任意单个小写字母。同样的，[[:upper:]]就表示任意单个大写字母。</strong></p><p><strong>聪明如你，一定发现了一些规律，那就是替换“[[: :]]”中的单词，就可表示不同的含义。在基础正则中，主要包含如下特殊符号：</strong></p><ul><li><strong>[[:alpha:]]：</strong>表示任意1个不区分大小写的字母。</li><li><strong>[[:lower:]]：</strong>表示任意1个小写字母。</li><li><strong>[[:upper:]]：</strong>表示任意1个大写字母。</li><li><strong>[[:digit:]]：</strong>表示0-9之间任意1个数字，包括0和9。</li><li><strong>[[:alnum:]]：</strong>表示任意1个数字或字母。</li><li><strong>[[:space:]]：</strong>表示任意1个空白字符，包括“空格”、“tab键”等。</li><li><strong>[[:punct:]]：</strong>表示任意一个标点符号。</li></ul><p>除了上面的特殊符号表示法外，还有其他办法实现 上述的匹配效果。还记得我们在正则表达式基础入门一文中举得一个[a-z]*的例子不？示例中，<strong>[a-z]其实就表示任意一个小写字母，效果与[[:lower:]]一样。同理，[A-Z]就与[[:upper:]]是等价的，表示任意一个大写字母；[0-9]与[[:digit:]]是等价的，表示任意一个0-9的数字。如下所示：</strong></p><p><img src="https://i.loli.net/2019/06/27/5d1459f80399818712.jpg" alt></p><p>那么，有了上面的基础，“[a-z][A-Z][0-9]”就表示任意一个不区分大小写的字母或0-9的数字，等价于[[:alnum:]]。</p><p><strong>上述两种表示的特殊符号使用没有强制性，看个人喜好而定。</strong>这里还需要强调下<strong>[ ]的作用，它表示匹配指定范围内的任意单个字符。</strong>还是先看下面的示例：</p><p><img src="https://i.loli.net/2019/06/27/5d145a7cbf8b763261.jpg" alt></p><p>如上图，字母a后面接一个方括号[]，里面有bcd三个字母。既然方括号[ ]表示匹配任意范围内的单字符，那么上面的匹配条件就是<strong>“匹配字母a后面紧跟b或c或d三个字母一次的行”。</strong>我们活学活用，<strong>[Bd#3]表示什么意思呢？它就表示字符是大写B、或者是小写d、或者是特殊符号#、再或者是数字3各一次的场景，都可以被匹配到。</strong>如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145acb223a362278.jpg" alt></p><p>上图中，表示匹配包含om或oe或oo或ob字符串的行。</p><p><strong>现在，我们理解了方括号[  ]的含义，那么我们再来认识一下它的性格迥异的亲兄弟[^  ]，它表示匹配指定范围外的任意一个字符，与方括号[  ]的含义正好相反。</strong>还是看下面示例：</p><p><img src="https://i.loli.net/2019/06/27/5d145b22aa7ea24329.jpg" alt></p><p>如上图，与前面示例的效果正好相反，将测试文件中不包含om，或不包含oe，或不包含oo，或不包含ob字符的行匹配输出。同理，[^0-9]就表示匹配单个非数字字符，[^a-z]就表示匹配单个非小写字母等等，这里就不再举例了，很好理解。这里有个疑问，比如前面说了[0-9]与[[:digit:]]是等价的，<strong>那么[^0-9]与[^[:digit:]]是否等价？</strong>试试就知道了，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145bae6ce1253207.jpg" alt></p><p><strong>针对这些常用符号，我们总结出一张表如下：</strong></p><table><thead><tr><th>符号</th><th>等价于</th><th>含义</th></tr></thead><tbody><tr><td>[a-z]</td><td>[[:lower:]]</td><td>任意一个小写字母</td></tr><tr><td>[^a-z]</td><td>[^[:lower:]]</td><td>任意一个非小写字母</td></tr><tr><td>[A-Z]</td><td>[[:upper:]]</td><td>任意一个大写字母</td></tr><tr><td>[^A-Z]</td><td>[^[:upper:]]</td><td>任意一个非大写字母</td></tr><tr><td>[0-9]</td><td>[[:digit:]]</td><td>任意一个0-9的数字</td></tr><tr><td>[^0-9]</td><td>[^[:digit:]]</td><td>任意一个非0-9的数字</td></tr><tr><td>[a-zA-Z]</td><td>[[:alpha:]]</td><td>任意一个不区分大小写的字母</td></tr><tr><td>[^a-zA-Z]</td><td>[^[:alpha:]]</td><td>任意一个非字母的字符</td></tr><tr><td>[a-zA-Z0-9]</td><td>[[:alnum:]]</td><td>任意一个数字或字母</td></tr><tr><td>[^a-zA-Z0-9]</td><td>[^[:alnum:]]</td><td>任意一个非数字或字母的字符，比如符号</td></tr></tbody></table><p>其实，不仅[0-9]、[[:digit:]]可以匹配数字，还有一种简写的格式符号也能表示数字，比如”\d”就是表示十进制数字0-9。但是，<strong>并不是所有的正则表达式解释器都能够识别这些简写格式。</strong>因此，建议大家还是采用上述非简写格式来编写规则。</p><h2 id="基础正则中的分组与后向引用"><a href="#基础正则中的分组与后向引用" class="headerlink" title="基础正则中的分组与后向引用"></a><strong>基础正则中的分组与后向引用</strong></h2><p>在正则表达式中除了使用“位置匹配”、“连续次数匹配”和“常用符号”外，还可以使用分组和后向引用。那么，什么是分组？我们还是通过示例来说明，如下先创建一个测试文件test02：</p><p><img src="https://i.loli.net/2019/06/27/5d145c61b70a939419.jpg" alt></p><p>我们先使用如下规则进行匹配：</p><p><img src="https://i.loli.net/2019/06/27/5d145c83109d716151.jpg" alt></p><p>如上图，后面的匹配次数2只影响前面单字母b，所以上例中kkutysllbb、kkutysllbbb都会被匹配到。如果，我们想将上面示例中匹配要求改成连续匹配两次kkutysllb怎么办？这个时候，我们就需要使用分组的匹配的规则，将kkutysllb当做一个整体group来匹配。命令规则如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145d00bb06e90017.jpg" alt></p><p>上图中，<strong>\(  \)就表示分组，它将其中的内容看做一个整体。</strong>分组还可以嵌套，什么意思呢？接着往下看：</p><p><img src="https://i.loli.net/2019/06/27/5d145d47db40a55740.jpg" alt></p><p>如上图，为了说明分组嵌套，我们将上例中存在重复字母的地方做了单字母分组，实际环境时没必要这样分组。在上图中，我们将字母k和字母l做了两个子分组，且指定每个子分组各连续出现两次。同时，将kkutysllb做了一个大分组，包括上述的两个子分组，且指定整个大分组也连续出现两次。</p><p>我想我把分组的概念应该是说清楚了，接下来我们再看看什么是后向引用？之所以先讲分组，后介绍后向引用，是因为<strong>后向引用是以分组为前提的。</strong>还是以示例切入介绍，我们再创建一个测试文件test03，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145dfc9085770478.jpg" alt></p><p>现在，我们通过正则表达式将上述文件中的各行进行匹配，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145e2ff1cd886406.jpg" alt></p><p>H.\{4\}表示大写字母H的后面跟随了4个任意字符，其中”.”表示任意单个字符，前面已经说过了。通过上面的匹配规则，Hello和Hilll两个单词都会被匹配到，也就是上述测试文件三行都符合规则。我们现在有了新需求，需要找出kkutysllb单词前后两个词一致的行，通过上面的规则明显无法满足需求，这时候就需要用到后向引用了。如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145e8181a2c65417.jpg" alt></p><p>上图中，<strong>后面那个\1就表示后向引用，它的意思是kkutysllb后面的单词必须与前面\(H.\{4\}\)完全一致才符合规则。</strong>这究竟是什么道理呢？其实，<strong>\1的意思是等价整个正则中第1个分组的正则匹配到的结果。</strong>大白话就是，上例中整个正则只有一个分组\(H.\{4\}\)，当它与测试文件的第一行文本匹配时，会匹配到Hello，这时\1也为Hello。当它与测试文件第二行文本匹配时，会匹配到Hilll，这时\1也为HIlll。换句话说，\1引用整个正则中第1个分组的正则，同理，\2就引用了整个正则中第2个分组的正则，以此类推。。。如果你还是不明白，那就看下图理解吧，如果仍然不明白，那我也没招了。。。</p><p><img src="https://i.loli.net/2019/06/27/5d145eee25fe838328.jpg" alt></p><p>以上就是后向引用，再次强调，<strong>使用后向引用的前提是将需要引用的部分进行分组。</strong></p><p>在前述内容的例子中，我们使用了分组嵌套，那么在对分组嵌套后向引用时，到底哪个是分组1，哪个是分组n呢？我们来看下图：</p><p><img src="https://i.loli.net/2019/06/27/5d145f2da865898868.jpg" alt></p><p>在上图中，红色部分是一对分组，蓝色部分是另一对分组，当我们需要后向引用时，外层分组也就是红色分组是第1个分组，内层分组也就是蓝色分组是第2个分组。<strong>这是因为分组的顺序取决于分组符号的左侧部分的顺序</strong>，由于红色分组的左侧部分排在最前面，所以红色分组是第1个。</p><p>至此，分组和后向引用我们掰扯完了，我们还是保持国际惯例，对分组和后向引用做个总结，便于我们后续查询，如下：</p><ul><li><strong>\( \)：</strong>表示分组，我们可以将其中的内容当做一个整体，分组是可以嵌套的。</li><li><strong>\(ab\)：</strong>表示将ab当做一个整体去处理。</li><li><strong>\1：</strong>表示引用整个正则表达式中第1个分组中的正则匹配到的结果。</li><li><strong>\2：</strong>表示引用整个正则表达式中第2个分组中的正则匹配到的结果。</li></ul><h2 id="基础正则中的转义符"><a href="#基础正则中的转义符" class="headerlink" title="基础正则中的转义符"></a><strong>基础正则中的转义符</strong></h2><p>现在，我们来认识下正则中的常用符合，就是<strong>反斜杠”\“。</strong>反斜杠有什么用？我们还是先不解释，通过示例来描述。如下，我们还是创建的一个测试文件test04，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># cat test04</span></span><br><span class="line">base</span><br><span class="line">a1<span class="comment">#$</span></span><br><span class="line">ddd</span><br><span class="line">a-!@</span><br><span class="line">cccc</span><br><span class="line">a..</span><br></pre></td></tr></table></figure><p>如上，此时我们想匹配a..这行，利用我们前面了解的知识，规则可能会这样写。匹配条件如下：</p><p><img src="https://i.loli.net/2019/06/27/5d146343910bc63820.jpg" alt></p><p>上图中，虽然匹配结果包含了我们的需求，但是也多了其他三行，这让我们很不爽。主要是因为点符号”.”在基础正则中表示任意1个字符。因此，我们如果想要在匹配条件中匹配点点，就需要使用转移符号反斜杠“\”。<strong>转义符号“\”与正则中的符号结合起来就表示这个符号本身。</strong>因此，上面的需求我们可以使用如下的匹配条件：</p><p><img src="https://i.loli.net/2019/06/27/5d14607c70a2692622.jpg" alt></p><p>如上图，\.表示单个点符号，同理\*就表示单个星符号。在前面提到过，在基本正则中，\?表示其前面的字符出现0次或1次，\+表示前面的字符出现至少1次。那么，如果我们相匹配问号?和加号+呢？难道我们还要再在前面加一个反斜杠\\？或\\+，答案是否定的。<strong>在Linux中，如果想要在正则表达式中匹配问号?或加号+，只需要在匹配条件中直接输入?或+即可</strong>，大家可以自己尝试。但是，在某些时候，<strong>我们需要匹配反斜杠自身，这时需要在反斜杠前面再加上反斜杠就行。</strong></p><p>下面我们写个经常会使用的正则表达式—<strong>-匹配ifconfig输出中的各个网卡的ip地址。</strong>如下：</p><p><img src="https://i.loli.net/2019/06/27/5d14610017cd230432.jpg" alt></p><p>此时，我们需要首先对ipv4的地址做个分析，首先它是由4组三位数组成，且每组数字不大于255，前3组三位数后面都带一个点号”.”，那么我们的正则规则可以如下来写：</p><p><img src="https://i.loli.net/2019/06/27/5d14612e7203434521.jpg" alt></p><p>上图中，([0-2]{,1})([0-9]{1,2}).表示至少为1位数字，最多为3位数字，且首位数字不大于2，后面以点号“.”结尾的字段，然后将这部分作为一个分组连续匹配3次，最后这一段([0-2]{,1})([0-9]{1,2})也表示至少为1位数字，最多为3位数字，且首位数字不大于2。<strong>注意：上面的ip地址匹配并不精确，我们其实并没有限制每组数字范围在1-254。不过，没关系，当我们了解了扩展正则就能写一个更精确的ip地址匹配规则。</strong></p><p>至此，基础的正则的五朵金花也就介绍完了。在正则入门时我们说过，Linux中正则表达式分为基础正则表达式与扩展正则表达式，在下一篇文章中我们会介绍扩展正则，其实它们的用法都是相似的，而且写法也差不多，基础正则理解并掌握了，扩展正则几乎不费力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前一篇《正则表达式基础入门》中，我们总结了跟&lt;strong&gt;“位置匹配”&lt;/strong&gt;的相关的正则，它其实也是基础正则五朵金花之一，我们只是在入门介绍中单独拿出来进行科普。这篇文章我们就来掰扯掰扯基础正则的其它四朵金花。
    
    </summary>
    
      <category term="shell编程" scheme="https://kkutysllb.cn/categories/shell%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="DevOps" scheme="https://kkutysllb.cn/tags/DevOps/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-24-Linux原生网络虚拟化实践</title>
    <link href="https://kkutysllb.cn/2019/06/24/2019-06-24-Linux%E5%8E%9F%E7%94%9F%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%E5%AE%9E%E8%B7%B5/"/>
    <id>https://kkutysllb.cn/2019/06/24/2019-06-24-Linux原生网络虚拟化实践/</id>
    <published>2019-06-24T14:35:56.000Z</published>
    <updated>2019-06-24T15:30:56.477Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是虚拟化技术系列最后一片文章（容器技术作为专题单独介绍），介于后续OpenStack的Neutron在利用libvirt构建虚拟化网络服务时，利用了很多Linux虚拟网络功能（Linux内核中的虚拟网络设备以及其他网络功能）。因此，在网络虚拟化的收尾部分，特意安排一篇Linux虚拟网络的基础实践，一方面巩固大家对网络虚拟化的认知，另一方面也为后续学习OpenStack打好基础。下面，就来带大家了解一下Linux系统原生的与Neutron密切相关的虚拟网络设备。<a id="more"></a></p><h2 id="Tap"><a href="#Tap" class="headerlink" title="Tap"></a><strong>Tap</strong></h2><p>Tap这个概念大家应该不陌生，Tap交换机总听说过吧，信令平台采集探针数据收敛必备设备，是个纯二层设备。Linux中的tap属于虚拟网络设备，也是个二层虚拟设备。而在linux中所指的“设备”并不是我们实际生产或生活中常见路由器或交换机这类设备，它其实本质上往往是一个数据结构、内核模块或设备驱动这样含义。</p><p><strong>在linux中，tap和tun往往是会被并列讨论，tap位于二层，tun位于三层。</strong>为了说明这一点我们先看看下面linux用于描述tap和tun的数据结构内容：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Struct  tun_strruct  &#123;</span><br><span class="line">Char name [<span class="number">8</span>];      <span class="comment">//设备名称</span></span><br><span class="line">Unsigned  <span class="keyword">long</span>  flags；  <span class="comment">//区分tun和tap的标志位；</span></span><br><span class="line">Struct fasync_struct  *fasync;  <span class="comment">//文件异步调用通知接口</span></span><br><span class="line">Wait_queue_head_t read_wait;  <span class="comment">//消息队列读等待标志</span></span><br><span class="line">Struct net_device dev;   <span class="comment">//定义一个抽象网络设备</span></span><br><span class="line">Struct sk_buff_head txq;  <span class="comment">//定义一个缓存区</span></span><br><span class="line">Struct net_device_stats stats; <span class="comment">//定义一个网卡状态信息结构</span></span><br><span class="line">&#125;；</span><br></pre></td></tr></table></figure><p>从上面的数据结构可以看出，tap和tun的数据结构其实一样的，用来区别两者的其实就是flags。Tap从功能上说，位于二层也就是数据链路层，而二层的主要网络协议包括：</p><ul><li><strong>点对点协议（Point-to-Point Protocol）</strong></li><li><strong>以太网协议（Ethernet）</strong></li><li><strong>高级数据帧链路协议（High-Level Data Link Protocol）</strong></li><li><strong>帧中继（Frame Relay）</strong></li><li><strong>异步传输模式（Asynchronous Transfer Mode）</strong></li></ul><p><strong>而tap只是与以太网协议（Ethernet）对应，所以tap常被称为“虚拟以太网设备” 。</strong>要想使用Linux命令行操作一个tap，首先Linux得有tun模块（Linux使用tun模块实现了tun/tap），检查方法如下所示，输入modinfo tun命令如果有如下输出，就表示Linux内核具备tun模块。</p><p><img src="https://i.loli.net/2019/06/24/5d10e1d00f5a237937.jpg" alt></p><p>接下来，还要看看Linux内核是否加载tun模块。检查办法就是输入指令lsmod|grep tun，查看是否有如下图所示信息输出。如果有，就表示已加载，否则输入modprobe tun指令进行加载，然后再次输入lsmod|grep tun指令进行确认（<strong>这种加载-确认的操作方式是一种好的操作习惯，要养成！！！</strong>）</p><p><img src="https://i.loli.net/2019/06/24/5d10e2df2274078453.jpg" alt></p><p>当我们确认了Linux加载了tun模块后，我们还需要确认Linux是否有操作tun/tap的命令行工具tunctl。在Linux命令行输入tunctl help进行确认。</p><p><img src="https://i.loli.net/2019/06/24/5d10e2f95825263133.jpg" alt></p><p>如上图所示，如果提示command not found，在CentOS 6.x系统下直接输入yum install -y tunctl安装即可。但是，在CentOS 7.x系统下，需要先指定一个特定仓库，然后按照指定的仓库进行安装，否则会提示找不到tunctl rpm包。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先创建一个指定仓库</span></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/yum.repos.d/nux-misc.repo</span><br><span class="line">&gt; [nux-misc]</span><br><span class="line">&gt; name=Nux Misc</span><br><span class="line">&gt; baseurl=http://li.nux.ro/download/nux/misc/el7/x86_64/</span><br><span class="line">&gt; enabled=0</span><br><span class="line">&gt; gpgcheck=1</span><br><span class="line">&gt; gpgkey=http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro</span><br><span class="line">&gt; EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照指定仓库进行安装</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># yum -y --enablerepo=nux-misc install tunctl</span></span><br></pre></td></tr></table></figure><p>安装完后，再次执行tunctl help命令后，有如下信息输出，表示安装成功。</p><p><img src="https://i.loli.net/2019/06/24/5d10e355ee04934514.jpg" alt></p><p>具备了tun和tunctl后，我们就亦可以创建一个tap设备了，创建命令使用-t选项指定创建的tap设备名称。</p><p><img src="https://i.loli.net/2019/06/24/5d10e3731b61a65536.jpg" alt></p><p>此时，我们输入ip link list或ifconfig命令可以查看到刚才创建的tap设备llb_tap1。</p><p><img src="https://i.loli.net/2019/06/24/5d10e3934dcba94992.jpg" alt></p><p>通过上面的命令输出，我们发现这个tap设备还没有绑定ip，可以通过执行ip addr或ifconfig命令为该tap设备绑定一个ip地址，我们给它规划一个192.168.1.1/24的地址。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过ip addr命令绑定ip地址</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip addr local 192.168.1.1/24 dev llb_tap1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者使用ifconfig命令绑定ip地址</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig llb_tap1 192.168.1.1/24</span></span><br></pre></td></tr></table></figure><p><strong>注意：上述操作方式，个人推荐第一种，毕竟我们要与时俱进。。。</strong></p><p><img src="https://i.loli.net/2019/06/24/5d10e3ff2808d82295.jpg" alt></p><p>到此为止，一个tap设备就创建完毕了，在OpenStack的Neutron中创建的虚拟网络，虚拟机的vNIC与虚拟交换机之间就是通过一个tap桥接，而这个tap设备就是俗称的<strong>端口组</strong>，所以也就有了同一个端口组内的port归属同一个network的说法。后续通过测试用例，我们会再次讲述tap的用法。</p><h2 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a><strong>Namespace</strong></h2><p><strong>namespace是Linux虚拟网络的一个重要概念。</strong>传统的Linux的许多资源是全局的，比如进程ID资源。而<strong>namespace的目的首先就是将这些资源进行隔离</strong>。容器技术的两大实现基石之一就是namespace，它主要负责容器虚拟化中资源隔离，另一个就是cgroups，主要负责容器虚拟化中资源的管理。</p><p>在Linux中，可以在一个Host内创建许多namespace，于是那些原本是Linux全局的资源，就<strong>变成了namespace范围内的“全局”资源</strong>，而且不同namespace的资源互相不可见、彼此透明。Linux中会被namespace隔离的全局资源包括：</p><ul><li><strong>uts_ns：</strong>UTS是Unix Timesharing System的简称，包含内核名称、版本、底层体系结构等信息。</li><li><strong>ipc_ns：</strong>所有与进程通信（IPC）有关的信息。</li><li><strong>mnt_ns：</strong>当前装载的文件系统；</li><li><strong>pid_ns：</strong>有关进程的id信息。</li><li><strong>user_ns：</strong>资源配额信息。</li><li><strong>net_ns：</strong>网络信息。</li></ul><p>至于为什么这些全局资源会被隔离，是由Linux全局资源的数据结构定义决定的，如下所示，Linux的全局资源数据结构定在在文件<strong>nsproxy.h</strong>中。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Struct nsproxy &#123;</span><br><span class="line">Atomic_t count;</span><br><span class="line">Struct uts_namespace *uts_ns;</span><br><span class="line">Struct ipc_namespace *ipc_ns;</span><br><span class="line">Struct mnt_namespace *mnt_ns;</span><br><span class="line">Struct pid_namespace *pid_ns;</span><br><span class="line">Struct user_namespace *user_ns;</span><br><span class="line">Struct net *net_ns;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>从资源的隔离的角度来说，Linux namespace的示意图如下所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e5fd7f09157174.jpg" alt></p><p>上图表明，每个namespace里面将原本是全局资源的进行了隔离，彼此互相不可见。同时在Linux的Host或者每一个VM中，都各自有一套相关的全局资源。借助虚拟化的概念，我们可以将Linux Host的namespace称为root namespace，其它虚拟机namespace称为guest namespace。</p><p><strong>单纯从网络的视角来看，一个namespace提供了一份独立的网络协议栈。包括：网络设备接口、IPv4、IPv6、IP路由、防火墙规则、sockets等。</strong>一个Linux Device，无论是虚拟设备还是物理设备，只能位于一个namespace中，不同namespace的设备之间通过<strong>veth pair</strong>设备进行连接，veth pair设备也是一个虚拟网络设备，可以暂时将其理解为<strong>虚拟网线</strong>，后面会详细介绍。</p><p>Linux中namespace的操作命令是<strong>ip netns</strong>，这个命令的帮助如下所示：</p><p><img src="https://i.loli.net/2019/06/24/5d10e6656350639989.jpg" alt></p><p>首先，我们通过ip netns list命令查看一下当前系统的namespace列表信息，由于当前系统没有创建namespace，所以没有任何信息返回。然后，我们通过ip netns add命令添加一个namespace（llb_ns1）。最后，再通过ip netns list命令进行查看，结果如下图所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e68d9034919520.jpg" alt></p><p>接下来，我们可以通过ip link set名ing把刚才创建的虚拟tap设备迁移到llb_ns1中去。这时候，我们在root namespace下执行ifconfig命令就找不到llb_tap1设备了。</p><p><img src="https://i.loli.net/2019/06/24/5d10e6af6c3bd92998.jpg" alt></p><p>我们可以通过ip netns exec [NAME] CMD的方式操作，namespace中的设备，其中NAME参数为namespace名称，CMD是要执行的指令。</p><p><img src="https://i.loli.net/2019/06/24/5d10e6cbb0ab748635.jpg" alt></p><p>在namespace中给llb_tap1重新绑定ip地址。因为，llb_tap1在root namesapce中绑定的ip归属root namesapce的资源，当llb_tap1更改归属namespace时，原来的资源自然无法使用，必须重新配置。</p><p><img src="https://i.loli.net/2019/06/24/5d10e6f14e8f678387.jpg" alt></p><p>到此为止，namespace的创建就讲述完了，后续还会根据实例继续讲解其它用法。</p><h2 id="Veth-pair"><a href="#Veth-pair" class="headerlink" title="Veth pair"></a><strong>Veth pair</strong></h2><p>veth pair不是一个设备，而是一对设备，以连接两个虚拟以太端口，类似网线。操作veth pair，需要跟namespace一起配合，不然就没有意义。其实，veth pair设备本质上有三个接口，一端连接linux内核，另两端连接两个tap设备，是一个Y型结构，实现上更像一个HUB。如下图所示：</p><p><img src="https://i.loli.net/2019/06/24/5d10e73ac8a6350129.jpg" alt></p><p>两个namespace llb_ns1/llb_ns2中各有一个tap设备，组成veth pair，两者的ip如上图所示，测试两个ip进行互ping。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap1 type veth peer name llb_tap2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建两个namespace：llb_ns1，llb_ns2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将veth pair设备两端的两个tap设备移动到对应的namespace中</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap1 netns llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap2 netns llb_ns2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置两个tap设备的ip地址，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ifconfig llb_tap1 192.168.1.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ifconfig llb_tap2 192.168.1.2/24 up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试两个tap设备互ping</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ping -c 5 192.168.1.2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ping -c 5 192.168.1.1</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/24/5d10e7a926e6297494.jpg" alt></p><p>通过veth pair我们可以连接两个namespace中的两个tap设备，但是veth pair一端只能连接两个tap，如果是三个或多个namespace内的tap要实现互联怎么办？这时候，就只能使用Linux Bridge来完成。</p><h2 id="Linux-Bridge-vSwitch"><a href="#Linux-Bridge-vSwitch" class="headerlink" title="Linux Bridge/vSwitch"></a><strong>Linux Bridge/vSwitch</strong></h2><p>在Linux的网络部分，Bridge和Switch是一个概念。所以，大家把Linux Bridge看做一个交换机来理解就行，也就是2层的一个汇聚设备。Linux实现Bridge功能的是<strong>brctl模块</strong>。在命令行里敲一下brctl，如果能显示相关内容，则表示有此模块，否则还需要安装。安装命令是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum install -y bridge-utils</span></span><br></pre></td></tr></table></figure><p>Bridge本身的概念，我们通过一个综合测试用例来讲述Bridge的基本用法，同时也涵盖前面所述的几个概念：tap、namesapce、veth pair。实验拓扑图如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10e7ffbbd5e47582.jpg" alt></p><p>上图中，有4个namespace，每个namespace都有一个tap与交换机上一个tap口组成veth pair。这样4个namespace就通过veth pair及Bridge互联起来。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap1 type veth peer name tap1_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap2 type veth peer name tap2_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap3 type veth peer name tap3_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap4 type veth peer name tap4_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建namespace</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns3</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把tap设备移动到对应namespace中</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap1 netns llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap2 netns llb_ns2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap3 netns llb_ns3</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap4 netns llb_ns4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Bridge</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addbr br1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把相应的tap设备添加到br1的端口上</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap1_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap2_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap3_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap4_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置相应tap设备的ip，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ifconfig llb_tap1 192.168.1.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ifconfig llb_tap2 192.168.1.2/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns3 ifconfig llb_tap3 192.168.1.3/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns4 ifconfig llb_tap4 192.168.1.4/24 up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将Bridge及所有tap设备状态设置为up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set br1 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap1_peer up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap2_peer up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap3_peer up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap4_peer up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行互ping测试</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ping -c 3 192.168.1.4</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns3 ping -c 3 192.168.1.2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ping -c 3 192.168.1.1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns4 ping -c 3 192.168.1.2</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/24/5d10e86b3a0bf84209.jpg" alt></p><h2 id="Router"><a href="#Router" class="headerlink" title="Router"></a><strong>Router</strong></h2><p>Linux创建Router不像Bridge一样有一个直接命令，甚至连间接命令都没有。因为它自身就是一个路由器（Router）。不过Linux默认没有打开路由转发功能。可以用less /proc/sys/net/ipv4/ip_forward这个命令验证。这个命令就是查看一下这个文件（/proc/sys/net/ipv4/ip_forward）的内容。该内容是一个数字。如果是“0”，则表示没有打开路由功能。把“0”修改为“1”，就是打开了Linux的路由转发功能，修改命令为echo “1” &gt; /proc/sys/net/ipv4/ip_forward。这种打开方法，在机器重启以后就会失效了。一劳永逸的方法是修改配置文件“/etc/sysctl.conf”，将net.ipv4.ip_forward=0修改为1，保存后退出即可。</p><p><img src="https://i.loli.net/2019/06/24/5d10e8984235e46830.jpg" alt></p><p>下面，我们还是通过一个示例来直观感受一下Route的功能。实验拓扑图如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10e8b93b3d745185.jpg" alt></p><p>在上图中，llb_ns5/llb_tap5与llb_ns6/llb_tap6不在同一个网段中，中间需要经一个路由器进行转发才能互通。图中的Router是一个示意，其实就是Linux开通了路由转发功能。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap5 type veth peer name tap5_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap6 type veth peer name tap6_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建namespace：llb_ns5、llb_ns6</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns5</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将tap设备迁移到对应namespace中</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap5 netns llb_ns5</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap6 netns llb_ns6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置tap设备ip地址，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns5 ifconfig llb_tap5 192.168.100.5/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns6 ifconfig llb_tap6 192.168.200.5/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap5_peer 192.168.100.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap6_peer 192.168.200.1/24 up</span></span><br></pre></td></tr></table></figure><p>现在，我们先来做个ping测试，提示网络不可达，如下图所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e8f78316127513.jpg" alt></p><p>我们查看下llb_ns5的路由表信息，如下图所示，llb_ns5并没有到达192.168.200.0/24网段的路由表项，因此需要手工进行添加。</p><p><img src="https://i.loli.net/2019/06/24/5d10e914a452d47741.jpg" alt></p><p>在llb_ns5中添加到192.168.200.0/24网段静态路由信息，同时在llb_ns6中添加到192.168.100.0/24回程路由信息。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在llb_ns5中添加到192.168.200.0/24的静态路由</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns5 route add -net 192.168.200.0 netmask 255.255.255.0 gw 192.168.100.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同理，在llb_ns6中添加到192.168.100.0/24的回程路由</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns6 route add -net 192.168.100.0 netmask 255.255.255.0 gw 192.168.200.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看llb_ns5、llb_ns6的路由信息</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns5 route -ne</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns6 route -ne</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/24/5d10e957464e269449.jpg" alt></p><p>再次进行ping尝试，可以ping通，结果如下图所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e97fb5eb697934.jpg" alt></p><h2 id="Tun"><a href="#Tun" class="headerlink" title="Tun"></a><strong>Tun</strong></h2><p>在前面tap的时候就介绍过tun和tap其实同一数据结构，只是通过flags标志位来区分。Tap是二层虚拟以太网设备，那么<strong>tun就是三层的点对点的虚拟隧道设备</strong>。也就是说Linux原生支持三层隧道技术。至于什么是隧道技术？在上一篇数据中心的网络虚拟化技术有详细介绍，这里不再赘述。</p><p><strong>Linux一共原生支持5种三层隧道技术，分别是：</strong></p><ul><li><strong>ipip：</strong>IP in IP，在IPv4报文的基础上再封装一个IPv4报文头，属于IPv4 in IPv4。</li><li><strong>GRE：</strong>通用路由封装（Generic Routing Encapsulation），定义在任意一种网络层协议上封装任意一个其他网络层协议的协议，属于IPv4/IPv6 over IPv4。</li><li><strong>sit：</strong>与ipip类似，只不过用IPv4报文头封装一个IPv6报文，属于IPv6 over IPv4。</li><li><strong>isatap：</strong>站内自动隧道寻址协议，一般用于IPv4网络中的IPv6/IPv4节点间的通信。</li><li><strong>vti：</strong>全称是Virtual Tunnel Interface，为IPSec隧道提供一个可路由的接口类型。</li></ul><p>国际惯例，我们还是通过一个具体的测试实例来理解tun。实验拓扑图如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10e9a968d9b77681.jpg" alt></p><p>上图的tun1、tun2，如果我们先忽略的话，剩下的就是我们在前面讲述过的内容。测试用例的第一步，就是使图中的tap7与tap8配置能通，借助上面route的配置，这里我们不再重复。当tap7和tap8配通以后，如果我们不把图中的tun1和tun2暂时当做tun设备，而是当做两个“死”设备（比如当做是两个不做任何配置的网卡），那么这个时候tun1和tun2就像两个孤岛，不仅互相不通，而且跟tap7、tap8也没有关系。因此，我们就需要对tun1、tun2做相关配置，以使这两个两个孤岛能够互相通信。我们以ipip tunnel为例进行配置。</p><p>首先我们要加载ipip模块，Linux默认是没有加载这个模块的。通过命令行lsmod|grep ip进行查看，如果没有加载，可以通过命令modprobe ipip来加载ipip模块。具体过程参见tap部分，这里不再赘述。加载了ipip模块以后，我们就可以创建tun，并且给tun绑定一个ipip隧道。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap7 type veth peer name tap7_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap8 type veth peer name tap8_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建namespace：llb_ns7，llb_ns8</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns7</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将tap设备移动到对应的namespace</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap7 netns llb_ns7</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap8 netns llb_ns8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置ip地址，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ifconfig llb_tap7 192.168.100.6/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ifconfig llb_tap8 192.168.200.6/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap7_peer 192.168.100.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap8_peer 192.168.200.1/24 up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置路由和回程路由</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 route add -net 192.168.200.0 netmask 255.255.255.0 gw 192.168.100.1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 route add -net 192.168.100.0 netmask 255.255.255.0 gw 192.168.200.1</span></span><br></pre></td></tr></table></figure><p>测试互通，结果如下，表明底层underlay网络可达。</p><p><img src="https://i.loli.net/2019/06/24/5d10ea08d124792954.jpg" alt></p><p>下来，我们创建隧道设备tun，构建overlay网络，代码如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在llb_ns7和llb_ns8中创建tun1、tun2和ipip tunnel</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ip tunnel add tun1 mode ipip remote 192.168.200.6 local 192.168.100.6</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ip tunnel add tun2 mode ipip remote 192.168.100.6 local 192.168.200.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活tun设备，并配置ip地址</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ip link set tun1 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ip link set tun2 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ip addr add 10.10.10.50 peer 10.10.20.50 dev tun1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ip addr add 10.10.20.50 peer 10.10.10.50 dev tun2</span></span><br></pre></td></tr></table></figure><p>互通测试，结果如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10eaa6b8ab112350.jpg" alt></p><p><strong>把上面的命令行脚本中的ipip换成gre，其余不变，就创建了一个gre隧道的tun设备对。</strong>因为我们说tun是一个设备，那么我们可以通过ifconfig这个命令，来看看这个设备的信息，代码如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10eb1c828ed42461.jpg" alt></p><p>可以看到，tun1是一个ipip tunnel的一个端点，IP是10.10.10.50，其对端IP是10.10.20.50。 再看路由表信息，代码如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10eb4d248e961131.jpg" alt></p><p>图中的内容告诉我们，到达目的地10.10.10.50的路由是一个直连路由直接从tun1出去即可，其实tun1和tun2就是overlay网络的VTEP。</p><p>至此，Linux原生网络虚拟化介绍完毕。其实，还有个iptables，一般我们把它理解为Linux的包过滤防火墙，其实当其所属服务器处于网络报转发的中间节点时，它也是一个网络防火墙。它的“防火”机制就是通过一个个链结合策略表完成，而那一个个链其本质就是一个三层的虚拟网络设备。由于篇幅原因，iptables会放在Linux常用运维工具分类中介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是虚拟化技术系列最后一片文章（容器技术作为专题单独介绍），介于后续OpenStack的Neutron在利用libvirt构建虚拟化网络服务时，利用了很多Linux虚拟网络功能（Linux内核中的虚拟网络设备以及其他网络功能）。因此，在网络虚拟化的收尾部分，特意安排一篇Linux虚拟网络的基础实践，一方面巩固大家对网络虚拟化的认知，另一方面也为后续学习OpenStack打好基础。下面，就来带大家了解一下Linux系统原生的与Neutron密切相关的虚拟网络设备。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-24-打造文件实时同步架构之rsync篇</title>
    <link href="https://kkutysllb.cn/2019/06/24/2019-06-24-%E6%89%93%E9%80%A0%E6%96%87%E4%BB%B6%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E4%B9%8Brsync%E7%AF%87/"/>
    <id>https://kkutysllb.cn/2019/06/24/2019-06-24-打造文件实时同步架构之rsync篇/</id>
    <published>2019-06-24T10:19:24.000Z</published>
    <updated>2019-06-24T10:50:42.652Z</updated>
    
    <content type="html"><![CDATA[<p>rsync是可以实现增量备份的工具。配合任务计划，rsync能实现定时或间隔同步，配合inotify或sersync，可以实现触发式的实时同步。事实上，rsync有一套自己的算法，其算法原理以及rsync对算法实现的机制可能比想象中要复杂一些。平时使用rsync实现简单的备份、同步等功能足以，没有多大必要去深究这些原理性的内容。如果你对这些原理感兴趣，可以通过rsync命令的man文档、并且借助”-vvvv”分析rsync执行过程，结合rsync的算法原理去深入理解。本篇文章由于篇幅有限，只是介绍rsync的使用方法和它常用的功能，以及rsync和sersync如何配合打造文件实时同步架构。<a id="more"></a></p><p><img src="https://i.loli.net/2019/06/24/5d10a4fd1a40892780.jpg" alt></p><h2 id="rsync文件同步机制简介"><a href="#rsync文件同步机制简介" class="headerlink" title="rsync文件同步机制简介"></a><strong>rsync文件同步机制简介</strong></h2><p>rsync是一款开源的、快速的、多功能的、可实现全量及增量的本地或远程数据镜像同步备份的优秀工具。rsync适用于Unix/Linux/Windows等多种操作系统平台。</p><p><strong>rsync可以实现scp的远程拷贝(rsync不支持远程到远程的拷贝，但scp支持)、cp的本地拷贝、rm删除和”ls -l”显示文件列表等功能。</strong>但需要注意的是，r<strong>sync的最终目的或者说其原始目的是实现两端主机的文件同步，因此实现的scp/cp/rm等功能仅仅只是同步的辅助手段，且rsync实现这些功能的方式和这些命令是不一样的。</strong></p><p>rsync的目的是实现本地主机和远程主机上的文件同步(<strong>包括本地推到远程，远程拉到本地两种同步方式)</strong>，也可以实现本地不同路径下文件的同步，但不能实现远程路径1到远程路径2之间的同步，这个功能scp是可以实现的。</p><p>不考虑rsync的实现细节，就文件同步而言，涉及了<strong>源文件和目标文件的概念，还涉及了以哪边文件为同步基准。</strong>例如，想让目标主机上的文件和本地文件保持同步，则是以本地文件为同步基准，将本地文件作为源文件推送到目标主机上。反之，如果想让本地主机上的文件和目标主机上的文件保持同步，则目标主机上的文件为同步基准，实现方式是将目标主机上的文件作为源文件拉取到本地。当然，要保持本地的两个文件相互同步，rsync也一样能实现，这就像Linux中cp命令一样，以本地某文件作为源，另一文件作为目标文件，但是这两者实现的本质是完全不同的。</p><p>既然是文件同步，<strong>在同步过程中必然会涉及到源和目标两文件之间版本控制的问题</strong>，例如是否要删除源主机上没有但目标上多出来的文件，目标文件比源文件更新(newer than source)时是否仍要保持同步，遇到软链接时是拷贝软链接本身还是拷贝软链接所指向的文件，目标文件已存在时是否要先对其做个备份等等。</p><p><strong>rsync同步过程中由两部分模式组成：决定哪些文件需要同步的检查模式</strong>以及<strong>文件同步时的同步模式。</strong></p><p><strong>1）检查模式是指按照指定规则来检查哪些文件需要被同步</strong>，例如哪些文件是明确被排除不传输的。默认情况下，rsync使用<strong>“quick check”算法</strong>快速检查源文件和目标文件的大小、mtime(修改时间)是否一致，如果不一致则需要传输。当然，也可以通过在rsync命令行中指定某些选项来改变quick check的检查模式，比如”–size-only”选项表示”quick check”将仅检查文件大小不同的文件作为待传输文件。<strong>rsync支持非常多的选项，其中检查模式的自定义性是非常有弹性的。</strong></p><p><strong>2）同步模式是指在文件确定要被同步后，在同步过程发生之前要做哪些额外工作。</strong>例如：上文所说的是否要先删除源主机上没有但目标主机上有的文件，是否要先备份已存在的目标文件，是否要追踪链接文件等额外操作。<strong>rsync也提供非常多的选项使得同步模式变得更具弹性。</strong></p><p>相对来说，为rsync手动指定同步模式的选项更常见一些，只有在有特殊需求时才指定检查模式，因为大多数检查模式选项都可能会影响rsync的性能。</p><h2 id="rsync的三种工作模式"><a href="#rsync的三种工作模式" class="headerlink" title="rsync的三种工作模式"></a><strong>rsync的三种工作模式</strong></h2><p>rsync命令有三种常见模式，具体如下：</p><p><strong>1）本地模式：</strong>本地文件系统上实现同步，命令行语法格式如下：</p><ul><li><ul><li>rsync [option] [SRC] [DEST]</li><li>rsync [选项] [源文件] [目标文件]</li></ul></li></ul><p><strong>2）通过远程Shell访问模式：</strong>本地主机使用远程shell和远程主机通信，命令行语法格式如下：</p><ul><li><p><strong>拉取（Pull）:</strong></p></li><li><ul><li><ul><li>rsync [option] [USER@]HOST:SRC [DEST]</li><li>rsync [选项] 用户@主机:源文件 [目标文件]</li></ul></li></ul></li><li><p><strong>推送（Push）：</strong></p></li><li><ul><li><ul><li>rsync [option] [SRC] [USER@]HOST:DEST</li><li>rsync [选项] [源文件] 用户@主机:目标文件</li></ul></li></ul></li></ul><p><strong>3）rsync守护进程模式：</strong>本地主机通过网络套接字连接远程主机上的rsync daemon，命令行语法格式如下：</p><ul><li><p><strong>拉取（Pull）：</strong></p></li><li><ul><li><ul><li>rsync [option] [USER@]HOST::SRC [DEST]</li><li>rsync [选项] 用户@主机::源文件 [目标文件]</li><li>rsync [option] rsync://[USER@]HOST[:PORT]/SRC [DEST]</li><li>rsync [选项] rsync://用户@主机:端口/源文件 [目标文件]</li></ul></li></ul></li><li><p><strong>推送（Push）：</strong></p></li><li><ul><li><ul><li>rsync [option] SRC [USER@]HOST::DEST</li><li>rsync [选项] [源文件] 用户@主机::目标文件</li><li>rsync [option] SRC rsync://[USER@]HOST[:PORT]/DEST</li><li>rsync [选项] [源文件] rsync://用户@主机:端口/目标文件</li></ul></li></ul></li></ul><p>前两者的本质是通过管道通信，而第三种则是让远程主机上运行rsync服务，使其监听在一个端口上，等待客户端的连接。</p><p>其实，<strong>还有第四种工作方式：通过远程shell也能临时启动一个rsync daemon</strong>，它不同于方式3，它不要求远程主机上事先启动rsync服务，而是临时派生出rsync daemon，属于单用途的一次性daemon，仅用于临时读取daemon的配置文件，当此次rsync同步完成，远程shell启动的rsync daemon进程也会自动消逝。此通信方式的命令行语法格式同方式3，但<strong>要求options部分必须明确指定”–rsh”选项或其短选项”-e”。</strong></p><p><strong>上述语法格式中，如果仅有一个SRC或DEST参数，则将以类似于”ls -l”的方式列出源文件列表(只有一个路径参数，总会认为是源文件)，而不是复制文件。</strong></p><p>另外，使用rsync一定要注意的一点是，<strong>源路径如果是一个目录的话，带上尾随斜线和不带尾随斜线是不一样的，不带尾随斜线表示的是整个目录包括目录本身，带上尾随斜线表示的是目录中的文件，不包括目录本身。</strong></p><p>由于rsync支持一百多个选项，所以此处只介绍几个常用选项。完整的选项说明以及rsync的使用方法参见系统的man rsync帮助文档。<strong>其中，重要参数选项如下：</strong></p><ul><li><strong>-v：</strong>显示rsync过程中详细信息。可以使用”-vvvv”获取更详细信息。</li><li><strong>-P：</strong>显示文件传输的进度信息。(实际上”-P”=”–partial –progress”，其中的”–progress”才是显示进度信息的)。</li><li><strong>-n –dry-run：</strong>仅测试传输，而不实际传输。常和”-vvvv”配合使用来查看rsync是如何工作的。</li><li><strong>-a –archive：</strong>归档模式，表示递归传输并保持文件属性。等同于”-rtopgDl”。</li><li><strong>-r –recursive：</strong>递归到目录中去。</li><li><strong>-t –times：</strong>保持mtime属性。<strong>强烈建议任何时候都加上”-t”，否则目标文件mtime会设置为系统时间，导致下次更新检查出mtime不同从而导致增量传输无效。</strong></li><li><strong>-o –owner：</strong>保持owner属性(属主)。</li><li><strong>-g –group：</strong>保持group属性(属组)。</li><li><strong>-p –perms：</strong>保持perms属性(权限，不包括特殊权限)。</li><li><strong>-D：</strong>是”–device –specials”选项的组合，即也拷贝设备文件和特殊文件。</li><li><strong>-l –links：</strong>如果文件是软链接文件，则拷贝软链接本身而非软链接所指向的对象。</li><li><strong>-z：</strong>传输时进行压缩提高效率。</li><li><strong>-R –relative：</strong>使用相对路径。意味着将命令行中指定的全路径而非路径最尾部的文件名发送给服务端，包括它们的属性。</li><li><strong>–size-only：</strong>默认算法是检查文件大小和mtime不同的文件，使用此选项将只检查文件大小。</li><li><strong>-u –update：</strong>仅在源mtime比目标已存在文件的mtime新时才拷贝。注意，该选项是接收端判断的，不会影响删除行为。</li><li><strong>-d –dirs：</strong>以不递归的方式拷贝目录本身。默认递归时，如果源为”dir1/file1”，则不会拷贝dir1目录，使用该选项将拷贝dir1但不拷贝file1。</li><li><strong>–max-size：</strong>限制rsync传输的最大文件大小。可以使用单位后缀，还可以是一个小数值(例如：”–max-size=1.5m”)</li><li><strong>–min-size：</strong>限制rsync传输的最小文件大小。这可以用于禁止传输小文件或那些垃圾文件。</li><li><strong>–exclude：</strong>指定排除规则来排除不需要传输的文件。</li><li><strong>–delete：</strong>以SRC为主，对DEST进行同步。多则删之，少则补之。注意”–delete”是在接收端执行的，所以它是在exclude/include规则生效之后才执行的。</li><li><strong>-b –backup ：</strong>对目标上已存在的文件做一个备份，备份的文件名后默认使用”~”做后缀。</li><li><strong>–backup-dir：</strong>指定备份文件的保存路径。不指定时默认和待备份文件保存在同一目录下。</li><li><strong>-e ：</strong>指定所要使用的远程shell程序，默认为ssh。</li><li><strong>–port：</strong>连接daemon时使用的端口号，默认为873端口。</li><li><strong>–password-file：</strong>daemon模式时的密码文件，可以从中读取密码实现非交互式。注意，这不是远程shell认证的密码，而是rsync模块认证的密码。</li><li><strong>-W –whole-file：</strong>rsync将不再使用增量传输，而是全量传输。在网络带宽高于磁盘带宽时，该选项比增量传输更高效。</li><li><strong>–existing：</strong>要求只更新目标端已存在的文件，目标端还不存在的文件不传输。注意，使用相对路径时如果上层目录不存在也不会传输。</li><li><strong>–ignore-existing：</strong>要求只更新目标端不存在的文件。和”–existing”结合使用有特殊功能，见下文示例。</li><li><strong>–remove-source-files：</strong>要求删除源端已经成功传输的文件。</li></ul><p><strong>虽然选项非常多，但最常用的选项组合是”avz”，即压缩和显示部分信息，并以归档模式传输。</strong></p><h2 id="rsync本地和远程shell使用示例"><a href="#rsync本地和远程shell使用示例" class="headerlink" title="rsync本地和远程shell使用示例"></a><strong>rsync本地和远程shell使用示例</strong></h2><p>以下示例既可以通过shell远程链接的工作模式实现，也可以在本地模式实现，具体的实现方式不同，应用的实际场景就不同，这里只是作为示例讲解，在实际使用时要灵活变通。</p><p><strong>1）将/etc/hosts文件拷贝到本地/tmp目录下</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync /etc/hosts /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l /tmp/hosts</span></span><br><span class="line">-rw-r--r-- 1 root root 158 Jun  3 14:13 /tmp/hosts</span><br></pre></td></tr></table></figure><p><strong>2）将/etc/cron.d目录拷贝到/tmp目录下</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r /etc/cron.d /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -ld /tmp/cr*</span></span><br><span class="line">drwxr-xr-x 2 root root 4096 Jun  3 14:14 /tmp/cron.d</span><br></pre></td></tr></table></figure><p><strong>3）将/etc/cron.d目录拷贝到/tmp下，但要求在/tmp下也生成etc子目录</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用-R选项，利用相对路径拷贝机制</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -R -r /etc/cron.d /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree -l /tmp/etc/</span></span><br><span class="line">/tmp/etc/</span><br><span class="line">└── cron.d</span><br><span class="line">├── 0hourly</span><br><span class="line">├── raid-check</span><br><span class="line">└── sysstat</span><br><span class="line">1 directory, 3 files</span><br></pre></td></tr></table></figure><p>其中”-R”选项表示使用相对路径，<strong>此相对路径是以目标目录为根的。</strong>对于上面的示例，表示在目标上的/tmp下创建etc/cron.d目录，即/tmp/etc/cron.d，etc/cron.d的根”/“代表的就是目标/tmp。</p><p><strong>4）如果要拷贝的源路径较长，但只想在目标主机上保留一部分目录结构，例如，要拷贝/var/log/anaconda/*到/tmp下，但只想在/tmp下保留从log开始的目录，这时可以使用一个点代表相对路径的起始位置即可，也就是将长目录进行划分。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -R -r /var/./log/anaconda /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree -l /tmp/log/</span></span><br><span class="line">/tmp/<span class="built_in">log</span>/</span><br><span class="line">└── anaconda</span><br><span class="line">├── anaconda.log</span><br><span class="line">├── ifcfg.log</span><br><span class="line">├── journal.log</span><br><span class="line">├── ks-script-n7L3eP.log</span><br><span class="line">├── ks-script-v6NMcO.log</span><br><span class="line">├── packaging.log</span><br><span class="line">├── program.log</span><br><span class="line">├── storage.log</span><br><span class="line">└── syslog</span><br><span class="line">1 directory, 9 files</span><br></pre></td></tr></table></figure><p><strong>这种方式，从点开始的目录都是相对路径，其相对根目录为目标路径。</strong>所以对于上面的示例，将在目标上创建/tmp/log/anaconda/*。</p><p><strong>5）对远程目录下已存在的文件做一个备份</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]# rsync -R -r --backup /var/./log/anaconda /tmp</span><br><span class="line">[root@c7-test01 ~]# ls -l /tmp/log/anaconda/</span><br><span class="line">total 4880</span><br><span class="line">-rw------- 1 root root   10945 Jun  3 14:22 anaconda.log</span><br><span class="line">-rw------- 1 root root   10945 Jun  3 14:18 anaconda.log~</span><br><span class="line">-rw------- 1 root root   18216 Jun  3 14:22 ifcfg.log</span><br><span class="line">-rw------- 1 root root   18216 Jun  3 14:18 ifcfg.log~</span><br><span class="line">-rw------- 1 root root 1711915 Jun  3 14:22 journal.log</span><br><span class="line">-rw------- 1 root root 1711915 Jun  3 14:18 journal.log~</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-n7L3eP.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:18 ks-script-n7L3eP.log~</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-v6NMcO.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:18 ks-script-v6NMcO.log~</span><br><span class="line">-rw------- 1 root root  312693 Jun  3 14:22 packaging.log</span><br><span class="line">-rw------- 1 root root  312693 Jun  3 14:18 packaging.log~</span><br><span class="line">-rw------- 1 root root   29448 Jun  3 14:22 program.log</span><br><span class="line">-rw------- 1 root root   29448 Jun  3 14:18 program.log~</span><br><span class="line">-rw------- 1 root root   78942 Jun  3 14:22 storage.log</span><br><span class="line">-rw------- 1 root root   78942 Jun  3 14:18 storage.log~</span><br><span class="line">-rw------- 1 root root  320770 Jun  3 14:22 syslog</span><br><span class="line">-rw------- 1 root root  320770 Jun  3 14:18 syslog~</span><br></pre></td></tr></table></figure><p><strong>通过–backup参数可以在目标目录下，对已存在的文件就被做一个备份，备份文件默认使用”~”做后缀，可以使用”–suffix”指定备份后缀。</strong>同时，可以使用<strong>“–backup-dir”指定备份文件保存路径，但要求保存路径必须存在。指定备份路径后，默认将不会加备份后缀，除非使用”–suffix”显式指定后缀，如”–suffix=~”</strong>。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -R -r --backup-dir=/tmp/backup --backup /var/./log/anaconda /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l /tmp/backup/log/anaconda/</span></span><br><span class="line">total 2440</span><br><span class="line">-rw------- 1 root root   10945 Jun  3 14:22 anaconda.log</span><br><span class="line">-rw------- 1 root root   18216 Jun  3 14:22 ifcfg.log</span><br><span class="line">-rw------- 1 root root 1711915 Jun  3 14:22 journal.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-n7L3eP.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-v6NMcO.log</span><br><span class="line">-rw------- 1 root root  312693 Jun  3 14:22 packaging.log</span><br><span class="line">-rw------- 1 root root   29448 Jun  3 14:22 program.log</span><br><span class="line">-rw------- 1 root root   78942 Jun  3 14:22 storage.log</span><br><span class="line">-rw------- 1 root root  320770 Jun  3 14:22 syslog</span><br></pre></td></tr></table></figure><p><strong>6）源地址带与不带斜线（/）的区别的例子</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建实验环境</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># mkdir -p /data1/&#123;test1,test2&#125;/data2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果源目录的末尾有斜线，就会复制目录内的内容，而不是复制目录本身</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av /data1/ /data2/  # 本地同步</span></span><br><span class="line">sending incremental file list</span><br><span class="line">created directory /data2</span><br><span class="line">./</span><br><span class="line">test1/</span><br><span class="line">test1/data2/</span><br><span class="line">test2/</span><br><span class="line">test2/data2/</span><br><span class="line"></span><br><span class="line">sent 149 bytes  received 64 bytes  426.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果源目录没有斜线，则会复制目录本身及目录下的内容</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av /data1 /data2 # 本地同步</span></span><br><span class="line">sending incremental file list</span><br><span class="line">data1/</span><br><span class="line">data1/test1/</span><br><span class="line">data1/test1/data2/</span><br><span class="line">data1/test2/</span><br><span class="line">data1/test2/data2/</span><br><span class="line"></span><br><span class="line">sent 155 bytes  received 36 bytes  382.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看/data2目录下文件信息</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ls -l /data2</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 4 root root 32 Apr 28 01:27 data1</span><br><span class="line">drwxr-xr-x 3 root root 19 Apr 28 01:27 test1</span><br><span class="line">drwxr-xr-x 3 root root 19 Apr 28 01:27 test2</span><br></pre></td></tr></table></figure><p><strong>源路径如果是一个目录的话，带上尾随斜线和不带尾随斜线是不一样的，不带尾随斜线表示的是整个目录包括目录本身，带上尾随斜线表示的是目录中的文件，不包括目录本身。</strong></p><p><strong>7）删除文件的特殊例子</strong></p><p>当一个目录下有几十万或十几万个文件，使用rsync的–deleye选项可以很快进行删除。这里注意一点，是删除目录下的所有文件，而不是目录本身。</p><p>选项–delete使tmp目录内容和空目录保持一致，不同的文件及目录将会被删除，即null里有什么内容，tmp里就有什么内容，null里没有的，而tmp里有的就必须要删除，因为null目录为空，因此此命令会删除/tmp目录中的所有内容。<strong>使用”–delete”选项后，接收端的rsync会先删除目标目录下已经存在，但源端目录不存在的文件。也就是”多则删之，少则补之”。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看tmp目录下文件信息</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ls -l /tmp</span></span><br><span class="line">total 4</span><br><span class="line">drwxr-xr-x 3 root root  23 Apr 28 00:22 home</span><br><span class="line">-rw-r--r-- 1 root root 187 Apr 22 22:56 hosts</span><br><span class="line">drwx------ 3 root root  17 Apr 27 21:43 systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806</span><br><span class="line">。。。</span><br><span class="line">drwx------ 2 root root   6 Apr 23 18:24 vmware-root_9660-3101179142</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个空目录null</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># mkdir /null</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除tmp目录下所有文件和子目录</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av --delete /null/ /tmp/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting vmware-root_9660-3101179142/</span><br><span class="line">deleting vmware-root_9609-4121731445/</span><br><span class="line">deleting vmware-root_9604-3101310240/</span><br><span class="line">deleting vmware-root_9573-4146439782/</span><br><span class="line">deleting vmware-root_9458-2857896559/</span><br><span class="line">deleting vmware-root_9456-2866351085/</span><br><span class="line">。。。</span><br><span class="line">deleting hosts</span><br><span class="line">./</span><br><span class="line"></span><br><span class="line">sent 47 bytes  received 3,911 bytes  7,916.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看/tmp目录下文件和子目录信息</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ls -l /tmp</span></span><br><span class="line">total 0</span><br></pre></td></tr></table></figure><p><strong>8）”–existing”和”–ignore-existing”使用示例</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建测试环境，测试环境的结构如下：</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree /tmp/&#123;a,b&#125;</span></span><br><span class="line">/tmp/a</span><br><span class="line">├── bashrc</span><br><span class="line">├── c</span><br><span class="line">│   └── find</span><br><span class="line">├── fstab</span><br><span class="line">├── profile</span><br><span class="line">└── rc.local</span><br><span class="line">/tmp/b</span><br><span class="line">├── crontab</span><br><span class="line">├── fstab</span><br><span class="line">├── profile</span><br><span class="line">└── rc.local</span><br><span class="line">1 directory, 9 files</span><br></pre></td></tr></table></figure><p><strong>“–existing”是只更新目标端已存在的文件。</strong>目前/tmp/{a,b}目录中内容如上，bashrc在a目录中，crontab在b目录中，且a目录中多了一个c子目录。如下结果只有3个目标上已存在的文件被更新了，由于目标上没有c目录，所以c目录中的文件也没有进行传输。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --existing /tmp/a/ /tmp/b </span></span><br><span class="line">sending incremental file list</span><br><span class="line">fstab</span><br><span class="line">profile</span><br><span class="line">rc.local</span><br><span class="line">sent 2972 bytes  received 70 bytes  6084.00 bytes/sec</span><br><span class="line">total size is 204755  speedup is 67.31</span><br></pre></td></tr></table></figure><p>而<strong>“–ignore-existing”是更新目标端不存在的文件。</strong>如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --ignore-existing /tmp/a/ /tmp/b</span></span><br><span class="line">sending incremental file list</span><br><span class="line">bashrc</span><br><span class="line">c/</span><br><span class="line">c/find</span><br><span class="line">sent 231 bytes  received 62 bytes  586.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br></pre></td></tr></table></figure><p><strong>“–existing”和”–ignore-existing”结合使用时，有个特殊功效，当它们结合”–delete”使用的时候，文件不会传输，但会删除receiver端额外多出的文件。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建实验环境，如下</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree &#123;a,b&#125;</span></span><br><span class="line">a</span><br><span class="line">├── stu01</span><br><span class="line">├── stu02</span><br><span class="line">├── stu03</span><br><span class="line">└── stu04</span><br><span class="line">b</span><br><span class="line">└── a.log</span><br><span class="line">0 directories, 5 files</span><br><span class="line"><span class="comment"># 使用-n选项测试传输，并没有实际效果</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -nrv --delete a/ b/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting a.log</span><br><span class="line">stu01</span><br><span class="line">stu02</span><br><span class="line">stu03</span><br><span class="line">stu04</span><br><span class="line"></span><br><span class="line">sent 104 bytes  received 33 bytes  274.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00 (DRY RUN)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --existing &amp;&amp; --ignore-existing &amp;&amp; --delete结合使用，是删除接收端（目的端）多余的文件，且不会传输文件</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -nrv --existing --ignore-existing --delete a/ b/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting a.log</span><br><span class="line"></span><br><span class="line">sent 92 bytes  received 21 bytes  226.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00 (DRY RUN)</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -nrv --existing --ignore-existing --delete b/ a/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting stu04</span><br><span class="line">deleting stu03</span><br><span class="line">deleting stu02</span><br><span class="line">deleting stu01</span><br><span class="line"></span><br><span class="line">sent 58 bytes  received 48 bytes  212.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00 (DRY RUN)</span><br></pre></td></tr></table></figure><p>实际上，<strong>“–existing”和”–ingore-existing”是传输规则，只会影响receiver要求让sender传输的文件列表，属于传输模式。而receiver决定哪些文件在传输之前如何处理属于同步模式，</strong>所以各种同步规则，比如：”–delete”等操作都不会被这两个选项影响。</p><p><strong>9）”–remove-source-files”删除源端文件</strong></p><p><strong>使用该选项后，源端已经更新成功的文件都会被删除，源端所有未传输或未传输成功的文件都不会被移除。</strong>未传输成功的原因有多种，如exclude排除了，”quick check”未选则该文件，传输中断等等。总之，显示在”rsync -v”被传输列表中的文件都会被移除。如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --remove-source-files a/ b/ .</span></span><br><span class="line">sending incremental file list</span><br><span class="line">a.log</span><br><span class="line">stu01</span><br><span class="line">stu02</span><br><span class="line">stu03</span><br><span class="line">stu04</span><br><span class="line">sent 331 bytes  received 151 bytes  964.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l &#123;a/,b/&#125;</span></span><br><span class="line">a/:</span><br><span class="line">total 0</span><br><span class="line">b/:</span><br><span class="line">total 0</span><br></pre></td></tr></table></figure><p><strong>上述显示出来的文件在源端全部被删除。</strong></p><p><strong>10）”–exclude”排除规则</strong></p><p>使用”–exclude”选项指定排除规则，排除那些不需要传输的文件。如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --exclude=/tmp/a/* /tmp/a/ /tmp/b/* a/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">c/</span><br><span class="line">sent 81 bytes  received 16 bytes  194.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls /tmp/b</span></span><br><span class="line">c</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls /tmp/a</span></span><br><span class="line">c</span><br></pre></td></tr></table></figure><p>上面的代码意思是不传送/tmp/a下的所有文件，但是传送/tmp/b下的所有文件，因此在传送列表中只有一个c/目录准备传送。</p><p><strong>需要注意的是：一个”–exclude”只能指定一条规则，要指定多条排除规则，需要使用多个”–exclude”选项，或者将排除规则写入到文件中，然后使用”–exclude-from”选项读取该规则文件。</strong></p><p><strong>另外，除了”–exclude”排除规则，还有”–include”包含规则，顾名思义，它就是筛选出要进行传输的文件，所以include规则也称为传输规则。它的使用方法和”–exclude”一样。如果一个文件即能匹配排除规则，又能匹配包含规则，则先匹配到的立即生效，生效后就不再进行任何匹配。</strong></p><p>关于规则，最重要的一点是它的作用时间。<strong>当发送端敲出rsync命令后，rsync将立即扫描命令行中给定的文件和目录(扫描过程中还会按照目录进行排序，将同一个目录的文件放在相邻的位置)，这称为拷贝树(copy tree)，扫描完成后将待传输的文件或目录记录到文件列表中，然后将文件列表传输给接收端。</strong>而<strong>筛选规则的作用时刻是在扫描拷贝树时，会根据规则来匹配并决定文件是否记录到文件列表中(严格地说是所有文件都会记录到文件列表中的，只不过排除的文件会被标记为hide隐藏起来)，只有记录到了文件列表中的文件或目录才是真正需要传输的内容。</strong>换句话说，筛选规则的生效时间在rsync整个同步过程中是非常靠前的，它会影响很多选项的操作对象，最典型的如”–delete”。</p><p><strong>实际上，排除规则和包含规则都只是”–filter”筛选规则的两种特殊规则。</strong>“–filter”比较复杂，它有自己的规则语法和匹配模式，由于篇幅有限，以及考虑到本文实际应用定位，”–filter”规则不便在此多做解释，仅简单说明下规则类。</p><p><strong>以下是rsync中的规则种类，不解之处请结合下文的”–delete”分析：</strong></p><p><strong>1）exclude规则：</strong>即排除规则，只作用于发送端，被排除的文件不会进入文件列表(实际上是加上隐藏规则进行隐藏)。</p><p><strong>2）include规则：</strong>即包含规则，也称为传输规则，只作用于发送端，被包含的文件将明确记录到文件列表中。</p><p><strong>3）hide规则：</strong>即隐藏规则，只作用于发送端，隐藏后的文件对于接收端来说是看不见的，也就是说接收端会认为它不存在于源端。</p><p><strong>4）show规则：</strong>即显示规则，只作用于发送端，是隐藏规则的反向规则。</p><p><strong>5）protect规则：</strong>即保护规则，该规则只作用于接收端，被保护的文件不会被删除掉。</p><p><strong>6）risk规则：</strong>即取消保护规则。是protect的反向规则。</p><p><strong>除此之外，还有一种规则是”clear规则”，作用是删除include/exclude规则列表。</strong></p><p><strong>rsync在发送端将文件列表发送给接收端后，接收端的generato进程会扫描每个文件列表中的信息，然后对列表中的每个信息条目都计算数据块校验码，最后将数据库校验码发给发送端，发送端通过校验码来匹配哪些数据块是需要传输的，这样就实现了增量传输的功能：只传输改变的部分，不会传输整个文件。而delete删除的时间点是generator进程处理每个文件列表时、生成校验码之前进行的，</strong>先将目标上存在但源上不存在的多余文件删除，这样就无需为多余的文件生成校验码。</p><p>但是，<strong>如果exclude和delete规则同时使用时，却不会删除被exclude排除的文件</strong>。这是因为，<strong>delete动作是比”–exclude”规则更晚执行的，被”–exclude”规则排除的文件不会进入文件列表中，在执行了delete时会认为该文件不存在于源端，从而会导致目标端将这些文件删除。但这是想当然的</strong>，尽管理论上确实是这样的，但是rsync为了防止众多误删除情况，提供了两种规则：<strong>保护规则(protect)和取消保护规则(risk)。</strong>默认情况下，<strong>“–delete”和”–exclude”一起使用时，虽然发送端的exclude规则将文件标记为隐藏，使得接收端认为这些被排除文件在源端不存在，但rsync会将这些隐藏文件标记为保护文件，使得它们不受delete行为的影响，这样delete就删除不了这些被排除的文件。</strong>如果还是想要强行删除被exclude排除的文件，可以使用”–delete-excluded”选项强制取消保护，这样即使被排除的文件也会被删除。</p><p><strong>11）指定ssh连接参数，如端口、连接的用户、ssh选项等</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -e "ssh -p 22 -o StrictHostKeyChecking=no" /etc/fstab 192.168.101.252:/tmp</span></span><br><span class="line">Warning: Permanently added <span class="string">'192.168.101.252'</span> (ECDSA) to the list of known hosts.</span><br><span class="line">root@192.168.101.252<span class="string">'s password: </span></span><br><span class="line"><span class="string">Permission denied, please try again.</span></span><br><span class="line"><span class="string">root@192.168.101.252'</span>s password:</span><br></pre></td></tr></table></figure><p>在要求保障数据安全的场景下，可以使用-e选项借助SSH隧道进行加密传输数据，-p是SSH命令的选项，指定SSH传输的端口号为22，-o是SSH的认证方式，上述示例表示不通过秘钥认证，可见直接指定ssh参数是生效的。</p><p><strong>12）拉取或推送文件及目录（类似scp命令）</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Server02上拉取/home/目录到本地/tmp目录下</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av 192.168.101.82:/home/ /tmp/</span></span><br><span class="line">root@192.168.101.82<span class="string">'s password: </span></span><br><span class="line"><span class="string">receiving incremental file list</span></span><br><span class="line"><span class="string">./</span></span><br><span class="line"><span class="string">kkutysllb/</span></span><br><span class="line"><span class="string">kkutysllb/.bash_history</span></span><br><span class="line"><span class="string">kkutysllb/.bash_logout</span></span><br><span class="line"><span class="string">kkutysllb/.bash_profile</span></span><br><span class="line"><span class="string">kkutysllb/.bashrc</span></span><br><span class="line"><span class="string">kkutysllb/202012312234.55</span></span><br><span class="line"><span class="string">kkutysllb/data001</span></span><br><span class="line"><span class="string">kkutysllb/data002</span></span><br><span class="line"><span class="string">kkutysllb/data003</span></span><br><span class="line"><span class="string">kkutysllb/data004</span></span><br><span class="line"><span class="string">。。。</span></span><br><span class="line"><span class="string">sent 767 bytes  received 4,507 bytes  958.91 bytes/sec</span></span><br><span class="line"><span class="string">total size is 1,440  speedup is 0.27</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 查看本地/tmp目录下内容</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[root@C7-Server01 ~]# ls -l /tmp</span></span><br><span class="line"><span class="string">total 4</span></span><br><span class="line"><span class="string">drwx------ 7 root root 4096 Apr 28 00:00 kkutysllb</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 推送本地/etc/udev/目录下所有内容到Server02的/home/kkutysllb/目录下</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[root@C7-Server01 ~]# rsync -av /etc/udev 192.168.101.82:/home/kkutysllb/</span></span><br><span class="line"><span class="string">root@192.168.101.82'</span>s password: </span><br><span class="line">sending incremental file list</span><br><span class="line">udev/</span><br><span class="line">udev/hwdb.bin</span><br><span class="line">udev/udev.conf</span><br><span class="line">udev/rules.d/</span><br><span class="line"></span><br><span class="line">sent 7,944,769 bytes  received 66 bytes  1,765,518.89 bytes/sec</span><br><span class="line">total size is 7,942,619  speedup is 1.00</span><br></pre></td></tr></table></figure><p>与scp命令复制的结果进行对比可以发现，使用rsync复制时，重复执行复制直至目录下文件相同就不再进行复制了。</p><h2 id="rsync使用技巧"><a href="#rsync使用技巧" class="headerlink" title="rsync使用技巧"></a><strong>rsync使用技巧</strong></h2><p><strong>1）实际运维场景常用选项-avz，相当于-vzrtopg（这是网上文档常见的选项），但是此处建议大家使用-avz选项，更简单明了。如果在脚本中使用也可以省略-v选项。</strong></p><p><strong>2）关于z压缩选项的使用建议，如果为内网环境，且没有其他业务占用带宽，可以不使用z选项。不压缩传输，几乎可以满带宽传输（千M网络），压缩传输则网络发送速度就会骤降，压缩的速率赶不上传输的速度。</strong></p><p><strong>3）选项n是一个提高安全性的选项，它可以结合-v选项输出模拟的传输过程，如果没有错误，则可以去除n选项真正的传输文件。</strong></p><h2 id="rsync-daemon模式"><a href="#rsync-daemon模式" class="headerlink" title="rsync daemon模式"></a><strong>rsync daemon模式</strong></h2><p>既然rsync通过远程shell就能实现两端主机上的文件同步，还要使用rsync的服务干啥？试想下，你有的机器上有一堆文件需要时不时地同步到众多机器上去，比如目录a、b、c是专门传输到web服务器上的，d/e、f、g/h是专门传输到ftp服务器上的，还要对这些目录中的某些文件进行排除，如果通过远程shell连接方式，无论是使用排除规则还是包含规则，甚至一条一条rsync命令地传输，这都没问题，但太过繁琐且每次都要输入同样的命令显得太死板。使用rsync daemon就可以解决这种死板问题。而且，<strong>rsync daemon是向外提供服务的，这样只要告诉了别人rsync的url路径，外人就能向ftp服务器一样获取文件列表并进行选择性地下载，</strong>所以，你所制定的列表，所有人都可以获取到并使用。</p><p>在Linux内核官网<a href="http://www.kernel.org上，就提供rsync的下载方式，官方给出的地址是rsync://rsync.kernel.org/pub，可以根据这个地址找出你想下载的内核版本。例如：要找出linux-3.0.15版本的内核相关文件，可以在客户端执行如下命令：" target="_blank" rel="noopener">www.kernel.org上，就提供rsync的下载方式，官方给出的地址是rsync://rsync.kernel.org/pub，可以根据这个地址找出你想下载的内核版本。例如：要找出linux-3.0.15版本的内核相关文件，可以在客户端执行如下命令：</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --no-motd -r -v -f "+ */" -f "+ linux-3.0.15*" -f "- *" -m rsync://rsync.kernel.org/pub/</span></span><br><span class="line">receiving file list ... <span class="keyword">done</span></span><br><span class="line">drwxr-xr-x          4,096 2019/05/04 03:15:23 .</span><br><span class="line">drwxr-xr-x          4,096 2014/11/12 05:50:10 linux</span><br><span class="line">drwxr-xr-x          4,096 2019/03/12 23:06:47 linux/kernel</span><br><span class="line">drwxr-xr-x        258,048 2019/05/23 13:52:08 linux/kernel/v3.x</span><br><span class="line">-rw-r--r--     76,803,806 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.bz2</span><br><span class="line">-rw-r--r--     96,726,195 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.gz</span><br><span class="line">-rw-r--r--            836 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.sign</span><br><span class="line">-rw-r--r--     63,812,604 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.xz</span><br><span class="line">sent 58 bytes  received 82,915 bytes  2,720.43 bytes/sec</span><br><span class="line">total size is 237,343,441  speedup is 2,860.49</span><br></pre></td></tr></table></figure><p>我们无需关注上面的规则代表什么意思，需要关注的重点是<strong>通过rsync可以向外提供文件列表并提供相应的下载。</strong>同样，我们还可以根据路径，将rsync daemon上的文件拉取到本地实现下载的功能。</p><p><img src="https://i.loli.net/2019/06/24/5d10a9d9a1b2597343.jpg" alt></p><p>rsync daemon是”rsync –daemon”或再加上其他一些选项启动的，它会读取配置文件，<strong>默认是/etc/rsyncd.conf，并默认监听在873端口上</strong>，当外界有客户端对此端口发起连接请求，通过这个网络套接字就可以完成连接，以后与该客户端通信的所有数据都通过该网络套接字传输。</p><p><strong>rsync daemon的通信方式和传输通道与远程shell不同。远程shell连接的两端是通过管道完成通信和数据传输的，当连接到目标端时，将在目标端上根据远程shell进程fork出rsync进程使其成为rsync server。而rsync daemon是事先在server端上运行好的rsync后台进程(根据启动选项，也可以设置为非后台进程)，它监听套接字等待client端的连接，连接建立后所有通信方式都是通过套接字完成的。</strong></p><p><strong>rsync中的server的概念从来就不代表是rsync daemon，server在rsync中只是一种通用称呼，只要不是发起rsync请求的client端，就是server端，可以认为rsync daemon是一种特殊的server，其实daemon更准确的称呼应该是service。</strong></p><p><strong>以下是rsync client连接rsync daemon时的命令语法：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST]</span><br><span class="line">rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]</span><br><span class="line">Push: rsync [OPTION...] SRC... [USER@]HOST::DEST</span><br><span class="line">rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST</span><br></pre></td></tr></table></figure><p><strong>连接命令有两种类型，一种是rsync风格使用双冒号的”rsync user@host::src dest”，一种是url风格的”rsync://user@host:port/src dest”。对于rsync风格的连接命令，如果想要指定端口号，则需要使用选项”–port”。</strong></p><p>上述语法中，其中daemon端的路径，如user@host::src，它的src代表的是模块名，而不是真的文件系统中的路径。关于rsync中的模块就是其配置文件中定义的各个功能。</p><h2 id="daemon配置文件rsyncd-conf"><a href="#daemon配置文件rsyncd-conf" class="headerlink" title="daemon配置文件rsyncd.conf"></a><strong>daemon配置文件rsyncd.conf</strong></h2><p>默认”rsync –daemon”读取的配置文件为/etc/rsyncd.conf，有些版本的系统上可能该文件默认不存在，需要手动创建。rsyncd.conf默认配置内容如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10aa16df0cd37237.jpg" alt></p><p>在上述示例配置文件中，先定义了一些全局选项，然后定义了[ftp1]，这个用中括号包围的”[ftp1]”就是rsync中所谓的模块，<strong>ftp1为模块ID，必须保证唯一，每个模块中必须定义一项”path”，path定义的是该模块代表的路径</strong>，此示例文件中，如果想请求ftp1模块，则在客户端使用”rsync user@host::ftp1”，这表示访问user@host上的/home/ftp目录，如果要访问/home/ftp目录下的子目录www，则”rsync user@host::ftp1/www”。</p><p>以下是我自用的配置项，也算是一个配置示例：</p><p>[</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">root@c7-server01 ~]<span class="comment"># cat /etc/rsyncd.conf </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局配置参数</span></span><br><span class="line"></span><br><span class="line"> port=888    <span class="comment"># 指定rsync端口。默认873</span></span><br><span class="line"> uid = 0 <span class="comment"># rsync服务的运行用户，默认是nobody，文件传输成功后属主将是这个uid</span></span><br><span class="line"> gid = 0 <span class="comment"># rsync服务的运行组，默认是nobody，文件传输成功后属组将是这个gid</span></span><br><span class="line"> use chroot = no <span class="comment"># rsync daemon在传输前是否切换到指定的path目录下，并将其监禁在内</span></span><br><span class="line"> max connections = 200 <span class="comment"># 指定最大连接数量，0表示没有限制</span></span><br><span class="line"> timeout = 300         <span class="comment"># 确保rsync服务器不会永远等待一个崩溃的客户端，0表示永远等待</span></span><br><span class="line"> motd file = /var/rsyncd/rsync.motd   <span class="comment"># 客户端连接过来显示的消息</span></span><br><span class="line"> pid file = /var/run/rsyncd.pid       <span class="comment"># 指定rsync daemon的pid文件</span></span><br><span class="line"> lock file = /var/run/rsync.lock      <span class="comment"># 指定锁文件</span></span><br><span class="line"> <span class="built_in">log</span> file = /var/<span class="built_in">log</span>/rsyncd.log       <span class="comment"># 指定rsync的日志文件，而不把日志发送给syslog</span></span><br><span class="line"> dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2  <span class="comment"># 指定哪些文件不用进行压缩传输</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定模块，并设定模块配置参数，可以创建多个模块</span></span><br><span class="line"></span><br><span class="line">[kksql]        <span class="comment"># 模块ID</span></span><br><span class="line">path = /kksql/ <span class="comment"># 指定该模块的路径，该参数必须指定。启动rsync服务前该目录必须存在。rsync请求访问模块本质就是访问该路径。</span></span><br><span class="line">ignore errors      <span class="comment"># 忽略某些IO错误信息</span></span><br><span class="line"><span class="built_in">read</span> only = <span class="literal">true</span>  <span class="comment"># 指定该模块是否可读写，即能否上传文件，false表示可读写，true表示可读不可写。所有模块默认不可上传</span></span><br><span class="line">write only = <span class="literal">false</span> <span class="comment"># 指定该模式是否支持下载，设置为true表示客户端不能下载。所有模块默认可下载</span></span><br><span class="line">list = <span class="literal">false</span>       <span class="comment"># 客户端请求显示模块列表时，该模块是否显示出来，设置为false则该模块为隐藏模块。默认true</span></span><br><span class="line">hosts allow = 10.0.5.0/24 <span class="comment"># 指定允许连接到该模块的机器，多个ip用空格隔开或者设置区间</span></span><br><span class="line">hosts deny = 0.0.0.0/32   <span class="comment"># 指定不允许连接到该模块的机器</span></span><br><span class="line">auth users = rsync_backup <span class="comment"># 指定连接到该模块的用户列表，只有列表里的用户才能连接到模块，用户名和对应密码保存在secrets file中，这里使用的不是系统用户，而是虚拟用户。不设置时，默认所有用户都能连接，但使用的是匿名连接</span></span><br><span class="line">secrets file = /etc/rsyncd.passwd <span class="comment"># 保存auth users用户列表的用户名和密码，每行包含一个username:passwd。由于"strict modes"默认为true，所以此文件要求非rsync daemon用户不可读写。只有启用了auth users该选项才有效。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">[kkweb]    <span class="comment"># 以下定义的是第二个模块</span></span><br><span class="line">path=/kkweb/</span><br><span class="line"><span class="built_in">read</span> only = <span class="literal">false</span></span><br><span class="line">ignore errors</span><br><span class="line">comment = anyone can access</span><br></pre></td></tr></table></figure><p>1）从客户端推到服务端时，文件的属主和属组是配置文件中指定的uid和gid。但是客户端从服务端拉的时候，文件的属主和属组是客户端正在操作rsync的用户身份，因为执行rsync程序的用户为当前用户。</p><p>2）auth users和secrets file这两行不是一定需要的，省略它们时将默认使用匿名连接。但是如果使用了它们，则secrets file的权限必须是600。客户端的密码文件也必须是600。</p><p>3）关于secrets file的权限，实际上并非一定是600，只要满足除了运行rsync daemon的用户可读即可。是否检查权限的设定是通过选项strict mode设置的，如果设置为false，则无需关注文件的权限。但默认是yes，即需要设置权限。</p><p>配置完后，再就是提供模块相关目录、身份验证文件等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># useradd -r -s /sbin/nologin rsync</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># mkdir /&#123;kksql,kkweb&#125;</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># chown -R rsync.rsync /&#123;kksql,kkweb&#125;</span></span><br></pre></td></tr></table></figure><p>提供模块kksql的身份验证文件，由于rsync daemon是以root身份运行的，所以要求身份验证文件对非root用户不可读写，所以设置为600权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># echo "rsync_backup:Oms_2600" &gt;&gt; /etc/rsyncd.passwd</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># chmod 600 /etc/rsyncd.passwd</span></span><br></pre></td></tr></table></figure><p>然后启动rsync daemon，启动方式很简单。如果是CentOS 7，则自带启动脚本：systemctl stard rsyncd。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># rsync --daemon</span></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># systemctl enable rsyncd &amp;&amp; systemctl start rsyncd</span></span><br></pre></td></tr></table></figure><p>启动好rysnc daemon后，它就监听在指定的端口上，等待客户端的连接。由于上述示例中的模块kksql配置了身份验证功能，所以客户端连接时会询问密码。如果不想手动输入密码，则可以使用”–password-file”选项提供密码文件，密码文件中只有第一行才是传递的密码，其余所有的行都会被自动忽略。</p><p>在客户端上访问daemon上各模块的文件示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rsync模式</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only --port 888 root@192.168.101.11::kkweb/</span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only --port 888 rsync_bakup@192.168.101.11::kkweb/</span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># url模式</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only rsync://root@192.168.101.11:888/kkweb/ </span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only rsync://rsync@192.168.101.11:888/kkweb/ </span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br></pre></td></tr></table></figure><p>晕吧？下一篇我们介绍sersync，继续晕。。。嘿嘿</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;rsync是可以实现增量备份的工具。配合任务计划，rsync能实现定时或间隔同步，配合inotify或sersync，可以实现触发式的实时同步。事实上，rsync有一套自己的算法，其算法原理以及rsync对算法实现的机制可能比想象中要复杂一些。平时使用rsync实现简单的备份、同步等功能足以，没有多大必要去深究这些原理性的内容。如果你对这些原理感兴趣，可以通过rsync命令的man文档、并且借助”-vvvv”分析rsync执行过程，结合rsync的算法原理去深入理解。本篇文章由于篇幅有限，只是介绍rsync的使用方法和它常用的功能，以及rsync和sersync如何配合打造文件实时同步架构。
    
    </summary>
    
      <category term="Linux常用运维工具" scheme="https://kkutysllb.cn/categories/Linux%E5%B8%B8%E7%94%A8%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="https://kkutysllb.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-21-服务器外部交换网络虚拟化</title>
    <link href="https://kkutysllb.cn/2019/06/22/2019-06-21-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A4%96%E9%83%A8%E4%BA%A4%E6%8D%A2%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    <id>https://kkutysllb.cn/2019/06/22/2019-06-21-服务器外部交换网络虚拟化/</id>
    <published>2019-06-22T03:54:39.000Z</published>
    <updated>2019-06-22T04:49:18.065Z</updated>
    
    <content type="html"><![CDATA[<p>数据中心服务器外部交换网络的虚拟化主要包括：<strong>数据中心内部二、三层交换设备虚拟化、大二层组网架构</strong>和<strong>软件定义网络SDN技术。</strong>其中，软件定义网络SDN技术在目前中国移动NovoNet战略中还处于现网试点阶段，预计2020年初会引入电信云数据中心。而数据中心内部二、三层交换设备虚拟化及大二层组网架构已完全成熟，并已在当前电信云数据中心中实际部署，本文也主要讲述这两种技术，其中大二层组网基石VxLAN需要大家重点掌握。<a id="more"></a></p><h2 id="数据中心内部二、三层交换设备虚拟化"><a href="#数据中心内部二、三层交换设备虚拟化" class="headerlink" title="数据中心内部二、三层交换设备虚拟化"></a><strong>数据中心内部二、三层交换设备虚拟化</strong></h2><p>现有数据中心内部的三层网络架构起源于园区网，传统数据中心。如下图所示，三层架构将网络分为<strong>接入(access)、汇聚(aggregation)</strong>和<strong>核心(core)</strong>三层。</p><p><img src="https://i.loli.net/2019/06/22/5d0da8867958c61533.jpg"></p><p><strong>接入层</strong>通常使用二层交换机，主要负责接入服务器、标记VLAN以及转发二层的流量。<strong>汇聚层</strong>通常使用三层交换机，主要负责同一POD（所谓POD就是划分范围就是汇聚交换机管理的范围，对应到上图就是底层的三个iStack堆叠组范围）内的路由，并实现ACL等安全策略。<strong>核心层</strong>通常使用业务路由器，主要负责实现跨POD的路由，并提供Internet接入以及VPN等服务。</p><p>在三层的网络规划中，接入层和汇聚层间通常是二层网络，接入交换机双上联到汇聚交换机，并运行STP来消除环路，汇聚交换机作为网关终结掉二层，和核心路由器起IGP来学习路由，少数情况下会选择BGP。</p><p>众所周知，MAC自学习是以太网的根基，它以泛洪帧为探针同步转发表，实现简单，无需复杂控制。但是，如果网络中存在环路，泛洪会在极短的时间内使网络瘫痪。为此，引入STP用来破除转发环路，不过STP的引入却带来了更多问题，如收敛慢、链路利用率低、规模受限、难以定位故障等。虽然，业界为STP的优化费劲了心思，但打补丁的方式只能治标而不能治本。随着数据中心I/O密集型业务的大发展，STP成了网络最为明显的一块短板，处理掉STP自然也就成了数据中心网络架构演进打响的第一枪。</p><h2 id="物理设备的“多虚一”：VRRP、iStack和虚拟机框"><a href="#物理设备的“多虚一”：VRRP、iStack和虚拟机框" class="headerlink" title="物理设备的“多虚一”：VRRP、iStack和虚拟机框"></a><strong>物理设备的“多虚一”：VRRP、iStack和虚拟机框</strong></h2><p>物理设备的“多虚一”功能，能够将多台设备中的控制平面进行整合，形成一台统一的逻辑设备，这台逻辑设备不但具有统一的管理IP，而且在各种L2和L3协议中也将表现为一个整体。因此在完成整合后，STP所看到的拓扑自然就是无环的了，这就间接地规避了STP的种种问题。</p><p>目前，普遍采用VRRP、堆叠iStack和虚拟机框技术完成物理设备的“多虚一”功能。其中，虚拟机框技术和传统的堆叠技术一样，不同的物理设备分享同一个控制平面，实际上就相当于为物理网络设备做了个集群，也有选主和倒换的过程。相比之下，虚拟机框在组网上的限制较少，而且在可用性方面的设计普遍要好于堆叠，因此可以看作对于堆叠技术的升级，我们只要了解堆叠技术即可。而且，虚拟机框技术以Cisco的VSS、Juniper的Virtual Chassis以及H3C的IRF 2为代表，在中国移动NFV电信云数据中心内华为交换机堆叠技术与此等价。</p><h3 id="VRRP技术"><a href="#VRRP技术" class="headerlink" title="VRRP技术"></a><strong>VRRP技术</strong></h3><p><strong>定义：</strong>虚拟路由冗余协议VRRP，通过把几台设备联合组成一台虚拟的路由设备，将虚拟路由设备的IP地址作为用户的默认网关实现与外部网络通信。</p><p><img src="https://i.loli.net/2019/06/22/5d0da901a17cc99156.jpg"></p><p>如上图所示，VRRP将局域网内的一组路由器划分在一起，形成一个VRRP备份组，它在功能上相当于一台虚拟路由器，使用虚拟路由器号进行标识。VRRP设备有自己的虚拟IP地址和虚拟MAC地址，它的外在表现形式和实际的物理路由器完全一样。局域网内的主机将VRRP设备的IP地址设置为默认网关，通过VRRP设备与外部网络进行通信。</p><p>VRRP设备是工作在实际的物理三层设备之上的。它由多个实际的三层设备组成，包括一个Master设备和多个Backup设备。Master设备正常工作时，局域网内的主机通过Master设备与外界通信。当Master设备出现故障时，Backup设备中的一台设备将成为新的Master设备，接替转发报文的工作。</p><p><strong>VRRP的工作过程为：</strong></p><p>1）VRRP设备中的物理设备根据优先级选举出Master。Master设备通过发送免费ARP报文，将自己的虚拟MAC地址通知给与它连接的设备或者主机，从而承担报文转发任务；</p><p>2）Master设备周期性发送VRRP报文，以公布其配置信息（优先级等）和工作状况；</p><p>3）如果Master设备出现故障，VRRP设备中的Backup设备将根据优先级重新选举新的Master；</p><p>4）VRRP设备状态切换时，Master设备由一台设备切换为另外一台设备，新的Master设备只是简单地发送一个携带VRRP设备的MAC地址和虚拟IP地址信息的免费ARP报文，这样就可以更新与它连接的主机或设备中的ARP相关信息。网络中的主机感知不到Maste设备已经切换为另外一台设备。</p><p>5）Backup设备的优先级高于Master设备时，由Backup设备的工作方式（抢占方式和非抢占方式）决定是否重新选举Master。</p><p><strong>VRRP根据优先级来确定VRRP设备中每台物理设备的角色（Master设备或Backup设备）。优先级越高，则越有可能成为Master设备。</strong></p><p>1）初始创建的设备工作在Backup状态，通过VRRP报文的交互获知VRRP设备中其他成员的优先级：</p><ol><li>如果VRRP报文中Master设备的优先级高于自己的优先级，则路由器保持在Backup状态；</li><li>如果VRRP报文中Master设备的优先级低于自己的优先级，采用抢占工作方式的路由器将抢占成为Master状态，周期性地发送VRRP报文，采用非抢占工作方式的路由器仍保持Backup状态；</li><li>如果在一定时间内没有收到VRRP报文，则本设备切换为Master状态。</li></ol><p>2）VRRP优先级的取值范围为0到255（数值越大表明优先级越高），可配置的范围是1到254，优先级0为系统保留给路由器放弃Master位置时候使用，255则是系统保留给IP地址拥有者使用。当设备为IP地址拥有者时，其优先级始终为255。因此，当VRRP设备内存在IP地址拥有者时，只要其工作正常，则为Master设备。</p><p><strong>VRRP的的工作方式</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0da9511990669228.jpg"></p><p>主备备份方式表示业务仅由Master设备承担。当Master设备出现故障时，才会由选举出来的Backup设备接替它工作。</p><p>如上图左边所示，初始情况下，Device A是Master路由器并承担转发任务，Device B和Device C是Backup路由器且都处于就绪监听状态。如果Device A发生故障，则虚拟路由器内处于Backup状态的Device B和Device C路由器将根据优先级选出一个新的Master路由器，这个新Master路由器继续为网络内的主机转发数据。</p><p>负载分担方式是指多台设备同时承担业务，因此负载分担方式需要两个或者两个以上的VRRP设备，每个VRRP设备都包括一个Master设备和若干个Backup设备，各VRRP设备中的Master设备可以各不相同。通过在一个设备的一个接口上可以创建多个VRRP设备，使得该设备可以在一个VRRP设备中作为Master设备，同时在其他的VRRP设备中作为Backup设备。</p><p>在上图右边中，有三个VRRP设备存在：</p><ul><li>VRRP1：Device A作为Master设备，Device B和Device C作为Backup设备。</li><li>VRRP2：Device B作为Master设备，Device A和Device C作为Backup设备。</li><li>VRRP3：Device C作为Master设备，Device A和Device B作为Backup设备。</li></ul><p>为了实现业务流量在Device A、Device B和Device C之间进行负载分担，需要将局域网内的主机的默认网关分别设置为VRRP1、2和3。在配置优先级时，需要确保三个VRRP设备中各路由器的VRRP优先级形成一定的交叉，使得一台路由器尽可能不同时充当2个Master路由器。</p><h3 id="iStack交换机堆叠技术"><a href="#iStack交换机堆叠技术" class="headerlink" title="iStack交换机堆叠技术"></a><strong>iStack交换机堆叠技术</strong></h3><p><strong>堆叠iStack（Intelligent Stack），是指将多台支持堆叠特性的交换机设备组合在一起，从逻辑上组合成一台交换设备。</strong>如下图所示，SwitchA与SwitchB通过堆叠线缆连接后组成堆叠iStack，对于上游和下游设备来说，它们就相当于一台交换机Switch。通过交换机堆叠，可以实现网络高可靠性和网络大数据量转发，同时简化网络管理。</p><p><img src="https://i.loli.net/2019/06/22/5d0da999163d217174.jpg"></p><blockquote><p><strong>1）高可靠性。</strong>堆叠系统多台成员交换机之间<strong>冗余备份</strong>；堆叠支持<strong>跨设备的链路聚合</strong>功能，实现<strong>跨设备的链路冗余备份</strong>。</p><p><strong>2）强大的网络扩展能力。</strong>通过增加成员交换机，可以轻松的扩展堆叠系统的<strong>端口数</strong>、<strong>带宽</strong>和<strong>处理能力</strong>；同时<strong>支持成员交换机热插拔</strong>，<strong>新加入的成员交换机自动同步主交换机的配置文件和系统软件版本。</strong></p><p><strong>3）简化配置和管理。</strong>一方面，用户可以<strong>通过任何一台成员交换机登录</strong>堆叠系统，对堆叠系统所有成员交换机进行<strong>统一配置和管理</strong>；另一方面，堆叠形成后，<strong>不需要配置复杂的二层破环协议和三层保护倒换协议</strong>，简化了网络配置。</p></blockquote><p><strong>堆叠涉及以下几个基本概念：</strong></p><p><strong>1）角色：</strong>堆叠中所有的单台交换机都称为成员交换机，按照功能不同，可以分为三种角色：</p><ul><li><strong>主交换机</strong>：主交换机（Master）负责<strong>管理整个堆叠</strong>。堆叠中<strong>只有一台</strong>主交换机。</li><li><strong>备交换机：</strong>备交换机（Standby）是<strong>主交换机的备份交换机</strong>。当主交换机故障时，备交换机会接替原主交换机的所有业务。堆叠中<strong>只有一台</strong>备交换机。</li><li><strong>从交换机：</strong>从交换机（Slave）主要<strong>用于业务转发</strong>，从交换机数量越多，堆叠系统的转发能力越强。除主交换机和备交换机外，堆叠中其他所有的成员交换机都是从交换机。</li></ul><p><strong>2）堆叠ID：</strong>即<strong>成员交换机的槽位号</strong>（Slot ID），用来<strong>标识和管理成员交换机</strong>，堆叠中所有成员交换机的<strong>堆叠ID都是唯一</strong>的。</p><p><strong>3）堆叠优先级</strong>：是成员交换机的<strong>一个属性</strong>，<strong>主要用于角色选举过程中确定成员交换机的角色，优先级值越大表示优先级越高，优先级越高当选为主交换机的可能性越大。</strong></p><p><strong>堆叠建立的过程包括以下四个阶段：</strong></p><p><strong>Step1：物理连接。</strong>如下图所示，根据网络需求，选择适当的连接方式和连接拓扑，组建堆叠网络。根据连接介质的不同，堆叠可分为<strong>堆叠卡堆叠</strong>和<strong>业务口堆叠</strong>。</p><p><img src="https://i.loli.net/2019/06/22/5d0da9d67084f96031.jpg"></p><p>如上图所示，每种连接方式都可组成<strong>链形</strong>和<strong>环形</strong>两种连接拓扑。下表从可靠性、链路带宽利用率和组网布线是否方便的角度对两种连接拓扑进行对比。</p><table><thead><tr><th style="text-align:left"><strong>连接拓扑</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td style="text-align:left">链形连接</td><td>首尾不需要有物理连接，<strong>适合长距离堆叠</strong>。</td><td><strong>可靠性低</strong>：其中一条堆叠链路出现故障，就会造成堆叠分裂。堆叠链路<strong>带宽利用率低</strong>：整个堆叠系统只有一条路径。</td><td>堆叠成员交换机距离较远时，组建环形连接比较困难，可以使用链形连接。</td></tr><tr><td style="text-align:left">环形连接</td><td><strong>可靠性高</strong>：其中一条堆叠链路出现故障，环形拓扑变成链形拓扑，不影响堆叠系统正常工作。堆叠链路<strong>带宽利用率高</strong>：数据能够按照最短路径转发。</td><td>首尾需要有物理连接，<strong>不适合长距离堆叠</strong>。</td><td>堆叠成员交换机距离较近时，从可靠性和堆叠链路利用率上考虑，建议使用环形连接。</td></tr></tbody></table><p><strong>Step2：主交换机选举。</strong>确定出堆叠的连接方式和连接拓扑，完成成员交换机之间的物理连接之后，所有成员交换机上电。此时，堆叠系统开始进行主交换机的选举。在堆叠系统中每台成员交换机都具有一个确定的角色，其中，主交换机负责管理整个堆叠系统。<strong>主交换机选举规则如下（依次从第一条开始判断，直至找到最优的交换机才停止比较）：</strong></p><ol><li><strong>运行状态比较，已经运行的交换机优先处于启动状态的交换机竞争为主交换机。</strong>堆叠主交换机选举超时时间为20s，堆叠成员交换机上电或重启时，由于不同成员交换机所需的启动时间可能差异比较大，因此不是所有成员交换机都有机会参与主交换机的选举：<strong>启动时间与启动最快的成员交换机相比，相差超过20s的成员交换机没有机会参与主交换机的选举，只能被动加入堆叠成为非主交换机，加入过程可参见堆叠成员加入与退出。因此，如果希望指定某一成员交换机成为主交换机，则可以先为其上电，待其启动完成后再给其他成员交换机上电。</strong></li><li><strong>堆叠优先级高的交换机优先竞争为主交换机。</strong></li><li><strong>叠优先级相同时，MAC地址小的交换机优先竞争为主交换机。</strong></li></ol><p><strong>Step3：拓扑收集和备交换机选举。</strong>主交换机选举完成后，会收集所有成员交换机的拓扑信息，根据拓扑信息计算出堆叠转发表项和破环点信息下发给堆叠中的所有成员交换机，并向所有成员交换机分配堆叠ID。之后进行备交换机的选举，作为主交换机的备份交换机。<strong>除主交换机外最先完成设备启动的交换机优先被选为备份交换机</strong>。<strong>当除主交换机外其它交换机同时完成启动时，备交换机的选举规则如下（依次从第一条开始判断，直至找到最优的交换机才停止比较）：</strong></p><ol><li><strong>堆叠优先级最高的设备成为备交换机。</strong></li><li><strong>堆叠优先级相同时，MAC地址最小的成为备交换机。</strong></li></ol><p>除主交换机和备交换机之外，剩下的其他成员交换机作为从交换机加入堆叠。</p><p><strong>Step4：稳定运行。</strong>角色选举、拓扑收集完成之后，所有成员交换机会自动同步主交换机的系统软件和配置文件。<strong>堆叠具有自动加载系统软件的功能，待组成堆叠的成员交换机不需要具有相同软件版本，只需要版本间兼容即可</strong>。当备交换机或从交换机与主交换机的软件版本不一致时，备交换机或从交换机会自动从主交换机下载系统软件，然后使用新系统软件重启，并重新加入堆叠。<strong>堆叠具有配置文件同步机制，</strong>备交换机或从交换机会将主交换机的配置文件同步到本设备并执行，以保证堆叠中的多台设备能够像一台设备一样在网络中工作，并且在主交换机出现故障之后，其余交换机仍能够正常执行各项功能。</p><p><strong>堆叠支持跨设备链路聚合技术，通过配置跨设备Eth-Trunk接口实现。</strong>用户可以将不同成员交换机上的物理以太网端口配置成一个聚合端口连接到上游或下游设备上，实现多台设备之间的链路聚合。当其中一条聚合链路故障或堆叠中某台成员交换机故障时，Eth-Trunk接口能够将流量重新分布到其他聚合链路上，实现了链路间和设备间的备份，保证了数据流量的可靠传输。</p><p><img src="https://i.loli.net/2019/06/22/5d0daa5681cf010026.jpg"></p><blockquote><p>如上图左边所示，流向网络核心的流量将均匀分布在聚合链路上，当某一条聚合链路失效时，Eth-Trunk接口将流量通过堆叠线缆重新分布到其他聚合链路上，实现了链路间的备份。</p><p>如上图右边所示，流向网络核心的流量将均匀分布在聚合链路上，当某台成员交换机故障时，Eth-Trunk接口将流量重新分布到其他聚合链路上，实现了设备间的备份。</p></blockquote><p>跨设备链路聚合实现了数据流量的可靠传输和堆叠成员交换机的相互备份。但是由于堆叠设备间堆叠线缆的带宽有限，跨设备转发流量增加了堆叠线缆的带宽承载压力，同时也降低了流量转发效率。<strong>为了提高转发效率，减少堆叠线缆上的转发流量，设备支持流量本地优先转发。</strong>即从本设备进入的流量，优先从本设备相应的接口转发出去；如果本设备相应的接口故障或者流量已经达到接口的线速，那么就从其它堆叠成员交换机的接口转发出去。</p><p><img src="https://i.loli.net/2019/06/22/5d0daa867bc0484968.jpg"></p><p>如上图所示，SwitchA与SwitchB组成堆叠，上下行都加入到Eth-Trunk。如果没有本地优先转发，则从SwitchA进入的流量，根据当前Eth-Trunk的负载分担方式，会有一部分经过堆叠线缆，从SwitchB的物理接口转发出去。设备支持本地优先转发之后，从SwitchA进入的流量，只会从SwitchA的接口转发，流量不经过堆叠线缆。</p><p><strong>设备堆叠ID缺省为0。</strong>堆叠时由堆叠主交换机对设备的堆叠ID进行管理，当堆叠系统有新成员加入时，如果新成员与已有成员堆叠ID冲突，则堆叠主交换机从0～最大的堆叠ID进行遍历，找到第一个空闲的ID分配给该新成员。新建堆叠或堆叠成员变化时，如果不在堆叠前手动指定各设备的堆叠ID，则由于启动顺序等原因，最终堆叠系统中各成员的堆叠ID是随机的。因此，在建立堆叠时，建议提前规划好设备的堆叠ID，或通过特定的操作顺序，使设备启动后的堆叠ID与规划的堆叠ID一致。<strong>修改堆叠ID设备需要重启。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0daabda7d4130313.jpg"></p><p>如上图所示，<strong>堆叠成员加入是指向已经稳定运行的堆叠系统添加一台新的交换机</strong>。堆叠成员加入分为<strong>新成员交换机带电加入和不带电加入</strong>，带电加入则需要采用堆叠合并的方式完成，此处堆叠成员加入是指不带电加入。新成员交换机加入堆叠时，<strong>建议采用不带电加入</strong>。</p><p>堆叠成员加入的过程如下：</p><p><strong>Step1：</strong>新加入的交换机连线上电启动后，进行角色选举，新加入的交换机会选举为从交换机，堆叠系统中原有主备从角色不变。</p><p><strong>Step2：</strong>角色选举结束后，主交换机更新堆叠拓扑信息，同步到其他成员交换机上，并向新加入的交换机分配堆叠ID（新加入的交换机没有配置堆叠ID或配置的堆叠ID与原堆叠系统的冲突时）。</p><p><strong>Step3：</strong>新加入的交换机更新堆叠ID，并同步主交换机的配置文件和系统软件，之后进入稳定运行状态。</p><p><strong>堆叠成员退出是指成员交换机从堆叠系统中离开</strong>。根据退出成员交换机角色的不同，对堆叠系统的影响也有所不同：当主交换机退出，备份交换机升级为主交换机，重新计算堆叠拓扑并同步到其他成员交换机，指定新的备交换机，之后进入稳定运行状态。当备交换机退出，主交换机重新指定备交换机，重新计算堆叠拓扑并同步到其他成员交换机，之后进入稳定运行状态。当从交换机退出，主交换机重新计算堆叠拓扑并同步到其他成员交换机，之后进入稳定运行状态。堆叠成员交换机退出的过程，主要就是拆除堆叠线缆和移除交换机的过程。</p><blockquote><p><strong>对于环形堆叠：</strong>成员交换机退出后，为保证网络的可靠性还需要把退出交换机连接的两个端口通过堆叠线缆进行连接。</p><p><strong>对于链形堆叠：</strong>拆除中间交换机会造成堆叠分裂。这时需要在拆除前进行业务分析，尽量减少对业务的影响。</p></blockquote><h3 id="高级STP欺骗：跨设备链路聚合M-LAG"><a href="#高级STP欺骗：跨设备链路聚合M-LAG" class="headerlink" title="高级STP欺骗：跨设备链路聚合M-LAG"></a><strong>高级STP欺骗：跨设备链路聚合M-LAG</strong></h3><p>STP会严重浪费链路资源，根源在于它会禁止冗余链路上的转发。如果通过一种办法来“欺骗”STP，让它认为物理拓扑中没有冗余链路，那么就可以解决上述问题。实际上，上面的VRRP、堆叠技术中就用到了跨设备链路聚合，不过在实现上过于复杂，因此又出现了一类技术，它们不再为控制平面做集群，只保留了跨设备链路聚合的能力，这类技术以M-LAG为代表。</p><p><strong>定义：M-LAG（Multichassis Link Aggregation Group）即跨设备链路聚合组，是一种实现跨设备链路聚合的机制，将一台设备与另外两台设备进行跨设备链路聚合，从而把链路可靠性从单板级提高到了设备级，组成双活系统。</strong>如下图左边所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0dab358e08a98162.jpg"></p><p>如上图右边所示，用户侧设备Switch（可以是交换机或主机）通过M-LAG机制与另外两台设备（SwitchA和SwitchB）进行跨设备链路聚合，共同组成一个双活系统。这样可以实现SwitchA和SwitchB共同进行流量的转发，保证网络的可靠性。</p><p>M-LAG作为一种跨设备链路聚合的技术，除了具备增加带宽、提高链路可靠性、负载分担的优势外，还具备以下优势：</p><ul><li><strong>更高的可靠性：</strong>把链路可靠性从单板级提高到了设备级。</li><li><strong>简化组网及配置：</strong>可以将M-LAG理解为一种横向虚拟化技术，将双归接入的两台设备在逻辑上虚拟成一台设备。M-LAG提供了一个没有环路的二层拓扑同时实现冗余备份，不再需要繁琐的生成树协议配置，极大的简化了组网及配置。</li><li><strong>独立升级：</strong>两台设备可以分别进行升级，保证有一台设备正常工作即可，对正在运行的业务几乎没有影响。</li></ul><p>下表对M-LAG涉及的相关概念做了个总结，如下：</p><table><thead><tr><th>概念</th><th>说明</th></tr></thead><tbody><tr><td>M-LAG主设备</td><td>部署M-LAG且状态为主的设备。</td></tr><tr><td>M-LAG备设备</td><td>部署M-LAG且状态为备的设备。<strong>说明：</strong>正常情况下，主设备和备设备同时进行业务流量的转发。</td></tr><tr><td>peer-link链路</td><td>peer-link链路是一条直连链路且必须做链路聚合，用于交换协商报文及传输部分流量。为了增加peer-link链路的可靠性，推荐采用多条链路做链路聚合。</td></tr><tr><td>peer-link接口</td><td>peer-link链路两端直连的接口均为peer-link接口。</td></tr><tr><td>M-LAG成员接口</td><td>M-LAG主备设备上连接用户侧主机（或交换设备）的Eth-Trunk接口。为了增加可靠性，推荐链路聚合配置为LACP模式。</td></tr></tbody></table><p><strong>基于M-LAG组成的双活系统提供了设备级的可靠性，如下图所示，M-LAG的建立过程有如下几个步骤：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0dab988404b57115.jpg"></p><p><strong>Step1：</strong>当M-LAG两台的设备完成配置后，两台的设备会通过peer-link链路定期发送M-LAG协商报文。当收到对端的M-LAG协商报文后，会判断M-LAG协商报文中的DFS Group编号是否和本端相同。如果两台的DFS Group编号相同，则这两台设备配对成功。</p><p><strong>Step2：</strong>配对成功后，两台设备会通过比较M-LAG协商报文中的DFS Group优先级确定出主备状态。以SwitchB为例，当SwitchB收到SwitchA发送的报文时，SwitchB会查看并记录对端信息，然后比较DFS Group的优先级，如果SwitchA的DFS Group优先级高于本端的DFS Group优先级，则确定SwitchA为M-LAG主设备，SwitchB为M-LAG从设备。如果SwitchA和SwitchB的DFS Group优先级相同，比较两台设备的MAC地址，确定MAC地址小的一端为M-LAG主设备。这里的主、备不影响正常的流量转发，只有在出现故障时才起作用。</p><p><strong>Step3：</strong>协商出主备后，两台设备之间会通过网络侧链路周期性地发送M-LAG心跳报文，也可以配置通过管理网络检测心跳，当两台设备均能够收到对端发送的报文时，双活系统即开始正常的工作。M-LAG心跳报文主要用于peer-link故障时的双主检测。</p><p><strong>Step4：</strong>正常工作后，两台设备之间会通过peer-link链路发送M-LAG同步报文实时同步对端的信息，M-LAG同步报文中包括MAC表项、ARP表项等，这样任意一台设备故障都不会影响流量的转发，保证正常的业务不会中断。</p><p>M-LAG双活系统建立成功后即进入正常的工作，M-LAG主备设备负载分担共同进行流量的转发。如果出现故障，无论是链路故障、设备故障还是peer-link故障，M-LAG都能够保证正常的业务不受影响。按照数据中心TOR与EOR之间连接要求，我们这里仅以交换机双归接入普通以太网络和IP网络为例进行介绍。</p><p>正常工作时，如下图所示，来自非M-LAG成员端口的单播流量，按照正常的转发流程进行转发，而来自M-LAG成员端口的单播流量按照M-LAG聚合设备负荷分担方式进行转发。</p><p><img src="https://i.loli.net/2019/06/22/5d0dabe4ab98052900.jpg"></p><p>而来自非M-LAG成员接口的组播/广播流量 ，在SwitchA收到组播流量后向各个下一跳转发，当流量到达SwitchB时，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量不会向S-2转发。而来自M-LAG成员接口的组播/广播流量 ，在SwitchA收到组播流量后向各个下一跳转发，当流量到达SwitchB时，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量也不会向S-2转发。如下图所示：</p><p><img src="https://i.loli.net/2019/06/22/5d0dac55927f483641.jpg"></p><p>对于网络侧发往M-LAG成员接口的单播流量，流量会负载分担到SwitchA和SwitchB，然后发送至双活接入的设备。对于网络侧发往非M-LAG成员接口的单播流量，以发往S-1为例，流量不会进行负载分担，而是直接发到SwitchA，由SwitchA发往S-1。而网络侧的组播/广播流量不会在SwitchA、SwitchB之间采用负载分担方式转发，此处以SwitchA转发为例进行说明。</p><p>以SwitchA为例，SwitchA会发送到每一个用户侧端口，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量不会向S-2转发。如下图：</p><p><img src="https://i.loli.net/2019/06/22/5d0dac78abf2e86468.jpg"></p><p>故障情况时，如下图所示，当peer-link链路故障时，按照M-LAG应用场景不同，M-LAG的设备表现也不同。</p><p><img src="https://i.loli.net/2019/06/22/5d0dac98e241381855.jpg"></p><p>当M-LAG应用于TRILL网络的双归接入时，peer-link故障但心跳状态正常会触发状态为备的设备上M-LAG接口处于Error-Down状态。当M-LAG应用于普通以太网络、VXLAN网络或IP网络的双归接入时，peer-link故障但心跳状态正常会触发状态为备的设备上除管理网口、peer-link接口和堆叠口以外的接口处于Error-Down状态。M-LAG主设备侧Eth-Trunk链路状态仍为Up，M-LAG备设备侧Eth-Trunk链路状态变为Down，双归场景变为单归场景。一旦peer-link故障恢复，处于Error-Down状态的M-LAG接口默认将在2分钟后自动恢复为Up状态，处于Error-Down状态的其它物理接口将自动恢复为Up状态。数据中心场景中接入交换机TOR上处于Error-Down状态的M-LAG接口默认在5分钟后自动恢复为Up状态。</p><p>当M-LAG主设备故障时，M-LAG备设备将升级为主，其设备侧Eth-Trunk链路状态仍为Up，流量转发状态不变，继续转发流量。</p><p><img src="https://i.loli.net/2019/06/22/5d0dace16c0f061750.jpg"></p><blockquote><p>M-LAG主设备侧Eth-Trunk链路状态变为Down，双归场景变为单归场景。M-LAG主备设备端口状态默认不回切，即当M-LAG主设备故障发生时，M-LAG由备状态升级为主状态的设备仍保持主状态，恢复故障的主设备成为M-LAG的备设备。</p><p>如果是M-LAG备设备发生故障，M-LAG的主备状态不会发生变化，M-LAG备设备侧Eth-Trunk链路状态变为Down。M-LAG主设备侧Eth-Trunk链路状态仍为Up，流量转发状态不变，继续转发流量，双归场景变为单归场景。</p></blockquote><p>当下行Eth-Trunk链路发生故障，M-LAG主备状态不会变化，流量切换到另一条链路上进行转发。发生故障的Eth-Trunk链路状态变为Down，双归场景变为单归场景。</p><p><img src="https://i.loli.net/2019/06/22/5d0dad0a9c43c76801.jpg"></p><p>当上行链路发生故障，由于双归接入普通以太网络时，心跳报文一般是走管理网络，故不影响M-LAG主设备的心跳检测，对于双活系统没有影响，M-LAG主备设备仍能够正常转发。如图中所示，由于M-LAG主设备的上行链路故障，通过M-LAG主设备的流量均经过peer-link链路进行转发。</p><p><img src="https://i.loli.net/2019/06/22/5d0dad348065a39648.jpg"></p><p><strong>双归接入IP网络的M-LAG设备在正常工作时，与双归接入普通以太网类似，唯一区别是在为了保证M-LAG上三层组播功能正常，需要在M-LAG主备设备间部署一条三层直连链路，部分组播流量可以通过该三层链路进行转发，这条链路一般称为C链。在故障场景下，双归接入IP网络的M-LAG设备与接入普通以太网也只在上行链路故障时，表现有所区别。</strong></p><p>双归接入IP网络的M-LAG设备，在上行链路故障时，M-LAG备设备由于收不到主设备的心跳报文而变为双主状态。</p><p><img src="https://i.loli.net/2019/06/22/5d0dad7dd2d6613666.jpg"></p><p>这时用户侧流量到达SwitchA时，不能经过peer-link链路转发且没有可用的上行出接口，SwitchA会将用户流量全部丢弃。此时，需要分别在M-LAG主备设备上配置Monitor Link关联上行接口和下行接口，当上行出接口Down时，下行接口状态也会变为Down，这样就可以防止用户侧流量被丢弃。</p><p><strong>M-LAG特性主要应用于将服务器或交换机双归接入普通以太网络、TRILL（Transparent Interconnection of Lots of Links）、VXLAN（Virtual eXtensible Local Area Network）和IP网络。</strong>一方面可以起到负载分担流量的作用，另一方面可以起到备份保护的作用。由于M-LAG支持多级互联，M-LAG的组网可以分为单级M-LAG和多级M-LAG。</p><p>在电信云数据中心中，为了保证可靠性，服务器一般采用链路聚合的方式接入网络，如果服务器接入的设备故障将导致业务的中断。为了避免这个问题的发生，服务器可以采用跨设备链路聚合的方式接入网络，在SwitchA与SwitchB之间部署M-LAG，实现服务器的双归接入。组成M-LAG的两台TOR形成负载分担，共同进行流量转发，当其中一台设备发生故障时，流量可以快速切换到另一台设备，保证业务的正常运行。服务器双归接入时的配置和一般的链路聚合配置没有差异，必须保证服务器侧和交换机侧的链路聚合模式一致，推荐两端均配置为LACP模式。除了接入交换机TOR组成M-LAG外，汇聚交换机EOR之间也需要部署M-LAG配置，与下层TOR的M-LAG进行级联，这样不仅可以简化组网，而且在保证可靠性的同时可以扩展双归接入服务器的数量。多级M-LAG互联必须基于V-STP方式进行配置，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0dadce3fbec15942.jpg"></p><p>V-STP方式下，组成M-LAG的设备在二层网络中可以不作为根桥，组网灵活且支持M-LAG的级联。V-STP功能还可以解决M-LAG中错误配置或错误连线导致的环路问题，所以推荐采用V-STP方式。</p><p><strong>VRRP技术、堆叠以及M-LAG数据中心二、层设备虚拟化技术优缺点对比及适用场景总结</strong></p><table><thead><tr><th></th><th><strong>优点</strong></th><th><strong>缺点</strong></th><th><strong>应用场景</strong></th></tr></thead><tbody><tr><td><strong>堆叠</strong></td><td>简化本地网络管理节点、易维护，同时增加系统端口密度和带宽，充分发挥设备性能。</td><td>缺点就是可靠性不高，堆叠系统分裂、系统升级都会影响业务中断</td><td>广泛应用于企业、教育</td></tr><tr><td><strong>VRRP</strong></td><td>网络设备冗余、可靠性高</td><td>配置复杂、网络建设投入成本高，不能充分发挥设备的网络性能</td><td>银行、证券、政府内网</td></tr><tr><td><strong>M-LAG</strong></td><td>网络可靠性非常高，设备控制层面独立，能单独设备升级且不影响业务，充分发挥设备性能</td><td>管理节点多（控制层面无法虚拟化）</td><td>银行、证券、数据中心（双规场景）</td></tr></tbody></table><h2 id="大二层网络的基石VxLAN"><a href="#大二层网络的基石VxLAN" class="headerlink" title="大二层网络的基石VxLAN"></a><strong>大二层网络的基石VxLAN</strong></h2><h3 id="大二层是个什么鬼？"><a href="#大二层是个什么鬼？" class="headerlink" title="大二层是个什么鬼？"></a><strong>大二层是个什么鬼？</strong></h3><p>虽然Leaf-Spine为无阻塞传输提供了拓扑的基础，但是还需有配套合适的转发协议才能完全发挥出拓扑的能力。STP的设计哲学与Leaf-Spine完全就是不相容的，冗余链路得不到利用，灵活性和扩展性极差，废除STP而转向“大二层”成了业界的基本共识。</p><p>说到大二层的“大”，首先体现在物理范围上。虚拟机迁移是云数据中心的刚性需求，由于一些License与MAC地址是绑定的，迁移前后虚拟机的MAC地址最好不变，同时为了保证业务连续，迁移前后虚拟机的IP地址也不可以变化。迁移的位置是由众多资源因素综合决定的，网络必须支持虚拟机迁移到任何位置，并能够保持位于同一个二层网络中。所以大二层要大到横贯整个数据中心网络，甚至是在多个数据中心之间。</p><p>大二层的“大”，还意味着业务支撑能力的提升。随着公有云的普及，“多租户”成了云数据中心网络的基础能力。而传统二层网络中，VLAN最多支持的租户数量为4096，当租户间IP地址重叠的时候，规划和配置起来也比较麻烦，因此VLAN不能很好地支撑公有云业务的飞速发展。同理，对于大规模私有云而言，VLAN也难以胜任其对于网络虚拟化提出的要求。</p><p>大二层网络的实现主要依赖于网络叠加技术。网络叠加技术指的是一种物理网络架构上叠加的虚拟化技术模式，其大体框架是对基础网络不进行大规模修改的条件下，实现应用在网络上的承载，并能与其他网络业务分离，以基于IP的基础网络技术为主。其实这种模式是以对传统技术的优化而形成的。早期就有标准支持的二层Overlay技术，如RFC3378（Ethernet in IP），并且基于Ethernet over GRE的技术。H3C与思科都在物理网络基础上分别发展了私有二层Overlay技术：<strong>EVI（Ethernet Virtual Interconnection）与OTV（Overlay Transport Virtualization）</strong>。主要用于解决数据中心之间的二层互联与业务扩展问题，并且对于承载网络的基本要求是IP可达，部署上简单且扩展方便。</p><p><strong>网络叠加技术可以从技术解决目前数据中心面临的三个主要问题：</strong></p><p><strong>1）解决了虚拟机迁移范围受到网络架构限制的问题。</strong>网络叠加是一种封装在IP报文之上的新的数据格式，因此，这种数据可以通过路由的方式在网络中分发，而路由网络本身并无特殊网络结构限制，具备良性大规模扩展能力，并且对设备本身无特殊要求，且具备很强的故障自愈能力、负载均衡能力。</p><p><strong>2）解决了虚拟机规模受网络规格限制的问题</strong>。虚拟机数据封装在IP数据包中后，对网络只表现为封装后的网络参数，即隧道端点的地址，因此，对于承载网络（特别是接入交换机），MAC地址规格需求极大降低，最低规格也就是几十个（每个端口一台物理服务器的隧道端点MAC）。但是，对于核心/网关处的设备表项（MAC/ARP）要求依然极高，当前的解决方案是采用分散方式，即通过多个核心/网关设备来分散表项的处理压力。</p><p><strong>3）解决了网络隔离/分离能力限制的问题。</strong>针对VLAN数量在4000以内的限制，在网络叠加技术中沿袭了云计算“<strong>租户</strong>”的概念，称之为Tenant ID（租户标识），用24或64比特表示，可以支持16M的虚拟隔离网络划分。针对VLAN技术下网络的TRUANK ALL（VLAN穿透所有设备）的问题，网络叠加对网络的VLAN配置无要求，可以避免网络本身的无效流量带宽浪费，同时网络叠加的二层连通基于虚拟机业务需求而创建，在云的环境中全局可控。</p><h3 id="Overlay网络常用技术协议"><a href="#Overlay网络常用技术协议" class="headerlink" title="Overlay网络常用技术协议"></a><strong>Overlay网络常用技术协议</strong></h3><p>目前，IETF在Overlay技术领域有如下三大技术路线正在讨论：<strong>VxLAN（Virtual eXtensible Local Area Network）</strong>，是由VMware、思科、Arista、Broadcom、Citrix和红帽共同提出的IETF草案，是一种将以太网报文封装在UDP传输层上的隧道转发模式，目的UDP端口号为4798。<strong>NVGRE（Network Virtualization using Generic Routing Encapsulation）</strong>，是微软、Dell等提出的草案，是将以太网报文封装在GRE内的一种隧道转发模式。<strong>STT（Stateless Transport Tunneling）</strong>，是Nicira公司提出的一种Tunnel技术，目前主要用于Nicira自己的NVP平台上。STT利用了TCP的数据封装形式，但改造了TCP的传输机制，数据传输遵循全新定义的无状态机制，无需三次握手，以太网数据封装在无状态TCP中。三种网络叠加技术的优缺点和数据封装格式如下：</p><p><img src="https://i.loli.net/2019/06/22/5d0dae365edc152866.jpg"></p><p><strong>这三种二层网络叠加技术，大体思路均是将以太网报文承载到某种隧道层面，其差异性在于选择和构造隧道的不同，而底层均是IP转发。</strong>VxLAN和STT对于现网设备的流量均衡要求较低，即负载链路负载分担适应性好，一般的网络设备都能对L2~L4的数据内容参数进行链路聚合或等价路由的流量均衡，而NVGRE则需要网络设备对GRE扩展头感知并对flow ID进行Hash，需要硬件升级；STT对于TCP有较大修改，隧道模式接近UDP性质，隧道构造技术具有革新性，且复杂度较高，而VxLAN利用了现有通用的UDP传输，成熟性极高。总体来说，<strong>VLXAN技术相对具有优势。</strong></p><h3 id="VxLAN的包封装格式"><a href="#VxLAN的包封装格式" class="headerlink" title="VxLAN的包封装格式"></a><strong>VxLAN的包封装格式</strong></h3><p>VXLAN报文是在原始的二层报文前面再封装一个新的报文，新的报文中和传统的以太网报文类似，拥有源目mac、源目ip等元组。当原始的二层报文来到vtep节点后会被封装上VXLAN包头（在VXLAN网络中把可以封装和解封装VXLAN报文的设备称为vtep，vtep可以是虚拟switch也可以是物理switch），打上VXLAN包头的报文到了目标的vtep后会将VXLAN包头解封装，并获取原始的二层报文。</p><p><img src="https://i.loli.net/2019/06/22/5d0dae68818e161615.jpg"></p><p><strong>1）VXLAN头部。</strong>共计8个字节，目前使用的是Flags中的一个8bit的标识位和24bit的VNI（Vxlan Network Identifier），其余部分没有定义，但是在使用的时候必须设置为0x0000。</p><p><strong>2）外层的UDP报头。目的端口使用4798，</strong>但是可以根据需要进行修改，同时UDP的校验和必须设置成全0。</p><p><strong>3）IP报文头。</strong>目的IP地址可以是单播地址，也可以是多播地址。单播情况下，目的IP地址是VTEP（Vxlan Tunnel End Point）的IP地址；多播情况下引入VxLAN管理层，利用VNI和IP多播组的映射来确定VTEPs。<strong>protocol设置值为0x11</strong>，说明这是UDP数据包。Source ip是源vTEP_IP。Destination ip是目的VTEP IP。</p><p><strong>4）Ethernet Header。</strong>Destination Address：目的VTEP的MAC地址，即为本地下一跳的地址（通常是网关MAC地址）。<strong>VLAN Type被设置为0x8100，并可以设置Vlan Id tag（这就是vxlan的vlan标签）</strong>。<strong>Ethertype设置值为0x8000，指明数据包为IPv4的</strong>。</p><p><strong>outer mac header以及outer ip header里面的相关元组信息都是vtep的信息，和原始的二层报文没有任何关系。</strong>所在数据包在源目vtep节点之间的传输和原始的二层报文是毫无关系的，依靠的是外层的包头完成。  除此之外还有几个字段需要关注：</p><p>1、隧道终端VTEP用于多VxLAN报文进行封装/解封装，包括MAC请求报文和正常VXLAN数据报文，在一端封装报文后通过隧道向另一端VTEP发送封装报文，另一端VTEP接收到封装的报文解封装后根据被封装的MAC地址进行转发，VTEP可由支持VXLAN的硬件设备或软件来实现。</p><p>2、在UDP header里面有一个source port的字段，用于VxLAN网络节点之间ECMP的hash；</p><p>3、在VXLAN Header里的reserved字段，作为保留字段，很多厂商都会加以运用来实现自己组网的一些特性。</p><h3 id="VxLAN的控制和转发平面"><a href="#VxLAN的控制和转发平面" class="headerlink" title="VxLAN的控制和转发平面"></a><strong>VxLAN的控制和转发平面</strong></h3><p><strong>1）数据平面—隧道机制</strong> </p><p>VTEP为虚拟机的数据包加上了层包头，这些新的报头只有在数据到达目的VTEP后才会被去掉。中间路径的网络设备只会根据外层包头内的目的地址进行数据转发，对于转发路径上的网络来说，一个Vxlan数据包跟一个普通IP包相比，除了个头大一点外没有区别。由于VxLAN的数据包在整个转发过程中保持了内部数据的完整，因此<strong>VxLAN的数据平面是一个基于隧道的数据平面</strong>。</p><p><strong>2）控制平面—改进的二层协议</strong></p><p> VxLAN不会在虚拟机之间维持一个长连接，所以VxLAN需要一个控制平面来记录对端地址可达情况。控制平面的表为（VNI，内层MAC，外层vtep_ip）。Vxlan学习地址的时候仍然保存着二层协议的特征，节点之间不会周期性的交换各自的路由表，对于不认识的MAC地址，VXLAN依靠组播来获取路径信息(如果有SDN Controller，可以向SDN单播获取)。</p><p>另一方面，VxLAN还有自学习的功能，当VTEP收到一个UDP数据报后，会检查自己是否收到过这个虚拟机的数据，如果没有，VTEP就会记录源vni/源外层ip/源内层mac对应关系，避免组播学习。</p><h3 id="VxLAN的报文转发"><a href="#VxLAN的报文转发" class="headerlink" title="VxLAN的报文转发"></a><strong>VxLAN的报文转发</strong></h3><p><strong>1）ARP报文转发，转发过程如下图：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0daed33215e73398.jpg"></p><p><strong>Step1：</strong>主机A向主机B发出ARP Request，Src MAC为MAC-A，Dst MAC为全F；</p><p><strong>Step2：</strong>ARP Request报文到达vtep-1后，vtep-1对其封装VxLAN包头，其中外层的Src MAC为vtep-1的MAC-1，Dst MAC为组播mac地址， Src ip为vtep-1的IP-1，Dst ip为组播ip地址，并且打上了VxLAN VNID：10。由于vtep之间是三层网络互联的，广播包无法穿越三层网络，所以只能借助组播来实现arp报文的泛洪。通常情况下一个组播地址对应一个VNID，同时可能会对应一个租户或者对应一个vrf网络，通过VNID进行租户之间的隔离。</p><p><strong>Step3：</strong>打了VxLAN头的报文转发到了其他的vtep上，进行VxLAN头解封装，原始的ARP Request报文被转发给了vtep下面的主机，并且在vtep上生成一条MAC-A（主机A的mac）、VxLAN ID、IP-1（vtep-1的ip）的对应表项；</p><p><strong>Step4：</strong>主机B收到ARP请求，回复ARP Response，Src MAC：MAC-B、Dst MAC：MAC-A；</p><p><strong>Step5：</strong>ARP Response报文到达vtep-2后，被打上VxLAN的包头，此时外层的源目mac和ip以及VxLAN ID是根据之前在vtep-2上的MAC-A、VXLAN ID、IP-1对应表项来封装的，所以ARP Response是以单播的方式回复给主机A；</p><p><strong>Step6：</strong>打了VxLAN头的报文转发到vtep-1后，进行VxLAN头的解封装，原始的ARP Response报文被转发给了主机A；</p><p><strong>Step7：</strong>主机A收到主机B返回的ARP Response报文，整个ARP请求完成。</p><p><strong>这种用组播泛洪ARP报文的方式是VxLAN技术早期的方式，这种方式也是有一些缺点，比如产生一些不可控的组播流量等，所以现在很多厂商已经使用了控制器结合南向协议（比如openflow或者一些私有南向协议）来解决ARP的报文转发问题。</strong></p><p><strong>2）单播报文转发（同一个Vxlan），转发过程如下：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0daf0abec5020399.jpg"></p><p>在经过arp报文后，vtep-1和vtep-2上都会形成一个VxLAN二层转发表，大致如下（不同厂商表项可能略有不同，但是最主要的是以下元素）：</p><p><strong>vtep-1：</strong></p><table><thead><tr><th><strong>MAC</strong></th><th><strong>VNI</strong></th><th><strong>vtep</strong></th></tr></thead><tbody><tr><td>MAC-A</td><td>10</td><td>e1/1</td></tr><tr><td>MAC-B</td><td>10</td><td>vtep-2 ip</td></tr></tbody></table><p><strong>vtep-2：</strong></p><table><thead><tr><th><strong>MAC</strong></th><th><strong>VNI</strong></th><th><strong>vtep</strong></th></tr></thead><tbody><tr><td>MAC-B</td><td>10</td><td>e1/1</td></tr><tr><td>MAC-A</td><td>10</td><td>vtep-1 ip</td></tr></tbody></table><p> <strong>Step1：</strong>host-A将原始报文上行送到vtep-1；</p><p><strong>Step2：</strong>根据目的mac和VNI的号（这里的VNI获取是vlan和vxlan的mapping查询出的结果），查找到外层的目的ip是vtep-2 ip，然后将外层的源目ip分别封装为vtep-1 ip和vtep-2 ip，源目mac为下一段链路的源目mac；</p><p><strong>Step3：</strong>数据包穿越ip网络；</p><p><strong>Step4：</strong>vtep-2根据VNI、外层的源目ip，进行解封装，通过VNI和目的mac查表，得到目的端口是e1/1；</p><p><strong>Step5：</strong>host-B接受此原始报文，并回复host-A，回复过程同上一致。</p><p><strong>3）不同Vxlan之间，VxLAN与VLAN之间转发</strong></p><p>不同VxLAN之间转发，VxLAN与VLAN之间转发，各个厂商的解决方案不大一致，一般情况下所以跨VxLAN的转发需要一个叫VxLAN网关的设备，这个VxLAN网关可以是物理交换机也可以是软件交换机。具体如下图：</p><p><img src="https://i.loli.net/2019/06/22/5d0daf467c21689706.jpg"></p><p>另外cisco对不同vxlan之间的转发又是另外一种模式，由于cisco是一家硬件设备厂商，所以它的所有vtep都是硬件交换机，但是通常物理交换机收到的都是报文已经是Hypervisor层打了vlan tag了，所以cisco的处理方式会比较复杂，大体上是通过一个L3的VNI来完成。</p><p>VxLAN和VLAN之间的转发这部分，说实话我也想不到太多实际的应用场景，如果是一个L3的网络更是没有意义，因为VLAN的网关终结在了leaf节点上，唯一想到的场景可能就是L2网络中部分设备不支持打VxLAN只能打VLAN Tag（硬件overlay一般不会出现这个情况，软件overlay情况下，会有一些物理服务器没有vswitch来打vxlan tag，只能依靠硬件交换机来打vlan tag）。这种情况下是需要通过VxLAN网关设备来完成一个VxLAN到VLAN或者VLAN到VxLAN的mapping关系。这其实很好理解，就是比如一个打了VxLAN包头的报文要去访问一个某一个VLAN，这时候报文需要被先送到一个叫VxLAN网关设备，这个设备查找VxLAN和VLAN的对应表后，将VxLAN包头拆去，再打上内层的VLAN Tag，然后将数据包送给VLAN的网络中。同理，VLAN到VxLAN网络反之处理类似。</p><h3 id="典型的VXLAN组网模式"><a href="#典型的VXLAN组网模式" class="headerlink" title="典型的VXLAN组网模式"></a><strong>典型的VXLAN组网模式</strong></h3><p><strong>1）软件模式：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0dafc75e03223461.jpg"></p><p>软件模式中vtep功能由vswitch实现，图中物理交换网络是一个L3的网络，实际软件overlay的场景下物理的交换网并不一定要是一个L3的网络，只要物理服务器的ip互相可达即可，当然L3的网络是一个比较好的选择，因为L3的网络扩展性比L2好，L2网络一大就会有各种二层问题的存在，比如广播泛洪、未知单播泛洪，比如TOR的mac问题，这个前文已经讲了很多，这里就不赘述了。</p><p><strong>优点：</strong></p><p>1）硬件交换机的转发平面和控制平面解耦，更加灵活，不受物理设备和厂商的限制；</p><p>2）现有的硬件网络设备无需进行替换。（如果现有物理设备是L2的互联网络也可以使用软件overlay的方式，当然L3互联更佳，overlay的网络还是建议L3的方式）</p><p><strong>缺点：</strong></p><p>1）vSwitch转发性能问题，硬件交换机转发基本都是由ASIC芯片来完成，ASIC芯片是专门为交换机转发而设计的芯片，而vSwitch的转发是由x86的CPU来完成，当vSwitch作为vtep后并且增加4-7层服务外加分布式路由的情况下，性能、可靠性上可能会存在问题；</p><p>2）无法实现虚拟网络和物理服务器网络的共同管理；</p><p>3）如果是用商业的解决方案，则软件层面被厂商绑死，如果选用开源或自研的vswitch，存在一定的难度和风险。</p><p>4）软件overlay容易造成管理上的混乱，特别对于传统行业来说，overlay网络到底是网络运维管还是云平台运维管，会有一些利益冲突。不过这个并不是一个技术问题，在云计算环境下，类似这种问题会越来越多。</p><p>案例：互联网公司用软件overlay的还挺多，下面附上几个的方案简介，设计一些商业解决方案和一些开源的解决方案：</p><p><a href="http://www.sdnap.com/sdn-study/5885.html" target="_blank" rel="noopener">UCloud的“公有云SDN网络实践”分享</a></p><p><a href="http://www.sdnlab.com/14855.html" target="_blank" rel="noopener">浙江电信云资源池引入VxLAN的部署初探（NSX）</a></p><p><strong>2）硬件模式：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db03a3bcd890662.jpg"></p><p>硬件模式中vtep功能由物理交换机来实现，硬件overlay解决方案中物理网络<strong>一般都是L3的网络</strong>。</p><p><strong>优点：</strong></p><p>1）硬件交换机作为vtep转发性能比较有保证；</p><p>2）虚拟化网络和物理服务器网络可以统一管理；</p><p>3）向下兼容各种的虚拟化平台（商业或开源）。</p><p><strong>缺点：</strong></p><p>1）各硬件交换机厂商的解决方案难有兼容性，可能会出现被一个厂商锁定的情况；</p><p>2）老的硬件交换机基本上芯片都不支持VxLAN技术，所以要使用vxlan组网，会出现大规模的设备升级；</p><p><strong>案例：</strong>硬件overlay主要是Cisco、华为、H3C这种传统的硬件厂商提供解决方案，下面链接一个华为的的案例：</p><p><a href="http://www.sdnap.com/sdn-technology/6132.html" target="_blank" rel="noopener">美团云携手华为SDN解决方案</a></p><p><strong>3）软硬件混合模式</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db0632f6bc42809.jpg"></p><p>软硬件混合模式是一种比较折中的解决方案，虚拟化平台上依然使用vSwitch作为vtep，而对于物理服务器使其接入物理的vtep交换机。</p><p><strong>优点：</strong>整合了软、硬件模式的优缺点，是一种比较理想的overlay网络模型；</p><p><strong>缺点：</strong>落地起来架构比较复杂，统一性比较差，硬件交换机和软件交换机同时作为vtep，管理平台上兼容性是个问题。</p><p><strong>案例：</strong>这个解决方案目前一些硬件的交换机厂商会提供（比如华为），还有就是一些研发能力比较强的互联网公司会选择这个方案。</p><p>网络叠加技术作为网络虚拟化在数据层的实现手段，解决了虚拟机迁移范围受到网络架构限制、虚拟机规模受网络规格限制、网络隔离/分离能力限制的问题。同时，支持网络叠加的各种协议、技术正不断演进，VxLAN作为一种典型的叠加协议，最具有代表性。Linux内核3.7已经加入了对VxLAN协议的支持。另外，由IETF工作组提出的网络虚拟化叠加（NVO3）草案也在讨论之中，各大硬件厂商也都在积极参与标准的制定并研发支持网络叠加协议的网络产品，这些都在推动着软件定义网络SDN技术在云数据中心各个业务领域逐步落地。</p><h2 id="Spline-leaf架构下的SDN引入"><a href="#Spline-leaf架构下的SDN引入" class="headerlink" title="Spline-leaf架构下的SDN引入"></a><strong>Spline-leaf架构下的SDN引入</strong></h2><p>从 2003 年开始，随着虚拟化技术的引入，原来三层（three-tier）数据中心中，在二层以pod形式做了隔离的计算、网络和存储资源，形成一种池化的组网模式。这种技术产生了从接入层到核心层的大二层域的需求，如下图所示 。</p><p><img src="https://i.loli.net/2019/06/22/5d0db08b6b17627858.jpg"></p><p>虚拟机的引入，使得应用的部署方式越来越分布式，导致东西向流量越来越大。这些流量需要被高效地处理，并且还要保证低的、可预测的延迟。 然而，vPC只能提供两个并行上行链路，因此<strong>三层数据中心架构中的带宽成为了瓶颈</strong>。 三层架构的另一个问题是<strong>服务器到服务器延迟（server-to-server latency）随着流量路径的不同而不同。</strong></p><p>针对以上问题，提出了一种新的数据中心设计，称作基于Clos网络的Spine-and-Leaf架构（Clos network-based Spine-and-Leaf architecture），如下图所示。事实已经证明，这种架构可以提供高带宽、低延迟、非阻塞的服务器到服务器连接，<strong>成为超大规模网络首选。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db0c0c1a3f64646.jpg"></p><p>在以上两级Clos架构中，每个低层级的交换机（leaf）都会连接到每个高层级的交换机（spine），形成一个<strong>full-mesh拓扑</strong>。leaf 层由接入交换机组成，用于连接服务器等设备。spine层是网络的骨干，负责将所有的leaf连接起来。 fabric中的每个leaf都会连接到每个spine，如果一个spine挂了，数据中心的吞吐性能只会有轻微的下降。如果某个链路拥塞了，添加一个spine交换机就可以扩展每个leaf的上行链路，增大了leaf和spine之间的带宽，缓解了链路拥塞的问题。如果接入层的端口数量成为了瓶颈，那就直接添加一个新的leaf，然后将其连接到每个spine并做相应的配置即可。<strong>leaf层的接入端口和上行链路都没有瓶颈时，这个架构就实现了无阻塞。</strong></p><p><strong>在Spine-and-Leaf架构中，任意一个服务器到另一个服务器的连接，都会经过相同数量的设备（除非这两个服务器在同一leaf下面），这保证了延迟是可预测的，因为一个包只需要经过一个spine和另一个leaf就可以到达目的端。</strong></p><p>在数据中心的实际应用中，Spline-and-leaf架构又分为<strong>分层解耦架构</strong>和<strong>全融合架构</strong>两种，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db123413b967489.jpg"></p><p><strong>在分层解耦架构中，业务功能模块化，由不同的叶子节点分别实现。各节点可灵活合并和解耦，Pod分区具备灵活扩展能力。</strong>适用于东西流量预期有显著增长、数据中心规模大、未来高扩展的场景，比如：中国移动私有云部署场景。</p><p><strong>在全融合架构中，业务功能集中由一对核心交换机实现，属于接入+核心的二层架构。</strong>这种架构中，<strong>网络设备少，流量模型简单，维护简单，但扩展性较差。</strong>适用于中小型数据中心、DC规划可预见/扩展性低的场景，比如：各个中小企业的私有云部署场景。</p><p><strong><em>在数据中心中，到底使用大二层还是Spline-and-leaf解决方案，是由服务器的规模决定的，二者之间的区别只是服务器接入能力不同，并不存在谁比谁更先进一说。</em></strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db168c2f3385731.jpg"></p><blockquote><p>大二层架构是由核心层+接入层共两层组成，两台核心交换机做堆叠，作为网关，FW、LB等设备旁挂在网关边上。接入层TOR两两堆叠，可提供2000+台物理服务的接入能力。</p><p>Spline-and-leaf架构是由核心层+汇聚层+接入层共三层组成，两台核心交换机做堆叠，作为网关，FW、LB等设备旁挂在网关边上。Spine层交换机数量为2的倍数，相互独立，不做堆叠。接入层TOR两两堆叠，可提供2000~5000台物理服务的接入能力。</p></blockquote><p>中国移动私有云SDN解决方案技术规范中，对数据中内部的组网架构图要求如下（电信云的要求类似，多了网管CE的部署要求）。</p><p><img src="https://i.loli.net/2019/06/22/5d0db195edab866822.jpg"></p><ul><li>虚拟化/裸金属服务器混合场景优先采用混合SDN组网方式，即硬件接入交换机（SDN ToR）和VSW作为VTEP。</li><li>SDN控制器或控制器插件支持南向数据一致性校验和对账。</li><li>SDN控制器支持allowed-address-pairs功能。</li><li>控制器支持OpenFlow+Evpn流表和路由相互转换。</li><li>支持IPv4/IPv6双栈网络服务的自动化部署。</li><li>支持vFW上IPv6防火墙策略的自动化部署。</li><li>Server Leaf和Border leaf采用MC-LAG方式组网，并且由控制器下发MC-LAG相关业务。</li></ul><p>上述技术规范中，<strong>EVPN+OpenFlow双控制面是未来演进方向</strong>。相比OpenFlow单控制面来说，区别如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db1d1e9b4a86962.jpg"></p><p>在单控制面OpenFlow组网中，<strong>SDN控制器通过OpenFlow给虚拟交换机和物理交换机统一下发流表，虚拟交换机和物理交换机按照流表进行转发。</strong>优点就是OpenFlow灵活性高，SDN控制器和转发设备间可实现解耦。但是缺点很明显，也就是SDN控制器成为瓶颈，收敛性能和稳定性欠佳，不适合大规模资源池网络，最关键的是和已有IP网络的兼容性不好，如检测等特性。</p><p>在双控制面EVPN+OpenFlow组网中，<strong>SDN控制器通过OpenFlow给虚拟交换机下发流表，物理交换机之间通过BGP-EVPN协议建立转发表项。</strong>优点就是可靠性高，兼具OpenFlow的灵活性和EVPN的扩展性，符合未来演进方向，也减少现有设备投资改造费用。缺点就是对控制器要求较高，需要控制器支持EVPN协议，具备OpenFlow流表和EVPN转发表的翻译能力，而且这属于设备商的私有接口部分，只有少数几个厂家支持，比如，华为。</p><p>在实际多机房部署时，具体规划如下图所示</p><p><img src="https://i.loli.net/2019/06/22/5d0db20054c3737805.jpg"></p><ul><li>各个机房通过东西向互联交换机互联。</li><li>每个机房部署一套SDN控制器和云平台，采用硬件分布式或混合分布式SDN方案。</li><li>核心生产区、测试区和DMZ区的服务器统一挂在业务TOR下面。</li><li>所有的SDN控制器和云平台统一部署在管理网络区（POD1内）。</li><li>FC SAN分散部署在各个机房内，分布式存储集中部署在POD7。</li><li>在每个机房内部署东西向防火墙，Internet出口部署南北向防火墙。</li></ul><p>以上就是数据中心层面服务器外部交换网络虚拟化的全部内容，至此网络虚拟化技术理论方面的知识点全部介绍完毕，后续会用一篇Linux原生网络虚拟化实战作为全部虚拟化专题的结束篇。可能有人会提到SDN还没讲，一方面是因为目前还在试点，另一方面是因为SDN涉及面很广，有时间的话我会写个专题，从协议、控制器部署、实际解决方案等专门来介绍SDN。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据中心服务器外部交换网络的虚拟化主要包括：&lt;strong&gt;数据中心内部二、三层交换设备虚拟化、大二层组网架构&lt;/strong&gt;和&lt;strong&gt;软件定义网络SDN技术。&lt;/strong&gt;其中，软件定义网络SDN技术在目前中国移动NovoNet战略中还处于现网试点阶段，预计2020年初会引入电信云数据中心。而数据中心内部二、三层交换设备虚拟化及大二层组网架构已完全成熟，并已在当前电信云数据中心中实际部署，本文也主要讲述这两种技术，其中大二层组网基石VxLAN需要大家重点掌握。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-21-KVM到底是个啥？</title>
    <link href="https://kkutysllb.cn/2019/06/22/2019-06-21-KVM%E5%88%B0%E5%BA%95%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F/"/>
    <id>https://kkutysllb.cn/2019/06/22/2019-06-21-KVM到底是个啥？/</id>
    <published>2019-06-22T03:52:01.000Z</published>
    <updated>2019-06-22T05:02:04.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="KVM的现状"><a href="#KVM的现状" class="headerlink" title="KVM的现状"></a><strong>KVM的现状</strong></h2><p>KVM最初是由Qumranet公司的Avi Kivity开发的，作为他们的VDI产品的后台虚拟化解决方案。为了简化开发，Avi Kivity并没有选择从底层开始新写一个Hypervisor，而是选择了<strong>基于Linux kernel，通过加载模块使Linux kernel本身变成一个Hypervisor。</strong>2006年10月，在先后完成了基本功能、动态迁移以及主要的性能优化之后，Qumranet正式对外宣布了KVM的诞生。同月，<strong>KVM模块的源代码被正式纳入Linux kernel，成为内核源代码的一部分。</strong>2008年9月4日，Redhat公司以1.07亿美元收购了Qumranet公司，包括它的KVM开源项目和开发人员。自此，Redhat开始在其RHEL发行版中集成KVM，逐步取代Xen，并从RHEL7开始，正式不支持Xen。<a id="more"></a></p><p>在KVM出现之前，Xen虚拟化解决方案已经业界比较成熟的一款开源VMM，但是KVM出现之后，很快被Linux内核社区接受，就是因为Xen不是通过Linux内核去管理系统资源（硬件/软件），而是通过自身的管理系统去完成，仅这一点就让Linux内核社区很不爽。同时，Xen在当时设计上采用半虚方式，需要修改Guest OS的内核来满足I/O驱动性能要求，从而不支持商用OS（Windows、Mac OS）的虚拟化，使得Xen相比KVM来说，在硬件辅助虚拟化的支撑上包袱更重，转型困难重重。</p><p>目前，<strong>KVM已经成为OpenStack用户选择的事实上的Hypervisor标准。</strong>OpenStack自身调查数据显示，KVM占87%以上的部署份额，详细参见<a href="http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014/。可以说，KVM已经主导公有云部署的Hypervisor市场，同时在电信云部署方面也是一枝独秀。" target="_blank" rel="noopener">http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014/。可以说，KVM已经主导公有云部署的Hypervisor市场，同时在电信云部署方面也是一枝独秀。</a></p><p>功能上，虚拟化发展到今天，各个Hypervisor的主要功能都差不多。KVM由于其开源性，反而商业上的限制较少。性能上，KVM和Xen都能达到原生系统95%以上的效率（CPU、内存、网络、磁盘等benchmark衡量），KVM甚至还略微好过Xen一点点。微软虽然宣布其Hype-V的性能更好，但这只是微软一家之言，并没有公开的数据支撑。</p><p>在电信云NFV领域来说，由于通信网络设备的实时性要求很高，且NFV的开源平台OPNFV选择了OpenStack。为了更好实现网络功能虚拟化的愿景，其实时性要求责无旁贷的落到了KVM头上，NFV-KVM项目也就顺理成章地诞生了，它作为OPNFV的子项目主要解决KVM在实时性方面受到的挑战。详情请参见<a href="https://wiki.opnfv.org/display/kvm/Nfv-kvm。" target="_blank" rel="noopener">https://wiki.opnfv.org/display/kvm/Nfv-kvm。</a></p><p><strong>总的来说，虚拟化技术发展到今天已经非常成熟，再加上DPDK代码的开源化、KVM也好、其他Hypervisor也好，在转发性能的优化和硬件辅助虚拟化的支撑上都半斤八两，但由于KVM的开源性特性以及社区的热度，使得其在云计算领域解决方案一枝独秀。甚至，华为在其FusionSphere6.3版本也开始拥抱KVM而抛弃了Xen。要知道，华为在剑桥大学可是专门有一支团队在研究Xen虚拟化。</strong></p><h2 id="KVM虚拟化实现"><a href="#KVM虚拟化实现" class="headerlink" title="KVM虚拟化实现"></a><strong>KVM虚拟化实现</strong></h2><p><strong>KVM全称是Kernel-based Virtual Machine，即基于内核的虚拟机</strong>，是采用硬件辅助虚拟化技术的全虚拟化解决方案。对于I/O设备（如硬盘、网卡等），KVM即支持QEMU仿真的全虚，也支持virtio方式的半虚。<strong>KVM从诞生开始就定位于基于硬件虚拟化支持的全虚实现</strong>，由于其在Linux内核2.6版本后被集成，<strong>通过内核加载模式使得Linux内核变成一个事实上的Hypervisor</strong>，但是硬件管理还是由Linux Kernel来完成。因此，<strong>它是一个典型Type 2型虚拟化</strong>，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db4f46083634024.jpg"></p><p>如上图，<strong>一个KVM客户机就对应一个Linux进程，每个vCPU对应这个进程下的一个线程，还有单独处理I/O的线程，属于同一个进程组（忘了的，请回顾本站CPU虚拟化系列文章）。</strong>所以，宿主机上Linux Kernel可以像调度普通Linux进程一样调度KVM虚拟机，<strong>这种机制使得Linux Kernel的进程优化和调度功能优化等策略，都能用于KVM虚拟机。</strong>比如：通过进程权限限定功能可以限制KVM客户机的权限和优先级等。</p><p><strong>由于KVM嵌入Linux内核中，除了硬件辅助虚拟化（如VT-d）透传的硬件设备能被虚拟机看见外，其他的I/O设备都是QEMU模拟出来的，所以QEMU是KVM的天生好基友。而在内存管理方面，由于KVM本身就是Linux Kernel中一个模块，所以其内存管理完全依赖于Linux内核。</strong>Linux系统的所有内存管理机制，如大页、零页（重复页）共享KSM、NUMA、mmap共享内存等，都可以用于KVM虚拟机的内存管理上。</p><p><strong>在数据存储方面，由于KVM是Linux Kernel的一部分，它可以利用所有存储厂商的存储架构，支持Linux支持的任何存储和文件系统来存储数据。同时，还支持全局文件系统GFS2等共享文件系统上的虚拟机镜像</strong>，允许虚拟机在多个Host之间共享存储或使用逻辑卷共享存储。KVM虚拟机的原生磁盘格式为QCOW2，支持磁盘镜像、快照、多级快照和压缩加密等虚拟化特性。但是，有一点需要注意：<strong>基于KVM的镜像盘必须使用RAW格式（一种非精简格式盘，在华为FC解决方案中称为普通盘。它就像我们的物理硬盘一样，分配多大空间就是多大空间），否则利用镜像发布虚机会不成功。</strong></p><p><strong>在实时迁移方面，KVM虚拟机支持在多个Host之间热迁移，无论是OVS分布式虚拟交换机，还是Linux Bridge分布式虚拟交换机，KVM虚拟机都能完美兼容实现虚拟接入，且对用户（实际用户，APP等，也就是我们常说的租户这个概念）是透明的。同时，还支持将客户机当前状态，也就是快照保存到磁盘，并在以后恢复。</strong></p><p><strong>在设备驱动方面，KVM支持混合虚拟化，</strong>其中半虚拟化的驱动程序安装在虚拟机的OS中，允许虚拟机使用优化I/O接口而不用模拟设备。KVM使用的半虚拟化驱动程序是IBM和RedHat联合Linux社区开发的virtio标准，它是一个与Hypervisor独立的，构建设备驱动程序的接口，是KVM内核的另一个好基友，不仅支持KVM对其调用，还支持VMware、Hyper-V对其调用。同时，就像前面提到的，KVM也支持VT-d技术，两者如胶似漆，完美契合，通过将Host上PCI总线上的设备透传给虚拟机，让虚拟机可以直接使用原生的驱动程序来驱动这些物理设备。忘了啥是VT-d的，请回顾本站I/O虚拟化一文。同时，就像前面开篇提到的KVM在<strong>在性能方面能达到原生的95%以上，不差于Xen虚拟化，但在伸缩性方面支持拥有多达288个vCPU和4TB RAM的虚拟机，远超于Xen（Xen因为DM0的存在，其伸缩性受限，单机最大支持32个vCPU，1.5TB RAM）。</strong></p><p><strong>通过上面大致了解，可以说KVM就是在硬件辅助虚拟化技术之上构建起来的VMM。</strong>但并非要求所有硬件虚拟化技术都支持才能运行KVM虚拟化，<strong>KVM对硬件最低的依赖是CPU的硬件虚拟化支持</strong>（比如：Intel的VT-x技术和AMD的AMD-V技术），而其他的内存和I/O的硬件虚拟化支持，会让整个KVM虚拟化下的性能得到更多的提升。所以，我们在虚拟机部署KVM功能时，首先就是要查看宿主机Host（也就是我们实验环境的虚拟机，一般是在VMware Workstations的虚拟机打开CPU虚拟化功能。如果是真实环境，则需要物理服务器的BIOS中要开启VT-x功能，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db52ae861d10143.jpg"></p><p>然后，进入服务器后通过如下指令确认VT-x功能支持，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db5521d6ee20494.jpg"></p><p>如果什么输出都没有，那说明你的系统并没有支持虚拟化的处理 ，不能使用KVM。<strong>另外Linux发行版本必须在64bit环境中才能使用KVM。</strong></p><h2 id="KVM软件架构拆解"><a href="#KVM软件架构拆解" class="headerlink" title="KVM软件架构拆解"></a><strong>KVM软件架构拆解</strong></h2><p>KVM是在硬件虚拟化支持下的全虚拟化技术，所以它能在相应硬件上运行几乎所有的操作系统，如：Linux、Windows、FreeBSD、MacOS等。KVM虚拟化的核心主要由以下两个模块组成：</p><p><strong>1）KVM内核模块，它属于标准Linux内核的一部分，是一个专门提供虚拟化功能的模块，主要负责CPU和内存的虚拟化，</strong>包括：客户机的创建、虚拟内存的分配、CPU执行模式的切换、vCPU寄存器的访问、vCPU的执行。</p><p><strong>2）QEMU用户态工具，它是一个普通的Linux进程，为客户机提供设备模拟的功能，</strong>包括模拟BIOS、PCI/PCIE总线、磁盘、网卡、显卡、声卡、键盘、鼠标等。<strong>同时它通过ioctl系统调用与内核态的KVM模块进行交互。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db571ed3c212430.jpg"></p><p>上图中，<strong>在KVM虚拟化架构下，每个客户机就是一个QEMU进程</strong>，在一个宿主机上有多少个虚拟机就会有多少个QEMU进程。客户机中的每一个虚拟CPU对应QEMU进程中的一个执行线程，一个宿主机Host中只有一个KVM内核模块，所有虚拟机都与这个内核模块进行交互。</p><h3 id="KVM内核模块"><a href="#KVM内核模块" class="headerlink" title="KVM内核模块"></a><strong>KVM内核模块</strong></h3><p>KVM内核模块是KVM虚拟化的核心模块，它在内核中由两部分组成：<strong>一个是处理器架构无关的部分，用lsmod命令中可以看到，叫作kvm模块</strong>；<strong>另一个是处理器架构相关的部分，在Intel平台上就是kvm_intel这个内核模块。</strong>如下图所示，KVM的主要功能是<strong>初始化CPU硬件，打开虚拟化模式，然后将虚拟机运行在虚拟环境下，并对虚拟机的运行提供一定的支持。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db59ae145834533.jpg"></p><p>KVM仅支持硬件辅助的虚拟化，所以，“<strong>打开并初始化系统硬件以支持虚拟机的运行”</strong>是KVM模块本职工作。以Intel CPU架构服务器为例，KVM打开并初始化硬件以支持虚拟机运行的过程如下：</p><p><strong>Step1：</strong>在被内核加载的时候，KVM模块会先初始化内部的数据结构。</p><p><strong>Step2：</strong>做好准备之后，KVM模块检测系统当前的CPU，然后打开CPU控制寄存器CR4中的虚拟化模式开关，并通过执行<strong>VMXON指令</strong>将宿主操作系统（包括KVM模块本身）置于CPU虚拟化模式中的<strong>根模式root operation</strong>，详细参见本站计算虚拟化之CPU虚拟化一文。</p><p><strong>Step3：</strong>KVM模块创建特殊设备文件/dev/kvm并等待来自用户空间的命令。</p><p><strong>Step4：</strong>后续虚拟机的创建和运行，其本质上就是是一个用户空间的QEMU和内核空间的KVM模块相互配合的过程。</p><p><strong>这里面的/dev/kvm这个设备比较关键，它可以被当作一个标准的字符设备，用来缓存用户空间与内核空间切换的上下文，也就是ioctl调用上下文，是KVM模块与用户空间QEMU的通信接口。</strong></p><p>针对/dev/kvm文件的最重要的loctl调用就是“<strong>创建虚拟机</strong>”。这里的创建虚拟机，<strong>简单理解就是KVM为了某个虚拟机创建对应的内核数据结构，并且返回一个文件句柄来代表所创建的虚拟机。</strong>针对该文件句柄的loctl调用可以对虚拟机做相应的管理，比如创建用户空间虚拟地址和客户机物理地址及真实内存物理地址的映射关系，再比如创建多个可供运行的vCPU。KVM模块同样会为每一个创建出来的vCPU生成对应的文件句柄，对其文件句柄进行相应的loctl调用，就可以对vCPU进行调度管理。</p><p>而针对vCPU的最重要的loctl调用就是“<strong>运行虚拟处理器</strong>”。通过它，虚拟机就可以在非根模式下运行，一旦执行敏感指令，就通过<strong>VMX Exit</strong>切入根模式，由KVM决定后续的操作并返回执行结果给虚拟机。</p><p>除了处理器虚拟化，<strong>内存虚拟化的实现也是由KVM内核模块完成，包括影子页表和EPT硬件辅助，均由KVM内核模块负责完成GVA—&gt;HPA的两级转换。</strong>处理器对设备的访问主要是通过<strong>I/O指令</strong>和<strong>MMIO</strong>，其中<strong>I/O指令会被处理器直接截获，MMIO会通过配置内存虚拟化来捕捉。一般情况下，除非对外设有极高性能要求，比如虚拟中断和虚拟时钟，外设均是由QEMU这个用户空间的进程来模拟实现。</strong></p><h3 id="QEMU用户态设备模拟"><a href="#QEMU用户态设备模拟" class="headerlink" title="QEMU用户态设备模拟"></a><strong>QEMU用户态设备模拟</strong></h3><p>QEMU原本就是一个著名的开源虚拟机软件项目，既是一个功能完整的虚拟机监控器，也在QEMU-KVM的软件栈中承担设备模拟的工作。它是由一个法国工程师独立编写的代码实现，<strong>并不是KVM虚拟化软件的一部分</strong>，但是从名字就能知道它和KVM是一辈子的好基友。</p><p>QEMU最初实现的虚拟机是一个纯软件的实现，也就是我们常说的通过二进制翻译来实现虚拟机的CPU指令模拟，所以性能比较低。但是，其优点是跨平台，甚至可以支持客户机与宿主机并不是同一个架构，比如在x86平台上运行ARM客户机。同时，QEMU能与主流的Hypervisor完美契合，包括：Xen、KVM、Hyper-v，以及VMware各种Hypervisor等，为上述这些Hypervisor提供虚拟化I/O设备。</p><p><strong>而QEMU与KVM密不可分的原因，就是我们常说的QEMU-KVM软件协议栈。</strong>虚拟机运行期间，QEMU会通过KVM内核模块提供的系统调用ioctl进入内核，由KVM内核模块负责将虚拟机置于处理器的特殊模式下运行。一旦遇到虚拟机进行I/O操作时，KVM内核模块会从上次的系统调用ioctl的出口处返回QEMU，由QEMU来负责解析和模拟这些设备。除此之外，虚拟机的配置和创建，虚拟机运行依赖的虚拟设备，虚拟机运行时的用户操作环境和交互，以及一些针对虚拟机的特殊技术，比如动态迁移，都是由QEMU自己实现的。</p><p><strong>QEMU除了提供完全模拟的设备以外，还支持virtio协议的设备模拟。</strong>在前端虚拟机中需要安装相应的virtio-blk、virtio-scsi、virtio-net等驱动，就能连接到QEMU实现的virtio的虚拟化后端。除此之外，QEMU还提供了virtio-blk-data-plane的高性能的块设备I/O方式，与传统virtio-blk相比，它为每个块设备单独分配一个线程用于I/O处理，不需要与原QEMU执行线程同步和竞争锁，而且使用ioeventfd/irqfd机制，利用宿主机Linux上的AIO（异步I/O）来处理客户机的I/O请求，使得块设备I/O效率进一步提高。</p><p><strong><em>总之，在KVM虚拟化的软件架构中，KVM内核模块与QEMU用户态程序是处于最核心的位置，有了它们就可通过qemu命令行操作实现完整的虚拟机功能。</em></strong></p><h2 id="KVM的各类上层管理APP"><a href="#KVM的各类上层管理APP" class="headerlink" title="KVM的各类上层管理APP"></a><strong>KVM的各类上层管理APP</strong></h2><p>KVM目前已经有libvirt API、virsh命令行工具、OpenStack云管理平台等一整套管理工具，与VMware提供的商业化管理工具相比虽然有所差距，但KVM这一整套管理工具都是API化的、开源的，可以灵活使用，且能二次定制开发。</p><p><img src="https://i.loli.net/2019/06/22/5d0db5f94cc3658528.jpg"></p><p><strong>1）libvirt</strong></p><p>libvirt是使用最广泛的对KVM虚拟化进行管理的工具和应用程序接口，已经是事实上的虚拟化接口标准。作为通用的虚拟化API，<strong>libvirt不但能管理KVM，还能管理VMware、Hyper-V、Xen、VirtualBox等其他虚拟化方案。但是，在通过Docker或Kolla部署OpenStack时，由于容器镜像中集成了libvirt功能，需要关闭Host的libvirt服务，否则会发生PID调用错误。</strong></p><p><strong>2）virsh</strong></p><p>virsh是一个常用的管理KVM虚拟化的命令行工具，用于在单个宿主机上进行运维操作。virsh是用C语言编写的一个调用libvirt API的虚拟化管理工具，其源代码也是同步公布在libvirt这个开源项目中的。我们常用的KVM虚拟机查看指令就是virsh list –all，如下图：</p><p><img src="https://i.loli.net/2019/06/22/5d0db616cb5c237088.jpg"></p><p><strong>3）virt-manager</strong></p><p>virt-manager是专门针对虚拟机的图形化管理软件，底层与虚拟化交互的部分仍然是调用libvirt API来操作的。virt-manager除了提供虚拟机生命周期管理的基本功能，还提供性能和资源使用率的监控，同时内置了VNC和SPICE客户端，方便图形化连接到虚拟客户机中。virt-manager在RHEL、CentOS、Fedora等操作系统上都非常流行，因其图形化操作的易用性，成为新手入门学习虚拟化操作的首选管理软件。但是，在真实服务器端由于需要安装图形化界面，所以并不常用（服务器环境实现可视化一般是通过VNC功能实现）。</p><p><strong>4）OpenStack</strong></p><p>OpenStack是目前业界使用最广泛的功能最强大的云管理平台，它不仅提供了管理虚拟机的丰富功能，还有非常多其他重要管理功能，如：对象存储、块存储、网络、镜像、身份验证、编排服务、控制面板等。OpenStack的Nova、Cinder和Neutron，也就是计算、存储和网络管理组件仍然使用libvirt API来完成对底层虚拟化的管理，其计算、存储和网络服务组件的配置文件conf中，均有[libvirt]分类引用配置项。</p><p>以上，就是KVM这个耳熟能详的Hypervisor的全貌，作为运维人员掌握以上知识点即可，也就是理解KVM虚拟机的真正工作原理即可。如果工作层面涉及性能或管理的二次开发，必须进一步了解并掌握其官方社区的源码以及各种热点技术或BUG解决方案，这就需要自己钻研了。后面，我们会从实战的角度来介绍如何用熟KVM虚拟机的各类操作。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;KVM的现状&quot;&gt;&lt;a href=&quot;#KVM的现状&quot; class=&quot;headerlink&quot; title=&quot;KVM的现状&quot;&gt;&lt;/a&gt;&lt;strong&gt;KVM的现状&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;KVM最初是由Qumranet公司的Avi Kivity开发的，作为他们的VDI产品的后台虚拟化解决方案。为了简化开发，Avi Kivity并没有选择从底层开始新写一个Hypervisor，而是选择了&lt;strong&gt;基于Linux kernel，通过加载模块使Linux kernel本身变成一个Hypervisor。&lt;/strong&gt;2006年10月，在先后完成了基本功能、动态迁移以及主要的性能优化之后，Qumranet正式对外宣布了KVM的诞生。同月，&lt;strong&gt;KVM模块的源代码被正式纳入Linux kernel，成为内核源代码的一部分。&lt;/strong&gt;2008年9月4日，Redhat公司以1.07亿美元收购了Qumranet公司，包括它的KVM开源项目和开发人员。自此，Redhat开始在其RHEL发行版中集成KVM，逐步取代Xen，并从RHEL7开始，正式不支持Xen。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-20-SBA架构下的核心网大一统</title>
    <link href="https://kkutysllb.cn/2019/06/20/2019-06-20-SBA%E6%9E%B6%E6%9E%84%E4%B8%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BD%91%E5%A4%A7%E4%B8%80%E7%BB%9F/"/>
    <id>https://kkutysllb.cn/2019/06/20/2019-06-20-SBA架构下的核心网大一统/</id>
    <published>2019-06-20T09:37:24.000Z</published>
    <updated>2019-06-21T05:47:39.266Z</updated>
    
    <content type="html"><![CDATA[<p>移动通信核心网络从1G/2G程控交换、电路交换时代的大一统，到2.5G/3G/4G软交换、GPRS、EPC和IMS的专业分家，再到5G时代SBA架构统一，完美印证了那句俗得不能再俗的俗话—“天下大势，合久必分、分久必合”。<a id="more"></a></p><p>现在，很多人都以为5G的语音业务无论是初期的VoLTE、后续EPS Fallback，还是最终的VoNR仍是由IMS网络来提供，再加上现在讲解5G SBA架构的课件资料等都是以4G EPC网络架构做参考对比讲解。所以，5G 时代的核心网仍然是分成PS和IMS两大领域。其实，从业务提供层面来讲可以这样认为，毕竟不同业务的信令协议不同。但是，从网元实现和组网架构的设计理念来讲，未必还需要上述的专业分家。</p><h2 id="业务角度的核心网专业划分"><a href="#业务角度的核心网专业划分" class="headerlink" title="业务角度的核心网专业划分"></a><strong>业务角度的核心网专业划分</strong></h2><p>移动通信网络的核心网从GSM时代的2.5G时代开始就分为电路交换域CS和分组交换域PS，分别负责移动语音业务和GPRS手机上网业务，到了3G时代，核心网在CS和PS的基础上又多了一个“小兄弟”IMS域，它的定义是“IP多媒体子系统”，在2008年的3GPP R5版本定义并冻结。在国内，只有中国移动在2009年6省市部署IMS域试点，2010年27省全面部署IMS域并全面商用，初期用于固网VOBB政企和家庭固话业务承载，而到了4G时代，由于其作为VoLTE高清语音/视频通话业务的核心网络，<strong>在移动通信核心网的地位一下由“小兄弟”蹿升为“老大哥”</strong>。</p><p><img src="https://i.loli.net/2019/06/20/5d0b54522a92c53216.jpg"></p><p>目前，4G时代，移动通信核心网主要细分为EPC、IMS和软交换三大专业，EPC仍然继承提供GPRS业务，只是带宽大了很多，速度快了许多，可以简单理解为GPRS业务的增强版。而IMS则从初期只提供固网业务，发展为开始为用户提供高清语音/视频通话业务，不仅业务种类多了手机终端侧的视频通话、视频彩铃、一号多终端（虚拟eSIM卡）等富媒体业务，且语音质量（清晰度、保真度）和接续时延等用户感知，相比传统电路交换域CS，都有质的提升。<strong>至于软交换专业，那只是2G/3G时代电路域CS的产物，最终会被时代抛弃。</strong>这一点从中国电信、中国联通纷纷宣布关闭GSM网络来看，就是最好的证明。至于中国移动，由于其组网和业务承载的复杂性（中移动目前网络是全球最复杂一张网），以及用户迁移率（VoLTE业务用户转化）等原因，暂时无法宣布关闭GSM网络时间表，但是2G网络的语音业务体验不如VoLTE，这是肯定的。如果你的终端支持VoLTE功能，且是中移动客户，那就赶紧开通VoLTE吧，资费不变，感知提升，何乐而不为呢？</p><p><img src="https://i.loli.net/2019/06/20/5d0b546f4124435466.jpg"></p><p>因此，在现网4G网络阶段，移动通信核心网专业从业务承载的角度来看，分为<strong>EPC专业（用户数据业务、彩信业务）、IMS专业（高清语音/视频业务、短信业务）和软交换专业（传统语音业务、短信业务）</strong>三大专业。所以，虽然大家都宣称自己是核心网专业从业人员，但是业务流程、信令协议、网元配置以及人员技能储备等方面都是不同的。</p><p>到了5G时代，由于SBA服务化核心网架构的提出（中移动首提），所有<strong>网元功能模块全部“软”化</strong>，从软件模块化设计理念来看，<strong>这必然会导致业务管理网元功能SMF不仅只继承EPC-C面功能，同时整个IMS-C面功能也是能够集成统一的。而用户面网元功能UPF不仅只继承EPC-U，同样也能集成IMS-U。</strong></p><p><img src="https://i.loli.net/2019/06/20/5d0b5492bf87456950.jpg"></p><p>而且，基于IT领域“生产者/消费者”理念，<strong>导致SBA架构下各网元功能模块之间通过一条逻辑总线互联</strong>，<strong>松耦合且无强依赖关系。</strong>比如：鉴权解耦（AUSF）、用户数据解耦（UDM/UDR）、接入控制解耦（AMF）、业务功能解耦且接口开放（NEF、AF）、数据转发解耦（UPF）、全网路由寻址统一（NRF）等等。<strong>这个理念（生产者/消费者）与现有IMS网络架构设计理念（控制、承载、业务三分离）是相符的，这也为统一提供语音/数据类业务，不再细分IMS/CS/PS等专业打下基础。</strong>不是很理解？那我们就来掰扯下IMS网络架构如何与SBA架构融合。。。</p><h2 id="IMS逻辑网元与SBA架构网元功能融合"><a href="#IMS逻辑网元与SBA架构网元功能融合" class="headerlink" title="IMS逻辑网元与SBA架构网元功能融合"></a><strong>IMS逻辑网元与SBA架构网元功能融合</strong></h2><p>IMS网络架构从2008年3GPP R5版本冻结以来，一直没有变化。从这一点来看，正好说明这个架构是非常成熟的。即使到了4G时代，为了给用户提供VoLTE业务，加入了EPC网络。但是，<strong>从VoLTE业务角度来看，EPC只是一个接入网，为用户建立一条承载隧道，使其接入IMS网络，从而享受VoLTE高清类、富媒体类等业务，整体VoLTE业务及其增值业务控制都在IMS网络内，所以，IMS网络才是真正的核心网。</strong></p><p><img src="https://i.loli.net/2019/06/20/5d0b54bbd6f3d87730.jpg"></p><p>整个IMS网络架构也是分层的，分为<strong>接入层，承载层、控制层</strong>和<strong>业务层</strong>共4层。从VoLTE业务角度来看，接入层就是上图的EPC部分，承载层就是上图的IP专网，控制层和业务层就是上图的IMS部分（控制层功能和业务层功能由不同的物理网元实现）。其整体全貌如下，我们就按照从接入层到业务层的层次顺序，来掰扯下面这张图。。。</p><p><img src="https://i.loli.net/2019/06/20/5d0b54e0c9ae354705.jpg"></p><p>首先，就是核心层与接入层之间的用作边界网关的SBC网元，它主要用于<strong>用户接入控制，业务代理鉴权和数据包的路由转发</strong>。用户的业务请求从接入层首先送达SBC，然后由它负责向核心侧进行消息转发，而核心侧的响应也由它抓发给接入侧。所以，<strong>SBC网元必然是一个业务信令数据包和业务媒体数据包合一转发的网元，从数通角度来说与路由器功能一致。</strong>那么，从网元兼具的具体功能角度来看，用户接入控制功能可以卸载到SBA架构的AMF，业务代理鉴权可以卸载到SBA的AUSF，业务路由控制可以卸载到SBA的SMF，媒体面数据包转发功能可以卸载到SBA的UPF上。</p><p>从SBC往上，就到了真正的核心控制层，主要就三类网元<strong>CSCF</strong>、<strong>ENUM/DNS和HSS</strong>。先说CSCF，CSCF在3GPP定义中又分为三个逻辑功能网元P-CSCF、I-CSCF和S-CSCF。<strong>P-CSCF网元也是用于业务的接入控制，本质上它才是IMS核心控制层的入口。</strong>而现网由于涉及跨网络层对接，从安全角度考虑设置边界网关SBC，正是因为SBC的存在，使得P-CSCF变成了一个纯信令控制面网元。<strong>在固网业务网内，P-CSCF是独立设置的，与边界网关SBC采用星型拓扑连接，目的就是对不同业务区域用户接入统一集中管理。在VoLTE业务网内，P-CSCF与SBC合设。</strong>所以，从网元功能角度来看，其接入控制功能同样可以卸载SBA架构的AMF上。</p><p>I-CSCF网元主要用于跨IMS核心控制层互通，所以它有拓扑隐藏的功能。也就是说，从其他IMS核心控制层来的业务请求消息只能找本端的I-CSCF，本端核心网的其他网元对外来的业务请求消息是不可见的。这个网元的功能有个学名—<strong>“用户归属域入口”</strong>，见名知意，<strong>它本质上也是一种接入控制类网元。</strong>所以，从网元功能的角度来看，也可以卸载到SBA架构的AMF。如果考虑安全风险，也可以卸载到SBA架构的SMF。</p><p>S-CSCF网元主要用于信令的路由控制和业务逻辑的触发，从这点来看是个<strong>典型的会话控制类网元</strong>。所以，必然可以卸载到SBA的SMF上。</p><p>ENUM/DNS网元用于IMS域内全网的路由寻址，但是它不做路由转发，而是将寻址到的对端地址发给本端的S-CSCF，由本端的S-CSCF负责路由转发。由于IMS网络目前主要用于通话类语音业务，因此就涉及电话号码的翻译问题。在2G/3G时代，用户拨打电话，是通过纯号码分析功能完成全网路由寻址的，在VoLTE时代，由于语言业务承载在IP上，就涉及将电话号码翻译成IP地址的需求，这就是ENUM/DNS网元存在的意义。</p><p><img src="https://i.loli.net/2019/06/20/5d0b55045351845286.jpg"></p><p>如上图，ENUM/DNS从名字上就知道它是由ENUM和DNS两个逻辑网元组成，其实它还有个小名，叫ENS（初期一提ENS，很多人都懵逼了。。。我也是懵逼er之一。。。）。ENUM的功能主要完成用户电话号码到归属域域名+传输&amp;应用协议+服务端口的NAPTR翻译，然后将翻译后的结果送给DNS完成SRV和A查询的两步翻译，从而得到被叫用户归属域入口地址（也就是I-CSCF地址），实现业务从主叫端到被叫端的E2E接续。从ENUM/DNS的网元功能来看，它与SBA架构下NRF何其相似，这也是为什么有人提出5G时代DNS功能弱化的原因。唯一的区别就是现有ENUM/DNS还负责用户数据的存储，而SBA架构的NRF可没这么牛X，但是可以拉着它的“表兄弟”—UDM/UDR一起帮忙啊。。。所以，这都不是问题。</p><p>而HSS网元用于用户签约数据存储，用户接入的合法性鉴权/授权等功能，它也有个学名叫“用户数据中心”。在SBA架构下也有个类似的网元功能单元UDM/UDR，其功能与HSS也类似，同样用于用户签约数据的存储，只是它没有用户接入合法性鉴权/授权功能，这部分功能解耦在SBA架构的AUSF上面。所以，HSS网元的用户签约数据存储功能可以卸载到UDM/UDR上，而鉴权/授权功能可以卸载到AUSF上，两个网元功能单元通过SBA服务总线接口交互消息，同样也不是问题。</p><p>从核心控制层往上，就是业务层了，主要是各类业务服务器的包括基础业务提供服务器、增值业务提供服务器、短信、彩信、彩铃、彩印等等。。。这些业务服务器通过标准接口与核心控制层对接，只用于业务逻辑的生成和下发（比如：互转、彩铃播放等等），并不涉及业务路由的控制，所以它们与核心控制层是一种解耦关系，也就是说任何一家业务服务网元厂家只要按照统一接口标准开发自己的产品就可以与核心控制层完成对接，并实现特色业务提供。同样，在SBA架构下也有类似的网元功能单元，那就是AF，如果有进一步能力开放需求，还可以拉着NEF一起搞个“大事情”。</p><p>以上，通过IMS网络各网元功能的解析，阐述了从网元功能层面，IMS网元与SBA架构网元功能融合的可能性。下面，我们从网元处理逻辑的角度继续掰扯。</p><p>现网的网元种类很多，按专业细分：有软件换的、EPC的、承载网的、IMS的、增值业务的等等；按照所处的网络位置分：有边界网关、接入端局、互联互通关口局、长途汇接局、信令转发点等等。。。同样，现网各类网元的提供厂商也很多，有华为、中兴、爱立信、诺基亚等知名厂商。但是，无论什么类型的网元，无论由谁来提供，<strong>网元内部处理逻辑绕不开三大块：消息接口和分发逻辑单元、业务处理逻辑单元和数据库逻辑单元。</strong></p><p><img src="https://i.loli.net/2019/06/20/5d0b5538b393840575.jpg"></p><p><strong>消息接口和分发逻辑主要用来接收各类信令/媒体消息，按照一定的过滤机制分发给内部的业务逻辑处理单元，业务逻辑处理完成处理后，将业务状态缓存到内部的数据库逻辑单元，并将处理结果转发给消息接口和分发逻辑单元，然后消息接口单元按照一定路由策略转发给外部其他网元的消息接口单元。</strong>上述各类型网元内部基本上都是类似的处理逻辑，这种逻辑处理机制同样符合软件模块化设计的思想。而5G SBA架构本身就采用软件模块化设计的思想，不仅将各类网元功能“软”化，同时将传统网络各网元的逻辑功能进行了拆分和重组，使得每个“软”化的网元功能单元能力更加清晰。因此，也就有了我们开篇提到的SMF不仅只继承EPC-C面功能，同样也能集成IMS-C面功能的观点，增加的逻辑功能点主要涉及消息接口和分发逻辑单元以及业务处理单元的开发，而接入控制功能卸载到AMF、鉴权功能卸载AUSF等，增加的逻辑功能点也主要涉及上面两处。</p><p>但是，现网网元这种处理逻辑其实是一种有状态的设计理念，涉及业务状态在本网元内部的存储，一旦本网元故障，而业务数据没有异地灾备机制的情况下，就会发生业务受损。而5G时代为了进一步提高业务可靠性，有些厂家提出了<strong>无状态的设计理念</strong>，这就需要将传统网元内部数据库逻辑单元统一进行集群化部署，而与各业务功能单元的业务处理逻辑单元采用高可靠、负载均衡对接架构，从而实现业务高可用，无损失特性。这些从软件模块设计理念来看，那都不是事儿。</p><p>了解了上面网元内部处理逻辑的概念，那么理解不同业务信令、流程在同一类SBA网元功能单元完成逻辑判断和业务处理，也就是水到渠成的事情。比如：在SMF只能处理PS业务流程基础上，在其内部业务处理逻辑单元中增加SIP信令处理单元、Diameter信令处理单元、HTTP信令处理单元，就能在SMF上同样实现CSCF的功能。同理，在AMF也能实现P-CSCF/I-CSCF的功能。而<strong>5G网络基于业务流的QoS策略，在一个PDU会话中通过识别不同业务流，从而建立不同5QI（类似现在QCI)等级的业务承载</strong>，更为这种大一统的核心网架构提供天然的基础。</p><p>以上，就是我对5G SBA架构下核心网大一统的粗浅理解。在我个人看来，结合SBA服务化网络架构理念和网元功能单元“软”化，未来核心网专业大一统是很有可能的，并且从技术角度来看，应该也不存在无法解决的问题。但是，很可能有设备厂商从网络可靠、安全的角度提出异议，或者是一些细节问题提出解决进展慢等困难，其实这都是背后的商业目的在作怪。至于人员的技能融合，那是必须要完成的，否则就算只是核心网专业，那也玩不转。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;移动通信核心网络从1G/2G程控交换、电路交换时代的大一统，到2.5G/3G/4G软交换、GPRS、EPC和IMS的专业分家，再到5G时代SBA架构统一，完美印证了那句俗得不能再俗的俗话—“天下大势，合久必分、分久必合”。
    
    </summary>
    
      <category term="5G网络架构" scheme="https://kkutysllb.cn/categories/5G%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="5G" scheme="https://kkutysllb.cn/tags/5G/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-18-服务器与外部链接的网络虚拟化</title>
    <link href="https://kkutysllb.cn/2019/06/19/2019-06-18-%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E9%93%BE%E6%8E%A5%E7%9A%84%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    <id>https://kkutysllb.cn/2019/06/19/2019-06-18-服务器与外部链接的网络虚拟化/</id>
    <published>2019-06-18T16:10:00.000Z</published>
    <updated>2019-06-18T16:48:29.665Z</updated>
    
    <content type="html"><![CDATA[<p>服务器与外部连接的网络虚拟化主要有基于<strong>TOR交换机的虚拟交换技术</strong>和<strong>高性能的计算网络融合技术</strong>。基于接入交换机TOR实现的虚拟交换是指某些物理交换机可通过特殊协议，感知虚拟机的存在，在物理交换机层实现虚拟交换，其代表技术就是<strong>VEPA</strong>和<strong>VN-tag</strong>。同时，高性能计算网络及其融合技术中<strong>Infiniband、FCOE、RoCE</strong>等技术旨在<strong>用一种连接线将数据中心存储网络和计算网络（互联网络）聚合起来</strong>，使服务器可以灵活的配置网络端口，简化网络部署。<a id="more"></a></p><h2 id="基于物理交换机的分布式虚拟交换机"><a href="#基于物理交换机的分布式虚拟交换机" class="headerlink" title="基于物理交换机的分布式虚拟交换机"></a><strong>基于物理交换机的分布式虚拟交换机</strong></h2><p>如下图所示，<strong>基于物理交换机实现虚拟交换的思想是将虚拟机的网络流量全部导入接入交换机TOR处理，即使是同一VLAN内的流量也要绕出来，二层的广播和组播都由接入交换机TOR来模拟。</strong></p><p><img src="https://i.loli.net/2019/06/19/5d090d97943ef92905.jpg"></p><p><strong>在TOR上实现虚拟交换功能的代表技术就是VEPA（Virtual Ethernet Port Aggregator）</strong>。VEPA将虚拟机之间的交换行为从服务器内部移出到上联交换机上，当两个处于同一服务器内的虚拟机要交换数据时，从虚拟机A出来的数据帧首先会经过服务器网卡送往上联交换机，上联交换机通过查看帧头中带的MAC地址（虚拟机MAC地址）发现目的主机在同一台物理服务器中，因此又将这个帧送回原服务器，完成寻址转发。该技术<strong>依赖物理交换机支持VEPA，需要虚拟交换机软件、服务器物理网口驱动作联动修改</strong>，<strong>节省了虚拟交换机查表对物理主机CPU资源消耗。</strong>物理交换机转发功能依赖芯片，功能扩展性差，目前只有H3C、HP等少数厂家宣传支持。</p><p>VEPA方案的目标是要将虚拟机之间的交换从服务器内部移出到接入硬件交换机TOR上，<strong>实现虚拟机之间的“硬交换”。</strong>采用这种方法，通常只需要对网卡驱动、VMM桥模块和外部交换机的软件做很小的改动，从而实现低成本，且对当前网卡、交换机、现有以太网报文格式和标准影响最小。</p><p>采用VEPA方案，虚拟机的流量都是通过物理接入层TOR交换机完成，它的访问控制和报文下发策略也是基于物理接入层TOR交换机，因此很好地避免了网络和服务器设备管理边界模糊的难题。<strong>VEPA标准协议VDP还能准确的感知虚拟机的工作状态，当虚拟机发生迁移时，VEPA协议会将与此相关的访问控制和报文下发等策略重新部署到新的接入交换机。</strong></p><p><img src="https://i.loli.net/2019/06/19/5d090e59c6f8660696.jpg"></p><p>如上图所示：采用了VEPA方案之后，位于一台物理机上不同虚拟机之间的网络流量不再只是由虚拟交换机来完成，而是都要通过物理接入层TOR交换机(即使是从同一个网络端口)。有人可能会提出这样的疑问：直接在虚拟机内存中交换数据不是更快，而且节省物理网络资源吗？答案是肯定的，数据转发确实受影响。但是，H3C的解释是：<strong>虚拟交换机有助于实现虚拟网卡与服务器上物理网卡之间的转换，但也增加了原有网络结构的复杂性，产生新的“边界”给管理带来了麻烦。而且虚拟交换机在服务器宕机的情况下也同时不可用了。</strong>（个人觉得这个解释很牵强，但是VEPA对虚拟机热迁移确实很有利）</p><p>VN-tag是由Cisco和VMWare共同提出的一项标准，如下图所示，<strong>其核心思想是在标准以太网帧中增加一段专用的标记—VN-Tag，用以区分不同的VIF，从而识别特定虚拟机的流量。</strong></p><p><img src="https://i.loli.net/2019/06/19/5d090f0675ade78536.jpg"></p><p><strong>如上图，每个虚机对应唯一的VIF。VN-Tag中最重要的内容是一对新地址：dvif_id和svif_id，这个地址空间对应的不是交换机的端口或者IP网段，而是虚机的VIF。VN-Tag通过这一对地址说明了数据帧从何而来，到哪里去。</strong></p><p>当数据帧从虚机流出来后，就被加上一个VN-Tag标签。基于VN-Tag的源地址dvif_id就能区分出产生于不同虚机的流量。一台具备VN-Tag协议栈的接入交换机TOR可以将VN-Tag与虚机的VIF对应起来，这样就形成了一个映射关系。物理机的出口网卡称为NIC，而上联交换机被称为TOR。</p><p><strong>级联是VN-Tag的另一大特点</strong>，NIC对应的TOR不必是直接连接服务器的接入交换机，可以是网络内的任意IP可达的设备。这种设计的好处是，接入层设备往往比较简单，通过级联可以将虚拟机的流量拉高到高端的汇聚甚至核心交换机上，利用汇聚、核心交换机丰富的功能特性对流量进行精细化的管理。IEEE最初将VN-Tag标准称为802.1Qbh，后来改为802.1Br。</p><p><strong>VN-Tag本质上是一种端口扩展技术，需要为以太网报文增加TAG，而对应的端口扩展设备TOR借助报文TAG中的信息，将端口扩展设备上的物理端口映射成上行物理交换机上的一个虚拟端口，并且使用TAG中的信息来实现报文转发和策略控制。</strong></p><p><strong>基于接入交换机TOR实现的虚拟交换，优点是：</strong>节省了虚拟交换机查表对物理主机CPU资源消耗缺点，对于对流量监管能力、安全策略部署能力要求较高的场景（如数据中心）而言，是一种优选的技术方案。<strong>缺点是：</strong>是由于流量从虚拟机上被引入到外部网络，带来了更多网络带宽开销的问题。虚拟机之间报文转发性能低于虚拟交换机，兼容性较差，需要特殊物理交换机，并且需要虚拟交换机软件、服务器物理网口驱动作联动修改，物理交换机转发功能依赖芯片，功能扩展性差。</p><h2 id="InfiniBand技术和协议架构分析"><a href="#InfiniBand技术和协议架构分析" class="headerlink" title="InfiniBand技术和协议架构分析"></a><strong>InfiniBand技术和协议架构分析</strong></h2><p>InfiniBand其实并不是什么新技术。1999年左右，当时IT界的七个大佬Compaq、HP、IBM，Dell、Intel、Microsoft、Sun联合起来成立了<strong>IBTA</strong>(InfiniBand Trade Association)，<strong>将InfiniBand定位于高速的I/O互联网络，旨在通过机外的长连接提供机内总线的I/O性能，同时保证机外互联的可扩展性。</strong>不过出于各种原因，各家巨头在IBTA成立后都纷纷离开，InfiniBand没能得到良好的市场推广，只是在超算和高性能数据库领域有所应用。Cisco在2005年收购Topspin后也拥有了InfiniBand的产品线，但是Cisco后面就没怎么继续研发了。Intel在收购QLogic之后，也没有继续研发InfiniBand，而是推出了自己的OmniPath。目前，InfiniBand的厂商主要就是Mellonax一家，基本上可以算是小圈子里面绝对的霸主了。</p><p>Infiniband大量用于FC/IP SAN、NAS和服务器之间的连接，作为<strong>iSCSI RDMA的存储协议iSER已被IETF标准化</strong>。相比FC的优势主要体现在<strong>性能是FC的3.5倍，Infiniband交换机的延迟是FC交换机的1/10，支持SAN和NAS。</strong></p><p><strong>InfiniBand也是一种分层协议(类似TCP/IP协议)，每层负责不同的功能，下层为上层服务，不同层次相互独立。 IB采用IPv6的报头格式。其数据包报头包括本地路由标识符LRH，全局路由标示符GRH，基本传输标识符BTH等。</strong></p><p><img src="https://i.loli.net/2019/06/19/5d090fb94590b89863.jpg"></p><p><strong>InfiniBand的协议栈如上图所示</strong>。InfiniBand的传输层首部叫做<strong>BTH(Base Transport Header)</strong>，通过<strong>Queue Pair（QP）来定位远端的目标内存</strong>，通过<strong>Partition Key实现内存的访问控制</strong>，通过<strong>Sequence Number实现可靠传输</strong>。InfiniBand提供了对多种(包括可靠/不可靠、基于连接/基于报文)传输类型的支持，不同的传输类型使用不同的<strong>ETH(Extended Transport Header)，ETH紧跟在BTH的后面。传输层和传输层以下都能够卸载到InfiniBand网卡中进行硬件加速。</strong>传输层之上是InfiniBand传输层和用户应用间的映射层<strong>ULP(Upper Level Protocol)</strong>，允许用户应用在不改变程序原有语义的情况下使用底层的InfiniBand网络进行传输。比如：不同类型的应用需要不同的ULP，如TCP/IP应用可通过IPoIB这一ULP来进行适配。而<strong>InfiniBand的原生应用则可以调用InfiniBand所提供的Verbs API直接对InfiniBand网卡进行操作。</strong></p><p><strong>InfiniBand的超低时延得益于以下几点：</strong></p><p>1）InfiniBand交换机都会采用<strong>直通式的转发</strong>，减小了串行化延迟；</p><p>2）使用了先进的、<strong>基于Credit的链路层流控机制</strong>，能够有效地防止丢包与死锁；</p><p>3）提供了<strong>对于RDMA的原生支持</strong>，从而大幅地提高了端点的性能。</p><p>传统服务器网卡的工作依赖于中断或者轮询，数据包从网卡到达应用程序需要经过内核协议栈的处理以及内核态与用户态间的切换，在上述过程中还要对数据包进行反复的拷贝，这些操作不仅对CPU资源造成了巨大的消耗，而且严重地制约了应用的I/O性能（详见DPDK相关介绍文章）。相比之下，RDMA首先作为一种DMA机制，能够直接在网卡的缓冲区和应用内存间进行数据交互，降低了中断或者轮询的频率，旁路掉了内核协议栈，并实现了数据的零拷贝，因此应用的I/O性能得以大幅提升，而被解放出来的CPU则可以用于处理应用本身。另外，<strong>RDMA提供了一组标准的数据操作接口</strong>，使得本端应用能够直接操作远端应用在内存中的数据，即字面所述的“<strong>远程DMA</strong>”。InfiniBand在设计之初即提供了<strong>对于RDMA的支持，是InfiniBand实现超低延时的重要基础。</strong></p><p><strong>InfiniBand的网卡称为CA(Channel Adaptor)</strong>，CA又可分为<strong>服务器端的HCA（Host Channel Adaptor）</strong>和<strong>交换机/存储端的TCA（Target Channel Adaptor）</strong>。<strong>CA不仅实现了InfiniBand的物理层与链路层</strong>，而且能够对InfiniBand的网络层和传输层进行硬件加速，CA的Driver工作在内核中，为ULP或者InfiniBand原生应用提供操作接口。<strong>无论是HCA还是TCA，其实质都是一个主机适配器，它是一个具备一定保护功能的可编程DMA引擎。</strong></p><p><strong>QP(Queue Pair)是CA提供RDMA能力的基础，它可完成应用层的虚拟地址到CA上的物理地址的自动映射，如果两端的应用需要通信，那么双方都要申请CA上的物理资源，并通过相应的QP来完成对CA的操作。</strong>如下图所示，每个QP中都包括<strong>Receive和Send两个Work Queue分别用于数据的收和发</strong>。应用在进行网络通信前，首先需要向CA请求建立一个QP，需要发送数据时通过Work Request将数据作为WQE(Work Queue Entry)投入QP的<strong>Send Queue</strong>，应用需要为WQE指定<strong>AV(Address Vector)</strong>，AV中携带着通信目标的地址信息，以及在通信路径上进行传输所需要的参数。CA根据AV开始进行L4～L2层的处理，封装好包头并从相应的物理端口送出。远端的目标CA收到数据包后，会根据传输层的QP字段将数据放入相应QP的<strong>Receive Queue</strong>中，然后将数据直接移动到相应的应用内存。每当一个WQE处理完毕之后，CA会通过<strong>Completion Queue</strong>发送CQE(Completion Queue Entry)给应用，通知其可以继续进行后面的处理。</p><p><img src="https://i.loli.net/2019/06/19/5d0910fdad3a135154.jpg"></p><p>CA将数据包从物理端口送出后，就进入到了InfiniBand Fabric。子网是InfiniBand Fabric中最重要的概念，<strong>子网内部是二层通信，InfiniBand Switch根据链路层LRH中的DLID完成转发，子网间通信需要进行三层的路由，InfiniBand Router通过网络层GRH中的DGID完成。GRH中的SGID和DGID是端到端不变的，而每经过一次路由，链路层LRH中的SLID和DLID都会逐跳发生变化。</strong>InfiniBand对子网采用了集中式的管理与控制方式，每个子网中至少有一个SM(Subnet Manager)，多个SM间可以互为备份。SM既可以硬件实现也可以软件实现，可以实现在子网的任何一个节点上，包括CA、Switch和Router。子网中每个节点上都有一个SMA(SM Agent)，SM和SMA间通过接口SMI(Subnet Management Interface)来实现子网的管理与控制，<strong>SMI主要提供Get、Set、Trap3种类型的操作。</strong>SMI信道上的控制信令称为SMP(Subnet Management Packet)，SMP的传输需要使用专用的QP0(QP0不可用于传输用户应用的数据)，并且要求放在逻辑链路VL 15上(QoS优先级最高)，保证子网的管理和控制流量能够得到优先传输，而不会被数据流量所阻塞。其子网、交换机与路由器的连接如图下图所示：</p><p><img src="https://i.loli.net/2019/06/19/5d0911c1bf4e865109.jpg"></p><p>最后，对SM的主要功能及相关做一简要说明，如下：</p><p><strong>1）获取节点信息发现并维护拓扑。</strong>SM和SMA间以带内的方式交互SMP，在拓扑发现过程中，交换机上还没有任何转发信息，SMP是没有办法进行传输的。为了解决这个问题，InfiniBand设计了一种专用的控制信令Directed Routed SMP，Directed Routed SMP携带了自身的一次性转发信息，因此可以在交换机上尚未形成LID转发表时在InfiniBand Fabric中进行传输。</p><p><strong>2）为节点分配LID、GID。</strong>对每个CA来说，在出厂时会分配一个<strong>64位全球唯一的GUID(Global Unique Identifier)，不过这个GUID只是用来标识CA的，并不用于二层和三层的转发。</strong>LID和GID是由SM为CA集中分配的，分别用于二层和三层的转发。LID的长度为16位，只在子网本地有效，每个CA可以分配一个或者一段连续的LID(通过LMC实现)，发送数据包时CA可以采用不同的LID，以实现Fabric上的多路径转发。GID的长度为128位，由64位的子网前缀+GUID组成，实际上SM为CA分配的是子网前缀，然后CA自己在本地组合出GID。SM还会维护GUID和LID/GID间的映射关系，用于地址解析。</p><p><strong>3）根据拓扑计算路由。</strong>InfiniBand在计算路由时通常都是<strong>以Up/Down算法</strong>为基础的，Up/Down算法会为链路指定Up或者Down两种方向，路径只允许链路方向从Up转为Down，而不允许从Down转为Up，从而可以避免形成路由环路。</p><p><strong>4）形成LID转发表，并将转发表配置给相应的Switch。</strong>Switch会根据数据包的DLID查找LID转发表，然后找到对应的端口进行转发。由于对转发表采用集中式控制方式，为了防止子网内部产生路由环路，InfiniBand通常的做法是等到所有Switch上的转发表都形成之后，再来激活子网中流量的转发。</p><p>除了SM以外，<strong>每个子网还都需要有一个SA(Subnet Administration)，SA在逻辑上是SM的一部分(物理上两者没有必然的联系)，可以看作是子网的数据库，CA间通信所需的信息(如DLID/DGID、Path MTU等)都由SA完成解析。</strong>InfiniBand中还有一些其他的子网管理组件，比如负责维护端到端QP连接的CM（Connection Management），负责性能检测的PM（Performance Manager），负责板上器件检测的BM（Baseboard Manager）等等。除了SM以外，InfiniBand中其余的管理组件统称为GSM（General Services Manager），子网中每个节点上都有相应的GSA（General Services Agent），GSM和GSA间的接口称为GSI（General Services Interface），GSI信道上的控制信令称为GMP（General Services Management Packet），<strong>GMP的传输也需要使用专用的QP1(QP1不可用于传输用户应用的数据)</strong>，不过和SMP不同的是，<strong>GMP不可以放在逻辑链路VL 15上，也就是说GMP需要和数据流量一起接受流控。</strong></p><h2 id="FCOE技术原理解析"><a href="#FCOE技术原理解析" class="headerlink" title="FCOE技术原理解析"></a><strong>FCOE技术原理</strong>解析</h2><p><strong>FCoE采用增强型以太网作为物理网络传输架构，能够提供标准的光纤通道有效内容载荷</strong>，避免了 TCP/IP协议开销，而且FCoE能够像标准的光纤通道那样为上层软件层（包括操作系统、应用程序和管理工具）服务。</p><p>FCoE可以提供多种光纤通道服务，比如发现、全局名称命名、分区等，而且这些服务都可以像标准的光纤通道那样运作。不过，<strong>由于FCoE不使用TCP/IP协议，因此FCoE数据传输不能使用IP网络。FCoE是专门为低延迟、高性能、二层数据中心网络所设计的网络协议。</strong></p><p>　　</p><p><strong>和标准的光纤通道FC一样，FCoE协议也要求底层的物理传输是无损失的。</strong>因此，国际标准化组织已经开发了针对以太网标准的扩展协议族，尤其是针对无损10Gb以太网的速度和数据中心架构。这些扩展协议族可以进行所有类型的传输。这些针对以太网标准的扩展协议族有个高大上的名字，被国际标准组织称为“<strong>融合型增强以太网（CEE）</strong>”。</p><p>FCoE的做法是使用以太网的帧头代替FC-2P和FC-2M，上层的FC-2V、FC-3和FC-4仍然保留，FCoE协议栈如下图所示。<strong>FCoE将服务器的FC节点称为ENode，FCoE交换机称为FCF。</strong></p><p><img src="https://i.loli.net/2019/06/19/5d0912fa79d9362058.jpg"></p><p>在FCoE网络中，服务器通过一块<strong>CNA网卡同时支撑IP和FC两套协议</strong>，相当于HBA和以太网NIC的合体。<strong>FCoE的以太网类型是0x8906</strong>，其外层MAC地址的写法比较讲究，后续通过具体的通信流程会进行介绍。<strong>外层的VLAN对于FCoE来说同样非常关键，其原因主要有两个</strong>：首先，FCoE流量必须在无损无丢包的以太网链路上进行传输，这完全依赖于PFC和ETS机制，而这两种机制都需要根据VLAN标签来对流量进行分类，因此FCoE流量必须承载在特定的VLAN中。其次，FCF要想实现存储网络内部的虚拟化，需要使用不同的VLAN来承载不同VSAN(Virtual Storage Area Network)的流量。</p><p><strong>那么封装外层以太网时具体该使用哪个VLAN呢？这由是FCoE的控制平面决定。FCoE使用FIP(FCoE Initialization Protocol)作为控制平面协议，其以太网类型为0x8914</strong>。FIP主要负责以下3个工作：</p><p><strong>1）VLAN发现。</strong>FIP在原先VLAN中通过VLAN发现报文，并与邻居协商后续FIP信令和FCoE流量所使用的VLAN，其<strong>缺省值为1002</strong>。</p><p><strong>2）FCF发现。</strong>FCF在所有FCoE VLAN内定期组播发现通告报文，使得当前VLAN内的所有的ENode发现自己。</p><p><strong>3）FLOGI/PLOGI。</strong>与FC中相应过程一样，FCF作为Login Server为ENode分配FCID，同时作为Name Server记录ENode的登录信息。</p><p>经过上述3个阶段后，FCoE网络的初始化工作就完成了，FCoE流量得以无损地在以太网中传输。下面来看一个在多跳FCoE网络中典型的报文转发流程，如下图所示：</p><p><img src="https://i.loli.net/2019/06/19/5d0913aeb478568261.jpg"></p><p>由于FCID是端到端的，因此FCoE报文在经过FCF转发时FCID不会发生变化，而外层的MAC地址会逐跳改写。我们知道IP和MAC是通过ARP协议联系在一起的，那么FCID和MAC该如何映射呢？FCoE为ENode规定了如下的映射方法：<strong>使用FC-MAP填充MAC地址的高24位，低24位填充为FCID，得到FPMA作为自己的以太网地址，而弃用CAN网卡出厂时的MAC地址。</strong>其中，FC-MAP为在FIP的FCF发现阶段中，FCF告诉ENode的信息，每个VSAN内部的ENode都使用相同的FC-MAP，不同的VSAN使用不同的FC-MAP。而对于FCF来说，不进行这种转换，直接使用本机MAC地址FCF-MAC进行外层以太网封装。同一个VSAN内的报文都在同一个VLAN内传输，FCF进行VLAN的MAC地址学习，保证了VSAN间的隔离，不同VLAN的优先级不同，通过PFC和ETS进行差异化的传输控制。</p><p>单跳FCoE的转发更为简单，负责接入的FCF收到FCoE流量后，根据FCID进行寻址，然后直接转换成FC-2的帧格式在FC网络中进行传输。</p><p>当在服务器中部署虚拟机的时候，FCF不再是FCoE接入网络的第一跳，很多FIP的交互过程就实现不了了，而FC网络也面临着这个问题。FC网络给出的解决办法是NPIV/NPV，NPIV部署在服务器中作为ENode和FCF之间的代理，为下挂多个虚拟机的ENode完成FLOGI/PLOGI过程，而NPV则将NPIV的功能放到了以太网交换机上。NPIV/NPV的示意如下图所示。同样，FCoE也可以配合NPIV的工作实现虚拟机的FCoE接入。</p><p><img src="https://i.loli.net/2019/06/19/5d0913fc5e99e73776.jpg"></p><h2 id="RoCE与RoCEv2"><a href="#RoCE与RoCEv2" class="headerlink" title="RoCE与RoCEv2"></a><strong>RoCE与RoCEv2</strong></h2><p>相比于以太网，InfiniBand的优势主要在于以下几点：<strong>带宽总是能够领先一步，链路层具有流控能力，RDMA可以通过旁路内核来加速。</strong>不过，InfiniBand太贵，对运维人员要求也较高。因此，随着10GE的普及，40GE/100GE的推广，以太网的带宽资源不再是瓶颈，而DCB协议族的发展也使得以太网具备了不丢包的传输能力，如果再能够提供对RDMA的支持，那么以太网就会有能力和InfiniBand在HPC领域一较高下了。</p><p>2010年，IBTA制定了RoCE(RDMA over Converged Ethernet)，使用以太网代替了IB的链路层，保留了网络层以及传输层对于RDMA的支持，结合DCB(要求至少10G的端口速率)即可以获得微秒级的传输延迟。不过，<strong>RoCE由于在网络层仍保留着IB的GRH，因此是不能进行IP路由的</strong>。为此，IBTA在2014年又制订了<strong>RoCEv2，在RoCE的基础上将IB的网络层替换为IP，</strong>RoCEv2的流量获得了跨越广域网进行传输的能力，因此RoCEv2又称为RRoCE(Routable RoCE)。下图展示了RoCE技术的演进策略。</p><p><img src="https://i.loli.net/2019/06/19/5d0914937d09d45006.jpg"></p><p>相比于FCoE使用FIP协议作为专有的控制平面<strong>，RoCE和RoCEv2并没有专门设计自己的控制协议。</strong>在InfiniBand中管理和控制的组件多是围绕InfiniBand子网来进行的，由于RoCE、RoCEv2中使用了以太网来替换InfiniBand的链路层，因此自然就没有了InfiniBand子网的概念，SM、SA、PM、BM等组件也就失去了存在的意义，只有传输层上用于端到端协商QP的CM保留了下来。在InfiniBand中，地址分配和解析工作是由SM和SA完成的，去掉了SM和SA后，需要由以太网/IP中相应的机制来进行地址分配和解析。三者地址分配和解析机制如下所示。</p><table><thead><tr><th></th><th>L2的生成/分配</th><th>L3的生成/分配</th><th>L2和L3的解析</th></tr></thead><tbody><tr><td>IB</td><td>SM分配LID和LMAC</td><td>默认为0xFE::80+GUID，SM分配GID Prefix，覆盖为GID Prefix+GUID</td><td>Query HCA Verb SA</td></tr><tr><td>RoCE</td><td>以太网MAC</td><td>默认为0xFE::80+GUID</td><td>ARP</td></tr><tr><td>RoCEV2</td><td>以太网MAC</td><td>DHCP或者为静态配置</td><td>ARP</td></tr></tbody></table><p>除了RoCE和RoCEv2以外，还有一种融合的方案是iWARP(RDMA over TCP/IP)，通过将TCP/IP卸载到网卡中也可以实现良好的转发性能。不过，iWARP是一种纯应用层的实现，不属于服务器与外部网络连接虚拟化范畴。</p><p>至此，服务器与外部连接的网络虚拟化典型技术介绍完毕。这一块大部分内容对于传统CT运维人员，甚至是不负责存储运维的IT人员都较有难度，涉及许多存储领域的概念和数据包转发流程等知识点。因此，大家在看博客时，需要借助搜索工具让自己对存储领域的一些知识点有个感性认识后，再来看本篇博文就显得轻松许多。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;服务器与外部连接的网络虚拟化主要有基于&lt;strong&gt;TOR交换机的虚拟交换技术&lt;/strong&gt;和&lt;strong&gt;高性能的计算网络融合技术&lt;/strong&gt;。基于接入交换机TOR实现的虚拟交换是指某些物理交换机可通过特殊协议，感知虚拟机的存在，在物理交换机层实现虚拟交换，其代表技术就是&lt;strong&gt;VEPA&lt;/strong&gt;和&lt;strong&gt;VN-tag&lt;/strong&gt;。同时，高性能计算网络及其融合技术中&lt;strong&gt;Infiniband、FCOE、RoCE&lt;/strong&gt;等技术旨在&lt;strong&gt;用一种连接线将数据中心存储网络和计算网络（互联网络）聚合起来&lt;/strong&gt;，使服务器可以灵活的配置网络端口，简化网络部署。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-15-服务器内部虚拟接入技术</title>
    <link href="https://kkutysllb.cn/2019/06/16/2019-06-15-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%86%85%E9%83%A8%E8%99%9A%E6%8B%9F%E6%8E%A5%E5%85%A5%E6%8A%80%E6%9C%AF/"/>
    <id>https://kkutysllb.cn/2019/06/16/2019-06-15-服务器内部虚拟接入技术/</id>
    <published>2019-06-15T17:40:56.000Z</published>
    <updated>2019-06-15T18:06:46.968Z</updated>
    
    <content type="html"><![CDATA[<p>在《网络虚拟化概述》一文中，我们提到了数据中心内部网络虚化技术是一种端到端的解决方案，在不同的网络层面其具体实现的技术各不相同。按照分层的架构，主要分为服务器内部的I/O虚拟化，服务器内部虚拟接入实现、服务器与外部的网络连接虚拟化和外部交换网络的虚拟化4部分。其中，服务器内部的I/O虚拟化技术可以参见本站《计算虚拟化之I/O虚拟化》一文，本文主要阐述服务器内部的虚拟接入技术。<a id="more"></a></p><h2 id="虚拟交换机概述"><a href="#虚拟交换机概述" class="headerlink" title="虚拟交换机概述"></a>虚拟交换机概述</h2><p>如下图所示，服务器内部虚拟网络的划分是<strong>以物理网卡为界</strong>，物理网卡往上为虚拟资源，物理网卡及其往下为物理资源。服务器内部的虚拟接入技术分为<strong>虚拟机网卡、虚拟交换端口和端口组、上行链路和虚拟交换机</strong>四部分。</p><p><img src="https://i.loli.net/2019/06/16/5d052e1856f7c61884.jpg"></p><p>其中，<strong>虚拟网卡</strong>就是用虚拟机中用软件模拟网络环境，提供类似真实网卡的功能，无需连接。<strong>虚拟交换端口</strong>是虚拟交换机上的端口，用来连接虚拟网卡，为虚拟机提供接入网络的服务。<strong>端口组</strong>是为了方便管理，将具有同样属性的一组虚拟交换端口称为端口组。通常情况下，一个端口组就是一个VLAN。在同一个物理服务器内部，相同端口组的虚拟机之间通信无需进过物理网络，但是不同端口组的虚拟机通信必须经过物理网络。<strong>上行链路</strong>是虚拟交换机与主机物理网卡之间之间虚拟链路，通过上行链路虚拟交换机与主机的物理网络适配器相连，使得主机中的虚拟机能够与另一主机中的虚拟机或外部网络进行通信。<strong>虚拟交换机</strong>原理与物理交换机一样，构建起虚拟机之间的网络，并提供虚拟机与外部网络互通的能力。</p><p><strong>虚拟交换机属于二层设备，在虚拟化网络中起到承上启下的作用。</strong>虚拟交换机的交换端口连接虚拟机网卡，上行链路与物理网卡绑定，从而打通虚拟网络和物理网络；物理网卡与物理交换机相连，使虚拟机发出的数据能够转发到物理网络中。同一主机内相同端口组的虚拟机通过虚拟交换机就能通信，不需要通过物理交换机；不同主机相同端口组的虚拟机需要通过物理交换机才能互相通信。</p><p>虚拟交换机按照实现的范围分为<strong>标准虚拟交换机</strong>和<strong>分布式虚拟交换机</strong>两种，<strong>标准虚拟交换机</strong>不支持跨物理服务器，只能为同一主机内部的虚拟机之间提供二层交换功能。比如：VMware Workstations的主机网络适配器、Virtual Box的内部网络适配器、Hyper-V的内部网路适配器、VMware vSphere中的标准虚拟交换机都是这类虚拟交换机。<strong>分布式交换机</strong>支持跨物理服务器提供二层交换的功能，一台分布式虚拟交换机可以分布在多台物理服务器上。比如：华为FusionSphere中的虚拟交换机（上图所示），VMware vSphere中的分布式虚拟交换机、开源的OVS交换机、Linux原生的Linux Bridge等都是这种分布式交换机。</p><p>虚拟交换机按照I/O虚拟化的方式可以分为三种类型：<strong>普通、VMDq</strong>和<strong>SR-IOV</strong>。如下图所示：</p><p><img src="https://i.loli.net/2019/06/16/5d052e412cf3723970.jpg"></p><p><strong>普通模式。</strong>基于CPU实现的虚拟交换，天生会消耗一部分CPU资源。在服务器的CPU中实现完整的虚拟交换的功能，虚拟机的虚拟网卡对应虚拟交换的一个虚拟端口，服务器的物理网卡作为虚拟交换的上行链路接入物理TOR交换机。虚拟机的报文收发流程如下：虚拟交换机首先从虚拟端口/物理端口接收以太网报文，之后根据虚拟机MAC、VLAN，查找二层转发表，找到对应的虚拟端口/物理端口，然后按照具体的端口，转发报文。采用普通模式的虚拟交换机，同一服务器上的虚拟机间报文由虚拟交换机实现实现虚拟机之间报文的二层软件转发， 报文不出服务器，转发路径短，性能高。但是，跨服务器通信时，需要经物理交换机进行转发，相比物理交换机实现虚拟交换，虚拟交换模块的消耗，性能稍低于物理交换机。与物理交换机相比，由于采用纯软件实现虚拟交换，相比采用L3芯片的物理交换机，功能扩展灵活、快速，可以更好的满足云计算的网络需求扩展。同时，由于服务器内存大，相比物理交换机，在L2交换容量、ACL容量等，远大于物理交换机。</p><p><strong>VMDq模式。</strong>相比普通模式，交换、安全、QoS等功能从服务器CPU上卸载至网卡上，CPU开销较少。采用零拷贝、VLAN、Bonding等关键技术使得虚拟机网络报文通过缓冲区到达物理网卡，并支持物理网卡的绑定操作，同时支持基于队列的VLAN和二层广播域的隔离。在ACL方面支持基于五元组的包过滤，支持状态规则，完全兼容当前的安全组功能。且ACL规则表基于队列，不受其它队列规则数的干扰。在QoS方面还支持带宽限制，可以预留最小带宽，动态调整带宽比例以及带宽优先级。VMDq采用网桥交换技术，在硬件层面完成MAC和VLAN的交换功能。最重要一点，相比SR-IOV，VMDq的支持虚拟机热迁移，华为的iNIC网卡就采用了这种技术。</p><p><strong>SR-IOV模式。</strong>设计思想是将虚拟交换功能从服务器的CPU移植到服务器物理网卡，通过网卡硬件改善虚拟交换机占用CPU资源而影响虚拟机性能的问题，同时借助物理网卡的直通的能力，加速虚拟交换的性能。传统的SR-IOV商业网卡，可以支持简单的虚拟交换的功能，高级特性较少，并且由于自身设计以及缺乏与Hypervisor的配合存在一些缺陷，如热迁移等。</p><h2 id="Linux原生网络设备虚拟化"><a href="#Linux原生网络设备虚拟化" class="headerlink" title="Linux原生网络设备虚拟化"></a>Linux原生网络设备虚拟化</h2><p>TAP/TUN是Linux内核实现的一对虚拟网络设备，TAP工作在二层，TUN工作在三层。Linux内核通过TAP/TUN设备向绑定该设备的用户空间程序发送数据，反之，用户空间程序也可以像操作物理网络设备那样，向TAP/TUN设备发送数据。</p><p>基于TAP驱动，即可实现虚拟机vNIC的功能，虚拟机的每个vNIC都与一个TAP设备相连，vNIC之于TAP就如同NIC之于eth。当一个TAP设备被创建时，在Linux设备文件目录下会生成一个对应的字符设备文件，用户程序可以像打开一个普通文件一样对这个文件进行读写。比如，当对这个TAP文件执行 write操作时，相当于TAP设备收到了数据，并请求内核接受它，内核收到数据后将根据网络配置进行后续处理，处理过程类似于普通物理网卡从外界收到数据。当用户程序执行read请求时，相当于向内核查询TAP设备是否有数据要发送，有的话则发送，从而完成TAP设备的数据发送。</p><p>TUN则属于网络中三层的概念，数据收发过程和TAP是类似的，只不过它要指定一段IPv4地址或IPv6 地址，并描述其相关的配置信息，其数据处理过程也是类似于普通物理网卡收到三层 IP 报文数据。</p><p>VETH设备总是成对出现，一端连着内核协议栈，另一端连着另一个设备，一个设备收到内核发送的数据后，会发送到另一个设备上去，这种设备通常用于容器中两个namespace之间的通信。</p><h2 id="Linux-Bridge详解"><a href="#Linux-Bridge详解" class="headerlink" title="Linux Bridge详解"></a>Linux Bridge详解</h2><p>Bridge也是 Linux内核实现的一个工作在二层的虚拟网络设备，但不同于TAP/TUN这种单端口的设备，Bridge实现为多端口，本质上是一个虚拟交换机，具备和物理交换机类似的功能。</p><p>Bridge可以绑定其他Linux网络设备作为从设备，并将这些从设备虚拟化为端口，当一个从设备被绑定到Bridge上时，就相当于真实网络中的交换机端口上插入了一根连有终端的网线。如下图所示，Bridge 设备br0绑定了实际设备eth0和虚拟设备设备tap0/tap1，当这些从设备接收到数据时，会发送给br0 ，br0会根据MAC地址与端口的映射关系进行转发。</p><p><img src="https://i.loli.net/2019/06/16/5d052e8c9ff1a60830.jpg"></p><p>Linux下的Bridge和vlan有点相似，它依赖于一个或多个从设备。与VLAN不同的是，它不是虚拟出和从设备同一层次的镜像设备，而是虚拟出一个高一层次的设备，并把从设备虚拟化为端口port，且同时处理各个从设备的数据收发及转发 。</p><p>Linux Bridge的功能主要在内核里实现。当一个从设备被attach到Linux Bridge上时，这时在内核程序里，netdev_rx_handler_register()被调用，一个用于接受数据的回调函数被注册。以后每当这个从设备收到数据时都会调用这个函数把数据转发到Linux Bridge上。当Linux Bridge接收到此数据时br_handle_frame()被调用，进行一个和现实世界中的交换机类似的处理过程：<strong>判断包的类别（广播/单点），查找内部MAC端口映射表，定位目标端口号，将数据转发到目标端口或丢弃，自动更新内部MAC端口映射表以自我学习。</strong></p><p>Linux Bridge和现实世界中的二层交换机有一个区别：<strong>数据被直接发到Linux Bridge上，而不是从一个端口接受。这种情况可以看做Linux Bridge自己有一个MAC可以主动发送报文，或者说Linux Bridge自带了一个隐藏端口和宿主Linux系统自动连接，Linux上的程序可以直接从这个端口向Linux Bridge上的其他端口发数据。</strong>所以，当一个Linux Bridge拥有一个网络设备时，如bridge0加入了eth0时，实际上bridge0拥有两个有效MAC地址，一个是bridge0的，一个是eth0的，他们之间可以通讯。</p><p>通过上面的描述，也就解释了Linux Bridge为什么可以设置IP地址。通常来说IP地址是三层协议的内容，不应该出现在二层设备上。但是Linux里Bridge是通用网络设备抽象的一种，只要是网络设备就能够设定IP地址。当一个bridge0拥有IP后，Linux便可以通过路由表或者IP表规则在三层定位bridge0，此时相当于Linux拥有了另外一个隐藏的虚拟网卡和Linux Bridge的隐藏端口相连，这个网卡就是名为 virbr0的通用网络设备，如下图所示，IP可以看成是这个网卡的。</p><p><img src="https://i.loli.net/2019/06/16/5d052edae7a8867935.jpg"></p><p>当有符合此IP的数据到达时，内核协议栈认为收到了一个目标地址为本机的数据包，此时应用程序可以通过Socket接收到它。其实，在实际中3层交换机设备，它也拥有一个隐藏的MAC地址，供设备中的三层协议处理程序和管理程序使用。设备里的三层协议处理程序，对应名为 bridge0 的通用网络设备的三层协议处理程序，即宿主Linux系统内核协议栈程序。设备里的管理程序，对应bridge0宿主Linux系统里的应用程序。</p><p>对于一个被attach到Linux Bridge上的设备来说，只有它收到数据时，此数据包才会被转发到Linux Bridge上，进而完成查表广播等后续操作。当请求是发送类型时，数据是不会被转发到Linux Bridge 上的，它会寻找下一个发送出口。<strong>我们在配置网络时经常忽略这一点从而造成网络故障。</strong></p><p>Bridge的实现当前有一个限制：<strong>当一个设备被attach到Bridge上时，那个设备的IP会变的无效，Linux不再使用那个IP在三层接受数据。比如：如果eth0本来的IP是192.168.1.2，此时收到一个目标地址是192.168.1.2的数据，Linux的应用程序能通过Socke 操作接受到它。而当eth0被attach到一个br0 时，尽管eth0的IP还在，但应用程序是无法接受到上述数据的。此时应该把192.168.1.2赋予 br0。</strong></p><p>借助Linux Bridge功能，同主机或跨主机的虚拟机之间能够轻松实现通信，也能够让虚拟机访问到外网，这就是我们所熟知的桥接模式，一般在装VMware虚拟机或者VirtualBox虚拟机的时候，都会提示我们要选择哪种模式，常用的两种模式是桥接和NAT。</p><p>Bridge本身是支持VLAN功能的，如下图所示，通过配置，Bridge可以将一个物理网卡设备eth0划分成两个子设备eth0.10，eth0.20，分别挂到Bridge虚拟出的两个VLAN上，VLAN id分别为VLAN 10和VLAN 20。同样，两个VM的虚拟网卡设备vnet0和vnet 1也分别挂到相应的VLAN 上。这样配好的最终效果就是VM1不能和VM2通信，达到了隔离。</p><p><img src="https://i.loli.net/2019/06/16/5d052f0fda9b368835.jpg"></p><h2 id="OVS详解"><a href="#OVS详解" class="headerlink" title="OVS详解"></a><strong>OVS详解</strong></h2><p>既然Linux原生的Bridge就能实现二层虚拟交换功能，为什么还有很多厂商都在做自己的虚拟交换机，比如比较流行的有 VMware virtual switch、Cisco Nexus 1000V以及 Open vSwitch。究其原因，主要有以下几点：</p><p><strong>1）方便网络管理与监控。</strong>OVS的引入，可以方便管理员对整套云环境中的网络状态和数据流量进行监控，比如可以分析网络中流淌的数据包是来自哪个VM、哪个OS及哪个用户，这些都可以借助OVS 提供的工具来达到。</p><p><strong>2）加速数据包的寻路与转发。</strong>相比Bridge单纯的基于MAC地址学习的转发规则，OVS引入流缓存的机制，可以加快数据包的转发效率。</p><p><strong>3）基于 SDN 控制面与数据面分离的思想</strong>。上面两点其实都跟这一点有关，OVS控制面负责流表的学习与下发，具体的转发动作则有数据面来完成，可扩展性强。</p><p><strong>4）隧道协议支持。</strong>Bridge只支持VxLAN，OVS支持gre/vxlan/IPsec等。</p><p><strong>5）适用于 Xen、KVM、VirtualBox、VMware 等多种 Hypervisors。</strong></p><p>Open vSwitch（OVS）是一个开源的虚拟交换机，遵循Apache2.0许可，其定位是要做一个产品级质量的多层虚拟交换机，通过支持可编程扩展来实现大规模的网络自动化。它的设计目标是方便管理和配置虚拟机网络，能够主动检测多物理主机在动态虚拟环境中的流量情况。针对这一目标，OVS具备很强的灵活性，可以在管理程序中作为软件交换机运行，也可以直接部署到硬件设备上作为控制层。此外，OVS还支持多种标准的管理接口，如NetFlow、sFlow、IPFIX、RSPAN、CLI、LACP、802.1ag。</p><p>对于其他的虚拟交换机设备，如VMware的vNetwork分布式交换机、思科Nexus 1000V虚拟交换机等，它也提供了较好的支持。由于OVS提供了对OpenFlow协议的支持，它还能够与众多开源的虚拟化平台（如KVM、Xen）相整合。在现有的虚拟交换机中，OVS作为主流的开源方案，发展速度很快，它在很多的场景下都可灵活部署，因此也被很多SDN/NFV方案广泛支持。</p><p><img src="https://i.loli.net/2019/06/16/5d052f460be7496724.jpg"></p><p>OVS在实现中分为<strong>用户空间</strong>和<strong>内核空间</strong>两个部分。用户空间拥有多个组件，它们主要负责实现数据交换和OpenFlow流表功能，还有一些工具用于虚拟交换机管理、数据库搭建以及和内核组件的交互。内核组件主要负责流表查找的快速通道。</p><p><img src="https://i.loli.net/2019/06/16/5d052f6cf019f58139.jpg"></p><p>其中，OVS最重要的组件是<strong>ovs-vswitchd</strong>，它实现了OpenFlow交换机的核心功能，并且通过Netlink协议直接和OVS的内核模块进行通信。用户通过<strong>ovs-ofctl</strong>可以使用OpenFlow协议去连接交换机并查询和控制。另外，OVS还提供了sFlow协议，可以通过额外的sFlowTrend等软件（不包含在OVS软件包中）去采样和监控数据报文。</p><p><strong>ovs-vswitchd</strong>通过UNIX socket通信机制和<strong>ovsdb-server</strong>进程通信，将虚拟交换机的配置、流表、统计信息等保存在数据库ovsdb中。当用户需要和ovsdb-server通信以进行一些数据库操作时，可以通过运行ovsdb-client组件访问ovsdb-server，或者直接使用ovsdb-tool而不经ovsdb-server就对ovsdb数据库进行操作。</p><p><strong>ovs-vsctl</strong>组件是一个用于交换机管理的基本工具，主要是获取或者更改ovs-vswitchd的配置信息，此工具操作的时候会更新ovsdb-server的数据库。同时，我们也可以通过另一个管理工具组件<strong>ovs-appctl</strong>发送一些内部命令给ovs-vswitchd以改变其配置。另外，在特定情况下，用户可能会需要自行管理运行在内核中的数据通路，那么也可以通过调用<strong>ovs-dpctl</strong>驱使ovs-vswitchd在不依赖于数据库的情况下去管理内核空间中的数据通路。</p><p><strong>openvswitch.ko</strong>则是在内核空间的快速通路，主要是包括<strong>datapath（数据通路）模块</strong>，datapath负责执行报文的快速转发，也就是把从接收端口收到的数据包在流表中进行匹配，并执行匹配到的动作，实现快速转发能力。通过<strong>netlink通信机制</strong>把ovs-vswitchd管理的流表缓存起来。</p><p><strong>OVS数据流转发的大致流程如下：</strong></p><p>1）OVS的datapah接收到从OVS连接的某个网络端口发来的数据包，从数据包中提取源/目的IP、源/目的MAC、端口等信息。</p><p>2）OVS在内核态查看流表结构（通过HASH），如果命中，则快速转发。</p><p>3）如果没有命中，内核态不知道如何处置这个数据包。所以，通过<strong>netlink upcall</strong>机制从内核态通知用户态，发送给ovs-vswitchd组件处理。</p><p>4）ovs-vswitchd查询用户态精确流表和模糊流表，如果还不命中，在SDN控制器接入的情况下，经过OpenFlow协议，通告给控制器，由控制器处理。如果没有SDN控制器接入，则进行丢弃处理。</p><p>5）如果模糊命中，ovs-vswitchd会同时刷新用户态精确流表和内核态精确流表；如果精确命中，则只更新内核态流表。</p><p>6）刷新后，重新把该数据包注入给内核态datapath模块处理。</p><p>7）datapath重新发起选路，查询内核流表，匹配；报文转发，结束。</p><p>虽然OVS作为虚拟交换机已经很好，但是它在NFV的场景下，在转发性能、时延、抖动上离商业应用还有一段距离。Intel利用DPDK的加速思想，对OVS进行了性能加速。从OVS2.4开始，通过配置支持两种软件架构：<strong>原始OVS（主要数据通路在内核态）</strong>和<strong>DPDK加速的OVS（数据通路在用户态）</strong>。所谓DPDK加速的OVS，也就是华为常用来宣传的EVS。</p><p>根据上面的描述，跟数据包转发性能相关的主要有两个组件：<strong>ovs-vswitchd（用户态慢速通路）</strong>和<strong>openvswitch.ko（内核态快速通路）</strong>。如下所示，显示了OVS数据通路的内部模块图，DPDK加速的思想就是专注在这个数据通路上。</p><p><img src="https://i.loli.net/2019/06/16/5d052fa56edc724827.jpg"></p><p><strong>ovs-vswitchd主要包含ofproto、dpif、netdev模块</strong>。<strong>ofproto</strong>模块实现openflow的交换机；<strong>dpif</strong>模块抽象一个单转发路径；<strong>netdev</strong>模块抽象网络接口（无论物理的还是虚拟的）。</p><p><strong>openvswitch.ko主要由数据通路模块组成，里面包含着流表。</strong>流表中的每个表项由一些匹配字段和要做的动作组成。</p><p>OVS在2.4版本中加入了DPDK的支持，作为一个编译选项，可以选用原始OVS还是DPDK加速的OVS。DPDK加速的OVS利用了DPDK的PMD驱动，向量指令，大页、绑核等技术，来优化用户态的数据通路，直接绕过内核态的数据通路，加速物理网口和虚拟网口的报文处理速度。如下图所示，虚线框内就是DPDK加速的部分。</p><p><img src="https://i.loli.net/2019/06/16/5d052fca2766471690.jpg"></p><p><strong>DPDK加速的OVS数据流转发的大致流程如下：</strong></p><p>1）OVS的ovs-vswitchd接收到从OVS连接的某个网络端口发来的数据包，从数据包中提取源/目的IP、源/目的MAC、端口等信息。</p><p>2）OVS在用户态查看精确流表和模糊流表，如果命中，则直接转发。</p><p>3）如果还不命中，在SDN控制器接入的情况下，经过OpenFlow协议，通告给控制器，由控制器处理。</p><p>4）控制器下发新的流表，该数据包重新发起选路，匹配；报文转发，结束。</p><p><strong>DPDK加速的OVS与原始OVS的区别在于，从OVS连接的某个网络端口接收到的报文不需要openvswitch.ko内核态的处理，报文通过DPDK PMD驱动直接到达用户态ovs-vswitchd里。</strong></p><p>对DPDK加速的OVS优化工作还在持续进行中，重点在用户态的转发逻辑（dpif）和vhost/virtio上，比如采用DPDK实现的cuckoo哈希算法替换原有的哈希算法去做流表查找，vhost后端驱动采用mbuf bulk分配的优化，等等。</p><p><strong>以上，就是服务器内部的虚拟接入技术实现，主要就是虚拟交换机的原理和作用。此外，我们还介绍了Linux原生的虚拟网络设备，分布式交换机Linux Bridge，以及目前被广泛应用在电信云NFV领域的开源OVS虚拟交换机的原理和数据转发加速思路。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在《网络虚拟化概述》一文中，我们提到了数据中心内部网络虚化技术是一种端到端的解决方案，在不同的网络层面其具体实现的技术各不相同。按照分层的架构，主要分为服务器内部的I/O虚拟化，服务器内部虚拟接入实现、服务器与外部的网络连接虚拟化和外部交换网络的虚拟化4部分。其中，服务器内部的I/O虚拟化技术可以参见本站《计算虚拟化之I/O虚拟化》一文，本文主要阐述服务器内部的虚拟接入技术。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-13-正则表达式基础入门</title>
    <link href="https://kkutysllb.cn/2019/06/14/2019-06-13-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"/>
    <id>https://kkutysllb.cn/2019/06/14/2019-06-13-正则表达式基础入门/</id>
    <published>2019-06-13T18:34:29.000Z</published>
    <updated>2019-06-14T01:18:14.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是正则表达式"><a href="#什么是正则表达式" class="headerlink" title="什么是正则表达式"></a><strong>什么是正则表达式</strong></h2><p>简单地说，正则表达式就是为处理大量的字符串及文本而定义的一套规则和方法。可能通过这句话大家还是不明白什么是正则表达式，这里我也就不罗列正则表达式的定义了，反正就是罗列了，你还是不明白。我们通过一个例子来说明：<a id="more"></a></p><p>首先，我们在Notepad++中建立一个文本，名字随便起，在文中添加如下三行内容：</p><p><img src="https://i.loli.net/2019/06/14/5d02979ec67fe28937.jpg"></p><p>这时，我们需要搜索这个文本文件中的kkutysllb这个单词，很简单，通过ctrl+F快捷键打开搜索栏，输入kkutysllb即可，如下：</p><p><img src="https://i.loli.net/2019/06/14/5d0297bfe6f2062416.jpg"></p><p>根据搜索结果，我们找到了3个kkutysllb字符组。那么，这时我们需求变一下，需要查找以kkutysllb开头的文本，我们如何查找？眼睛不瘸的你一定发现了，Nodepad++搜索栏中有<strong>“正则表达式”</strong>字串！！！</p><p>所以，我们开心的再次Ctrl+F打开搜索栏，选中正则表达式，在目标栏中输入^kkutysllb，点击查找所有，即可满足我们的需求。</p><p><img src="https://i.loli.net/2019/06/14/5d0297dab6efd13398.jpg"></p><p>是不是很神奇？这就是正则表达式存在的意义，我们可以把上面^kkutysllb看做一个正则表达式（本来就是），这个正则表达式的意思就是“<strong>以kkutysllb开头的行”</strong>。</p><p>到目前为止，我们已经初步接触到了正则表达式，现在让我回过头来看看开头那句话—<strong>简单地说，正则表达式就是为处理大量的字符串及文本而定义的一套规则和方法。</strong>现在，你是不是就理解一点儿了？其实，正则表达式在编写程序时经常会用到，因为我们写的代码程序主要用来处理各种数据和文本，有了正则表达式可以让程序代码足够精简且强大。同时，在Linux的文本处理三剑客中也会经常用到正则表达式（后面会专门讲三剑客工具），通过正则表达式可以将复杂的处理化繁为简，提高运维脚本编写效率，而且<strong>在Linux运维工具中只有三剑客工具支持正则表达式。</strong></p><p>最后，我们正式给出正则表达式的官方定义：<strong>正则表达式，又称规则表达式，英文为Regular Expression，常简写为regex，regexp或RE。正则表达式是计算机科学的一个概念，通常被用来检索、替换那些符合某个模式（规则）的文本。</strong></p><h2 id="正则表达式入门"><a href="#正则表达式入门" class="headerlink" title="正则表达式入门"></a><strong>正则表达式入门</strong></h2><p>为了让大家对Linux中使用正则表达式有个感性认识，我们需要借助一个常见的命令grep来讲述。至于grep是个啥？大家暂时把他理解成一个搜索工具即可，详细用法后面会有一篇专门介绍grep的文章。现在，大家暂时“照猫画猫”跟着我做就行。</p><p>当grep与正则结合时，可以说是如胶似漆。。。不对，应该严肃点儿说是。。。如鱼得水！！！<strong>grep会根据“正则的含义”在文本中搜索符合条件的字符串。</strong>我们首先在目录下创建一个测试文件test，写入如下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># cat test </span></span><br><span class="line">kkutysllb</span><br><span class="line">I Love kkutysllb</span><br><span class="line">He is an interesting man</span><br><span class="line">He is my role model</span><br><span class="line">The qq number of kkutysllb is: 31468130</span><br><span class="line">kkutysllb<span class="string">'s Homepage: https://kkutysllb.cn</span></span><br><span class="line"><span class="string">His common names: kkutysllb,kkutys,123kkutysllb123,kkutysllb78</span></span><br><span class="line"><span class="string">kkutysllb cool</span></span><br></pre></td></tr></table></figure><p>如果我们想搜索出test文件中包括kkutysllb的行，可以使用如下命令：</p><p><img src="https://i.loli.net/2019/06/14/5d02982896d8638434.jpg"></p><p>如上图，可以看出只要包含kkutysllb字符串的行都会被搜索出来。但是如果我们只想搜索以kkutysllb字符串的开始的行呢？这时候，正则表达式就派上用场了。<strong>在正则表达式中，^符号表示以什么字符串开头</strong>，^kkutysllb就表示以kkutysllb字符串开头。因此，为了满足我们的需求，可以使用如下命令：</p><p><img src="https://i.loli.net/2019/06/14/5d0298440a11611473.jpg"></p><p>那么，我们如果想查找以kkutysllb字符串结尾的行呢？可以使$符号来表示以什么字符串结尾，kkutysllb$就表示以kkutysllb字符串结尾。命令如下：</p><p><img src="https://i.loli.net/2019/06/14/5d02986ce8aa912334.jpg"></p><p>我们学会了^和$，知道它们是正则表达式中分别用于锚定行首和行尾，那么如果我们把它们结合起来使用呢？比如：^kkutysllb$代表几个意思？我们先来分下下，^kkutysllb表示以kkutysllb字符串开头，紧接着kkutysllb$代表以kkutysllb字符串结尾，也就是说<strong>^kkutysllb$代表</strong>以kkutysllb开头同时以kkutysllb结尾的字符串，也就是说<strong>整行只有kkutysllb一个字符串的场景</strong>。我们不妨验证下，命令如下：</p><p><img src="https://i.loli.net/2019/06/14/5d0298883d19419948.jpg"></p><p>如上图，聪明如我，果然如此！那如果符号^和$之间什么都没有呢？也就是我们要匹配^$代表什么意思？其实，<strong>^$就代表空行的意思。</strong>为了显示清楚，我们将空行的的行号打印出来，命令如下：</p><p><img src="https://i.loli.net/2019/06/14/5d0298a0db5c358766.jpg"></p><p>可以看到我们的测试文件中第9行与第10行为空行，与实际情况一样。</p><p>现在，我们已经能够灵活的锚定行首和行尾了，那么，正则表达式能不能锚定词首或词尾呢？那必须滴。。。<strong>在正则表达式中，”\&lt;”表示锚定词首，”\&gt;”表示锚定词尾。</strong>比如：我们想搜索单词以kkutys开头的行，命令如下：</p><p><img src="https://i.loli.net/2019/06/14/5d0298c33c86416242.jpg"></p><p>如上图，可以看见123kkutysllb123这个单词没有被匹配到，至于它所在行被匹配输出，是因为它前后单词都是以kkutys开头的。我们再来匹配以单词cool结尾的行，命令如下：</p><p><img src="https://i.loli.net/2019/06/14/5d0298e1696bb46682.jpg"></p><p>那么，聪明如我你一定想到了，要匹配整个单词，就将”\&lt;”和”\&gt;”结合起来使用就行了。其实，在正则表达式中，<strong>除了使用”\&lt;”和”\&gt;”去锚定词首和词尾外，还可以使用“\b”去完成同样的锚定功能。</strong>验证如下：</p><p><img src="https://i.loli.net/2019/06/14/5d0298fd5e74197615.jpg"></p><p>如上图，表示搜索以123开头，同时以123结尾，中间是任意多个字母组成的单词的行。根据搜索条件，就找到了123kkutysllb123这个单词所在的行。这里的示例主要是说明<strong>“\b”可以取代”\&lt;”和”\&gt;”来锚定词首和词尾，在实际中shell脚本中我也建议大家这样使用。</strong>至于[a-z]*的匹配规则后面会讲到，目前入门这里不是重点。</p><p>“\b”还有一个孪生兄弟“\B”，虽然它们有“血缘”，但是“性格迥异”，也就是功能完全不一样。“\b”是用来锚定词首和词尾，换句话说也就是用来锚定单词的边界。而<strong>“\B”正好相反，它是用来匹配非单词边界的</strong>，这样说可能并不容易理解，看了底下的示例你会秒懂！！！示例如下：</p><p><img src="https://i.loli.net/2019/06/14/5d02991e205d421193.jpg"></p><p>如上图，它的意思是匹配非kkutysllb单词所在的行，也就是说只要该行中存在不是以kkutysllb作为词首和词尾，而是作为中间内容的单词，就会匹配输出。至于“\B”匹配词首或词尾的用法，聪明如我你一定会掌握，这里不再赘述。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h2><p>通过上面的示例，大家可能发现我们使用的都是与“位置”有关，比如“行首、行尾、词首、词尾”等，我们可以把上面是示例中用到的符号归纳为“位置匹配”有关的正则表达式符合，我们不妨做个总结，便于以后查询。<strong>这些“位置匹配”相关的符号包括：^、$、\&lt;、\&gt;、\b、\B。</strong></p><p><strong>^：表示锚定行首，此字符后面的任意内容必须出现在行首，才能匹配。</strong></p><p><strong>$：表示锚定行尾，此字符前面的任意内容必须出现在行尾，才能匹配。</strong></p><p><strong>^$：表示匹配空行，这里所描述的空行只表示回车符，而空格和tab键制表符等只能算空字符串，不能当做空行处理。</strong></p><p><strong>^abc$：表示abc独占一行的场景才会被匹配到。</strong></p><p><strong>\&lt;或者\b：表示锚定词首，其后面的字符必须作为单词首部出现才能被匹配。</strong></p><p><strong>\&gt;或者\b：表示锚定词尾，其前面的字符必须作为单词尾部出现才能被匹配。</strong></p><p><strong>\B：用于匹配非单词边界，与\b是“性格迥异”的亲兄弟。</strong></p><p>在正则表达式中，包含<strong>基础正则表达式</strong>和<strong>扩展正则表达式</strong>两种，大家暂时不用纠结，后面会有专门总结扩展正则表达式的文章。我们现在主要需要掌握的都是基础正则表达式，只要学会了基础正则表达式，掌握扩展正则表达式只是分分钟的事情。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是正则表达式&quot;&gt;&lt;a href=&quot;#什么是正则表达式&quot; class=&quot;headerlink&quot; title=&quot;什么是正则表达式&quot;&gt;&lt;/a&gt;&lt;strong&gt;什么是正则表达式&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;简单地说，正则表达式就是为处理大量的字符串及文本而定义的一套规则和方法。可能通过这句话大家还是不明白什么是正则表达式，这里我也就不罗列正则表达式的定义了，反正就是罗列了，你还是不明白。我们通过一个例子来说明：
    
    </summary>
    
      <category term="shell编程" scheme="https://kkutysllb.cn/categories/shell%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="DevOps" scheme="https://kkutysllb.cn/tags/DevOps/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-12-网络虚拟化概述</title>
    <link href="https://kkutysllb.cn/2019/06/12/2019-06-12-%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%E6%A6%82%E8%BF%B0/"/>
    <id>https://kkutysllb.cn/2019/06/12/2019-06-12-网络虚拟化概述/</id>
    <published>2019-06-12T14:09:08.000Z</published>
    <updated>2019-06-22T04:03:53.722Z</updated>
    
    <content type="html"><![CDATA[<p>网络作为提供数据交换的模块，是数据中心内最为核心的基础设施之一，并直接关系到数据中心的能力、规模、可扩展性和管理性。为了满足日益增长的网络服务需求，特别是移动互联网业务的爆发式增长，数据中心逐渐向大型化、自动化、虚拟化、多租户的方向发展。传统网络设备不仅部署慢、调整难、成本高，而且其二层地址表项的规模直接决定了数据中心的规模。为此，网络虚拟化的概念应运而生。<a id="more"></a></p><p><img src="https://i.loli.net/2019/06/12/5d0108389809573087.jpg"></p><p>事实上，网络虚拟化这个概念由来已久。早在1990年代，世界上就出现了第一个虚拟局域网VLAN，后来逐渐出现GRE、VPN、L2TP等网络虚拟化技术。到今天，很多公司如Vmware、华为等都在使用这种不依赖物理设备的技术。最初，网络虚拟技术被设置为一个简单的开关功能，后续随着二层广播风暴的隔离和跨网络连接需求的出现，VLAN和VPN技术也应运而生。如今随着云计算技术的推动，在数据中心层面不再是简单的三层架构，而是演进出了大二层和spine-leaf架构，同时随着东西向流量的增加，I/O虚拟化技术、虚拟接入识别、物理网络可靠性以及路由网络随选SDN等技术为云数据中心提供自动化的强有力手段。</p><h2 id="传统二、三层网络中的虚拟化"><a href="#传统二、三层网络中的虚拟化" class="headerlink" title="传统二、三层网络中的虚拟化"></a>传统二、三层网络中的虚拟化</h2><h3 id="网络的基础知识—OSI模型和TCP-IP模型"><a href="#网络的基础知识—OSI模型和TCP-IP模型" class="headerlink" title="网络的基础知识—OSI模型和TCP/IP模型"></a><strong>网络的基础知识—OSI模型和TCP/IP模型</strong></h3><p><strong>开放式系统互联通信参考模型</strong>（Open System Interconnection Reference Model，OSI），简称为OSI模型（OSI model），如下图左边所示，<strong>是一种概念模型</strong>，由国际标准化组织提出，是一个试图使各种计算机在世界范围内互连为网络的标准框架。OSI的七层网络协议体系结构的概念清楚，理论也较为完整，但是它<strong>既复杂也不实用。</strong></p><p><img src="https://i.loli.net/2019/06/12/5d01085f46c1356706.jpg"></p><p>为此，<strong>互联网协议套件（Internet Protocol Suite，IPS</strong>）的概念被提出，它是<strong>一个网络通信模型</strong>，包含整个网络传输协议家族，是网络的基础通信架构，通常被称为TCP/IP协议族（TCP/IP Protocol Suite，或TCP/IP Protocols），简称TCP/IP模型。如上图右边所示，OSI模型与TCP/IP模型的对比示意图。</p><p>TCP/IP模型应用的非常广泛，它是一个四层的体系结构，包括：<strong>网络接口层、网际层（IP）、传输层（TCP或UDP）、应用层（各种应用层协议，如：TELNET、FTP、SMTP等）</strong>。通过这四层的协同工作，能够完成一些特定的任务。每一层创建在低一层提供的服务上，并且为高一层提供服务。 整个TCP/IP协议栈则负责解决数据如何通过许许多多个点对点通路（一个点对点通路，也称为一”跳”, 1 hop）顺利传输，由此不同的网络成员能够在许多”跳”的基础上创建相互的数据通路。 </p><h3 id="绕不开的二层和三层"><a href="#绕不开的二层和三层" class="headerlink" title="绕不开的二层和三层"></a><strong>绕不开的二层和三层</strong></h3><p>二层交换技术是发展比较成熟的技术，二层交换机属数据链路层设备，可以识别数据包中的MAC地址信息，根据MAC地址进行转发，并将这些MAC地址与对应的端口记录在自己内部的一个地址表中。三层交换技术就是将路由技术与交换技术合二为一的技术。在对第一个数据流进行路由后，它将会产生一个MAC地址与IP地址的映射表，当同样的数据流再次通过时，将根据此表直接从二层通过而不是再次路由，从而消除了路由器进行路由选择而造成网络的延迟，提高了数据包转发的效率。</p><p><img src="https://i.loli.net/2019/06/12/5d01087e357d454361.jpg"></p><p>二层网络就是数据链路层，只完成本地网络的互通，只识别相同的数据链路层协议，二层交换机是基于MAC地址转发，并且支持高密度以太网接口。而三层网络就是IP网络层，负责不同物理网络的连接，就像是树干一样，将物理世界与应用世界互联。可以识别多种链路层协议，使用IP协议屏蔽差异，兼容互联。三层路由器都是基于IP地址转发，可以支持ATM/SDN/以太网等多种链路层接口。</p><p>数据包在二、三层网络中转发，其封装格式示意图如下所示，数据包从四层发送到三层会被添加上IP首部然后进行转发，同样数据包到达二层会被添加上二层协议如以太首部然后进行转发，这个过程叫做<strong>封装</strong>。</p><p><img src="https://i.loli.net/2019/06/12/5d010898a434795631.jpg"></p><p>数据包从二层发送到三层时，会被二层设备剥离对应的首部，露出上层设备能够识别的首部，如IP首部，同样三层向四层转发的时，也会剥离本层的协议首部露出上层设备能够识别的首部，这个过程叫做<strong>解封装</strong>。能够执行二层封装、解封装的设备为二层设备，如二层交换机。能够执行三层封装、解封装的设备为三层设备，如路由器。三层交换机既能执行二层封装解封装，也能执行三层封装解封装，为三层设备。</p><p>数据包在两个网络端点之间传输，可以有三种传送方式：<strong>单播、组播</strong>和<strong>广播</strong>。</p><p><img src="https://i.loli.net/2019/06/12/5d0108bd833a193969.jpg"></p><p>单播方式中，发送源端明确知道目的端地址，直接使用该地址与接收者建立联系。组播方式中，发送源端将数据同时传递给一组目的地址，多个client组成接收者，接收者与源之间的路径由组播协议计算后得出。在广播方式中，发送源端不知道目的端地址，首先会发一个询问给在同一广播域的所有设备，真正的接收者收到该询问后会给源以单播的方式回一个答复，其他设备则不理会该询问。当网络很大的时候，广播包会占用一定的带宽，造成带宽的浪费，一个二层的本地网络就是广播域。</p><h3 id="最原始的网络虚拟化VLAN的技术实现"><a href="#最原始的网络虚拟化VLAN的技术实现" class="headerlink" title="最原始的网络虚拟化VLAN的技术实现"></a><strong>最原始的网络虚拟化VLAN的技术实现</strong></h3><p>在实际的物理二层网络中，经常会有广播包的发送需求，如果不加限制，除了会带来安全性和带宽占用浪费等问题外，最重要的是会产生<strong>广播风暴</strong>。所谓广播风暴，是指由于网络拓扑的设计和连接问题，或者其他原因导致广播包在网段内大量复制传播，导致网络性能下降甚至瘫痪。如下图所示，为了解决这个问题，传统网络使用虚拟局域网技术VLAN来实现。</p><p><img src="https://i.loli.net/2019/06/12/5d0108ef17a0911208.jpg"></p><p><strong>VLAN的主要作用就是隔离广播域</strong>，不同的VLAN之间不能直接通信。VLAN技术把用户划分成多个逻辑的网络（group），组内可以通信，组间不允许直接通信，二层转发的单播、组播、广播报文只能在组内转发。同时，VLAN技术可以很容易地实现组成员的添加或删除。也就是说，VLAN技术提供了一种管理手段，控制终端之间的互通。如上图，组1和组2的PC无法相互直接通信。</p><p>如下图所示，<strong>VLAN是通过对传统的数据帧添加tag字段来实现的。</strong>添加VLAN信息的方法最常用的就是IEEE802.1Q协议，还有一种是ISL协议。</p><p><img src="https://i.loli.net/2019/06/12/5d0109114f93969773.jpg"></p><p>IEEE802.1Q所附加的VLAN识别信息，位于数据帧中“<strong>发送源MAC地址</strong>”与“<strong>类别域（Type Field）</strong>”之间。具体内容为2字节的<strong>TPID</strong>和2字节的<strong>TCI</strong>，共计4字节。 在数据帧中添加了4字节的内容，那么CRC值自然也会有所变化。这时数据帧上的CRC是插入TPID、TCI后，对包括它们在内的整个数据帧重新计算后所得的值。<strong>TPID (Tag Protocol Identifier）</strong>是IEEE定义的新的类型，表明这是一个加了802.1Q标签的帧，<strong>TPID包含了一个固定的值0x8100</strong>。<strong>TCI (Tag Control Information）</strong>包括<strong>用户优先级(User Priority，3bit)、规范格式指示器(Canonical Format Indicator，1bit)</strong>和 <strong>VLAN ID（12bit）</strong>。VLAN ID 是对 VLAN 的识别字段，支持4096(2的12次方) VLAN 的识别。在4096个可能的VID 中，VID＝0 用于识别帧优先级。 4095(FFF)作为预留值，所以 VLAN 配置的最大可能值为4094，有效的VLAN ID范围一般为1-4094。</p><p>在VLAN中有以下两种链路类型：<strong>普通链路</strong>和<strong>中继链路</strong>。<strong>普通链路（Access Link）是用于连接用户主机和交换机的链路</strong>。通常情况下，主机并不需要知道自己属于哪个VLAN，主机硬件通常也不能识别带有VLAN标记的帧。因此，主机发送和接收的帧都是untagged帧。<strong>中继链路（Trunk Link）是用于交换机间的互连或交换机与路由器之间的连接</strong>。中继链路可以承载多个不同VLAN数据，数据帧在中继链路传输时，中继链路的两端设备需要能够识别数据帧属于哪个VLAN，所以在中继链路上传输的帧都是Tagged帧。</p><p>在VLAN网络中，针对上述两种不同的链路类型使用场景，分别有三种网络端口类型去适配：<strong>Access端口、Trunk端口</strong>和<strong>Hybrid端口</strong>。<strong>Access接口</strong>是交换机上用来连接用户主机的接口，它只能接入【普通链路。仅仅允许唯一的VLAN ID通过本接口，这个VLAN ID与接口的缺省VLAN ID相同，Access接口发往对端设备的以太网帧永远是不带标签的untagged帧。<strong>Trunk接口</strong>是交换机上用来和其他交换机连接的接口，它只能连接中继链路，允许多个VLAN的帧（带Tag标记）通过。<strong>Hybrid接口</strong>是交换机上既可以连接用户主机，又可以连接其他交换机的接口。Hybrid接口既可以连接普通链路又可以连接中继链路。Hybrid接口允许多个VLAN的帧通过，并可以在出接口方向将某些VLAN帧的Tag剥掉。<strong>而在虚拟交换机只用Access接口和Trunk接口。</strong></p><p><strong>各类型接口对数据帧的处理方式汇总如下：</strong></p><table><thead><tr><th><strong>接口类型</strong></th><th><strong>对接收不带Tag的报文处理</strong></th><th><strong>对接收带Tag的报文处理</strong></th><th><strong>发送帧处理过程</strong></th></tr></thead><tbody><tr><td><strong>Access接口</strong></td><td>接收该报文，并打上缺省的VLAN ID。</td><td>当VLAN ID与缺省VLAN ID相同时，接收该报文。当VLAN ID与缺省VLAN ID不同时，丢弃该报文。</td><td>先剥离帧的PVID Tag，然后再发送。</td></tr><tr><td><strong>Trunk接口</strong></td><td>打上缺省的VLAN ID，当缺省VLAN ID在允许通过的VLAN ID列表里时，接收该报文。打上缺省的VLAN ID，当缺省VLAN ID不在允许通过的VLAN ID列表里时，丢弃该报文。</td><td>当VLAN ID在接口允许通过的VLAN ID列表里时，接收该报文。当VLAN ID不在接口允许通过的VLAN ID列表里时，丢弃该报文。</td><td>当VLAN ID与缺省VLAN ID相同，且是该接口允许通过的VLAN ID时，去掉Tag，发送该报文。当VLAN ID与缺省VLAN ID不同，且是该接口允许通过的VLAN ID时，保持原有Tag，发送该报文。</td></tr><tr><td><strong>Hybrid接口</strong></td><td>打上缺省的VLAN ID，当缺省VLAN ID在允许通过的VLAN ID列表里时，接收该报文。打上缺省的VLAN ID，当缺省VLAN ID不在允许通过的VLAN ID列表里时，丢弃该报文。</td><td>当VLAN ID在接口允许通过的VLAN ID列表里时，接收该报文。当VLAN ID不在接口允许通过的VLAN ID列表里时，丢弃该报文。</td><td>当VLAN ID是该接口允许通过的VLAN ID时，发送该报文。可以通过命令设置发送时是否携带Tag。</td></tr></tbody></table><h3 id="爱折腾的VPN"><a href="#爱折腾的VPN" class="headerlink" title="爱折腾的VPN"></a>爱折腾的VPN</h3><p>VPN技术起初是为了解决明文数据在网络上传输带来的安全隐患而产生的。TCP/IP协议族中的很多协议都采用明文传输，如telnet、ftp、tftp等。一些黑客可能为了获取非法利益，通过诸如窃听、伪装等攻击方式截获明文数据，使企业或者个人蒙受损失。VPN技术可以从某种程度上解决该问题。例如，它可以对公网上传输的数据进行加密，即使黑客通过窃听工具截获到数据，也无法了解数据信息的含义。VPN也可以实现数据传输双方的身份验证，避免黑客伪装成网络中的合法用户攻击网络资源。</p><p><strong>VPN（virtual private network，虚拟专用网）就是在两个网络实体之间建立的一种受保护的连接，这两个实体可以通过点到点的链路直接相连</strong>，但通常情况下他们会相隔较远的距离。</p><p><img src="https://i.loli.net/2019/06/12/5d0109693d58327773.jpg"></p><p>VPN技术通过使用加密技术防止数据被窃听，并且通过数据完整性校验防止数据被破坏、篡改。通过认证机制实现通信双方身份确认，来防止通信数据被截获和回放。此外，在VPN中还可以定义何种流量需要被保护，数据被保护的机制以及数据封的过程。</p><p>VPN技术有两种基本的连接模式：<strong>隧道模式</strong>和<strong>传输模式</strong>。这两种模式实际上定义了两台实体设备之间传输数据时所采用的不同的封装过程。<strong>传输模式一个最显著的特点就是：在整个VPN的传输过程中，IP包头并没有被封装进去，这就意味着从源端到目的端数据始终使用原有的IP地址进行通信</strong>。如下图所示，而传输的实际数据载荷被封装在VPN报文中。对于大多数VPN传输而言，VPN的报文封装过程就是数据的加密过程，因此，攻击者截获数据后将无法破解数据内容，但却可以清晰地知道通信双方的地址信息。</p><p><img src="https://i.loli.net/2019/06/12/5d010980d1cb013743.jpg"></p><p>由于传输模式封装结构相对简单（每个数据报文较隧道模式封装结构节省20字节），因此传输效率较高，多用于通信双方在同一个局域网内的情况。</p><p>隧道模式中，如下图所示VPN设备将整个三层数据报文封装在VPN数据内，再为封装后的数据报文添加新的IP包头。由于新IP包头中封装的是VPN设备的ip地址信息，所以当攻击者截获数据后，不但无法了解实际载荷数据的内容，同时也无法知道实际通信双方的地址信息。</p><p><img src="https://i.loli.net/2019/06/12/5d01099ec713e51544.jpg"></p><p>由于隧道模式的VPN在安全性和灵活性方面具有很大的优势，在企业环境中应用十分广泛，总部和分公司跨广域网的通信、移动用户在公网访问公司内部资源等很多情况，都会应用隧道模式的VPN对数据传输进行加密。</p><p>通常情况下，VPN的类型分为<strong>站点到站点VPN</strong>和<strong>远程访问VPN</strong>。站点到站点VPN就是通过隧道模式在VPN网关之间保护两个或者更多的站点之间的流量，站点间的流量通常是指局域网之间（L2L）的通信流量。L2L的VPN多用于总部与分公司、分公司之间在公网上传输重要业务数据。比如：我们各地市VOBB固网用户接入方式就是这种类型，对于两个地市的固网终端用户来说，在VPN网关（针对现网来说可以看成一个统一的逻辑设备，包含现网中的SR、BRAS、FW、CE、SBC等设备)中间的网络是透明的，就好像通过一台路由器连接的两个局域网。各地市终端设备通过VPN连接访问核心网或其他地市接入网络资源。数据包封装的地址都是各地市规划的内网地址（一般为私有地址），而VPN网关对数据包进行的再次封装过程，客户端是全然不知的。</p><p>远程访问VPN通常用于单用户设备与VPN网关之间通信连接，单用户设备一般为一台pc或小型办公网络等。VPN连接的一端为PC，可能会让很多人误解远程访问VPN使用传输模式，但因为该种VPN往往也是从公网传输关键数据，而且单一用户更容易成为黑客的攻击对象，所以远程访问VPN对于安全性要求较高，更适用于隧道模式，如下图所示。</p><p><img src="https://i.loli.net/2019/06/12/5d0109b8111ea32153.jpg"></p><p>要想实现隧道模式的通信，就需要给远程客户端分配两个IP地址：一个是它自己的NIC地址，另一个是内网地址。也就是说远程客户端在VPN建立过程中同时充当VPN网关（使用NIC地址）和终端用户（使用内网地址）。比如：我们使用VPN代理软件从公网访问4A服务器，或者私有云远程用户从公网访问私有云资源都是这种方式。</p><h2 id="数据中心对网络的总体要求"><a href="#数据中心对网络的总体要求" class="headerlink" title="数据中心对网络的总体要求"></a><strong>数据中心对网络的总体要求</strong></h2><p>随着电信云NFV的全面部署，以及后续5G网络架构的演进，利用虚拟化和面向服务的技术，能够为智能设备提供广泛的业务服务。虚拟化是IaaS服务的基础，计算虚拟化将一台物理服务器”分裂“成多个虚拟服务器来调度，虚拟服务器作为业务的承载者和物理服务器一样，有着网络通信需求。也就是说不仅虚拟服务器之间需要网络通信，虚拟服务器与外部也存在网络通信的需求。在这种模式下，计算和存储能力向网络中心迁移，形成云化数据中心，大量的计算请求、信息请求依托网络向数据中心发送，网络成为数据中心的和用户的纽带。因此，如下图所示，云化数据中心对网络存在<strong>业务发展弹性、虚拟感知技术、网络资源整合和共享、高效运维和能耗管理</strong>等4大要求。</p><p><img src="https://i.loli.net/2019/06/12/5d0109d8eaa4462134.jpg"></p><h3 id="业务发展弹性需求"><a href="#业务发展弹性需求" class="headerlink" title="业务发展弹性需求"></a><strong>业务发展弹性需求</strong></h3><p>在信息化蓬勃发展的今天，业务流量增长异常迅猛。为了应对这一切，数据中心不仅需要提升服务器性能和网卡接入带宽，也需要充分利用现有的IT资源。分布式计算和虚拟化应运而生，流量模型随之从南北向流量为主向东西向流量为主转变。</p><p><img src="https://i.loli.net/2019/06/12/5d010a1496dca41543.jpg"></p><p>从Gartner 的报告中可以看到服务器10GE TOR 接入从2011年开始迅速成为主流，所占份额不断扩大，必然会导致网络侧上行40GE/100GE互联大量应用。横向交互流量增大使网络模型从传统的三层模型向胖树架构演进，横向无阻塞和大缓存成为数据中心网络规划设计的基本需求。</p><p>在这种计算、存储、网络整合的云数据中心内，对网络的要求<strong>由连接变为服务</strong>。因为，云计算本身就是一种服务，能按需、弹性提供，且可计量。网络资源被云计算整合在基础设施资源中，自然也是一种可按需、弹性提供，且可计量的服务。</p><h3 id="虚拟感知技术需求"><a href="#虚拟感知技术需求" class="headerlink" title="虚拟感知技术需求"></a><strong>虚拟感知技术需求</strong></h3><p>服务器虚拟化使得可以高效利用IT资源，降低企业运营成本成为可能。服务器内多虚拟机之间的交互流量，传统网络设备无法感知，也不能进行流量监控和必要的策略控制。虚拟机的灵活部署和动态迁移需要网络接入侧做相应的调整，在迁移时保持业务不中断。</p><p><img src="https://i.loli.net/2019/06/12/5d010a439d87c85492.jpg"></p><p>虚拟机的大量运用以及虚拟机交互流量的出现，使网络的前沿深入到服务器内部。虚拟拓扑发现，虚拟机策略下发，接入侧交换机网络配置和动态调整对传统网络模型和管理模式提出了很大的挑战。虚拟机迁移的物理范围不应过小，否则无法充分利用空闲的服务器资源。迁移后虚拟机的IP地址不变，以保持业务不中断。因此，迁移不能跨VLAN。综合以上两点，对数据中心网络提出了大二层的需求。</p><h3 id="网络资源的整合与共享"><a href="#网络资源的整合与共享" class="headerlink" title="网络资源的整合与共享"></a><strong>网络资源的整合与共享</strong></h3><p>数据中心对网络可靠性和安全性的需求是最基本的需求。可靠性设计包括：链路冗余、关键设备冗余和重要业务模块冗余。安全性设计包括物理空间的安全控制及网络的安全控制。企业多业务系统安全隔离和冗余设计导致网络资源成本高昂，利用率低，运维复杂。 </p><p><img src="https://i.loli.net/2019/06/12/5d010a64adca313019.jpg"></p><p>在云计算时代，为了充分利用网络资源实现业务的灵活部署，需要将多业务网络纵向融合成一张物理网，通过网络设备的虚拟化实现业务隔离和冗余备份。对于多类型的网络可以横向融合，通过采用FCOE和DCB等技术降低管理复杂性以及部署扩展性的挑战。</p><h3 id="高效的运维的电源管理"><a href="#高效的运维的电源管理" class="headerlink" title="高效的运维的电源管理"></a><strong>高效的运维的电源管理</strong></h3><p>数据中心内部存在网络设备和IT资源数量大，厂商多，运行配置复杂等问题。数据中心网络延伸到服务器内部，需要物理和虚拟网络拓扑完整展示，实现网络流量的精细化管理和监控，针对网络故障快速定位是对云计算时代数据中心运维的基本需求。 不断攀升的能耗成本，催高了数据中心的运营成本，绿色节能是云计算数据中心的必备条件。</p><p><img src="https://i.loli.net/2019/06/12/5d010a89b362689802.jpg"></p><h2 id="数据中心层面的网络虚拟化"><a href="#数据中心层面的网络虚拟化" class="headerlink" title="数据中心层面的网络虚拟化"></a><strong>数据中心层面的网络虚拟化</strong></h2><p>随着云计算、大数据、物联网等技术的发展，传统的网络虚拟化技术，已经难以满足云时代下多租户的需求。例如：广泛被使用的VLAN技术，虽然可以在物理交换机上通过划分多个VLAN来隔离，并虚拟出多个逻辑网络，但是其设计和配置，通常基于固定的规划，以及网络和服务器的位置不会频繁变更为前提。而面对云化的数据中心，大量虚拟机的动态的生命周期变化，以及弹性漂移和伸缩的特点，对网络提出了更高的按需配置和随动的要求。也就说，在云化数据中心内，网络不仅仅提供连接，更是一种服务，一种像虚拟化的计算资源一样逻辑隔离，弹性且可计量的服务。</p><h3 id="从计算虚拟化走向网络虚拟化"><a href="#从计算虚拟化走向网络虚拟化" class="headerlink" title="从计算虚拟化走向网络虚拟化"></a><strong>从计算虚拟化走向网络虚拟化</strong></h3><p><strong>网络虚拟化是指将网络的控制从网络硬件中脱离出来，交给虚拟化的网络层处理。这个虚拟化的网络层加载在整个物理网络之上，屏蔽掉底层的物理差异，在虚拟空间重建整个网络。</strong>如下图所示，就像计算虚拟化中将物理服务器整合抽象为计算资源池一样，物理网络也被泛化为网络资源池，通过控制面软件的集中调度，使得底层网络资源更加灵活。</p><p><img src="https://i.loli.net/2019/06/12/5d010aca0dde071037.jpg"></p><p>逻辑网络资源池是一种逻辑资源的灵活管理抽象，是对底层物理网络的一种形象描述。每一个虚拟网络可以根据业务或部门进行灵活分配，各个虚拟网络之间逻辑隔离。因此，每个虚拟网络内的网络资源变更，不会影响其它虚拟网络。与服务器虚拟化类似，网络虚拟化可以在很短的时间（秒级）创建L2、L3到L7的网络服务，如交换，路由，防火墙和负载均衡等。虚拟网络独立于底层网络硬件，可以按照业务需求配置、修改、保存、删除，而无需重新配置底层物理硬件或拓扑。这种网络技术的革新为实现软件定义网络SDN和软件定义数据中心（SDDC）奠定了基础。</p><p><img src="https://i.loli.net/2019/06/12/5d010af5908e959720.jpg"></p><p>云化数据中心内部的网络要同时解决<strong>多租户资源隔离</strong>和<strong>内外互通访问</strong>的需求。多租户环境中主要存在私有云场合，一个租户就是任何一个应用，租户内的安全、资源对外隔离且排他，因此需要有网络的隔离作为基础。同时，云化的数据中心不再局限四面墙之内，需要实现地理上相互隔离的故障转移性，位于虚拟网络内的应用需要访问外部网络环境，外部网络环境也需要访问虚拟网络内的资源。因此，虚拟网络也必须具备内外互通的开放性，通过利用隧道封装技术实现跨地理资源的迁移特性。</p><p>如下所示，隧道封装技术是一种通过使用互联网络的基础设施在网络之间传递数据的方式，使用隧道传递的数据（或负载）可以是不同协议的数据帧或包。隧道协议将这些数据帧或包重新封装在新的包头中发送，新的包头提供路由信息，使封装的负载数据可以通过互联网络传递。被封装的数据包在隧道的两个端点之间通过公共互联网络进行路由，其所经过的逻辑路径就是隧道。一旦到达网络端点，数据将被解包并转发到最终的目的地。</p><p><img src="https://i.loli.net/2019/06/12/5d010b1763a9e52967.jpg"></p><p>网络虚拟化将网络的边缘从硬件交换机推到了服务器里面，将服务器和虚拟机的所有部署、管理的职能从原来的系统管理员+网络管理员的模式变成了纯系统管理员的模式，让服务器的业务部署变得简单，不再依赖于形态和功能各异的硬件交换机，一切归于软件控制，实现自动化部署。这就是网络虚拟化在数据中心中最大的价值所在，也是为什么大家明知服务器的性能远远比不上硬件交换机但还是使用网络虚拟化技术的根本原因。</p><h3 id="数据中心网络虚拟化的层次"><a href="#数据中心网络虚拟化的层次" class="headerlink" title="数据中心网络虚拟化的层次"></a>数据中心网络虚拟化的层次</h3><p>随着越来越多的服务器被虚拟化，网络已经延伸到Hypervisor内部，网络通信的端点已经从以前的服务器变成了运行在服务器中的虚拟机，数据包从虚拟机的虚拟网卡流出，通过Hypervisor内部的虚拟交换机，在经过服务器的物理网卡流出到上联交换机。因此，<strong>虚拟化环境下的网络虚拟化需要解决端到端的问题。</strong>在整个过程中，虚拟交换机，网卡的I/O问题以及虚拟机的网络接入都是虚拟化的重点。如下图所示，每一个层面网络虚拟化的实现技术和方式均不同，我个人将其归纳为4个部分：<strong>服务器内部I/O虚拟化、服务器内部的虚拟接入、服务器与物理网络的连接、物理交换网络。</strong></p><p><img src="https://i.loli.net/2019/06/12/5d010b3af1e6193765.jpg"></p><p><strong>第一部分是服务器内部的IO虚拟化。</strong>多个虚拟机共享服务器中的物理网卡，需要一种机制既能保证I/O的效率，又要保证多个虚拟机对用物理网卡共享使用。I/O虚拟化的出现就是为了解决这类问题，详情可参见本站《计算虚拟化之I/O虚拟化》一文，这里不再赘述。</p><p><strong>第二部分是服务器内部的虚拟接入识别。</strong>用于识别不同虚拟机的网络包。在传统的服务器虚拟化方案中，从虚拟机的虚拟网卡发出的数据包在经过服务器的物理网卡传送到外部网络的上联交换机后，虚拟机的标识信息被屏蔽掉了，上联交换机只能感知从某个服务器的物理网卡流出的所有流量而无法感知服务器内某个虚拟机的流量，这样就不能从传统网络设备层面来保证QoS和安全隔离。虚拟接入要解决的问题是要把虚拟机的网络流量纳入传统网络交换设备的管理之中，需要对虚拟机的流量做标识。</p><p>在解决虚拟接入的问题时，思科和惠普分别提出了自己的解决方案。思科的是VN-Tag, 惠普的方案是VEPA(VirtualEthernet Port Aggregator)。为了制定下一代网络接入的话语权，思科和惠普这两个巨头在各自的方案上都毫不让步，纷纷将自己的方案提交为标准，分别为802.1Qbh和802.1Qbg。</p><p><strong>第三部分是服务器到网络的连接。</strong>网络连接技术一直都在追求更高的带宽中发展，比如Infiniband和10Gb以太网。在传统的企业级数据中心IT构架中，服务器到存储网络和互联网络的连接是异构和分开的。存储网络用光纤，互联网用以太网线（ISCSI虽然能够在IP层上跑SCSI，但是性能与光纤比还是差的很远）。数据中心连接技术的发展趋势是用一种连接线将数据中心存储网络和互联网络聚合起来，使服务器可以灵活的配置网络端口，简化IT部署。以太网上的<strong>FCOE</strong>技术和**Infiniband技术本身都使这种趋势成为可能。</p><p>Infiniband 技术产生于上个世纪末，是由Compaq、惠普、IBM、戴尔、英特尔、微软和Sun七家公司共同研究发展的高速先进的I/O标准。InfiniBand是一种长缆线的连接方式，具有高速、低延迟的传输特性。基于InfiniBand技术的网卡的单端口带宽可达20Gbps，为了发挥Infiniband设备的性能，需要一整套的软件栈来驱动和使用，这其中最著名的就是OFED（OpenFabrics Enterprise Distribution）,它基于Infiniband设备实现了RDMA（remote direct memoryaccess）。RDMA的最主要的特点就是零拷贝和旁路操作系统，数据直接在设备和应用程序内存之间传递，这种传递不需要CPU的干预和上下文切换。OFED还实现了一系列的其它软件栈：IPoIB（IP over Infiniband），SRP（SCSI RDMA Protocol）等，这就为Infiniband聚合存储网络和互联网络提供了基础。OFED由OpenFabrics联盟负责开发。</p><p>FCOE的出现则为数据中心互联网络和存储网络的聚合提供了另一种可能。FCOE是将光纤信道直接映射到以太网线上，这样光纤信道就成了以太网线上除了互联网网络协议之外的另一种网络协议。FCOE能够很容易的和传统光纤网络上运行的软件和管理工具相整合，因而能够代替光纤连接存储网络。虽然出现的晚，但FCOE发展极其迅猛。与Infiniband技术需要采用全新的链路相比，企业更愿意升级已有的以太网。在两者性能接近的情况下，采用FCOE方案似乎性价比更高。</p><p><strong>第四部分是网络交换。</strong>需要将物理网络和逻辑网络有效的分离，另外网络设备如交换机、路由器等需要具备1：N和N:1的虚拟化能力。在这一层面上要解决的问题则是要对现有的互联网络进行升级，使之满足新业务的需求，网络虚拟化则是这一变革的重要方向。在这一方向上目前有两种做法，一种是在原有的基础设施上添加新的协议来解决新的问题；另一种则完全推倒重来，希望设计出一种新的网络交换模型。</p><p>当虚拟数据中心开始普及后，虚拟数据中心本身的一些特性引入了对网络新的需求。物理机的位置一般是相对固定的，虚拟化方案的一个很大的特性在于虚拟机可以迁移。当虚拟机的迁移发生在不同网络，不同数据中心之间时，对网络产生了新的要求，比如需要保证虚拟机的IP在迁移前后不发生改变，需要保证虚拟机内运行在第二层（链路层）的应用程序也在迁移后仍可以跨越网络和数据中心进行通信等等。在这方面，Cisco连续推出了OTV，LISP和VXLAN等一系列解决方案，也就是隧道封装技术。</p><p><strong><em>以上就是云计算时代网络虚拟化的基本概述，不知你看明白没有，反正我是觉得我写明白了！！！</em></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;网络作为提供数据交换的模块，是数据中心内最为核心的基础设施之一，并直接关系到数据中心的能力、规模、可扩展性和管理性。为了满足日益增长的网络服务需求，特别是移动互联网业务的爆发式增长，数据中心逐渐向大型化、自动化、虚拟化、多租户的方向发展。传统网络设备不仅部署慢、调整难、成本高，而且其二层地址表项的规模直接决定了数据中心的规模。为此，网络虚拟化的概念应运而生。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-09-Linux原生的存储虚拟化软RAID和LVM</title>
    <link href="https://kkutysllb.cn/2019/06/09/2019-06-09-Linux%E5%8E%9F%E7%94%9F%E7%9A%84%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BD%AFRAID%E5%92%8CLVM/"/>
    <id>https://kkutysllb.cn/2019/06/09/2019-06-09-Linux原生的存储虚拟化软RAID和LVM/</id>
    <published>2019-06-09T10:06:49.000Z</published>
    <updated>2019-06-09T10:39:12.805Z</updated>
    
    <content type="html"><![CDATA[<p>为了让大家更好理解存储虚拟化的特点，本文将讲解各个常用RAID技术方案的特性，并通过实际部署软RAID 10、RAID 5+备份盘等方案来更直观地体验RAID的效果，以便进一步了解生产环境对硬盘设备的IO读写速度和数据冗余备份机制的需求。同时，本文还将介绍LVM的部署、扩容、缩小、快照以及卸载删除的相关知识，以便让大家通过开源存储虚拟化基础对存储虚拟化的块级虚拟化和文件系统级虚拟化有个更深刻的理解。<a id="more"></a></p><h2 id="RAID-技术"><a href="#RAID-技术" class="headerlink" title="RAID 技术"></a>RAID 技术</h2><p>这里主要介绍开源RAID技术，至于华为的RAID2.0技术详见存储虚拟化其他文章。<strong>RAID技术通过把多个硬盘设备组合成一个容量更大、安全性更好的磁盘阵列，并把数据切割成多个区段后分别存放在各个不同的物理硬盘设备上，然后利用分散读写技术来提升磁盘阵列整体的性能，同时把多个重要数据的副本同步到不同的物理硬盘设备上，从而起到了非常好的数据冗余备份效果。</strong></p><p>RAID技术的设计初衷是减少因为采购硬盘设备带来的费用支出，但是与数据本身的价值相比较，现代企业更看重的则是RAID技术所具备的冗余备份机制以及带来的硬盘吞吐量的提升。RAID技术的几种状态如下图所示：</p><p><img src="https://i.loli.net/2019/06/09/5cfcdaae3092b89150.jpg"></p><p><strong>RAID组变为降级状态后，在重建的过程中，如果还有别的成员盘出现故障，故障的成员盘的个数超过了阵列的冗余磁盘的个数，整个RAID组将为变为失效状态，此时原RAID组中的数据将会无法读取。</strong></p><h3 id="RAID-0"><a href="#RAID-0" class="headerlink" title="RAID 0"></a>RAID 0</h3><p><strong>RAID 0技术把多块物理硬盘设备（至少两块）通过硬件或软件的方式串联在一起，组成一个大的卷组，并将数据依次写入到各个物理硬盘中。</strong>在最理想的状态下，<strong>硬盘设备的读写性能会提升数倍</strong>，但是若任意一块硬盘发生故障将导致整个系统的数据都受到破坏。也就说，<strong>RAID 0技术能有效提升硬盘数据的吞吐率，但不具备数据备份和错误修复能力。</strong></p><p><img src="https://i.loli.net/2019/06/09/5cfcdad639e1883289.jpg"></p><p>如上图，RAID 0使用<strong>“分条”（stripe）技术</strong>把数据分布到各个磁盘上，RAID 0至少使用两个磁盘，<strong>并将数据分成从512字节到数兆字节（一般是512Byte的整数倍）的若干块</strong>，这些数据块可以并行写到不同的磁盘中。第1块数据被写到驱动器1中，第2块数据被写到驱动器2中，如此类推，当系统到达阵列中的最后一个磁盘时，就重新回到驱动器1的下一分条进行写操作，分割数据将I/O负载平均分配到所有的驱动器。</p><p><strong>RAID 0的数据写入是以分条形式将数据均匀分布到RAID 组的各个硬盘中。</strong>即一个分条的所有分块写满后，再开始在下一个分条上进行数据写入。如上图，现在有数据D0 ，D1 ，D2 ，D3 ，D4 ，D5需要在RAID 0中进行写入，首先将第一个数据D0写入第一块硬盘位于第一个分条的块，将第二个数据D1写入第二块硬盘位于第一个分条的块，至此，第一个分条的各个块写满了数据，当有数据D2需要写入时，就要对下一个分条进行写入，将数据D2写入第一块硬盘位于第二个分条的块中… 数据块D3，D4，D5的写入同理。<strong>写满一个分条的所有块再开始在下一个分条中进行写入。</strong></p><p><strong>RAID 0在收到数据读取指令后，就会在各个硬盘中进行搜索，看需要读取的数据块位于哪一个硬盘上，再依次对需要读取的数据进行读取。</strong>如上图，现在收到读取数据D0 ，D1 ，D2 ，D3 ，D4 ，D5的指令，首先从第一块磁盘读取数据块D0，再从第二块磁盘读取数据块D1…对各个数据块，从磁盘阵列读取后再由RAID控制器进行整合后传送给系统。至此，整个读取过程结束。</p><h3 id="RAID-1"><a href="#RAID-1" class="headerlink" title="RAID 1"></a>RAID 1</h3><p>RAID 1技术是把<strong>两块以上</strong>的硬盘进行绑定，在写入数据时，是将数据同时写入到多块硬盘设备上，其中某一块硬盘用作数据的备份或镜像。当其中某一块硬盘发生故障后，一般会自动切换的方式来恢复数据的正常使用。</p><p><img src="https://i.loli.net/2019/06/09/5cfcdb033aa6557058.jpg"></p><p><strong>如上图，RAID 1也被称为镜像，其目的是为了打造出一个安全性极高的RAID。</strong>RAID 1使用两组相同的磁盘系统互作镜像，速度没有提高，但是允许单个磁盘故障，数据可靠性最高。其原理为在主硬盘上存放数据的同时也在镜像硬盘上写一样的数据。当主硬盘（物理）损坏时，镜像硬盘则代替主硬盘的工作。因为有镜像硬盘做数据备份，所以RAID 1的数据安全性在所有的RAID级别上来说是最好的。</p><p><strong>RAID 1在进行数据写入的时候，并不是像RAID 0那样将数据分条写入所有磁盘，而是将数据分别写入成员盘，各个成员磁盘上的数据完全相同，互为镜像。</strong>如上图，需要将数据块D0，D1，D2写入RAID 1，先在两个磁盘上同时写入数据块D0，再在两个磁盘上同时写入数据块D1，以此类推。</p><p><strong>RAID 1在进行数据读取的时候，正常情况下可以实现数据盘和镜像盘同时读取数据，提高读取性能，如果一个磁盘损坏，则IO自动到存活的盘读取数据。</strong></p><p><strong>RAID 1的成员磁盘是互为镜像的，成员磁盘的内容完全相同，这样，任何一组磁盘中的数据出现问题，都可以马上从其它成员磁盘进行镜像恢复。</strong>比如：磁盘1损坏导致数据丢失，我们需要将故障磁盘用正常磁盘替换，再读取磁盘2的数据，将其复制到磁盘1上，从而实现了数据的恢复。</p><h3 id="RAID-3"><a href="#RAID-3" class="headerlink" title="RAID 3"></a>RAID 3</h3><p><strong>RAID 3是带有专用奇偶位的条带化阵列，是RAID 0的一种改进模式。它也采用了奇偶校验技术，不过没有使用海明码技术而采用较为简单的异或算法。</strong>在阵列中有一个驱动器专门用来保存其它驱动器中对应分条中数据的奇偶校验信息。奇偶位是编码信息，如果某个驱动器中的数据出错或者某一个驱动器故障，可以通过对奇偶校验信息的计算来恢复出故障驱动器中的数据信息。在数据密集型环境或者是单一用户环境中，组建RAID 3对访问较长的连续记录较好。在写入数据时，RAID 3会把数据的写入操作分散到多个磁盘上进行，然而不管是向哪一个数据盘写入数据，都需要同时重写校验盘中的相关信息。因此，对于那些经常需要执行大量写入操作的应用来说，校验盘的负载将会很大，无法满足程序的运行速度，从而导致整个RAID系统性能的下降。</p><p><img src="https://i.loli.net/2019/06/09/5cfcdb2c9c17861030.jpg"></p><p><strong>RAID 3是单盘容错并行传输。即采用Stripping技术将数据分块</strong>，对这些块进行异或校验，校验数据写到最后一个硬盘上。它的特点是有一个盘为校验盘，数据以位或字节的方式存于各盘（分散记录在组内相同扇区的各个硬盘上）。当一个硬盘发生故障，除故障盘外，写操作将继续对数据盘和校验盘进行操作。</p><p><strong>RAID3的数据读取是按照分条来进行的。</strong>将每个磁盘的驱动器主轴马达做精确的控制，同一分条上各个磁盘上的数据位同时读取，各个驱动器得到充分利用，读性能较高。 <strong>RAID 3的数据读写属于并行方式。</strong></p><p><strong>RAID 3的数据恢复是通过对剩余数据盘和校验盘的异或计算重构故障盘上应有的数据来进行的。</strong>如上图的RAID 3磁盘结构，当磁盘2故障，其上存储的数据位A1，B1，C1丢失，我们需要经过这样一个数据恢复过程：首先恢复数据A1，根据同一分条上其它数据盘和校验盘上的数据A0，A2，P1，进行异或运算，得到应有的数据A1，再用相同方法恢复出数据B1，C1的数据，至此，磁盘2上的数据全部得到了恢复。<strong>由于校验集中在一个盘，因此在数据恢复时，校验盘写压力比较大，影响性能。</strong></p><h3 id="RAID-5"><a href="#RAID-5" class="headerlink" title="RAID 5"></a>RAID 5</h3><p><strong>RAID5是一种旋转奇偶校验独立存取的阵列方式，它与RAID3不同的是没有固定的校验盘，</strong>而是按某种规则把奇偶校验信息均匀地分布在阵列所属的硬盘上，所以在每块硬盘上，既有数据信息也有校验信息。这一改变解决了争用校验盘的问题，能并发进行多个写操作。所以RAID 5即适用于大数据量的操作，也适用于各种事务处理，它是一种快速、大容量和容错分布合理的磁盘阵列。当有N块阵列盘时，可用容量为N-1块盘容量。 <strong>RAID 3、RAID 5中，在一块硬盘发生故障后，RAID组从ONLINE变为DEGRADED方式，直到故障盘恢复。但如果在DEGRADED状态下，又有第二块盘故障，整个RAID组的数据将丢失。</strong> </p><p><img src="https://i.loli.net/2019/06/09/5cfcdb527467110620.jpg"></p><p><strong>RAID 5的数据写入也是按分条进行的，各个磁盘上既存储数据块，又存储校验信息。</strong>一个分条上的数据块写入完成后，将产生的校验信息写入对应的校验磁盘中。<strong>由于RAID 5的数据是按照数据分条存储的，在读取的时候，按照分条进行读取。</strong></p><p>当RAID5中某一个磁盘故障，在恢复的时候，可利用其它存活成员盘数据进行异或逆运算，恢复故障盘上的数据。</p><h3 id="RAID-6"><a href="#RAID-6" class="headerlink" title="RAID 6"></a>RAID 6</h3><p><strong>RAID 6是带有两种校验的独立磁盘结构，采用两种奇偶校验方法，需要至少N+2(N&gt;2)个磁盘来构成阵列</strong>，一般用在数据可靠性、可用性要求极高的应用场合 。常用的RAID 6技术有<strong>RAID6 P＋Q</strong>和<strong>RAID6 DP。</strong></p><p>RAID 6实际上是在RAID 5基础上为了进一步保证数据可用性和可靠性设计的一种RAID方式。与<strong>RAID 5相比除了有通常的异或校验方式外，还增加了另一种特殊的异或校验方式和该方式校验数据存放区域</strong>，因此RAID 6的数据冗余性能相当好。但是，由于增加了一个校验，所以写入的效率比RAID 5要低，而且控制系统的设计也更为复杂，第二个校验区也减少了有效存储空间。</p><p>目前RAID 6还没有统一的标准，各家公司的实现方式都有所不同，主要有以下两种方式：</p><ul><li><strong>RAID P+Q： 华为、HDS</strong></li><li><strong>RAID DP： NetApp</strong></li></ul><p><strong>两种技术获取校验信息的方法不同，但是都能够在两块成员盘故障的情况下读取数据，数据不丢失。</strong></p><p><strong>1）RAID6 P＋Q的工作原理</strong></p><p>RAID6 P＋Q需要计算出两个校验数据P和Q，当有两个数据丢失时，根据P和Q恢复出丢失的数据。校验数据P和Q是由以下公式计算得来的：</p><p>​     <strong>P=D0⊕ D1 ⊕ D2 ……</strong>      </p><p>​     <strong>Q=(α⊗D0)⊕(β⊗D1)⊕(γ⊗D2)……</strong></p><p><img src="https://i.loli.net/2019/06/09/5cfcdb8e3f83772882.jpg"></p><p>在RAID6 P＋Q中，<strong>P和Q是两个相互独立的校验值，它们的计算互不影响，都是由同一分条上其它数据磁盘上的数据依据不同的算法计算而来的。</strong></p><p><strong>其中，P值的获得是通过同一分条上除P和Q之外的其它所有数据盘上数据的简单异或运算得到。Q值的获得过程就相对复杂一些，它首先对同一分条其他磁盘上的各个数据分别进行一个变换，然后再将这些变换结果进行异或操作而得到校验盘上的数据。</strong>这个变换被称为<strong>GF变换</strong>，它是一种常用的数学变换方法，可以通过查GF变换表而得到相应的变换系数，再将各个磁盘上的数据与变换系数进行运算就得到了GF变换后的数据，这个变换过程是由RAID控制器来完成的。</p><p>以上图为例，P1是由分条0中的数据D0、D1、D2进行简单的异或运算而得到的。同理，P2是由分条1中的数据D3、D4、D5进行简单的异或运算而得到， P3是由分条2中的数据D6、D7、D8进行简单的异或运算而得到。Q1是由分条0中的数据D0、D1、D2分别进行GF变换之后再进行异或运算而得到的。同理，Q2是由分条1中的数据D3、D4、D5分别进行GF变换之后再进行异或运算而得到， Q3是由分条2中的数据D6、D7、D8分别进行GF变换之后再进行异或运算而得到。</p><p>当某一个分条中有一块磁盘发生故障，根本不需要Q，直接用校验值P与其他正常数据进行异或运算就可以恢复出故障盘上面的数据，数据恢复比较方便。当分条中有两块磁盘发生故障，如果其中包含Q所在的磁盘，则可以先恢复出数据盘上面的数据，再恢复出校验盘Q上的校验值；如果故障盘不包含Q所在的盘，则可以将两个校验公式作为方程组，从而可以恢复出两个故障盘上面的数据。</p><p><strong>2）RAID6 DP的工作原理</strong></p><p>DP就是Double Parity，就是在RAID4所使用的一个行XOR校验磁盘的基础上又增加了一个磁盘用于存放斜向的XOR校验信息。</p><p><img src="https://i.loli.net/2019/06/09/5cfcdbac4614d91015.jpg"></p><p>横向校验盘中P0—P3为各个数据盘中横向数据的校验信息。比如：P0=D0  XOR D1 XOR D2 XOR D3。斜向校验盘中DP0—DP3为各个数据盘及横向校验盘的斜向数据校验信息。比如：DP0=D0 XOR D5 XOR D10 XOR D15</p><p><strong>RAID6 DP 同样也有两个相互独立的校验信息块，但是与RAID6 P＋Q 不同的是，它的第二块校验信息是斜向的。横向校验信息和斜向校验信息都使用异或校验算法而得到，</strong>横向校验盘中的信息的获得方法非常简单：P0是由条带0中的数据D0、D1、D2、 D3进行简单的异或运算而得到的。同理， P1是由分条1中的数据D4、D5、D6、D7进行简单的异或运算而得到。</p><p>斜向校验盘中校验信息的获得依然是采用数据之间的异或运算，只是数据块的选取相对复杂一些，是一个斜向的选取过程：由分条0上面第一个磁盘上的数据D0、分条1上面第二个磁盘上的数据D5、分条2上面第三个磁盘上的数据D10、分条3上面第四个磁盘上的数据D15经过异或校验而得到校验信息DP0；由分条0上面第二个磁盘上的数据D1、分条1上面第三个磁盘上的数据D6、分条2上面第四个磁盘上的数据D11、分条3上面校验盘上面的信息P3经过异或校验而得到校验信息DP1；由分条0上面第三个磁盘上的数据D2、分条1上面第四个磁盘上的数据D7、分条2上面校验盘上面的信息P2、分条3上面第一个磁盘上的数据D12经过异或校验而得到校验信息DP2。</p><p><strong>RAID6 DP允许阵列中同时有两个磁盘失效</strong>，我们以上图为例，假设磁盘1、2故障，则数据D0、D1、D4、D5、D8、D9、D12、D13失效，其他磁盘数据和校验信息正常。我们来看一下数据恢复是怎样一个过程：首先根据DP2和斜向校验恢复出D12， D12 =D2 ⊕ D7 ⊕ P2 ⊕DP2，然后利用P3和横向校验恢复出D13， D13 =D12 ⊕ D14 ⊕ D15 ⊕P3 ；根据DP3斜向校验恢复出D8， D8 =D3 ⊕ P1 ⊕DP3 ⊕ D13，利用P2和横向校验恢复出D9， D9 =D8 ⊕ D10 ⊕ D11 ⊕ P2；根据DP4和斜向校验恢复出D4，利用P1和横向校验恢复出D5，以此类推进而恢复出磁盘1、2上的所有数据。</p><h3 id="RAID-10"><a href="#RAID-10" class="headerlink" title="RAID 10"></a>RAID 10</h3><p>RAID 10是将镜像和条带进行组合的RAID级别，先进行RAID 1镜像然后再做RAID  0。RAID 10也是一种应用比较广泛的RAID级别。</p><p><img src="https://i.loli.net/2019/06/09/5cfcdbd591c9b84222.jpg"></p><p>RAID 10集RAID 0和RAID 1的优点于一身，适合应用在速度和容错要求都比较高的场合。先进行镜像，再进行分条。物理磁盘1和物理磁盘2组成RAID 1，物理磁盘3和物理磁盘4组成RAID 1，两个RAID 1再进行RAID 0。 </p><p>当不同RAID1中的磁盘，如物理磁盘2和物理磁盘4发生故障导致数据失效时，整个阵列的数据读取不会受到影响，因为物理磁盘1和物理磁盘3上面已经保存了一份完整的数据。但是如果组成RAID 1的磁盘（如物理磁盘1和物理磁盘2）同时故障，数据将不能正常读取。</p><h3 id="RAID-50"><a href="#RAID-50" class="headerlink" title="RAID 50"></a>RAID 50</h3><p>RAID 50是将RAID 5和RAID 0进行两级组合的RAID级别，第一级是 RAID 5，第二级为RAID 0。</p><p><img src="https://i.loli.net/2019/06/09/5cfcdbfae058d26478.jpg"></p><p>RAID 50是RAID 5和RAID 0的结合，先将3个或3个以上磁盘实现RAID 5，再把若干个RAID 5进行RAID 0分条。 RAID 50需要至少6个磁盘构成，把数据分条后分放到各个RAID 5中，在RAID 5中再进行分条和计算校验值及校验值的分布式存储。</p><p>如上图，物理磁盘1、2、3实现RAID 5，物理磁盘4、5、6实现RAID 5，再将两个RAID 5放在一起进行分条。允许不同RAID 5中的多块磁盘同时失效，但是一旦同一RAID 5中的两块磁盘故障，将会导致阵列失效。</p><h3 id="常用RAID级别的比较和应用场景"><a href="#常用RAID级别的比较和应用场景" class="headerlink" title="常用RAID级别的比较和应用场景"></a>常用RAID级别的比较和应用场景</h3><p><img src="https://i.loli.net/2019/06/09/5cfcdc2bd733957285.jpg"></p><p><strong>RAID组成员盘个数不建议过多，例如超过20块成员盘的RAID,不但性能比8-9块成员盘RAID组(建议RAID5成员盘个数)低，且在运行过程中RAID组失效的概率增加。</strong></p><table><thead><tr><th><strong>RAID级别</strong></th><th><strong>RAID 0</strong></th><th><strong>RAID 1</strong></th><th><strong>RAID 3</strong></th><th><strong>RAID 5 /6</strong></th><th><strong>RAID 10</strong></th></tr></thead><tbody><tr><td>典型应用环境</td><td>迅速读写，安全性要求不高，如图形工作站等</td><td>随机数据写入，安全性要求高，如服务器、数据库存储领域</td><td>连续数据传输，安全性要求高，如视频编辑、大型数据库等</td><td>随机数据传输，安全性要求高，如金融、数据库、存储等</td><td>数据量大，安全性要求高，如银行、金融等领域</td></tr></tbody></table><h3 id="虚拟机中的RAID实操"><a href="#虚拟机中的RAID实操" class="headerlink" title="虚拟机中的RAID实操"></a>虚拟机中的RAID实操</h3><p><strong>由于我们在虚拟机中搭建实验环境，所以采用Linux的软RAID来搭建，与实际生产环境硬件RAID卡相比，除了性能上不达标，一些关键特性不具备外，在数据写入、读取和恢复方面的机制类似，便于大家理解RAID的工作特性。</strong></p><p>首先我们给虚拟机挂载4块数据盘，用于组建RAID 10，如下图所示：</p><p><img src="https://i.loli.net/2019/06/09/5cfcdc63ca58f65011.jpg"></p><p><strong>mdadm命令用于管理Linux系统中的软件RAID硬盘阵列</strong>，格式为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mdadm [模式]  [选项] [成员设备名称]</span><br></pre></td></tr></table></figure><p>mdadm命令在Linux系统 中创建和管理软件RAID磁盘阵列，它涉及的理论知识的操作过程与生产环境中的完全一致。mdadm常用命令选项如下：</p><p><strong>1）创建模式命令选项和专用选项</strong></p><ul><li>-C 创建RAID</li><li>-l 指定级别</li><li>-n 指定设备个数</li><li>-v 显示创建过程</li><li>-a {yes|no} 自动为其创建设备文件</li><li>-c 指定数据块大小（chunk）</li><li>-x 指定空闲盘（热备磁盘）个数，空闲盘（热备磁盘）能在工作盘损坏后自动顶替</li></ul><p><strong>注意：创建阵列时，阵列所需磁盘数为-n参数和-x参数的个数和</strong></p><p>下面开始创建RAID过程：</p><p><strong>Step1：在系统中，通过mdadm创建一个软RAID 10，命名为/dev/md10，</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mdadm -Cv /dev/md10 -a yes -n 4 -l 10 /dev/sdb /dev/sdc /dev/sde /dev/sdd</span></span><br><span class="line">mdadm: layout defaults to n2</span><br><span class="line">mdadm: layout defaults to n2</span><br><span class="line">mdadm: chunk size defaults to 512K</span><br><span class="line">mdadm: size <span class="built_in">set</span> to 20954112K</span><br><span class="line">mdadm: Defaulting to version 1.2 metadata</span><br><span class="line">mdadm: array /dev/md10 started.</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># fdisk -l | grep "/dev/md10"</span></span><br><span class="line">Disk /dev/md10: 42.9 GB, 42914021376 bytes, 83816448 sectors</span><br></pre></td></tr></table></figure><p>通过上面的校验我们发现RAID 10创建成功，且大小为42.9G，符合预期。</p><p><strong>Step2：将创建好的md10格式化为ext4文件系统</strong>，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mkfs.ext4 /dev/md10</span></span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Filesystem label=</span><br><span class="line">OS <span class="built_in">type</span>: Linux</span><br><span class="line">Block size=4096 (<span class="built_in">log</span>=2)</span><br><span class="line">Fragment size=4096 (<span class="built_in">log</span>=2)</span><br><span class="line">Stride=128 blocks, Stripe width=256 blocks</span><br><span class="line">2621440 inodes, 10477056 blocks</span><br><span class="line">523852 blocks (5.00%) reserved <span class="keyword">for</span> the super user</span><br><span class="line">First data block=0</span><br><span class="line">Maximum filesystem blocks=2157969408</span><br><span class="line">320 block groups</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">8192 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class="line">4096000, 7962624</span><br><span class="line">Allocating group tables: <span class="keyword">done</span>                            </span><br><span class="line">Writing inode tables: <span class="keyword">done</span>                            </span><br><span class="line">Creating journal (32768 blocks): <span class="keyword">done</span></span><br><span class="line">Writing superblocks and filesystem accounting information: <span class="keyword">done</span></span><br></pre></td></tr></table></figure><p><strong>Step3：创建挂载点，把md10设备进行挂载，挂载后就可以发现md10可用空间为40G，</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mkdir /home/data</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># mount /dev/md10 /home/data</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># df -h</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda3        41G  2.4G   37G   7% /</span><br><span class="line">devtmpfs        2.0G     0  2.0G   0% /dev</span><br><span class="line">tmpfs           2.0G     0  2.0G   0% /dev/shm</span><br><span class="line">tmpfs           2.0G   12M  2.0G   1% /run</span><br><span class="line">tmpfs           2.0G     0  2.0G   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1       380M  102M  254M  29% /boot</span><br><span class="line">tmpfs           394M     0  394M   0% /run/user/0</span><br><span class="line">/dev/md10        40G   49M   38G   1% /home/data</span><br></pre></td></tr></table></figure><p><strong>Step4：查看/dev/md10磁盘阵列的详细信息，并把挂载信息写入到配置文件中，使其开机即挂载永久生效。</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mdadm -D /dev/md10</span></span><br><span class="line">/dev/md10:</span><br><span class="line">           Version : 1.2</span><br><span class="line">     Creation Time : Sun Jun  9 12:52:22 2019</span><br><span class="line">        Raid Level : raid10</span><br><span class="line">        Array Size : 41908224 (39.97 GiB 42.91 GB)</span><br><span class="line">     Used Dev Size : 20954112 (19.98 GiB 21.46 GB)</span><br><span class="line">      Raid Devices : 4</span><br><span class="line">     Total Devices : 4</span><br><span class="line">       Persistence : Superblock is persistent</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">   Update Time : Sun Jun  9 12:59:55 2019</span><br><span class="line">         State : clean </span><br><span class="line">Active Devices : 4</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">   Working Devices : 4</span><br><span class="line">    Failed Devices : 0</span><br><span class="line">     Spare Devices : 0</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">        Layout : near=2</span><br><span class="line">    Chunk Size : 512K</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">Consistency Policy : resync</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">          Name : c7-test01:10  (<span class="built_in">local</span> to host c7-test01)</span><br><span class="line">          UUID : 7f68ffce:e80f0c04:6c58f40c:526c5508</span><br><span class="line">        Events : 17</span><br><span class="line"></span><br><span class="line">Number   Major   Minor   RaidDevice State</span><br><span class="line">   0       8       16        0      active sync <span class="built_in">set</span>-A   /dev/sdb</span><br><span class="line">   1       8       32        1      active sync <span class="built_in">set</span>-B   /dev/sdc</span><br><span class="line">   2       8       64        2      active sync <span class="built_in">set</span>-A   /dev/sde</span><br><span class="line">   3       8       48        3      active sync <span class="built_in">set</span>-B   /dev/sdd</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># echo "/dev/md10 /home/data ext4 defaults 0 0" &gt;&gt; /etc/fstab</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># cat /etc/fstab </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># /etc/fstab</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Created by anaconda on Sat Jun  1 20:49:28 2019</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Accessible filesystems, by reference, are maintained under '/dev/disk'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">UUID=f078e329-6693-4fdd-89ad-0d5734b5e35f /                       ext4    defaults        1 1</span><br><span class="line">UUID=13919904-2c90-429c-b5ea-56f823a6e87c /boot                   ext4    defaults        1 2</span><br><span class="line">UUID=d3b139fc-7730-4bee-886b-1d4e0d681bae swap                    swap    defaults        0 0</span><br><span class="line">/dev/md10 /home/data ext4 defaults 0 0</span><br></pre></td></tr></table></figure><p><strong>2）管理模式</strong></p><ul><li>-a(–add)：添加磁盘</li><li>-d(–del)：删除磁盘</li><li>-r(–remove)：从RAID中移除损坏的成员盘</li><li>-f(–fail)：模拟成员盘故障</li><li>-S：停止阵列工作</li></ul><p><strong>注意：新增加的硬盘需要与原硬盘大小一致，且如果原有阵列缺少工作磁盘（如raid1只有一块在工作，raid5只有2块在工作），这时新增加的磁盘直接变为工作磁盘，如果原有阵列工作正常，则新增加的磁盘为热备磁盘。</strong></p><p><strong>Step1：模拟成员盘/dev/sdb故障，</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mdadm /dev/md10 -f /dev/sdb</span></span><br><span class="line">mdadm: <span class="built_in">set</span> /dev/sdb faulty <span class="keyword">in</span> /dev/md10</span><br></pre></td></tr></table></figure><p><strong>Step2：查看RAID状态，此时md10仍正常工作，但出于降级状态，符合预期。</strong>如下：</p><p><img src="https://i.loli.net/2019/06/09/5cfcdd3b93e4e31750.jpg"></p><p>在RAID 10级别的磁盘阵列中，当RAID 1磁盘阵列中存在一个故障盘时并不影响RAID 10磁盘阵列的使用，但是磁盘阵列此时出于降级使用状态。当购买了新的硬盘设备后再使用mdadm命令来予以替换即可，在此期间我们可以在/home/data目录中正常地创建或删除文件。由于我们是在虚拟机中模拟硬盘，所以先重启系统，然后再把新的硬盘添加到RAID磁盘阵列中。这个过程不是我们讨论的重点，现在/dev/sdb成员盘故障，使得md10处于降级使用状态，如果/dev/sdd故障呢？/dev/sdc故障呢？这两点才是我们要重点讨论的地方。</p><p><strong>Step3：再次模拟成员/dev/sdd故障，</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mdadm /dev/md10 -f /dev/sdd</span></span><br><span class="line">mdadm: <span class="built_in">set</span> /dev/sdd faulty <span class="keyword">in</span> /dev/md10</span><br></pre></td></tr></table></figure><p><strong>Step4：再次查看RAID的状态，</strong>如下：</p><p><img src="https://i.loli.net/2019/06/09/5cfcdd99db98b69860.jpg"></p><p>此时阵列中虽然坏了两块成员盘/dev/sdb和/dev/sdd，因为这两块成员盘同属于不同的RAID 1，所以阵列状态仍为降级状态，也就是表示还可以使用。接下来，我们模拟同一个RAID1中两块成员盘故障，看看阵列状态是否能达到我们预期的失效态。在模拟之前，需要先恢复一块成员盘，比如：/dev/sdd。<strong>注意：需要重启系统后才能恢复。恢复磁盘后，需要等待一段时间，此时刚恢复的磁盘正在恢复数据，如下：</strong></p><p><img src="https://i.loli.net/2019/06/09/5cfcddb71503b81495.jpg"></p><p><strong>Step5：模拟同一个RAID 1下两块成员盘同时故障，</strong>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]# mdadm /dev/md10 -f /dev/sdb /dev/sdc</span><br><span class="line">mdadm: set /dev/sdb faulty in /dev/md10</span><br><span class="line">mdadm: set /dev/sdc faulty in /dev/md10</span><br></pre></td></tr></table></figure><p><strong>Step6：再次查看阵列状态，此时软RAID 10虽然显示状态为降级使用，但是只有SET-A组，也就是说SET-B组的数据已丢失。</strong>如下：</p><p><img src="https://i.loli.net/2019/06/09/5cfcddf37dc5749392.jpg"></p><p><strong><em>RAID 5 的创建方法与此类似，不过RAID 5至少需要3块硬盘，同时，为了保证数据的可靠性，可以再增加一块热备盘。当成员盘故障时，数据盘可以立即顶上去并做数据同步和恢复操作。</em></strong></p><p><strong>Step1：创建RAID 5阵列/dev/md5，</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mdadm -Cv /dev/md5 -a yes -n 3 -l 5 -x 1 /dev/sdf /dev/sdg /dev/sdh /dev/sdi</span></span><br><span class="line">mdadm: layout defaults to left-symmetric</span><br><span class="line">mdadm: layout defaults to left-symmetric</span><br><span class="line">mdadm: chunk size defaults to 512K</span><br><span class="line">mdadm: size <span class="built_in">set</span> to 20954112K</span><br><span class="line">mdadm: Defaulting to version 1.2 metadata</span><br><span class="line">mdadm: array /dev/md5 started.</span><br></pre></td></tr></table></figure><p><strong>Step2：格式化md5，并创建挂载目录，并挂载磁盘</strong>，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mkfs.ext /dev/md5</span></span><br><span class="line">-bash: mkfs.ext: <span class="built_in">command</span> not found</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># mkfs.ext4 /dev/md5</span></span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Filesystem label=</span><br><span class="line">OS <span class="built_in">type</span>: Linux</span><br><span class="line">Block size=4096 (<span class="built_in">log</span>=2)</span><br><span class="line">Fragment size=4096 (<span class="built_in">log</span>=2)</span><br><span class="line">Stride=128 blocks, Stripe width=256 blocks</span><br><span class="line">2621440 inodes, 10477056 blocks</span><br><span class="line">523852 blocks (5.00%) reserved <span class="keyword">for</span> the super user</span><br><span class="line">First data block=0</span><br><span class="line">Maximum filesystem blocks=2157969408</span><br><span class="line">320 block groups</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">8192 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class="line">4096000, 7962624</span><br><span class="line"></span><br><span class="line">Allocating group tables: <span class="keyword">done</span>                            </span><br><span class="line">Writing inode tables: <span class="keyword">done</span>                            </span><br><span class="line">Creating journal (32768 blocks): <span class="keyword">done</span></span><br><span class="line">Writing superblocks and filesystem accounting information: <span class="keyword">done</span>   </span><br><span class="line">[root@c7-test01 ~]<span class="comment"># mkdir /home/code</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># mount /dev/md5 /home/code</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># df -h</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda3        41G  2.4G   37G   7% /</span><br><span class="line">devtmpfs        2.0G     0  2.0G   0% /dev</span><br><span class="line">tmpfs           2.0G     0  2.0G   0% /dev/shm</span><br><span class="line">tmpfs           2.0G   12M  2.0G   1% /run</span><br><span class="line">tmpfs           2.0G     0  2.0G   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1       380M  102M  254M  29% /boot</span><br><span class="line">/dev/md10        40G   49M   38G   1% /home/data</span><br><span class="line">tmpfs           394M     0  394M   0% /run/user/0</span><br><span class="line">/dev/md5         40G   49M   38G   1% /home/code</span><br></pre></td></tr></table></figure><p><strong>Step3：查看阵列md5的状态，</strong>如下：</p><p><img src="https://i.loli.net/2019/06/09/5cfcde4c1c21492447.jpg"></p><p><strong>Step4：我们再次将成员盘/dev/sdf模拟故障，然后可以看见热备盘/dev/sdi自动顶上故障盘位置并开始同步恢复数据。</strong>如下：</p><p><img src="https://i.loli.net/2019/06/09/5cfcde683820748137.jpg"></p><h2 id="LVM逻辑卷管理"><a href="#LVM逻辑卷管理" class="headerlink" title="LVM逻辑卷管理"></a>LVM逻辑卷管理</h2><h3 id="LVM逻辑卷管理的原理浅析"><a href="#LVM逻辑卷管理的原理浅析" class="headerlink" title="LVM逻辑卷管理的原理浅析"></a>LVM逻辑卷管理的原理浅析</h3><p>LVM是逻辑盘卷管理（Logical Volume Manager）的简称，它是Linux环境下对磁盘分区进行管理的一种机制，LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。</p><p><strong>LVM的工作原理其实很简单，它就是通过将底层的物理硬盘抽象的封装起来，然后以逻辑卷的方式呈现给上层应用。</strong>在传统的磁盘管理机制中，我们的上层应用是直接访问文件系统，从而对底层的物理硬盘进行读取，而在LVM中，其通过对底层的硬盘进行封装，当我们对底层的物理硬盘进行操作时，其不再是针对于分区进行操作，而是通过一个叫做逻辑卷的东西来对其进行底层的磁盘管理操作。比如说我增加一个物理硬盘，这个时候上层的服务是感觉不到的，因为呈现给上层服务的是以逻辑卷的方式。</p><p><strong>LVM最大的特点就是可以对磁盘进行动态管理。因为逻辑卷的大小是可以动态调整的，而且不会丢失现有的数据。</strong>如果我们新增加了硬盘，其也不会改变现有上层的逻辑卷。作为一个动态磁盘管理机制，逻辑卷技术大大提高了磁盘管理的灵活性。<strong>LVM的最大缺点就是影响磁盘I/O效率。</strong></p><p>在一个硬盘上创建多个逻辑卷，对创建好的卷调整大小，然后将它们挂载在’/home,/var,/tmp’等目录下，如下所示：</p><p><img src="https://i.loli.net/2019/06/09/5cfcde8ba6e8e38978.jpg"></p><blockquote><p><strong>PV（Physical Volume）- 物理卷：</strong>物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区，也可以是整个物理硬盘，也可以是raid设备，是LVM的基本存储逻辑块，但和基本的物理存储介质(如分区、磁盘等)比较，却包含有与LVM相关的管理参数。</p><p><strong>VG（Volumne Group）- 卷组：</strong>卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。</p><p><strong>PE（physical extent）：</strong>每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是在VG过程中配置的，默认为4MB。</p><p>LVM 默认使用4MB的PE区块，而LVM的LV最多仅能含有65534个PE (lvm1 的格式)，因此默认的LVM的LV最大容量为4M*65534/(1024M/G)=256G。PE是整个LVM 最小的储存区块，也就是说，其实我们的资料都是由写入PE 来处理的。简单的说，这个PE 就有点像文件系统里面的block 角色。所以调整PE 会影响到LVM 的最大容量！不过，在 CentOS 6.x 以后，由于直接使用 lvm2 的各项格式功能，因此这个限制已经不存在了。</p><p><strong>LV（Logical Volume）- 逻辑卷</strong>：逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。</p></blockquote><p><strong><em>简单来说就是：PV是物理的存储设备（整块磁盘或磁盘分区），VG是一个存放PV的仓库，将仓库中所有PV重新编号，也就是PE。LV就是用重新编号的PE组成的逻辑磁盘。</em></strong></p><p><strong>LVM 主要有三类命令行工具：</strong></p><ul><li><strong>pv 开头的命令用来操作 PV 物理卷</strong></li><li><strong>vg 开头的命令用来操作 VG 逻辑卷组</strong></li><li><strong>lv 开头的命令用来操作 LV 逻辑卷</strong></li></ul><table><thead><tr><th>功能/命令</th><th>物理卷管理</th><th>卷组管理</th><th>逻辑卷管理</th></tr></thead><tbody><tr><td>扫描</td><td>pvscan</td><td>vgscan</td><td>lvscan</td></tr><tr><td>建立</td><td>pvcreate</td><td>vgcreate</td><td>lvcreate</td></tr><tr><td>显示</td><td>pvdisplay</td><td>vgdisplay</td><td>lvdisplay</td></tr><tr><td>删除</td><td>pvremove</td><td>vgremove</td><td>lvremove</td></tr><tr><td>扩展</td><td></td><td>vgextend</td><td>lvextend</td></tr><tr><td>缩小</td><td></td><td>vgreduce</td><td>lvreduce</td></tr></tbody></table><h3 id="LVM逻辑卷管理实操"><a href="#LVM逻辑卷管理实操" class="headerlink" title="LVM逻辑卷管理实操"></a>LVM逻辑卷管理实操</h3><p><strong>1）安装逻辑卷管理软件，创建物理卷，并添加卷组</strong></p><p><strong>Step1：我们首先添加两块磁盘/dev/sdb和/dev/sdc，</strong>用来作为逻辑实验的环境，方法同RAID实验添加磁盘。</p><p><img src="https://i.loli.net/2019/06/09/5cfcded42d81c26718.jpg"></p><p><strong>Step2：:安装lvm逻辑卷管理软件，</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y lvm2</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfcdf114a77617673.jpg"></p><p><strong>Step3：对添加的两块磁盘（整盘）创建物理卷</strong>，这样这两块就可以被LVM进行管理，并有相关LVM管理参数。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># pvcreate /dev/sdb /dev/sdc</span></span><br><span class="line">Physical volume <span class="string">"/dev/sdb"</span> successfully created.</span><br><span class="line">Physical volume <span class="string">"/dev/sdc"</span> successfully created.</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfcdf41673aa85499.jpg"></p><p><strong>Step4：创建卷组datastorage ，并将两块物理盘加入到新创建的卷组中</strong>，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># vgcreate datastorage /dev/sdb /dev/sdc</span></span><br><span class="line">Volume group <span class="string">"datastorage"</span> successfully created</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfcdf6d9e48941077.jpg"></p><p><strong>2）逻辑卷管理实操</strong></p><p><strong>Step1：创建出一个150MB的逻辑卷设备lv_data01，</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># lvcreate -n lv_data01 -L 150M datastorage </span></span><br><span class="line">Rounding up size to full physical extent 152.00 MiB</span><br><span class="line">Logical volume <span class="string">"lv_data01"</span> created.</span><br></pre></td></tr></table></figure><p><strong>注意：这里切割单位的问题。在对逻辑卷进行切割时有两种计量单位。第一种是以容量为单位，所使用的参数为-L。例如，使用-L 150M生成一个大小为150MB的逻辑卷。另外一种是以基本单元的个数为单位，所使用的参数为-l。每个基本单元的大小默认为4MB。例如，使用-l 37可以生成一个大小为37×4MB=148MB的逻辑卷。我们采用的第一种切割办法。</strong></p><p><img src="https://i.loli.net/2019/06/09/5cfcdfa428bd573707.jpg"></p><p><strong>Step2：把创建的逻辑卷格式化为ext4文件系统格式，然后挂载到/home/data01下使用。</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># mkfs.ext4 /dev/datastorage/lv_data01 </span></span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Filesystem label=</span><br><span class="line">OS <span class="built_in">type</span>: Linux</span><br><span class="line">Block size=1024 (<span class="built_in">log</span>=0)</span><br><span class="line">Fragment size=1024 (<span class="built_in">log</span>=0)</span><br><span class="line">Stride=0 blocks, Stripe width=0 blocks</span><br><span class="line">38912 inodes, 155648 blocks</span><br><span class="line">7782 blocks (5.00%) reserved <span class="keyword">for</span> the super user</span><br><span class="line">First data block=1</span><br><span class="line">Maximum filesystem blocks=33816576</span><br><span class="line">19 block groups</span><br><span class="line">8192 blocks per group, 8192 fragments per group</span><br><span class="line">2048 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">8193, 24577, 40961, 57345, 73729</span><br><span class="line">Allocating group tables: <span class="keyword">done</span>                            </span><br><span class="line">Writing inode tables: <span class="keyword">done</span>                            </span><br><span class="line">Creating journal (4096 blocks): <span class="keyword">done</span></span><br><span class="line">Writing superblocks and filesystem accounting information: <span class="keyword">done</span> </span><br><span class="line">[root@c7-test01 ~]<span class="comment"># mkdir /home/data01 &amp;&amp; mount /dev/datastorage/lv_data01 /home/data01</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfcdfd71812b75693.jpg"></p><p><strong>Linux系统会把LVM中的逻辑卷设备存放在/dev设备目录中（实际上是做了一个符号链接），同时会以卷组的名称来建立一个目录，其中保存了逻辑卷的设备映射文件（即/dev/卷组名称/逻辑卷名称）。设置开机自动挂载的方式，同上面RAID实操，这里不再赘述。</strong></p><p><img src="https://i.loli.net/2019/06/09/5cfcdff11e04972825.jpg"></p><p><strong>Step3：扩容逻辑卷lv_data01，在扩容前需要先卸载挂载点，否则数据会丢失。</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卸载挂载点</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># umount /home/data01/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 扩容到290MB</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># lvextend -L 290M /dev/datastorage/lv_data01 </span></span><br><span class="line">  Rounding size to boundary between physical extents: 292.00 MiB.</span><br><span class="line">  Size of logical volume datastorage/lv_data01 changed from 152.00 MiB (38 extents) to 292.00 MiB (73 extents).</span><br><span class="line">  Logical volume datastorage/lv_data01 successfully resized.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查磁盘完整性，并重置磁盘容量</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># e2fsck -f /dev/datastorage/lv_data01 </span></span><br><span class="line">e2fsck 1.42.9 (28-Dec-2013)</span><br><span class="line">Pass 1: Checking inodes, blocks, and sizes</span><br><span class="line">Pass 2: Checking directory structure</span><br><span class="line">Pass 3: Checking directory connectivity</span><br><span class="line">Pass 4: Checking reference counts</span><br><span class="line">Pass 5: Checking group summary information</span><br><span class="line">/dev/datastorage/lv_data01: 11/38912 files (0.0% non-contiguous), 10567/155648 blocks</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># resize2fs /dev/datastorage/lv_data01 </span></span><br><span class="line">resize2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Resizing the filesystem on /dev/datastorage/lv_data01 to 299008 (1k) blocks.</span><br><span class="line">The filesystem on /dev/datastorage/lv_data01 is now 299008 blocks long.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新挂载磁盘，并查看状态</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># mount /dev/datastorage/lv_data01 /home/data01</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfce0329b51717664.jpg"></p><p><strong>在前面我们的卷组是由两块硬盘设备共同组成的。用户在使用存储设备时感知不到设备底层的架构和布局，更不用关心底层是由多少块硬盘组成的，只要卷组中有足够的资源，就可以一直为逻辑卷扩容。</strong></p><p><strong>Step4：缩容逻辑卷lv_data01，在执行缩容操作前记得先把挂载点卸载掉，同时要先检查磁盘文件系统的完整性。</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取消挂载点，并检查文件系统完整性</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># umount /home/data01/ &amp;&amp; e2fsck -f /dev/datastorage/lv_data01 </span></span><br><span class="line">e2fsck 1.42.9 (28-Dec-2013)</span><br><span class="line">Pass 1: Checking inodes, blocks, and sizes</span><br><span class="line">Pass 2: Checking directory structure</span><br><span class="line">Pass 3: Checking directory connectivity</span><br><span class="line">Pass 4: Checking reference counts</span><br><span class="line">Pass 5: Checking group summary information</span><br><span class="line">/dev/datastorage/lv_data01: 11/75776 files (0.0% non-contiguous), 15729/299008 blocks</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把逻辑卷缩小到120M</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># resize2fs /dev/datastorage/lv_data01 120M</span></span><br><span class="line">resize2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Resizing the filesystem on /dev/datastorage/lv_data01 to 122880 (1k) blocks.</span><br><span class="line">The filesystem on /dev/datastorage/lv_data01 is now 122880 blocks long.</span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># lvreduce -L 120M /dev/datastorage/lv_data01 </span></span><br><span class="line">  WARNING: Reducing active logical volume to 120.00 MiB.</span><br><span class="line">  THIS MAY DESTROY YOUR DATA (filesystem etc.)</span><br><span class="line">Do you really want to reduce datastorage/lv_data01? [y/n]: y</span><br><span class="line">  Size of logical volume datastorage/lv_data01 changed from 292.00 MiB (73 extents) to 120.00 MiB (30 extents).</span><br><span class="line">  Logical volume datastorage/lv_data01 successfully resized.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新挂载，并查看挂载状态</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># mount /dev/datastorage/lv_data01 /home/data01/</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfce086abf0d11068.jpg"></p><p><strong>相较于扩容逻辑卷，在对逻辑卷进行缩容操作时，其丢失数据的风险更大。所以在生产环境中执行相应操作时，一定要提前备份好数据。另外Linux系统规定，在对LVM逻辑卷进行缩容操作之前，要先检查文件系统的完整性（当然这也是为了保证我们的数据安全）。</strong></p><p><strong>Step5：创建快照卷，在创建之前我们往现有挂载目录写入一个测试文件。</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写入测试文件</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># echo "Welcome to kkutysllb.cn" &gt; /home/data01/redeme.txt</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># cat /home/data01/redeme.txt </span></span><br><span class="line">Welcome to kkutysllb.cn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建快照卷，使用-s 选项创建</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># lvcreate -L 120M -s -n lv_data01_snp /dev/datastorage/lv_data01</span></span><br><span class="line">  Logical volume <span class="string">"lv_data01_snp"</span> created.</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfce0b432b0b25609.jpg"></p><p>在逻辑卷lv_data01的挂载目录中创建一个100M的测试文件，可以看见快照卷的容量同步上升，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># dd if=/dev/zero of=/home/data01/test_files bs=1M count=100</span></span><br><span class="line">100+0 records <span class="keyword">in</span></span><br><span class="line">100+0 records out</span><br><span class="line">104857600 bytes (105 MB) copied, 1.12679 s, 93.1 MB/s</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfce0f2b64cb41781.jpg"></p><p><strong>LVM的“快照卷”功能类似于虚拟机软件的还原时间点功能。</strong>例如，可以对某一个逻辑卷设备做一次快照，如果日后发现数据被改错了，就可以利用之前做好的快照卷进行覆盖还原。LVM的快照卷功能有两个特点：</p><ul><li><strong>快照卷的容量必须等同于逻辑卷的容量；</strong></li><li><strong>快照卷仅一次有效，一旦执行还原操作后则会被立即自动删除。</strong></li></ul><p><strong>Step6：校验快照卷的恢复效果，记得先取消逻辑卷lv_data01的挂载点。</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取消挂载</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># umount /home/data01/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用快照卷lv_data01_snp对逻辑卷lv_data01进行还原</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># umount /home/data01/</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># lvconvert --merge /dev/datastorage/lv_data01_snp </span></span><br><span class="line">  Merging of volume datastorage/lv_data01_snp started.</span><br><span class="line">  datastorage/lv_data01: Merged: 30.56%</span><br><span class="line">  datastorage/lv_data01: Merged: 100.00%</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新挂载逻辑卷lv_data01</span></span><br><span class="line"></span><br><span class="line"> [root@c7-test01 ~]<span class="comment"># mount /dev/datastorage/lv_data01 /home/data01/</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/09/5cfce149ce3fb31332.jpg"></p><p><strong>使用快照卷还原后，原来创建的100M垃圾文件也就同步被清空了，同时快照卷只能使用一次，因此也会被系统回收。如下：</strong></p><p><img src="https://i.loli.net/2019/06/09/5cfce16a065f755843.jpg"></p><p><img src="https://i.loli.net/2019/06/09/5cfce1827171255801.jpg"></p><p><strong>Step7：删除逻辑卷，首先需要取消挂载点，然后按照逻辑卷—卷组—物理卷的顺序依次删除。</strong>代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取消挂载</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># umount /home/data01/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除逻辑卷lv_data01</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># lvremove /dev/datastorage/lv_data01 </span></span><br><span class="line">Do you really want to remove active logical volume datastorage/lv_data01? [y/n]: y</span><br><span class="line">  Logical volume <span class="string">"lv_data01"</span> successfully removed</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除卷组</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># vgremove datastorage </span></span><br><span class="line">  Volume group <span class="string">"datastorage"</span> successfully removed</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除物理卷</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># pvremove /dev/sdb /dev/sdc</span></span><br><span class="line">  Labels on physical volume <span class="string">"/dev/sdb"</span> successfully wiped.</span><br><span class="line">  Labels on physical volume <span class="string">"/dev/sdc"</span> successfully wiped.</span><br></pre></td></tr></table></figure><p>完成后，通过pvdisplay、vgdisplay、lvdisplay进行校验，如下：</p><p><img src="https://i.loli.net/2019/06/09/5cfce1bb34a0084086.jpg"></p><p><strong>至此，Linux原生的存储虚拟化介绍完毕，在后续KVM和OpenStack等开源技术实操时，还会用到Linux原生的存储虚拟化LVM，网络虚拟化vSwitch等实操，所以这里还需各位好好理解并掌握。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为了让大家更好理解存储虚拟化的特点，本文将讲解各个常用RAID技术方案的特性，并通过实际部署软RAID 10、RAID 5+备份盘等方案来更直观地体验RAID的效果，以便进一步了解生产环境对硬盘设备的IO读写速度和数据冗余备份机制的需求。同时，本文还将介绍LVM的部署、扩容、缩小、快照以及卸载删除的相关知识，以便让大家通过开源存储虚拟化基础对存储虚拟化的块级虚拟化和文件系统级虚拟化有个更深刻的理解。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-07-浅析5G-SBA服务网络架构</title>
    <link href="https://kkutysllb.cn/2019/06/07/2019-06-07-%E6%B5%85%E6%9E%905G-SBA%E6%9C%8D%E5%8A%A1%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/"/>
    <id>https://kkutysllb.cn/2019/06/07/2019-06-07-浅析5G-SBA服务网络架构/</id>
    <published>2019-06-06T23:16:40.000Z</published>
    <updated>2019-06-07T01:01:20.730Z</updated>
    
    <content type="html"><![CDATA[<h2 id="IMT-2020推进组提出的“三朵云“架构"><a href="#IMT-2020推进组提出的“三朵云“架构" class="headerlink" title="IMT-2020推进组提出的“三朵云“架构"></a>IMT-2020推进组提出的“三朵云“架构</h2><p>前文提出了5G网络架构是一种颠覆性的变化，之所以这么说，是从5G网络架构的实现方式来说的。由于“三朵云”概念的提出，导致5G的网络功能软件模块化实现，也就是我们常说的云化网络架构，其特征就是控制集中化、功能模块化和接口软件化。如下图所示：<a id="more"></a></p><p><img src="https://i.loli.net/2019/06/07/5cf99f183a7cc45564.jpg"></p><p>这种云化的变化针对现网架构来说其本质上是一种重构，通过控制云实现全网资源控制和能力开放，主要是各种核心网元的虚拟化实现，将传统网元的控制面功能抽象出来通过软件模块化封装，形成一个个网络控制功能模块。而传统网元的媒体转发面被一个个虚拟交换机、白盒交换机或是支持SDN流表的专有硬件替代，构成转发云，与控制云一起完成网络数据的路由控制和转发，并构成转控分离的架构，也就是俗称的C/U分离。而接入云内部无论是无线网络还是有线网络都是按照C/U分离的架构部署，同时将CDN和MEC等下沉到接入云控制面，一方面满足低时延的业务需求，同时与控制云配合实现端到端的切片网络。</p><p>为了实现上述云化网路的重构，3GPP推荐的电信云化网络的基础架构就是NFV，全球各大运营的选择也是NFV，NFV的全称就是网络功能虚拟化，通过NFV架构实现5G网络需求的SBA服务架构、网络服务的无状态设计、C/U分离和切片管理。如下图左边所示：</p><p><img src="https://i.loli.net/2019/06/07/5cf99f4286bc974621.jpg"></p><p>而NFV网络架构的提出并不是3GPP定义的，是ETSI定义的。ETSI作为NFV的发起标准组织，于2015年初发布了NFV参考架构，如上图右边所示。ETSI定义的NFV参考架构，与传统网络架构相比，进行了“三纵两横”的划分。纵向与传统网络类似，分为基础设施层、虚拟网元层和运营支撑层共3层，通过这三层实现网络功能虚拟化和网络服务的运营管理。而横向分为两个域：业务网络域和管理编排域。</p><p><strong>整个NFV架构的关键就是管理编排域MANO，MANO只是个逻辑功能，并没有实体。</strong>其本质上通过NFVO、VNFM和VIM通过一定的管理流程来创建各类虚拟网元，提供各类电信业务，从而实现对左边的业务网络域编排和管理。比如：我们需要扩容一个vMME网元，NFVO会将容量需求和业务接口需求等构建一个网元描述文件发送给VNFM，VNFM会根据这些需求生成资源清单并向NFVO申请资源，包括：需要多个虚拟机、划分为几组虚拟机组，每个虚机的CPU/内存/磁盘/IP/通信端口的规划、以及每个虚机要加载的操作系统镜像等。而NFVO接收申请后，会将资源清单发送给VIM，由VIM去筹备底层的虚拟化资源，完成后NFVO会通知VNFM资源准备好了，VNFM从而完成虚拟网元的实例化部署。</p><p>上述这种调度方式，是一种间接调度方式，资源的编排和控制统一由NFVO完成，VNFM不能直接对VIM发起资源编排需求，类似我们每个部门的领导一样要掌控一切信息。还有一种直接调度方式，就是由VNFM去直接控制VIM来完成资源的编排。从性能角度来看，直接模式更好，但是从部署落地以及后续演进角度来看，间接模式更好。目前，中国移动就是采用的这种方式。详见电信云落地若干问题一文。</p><p>而5G网络为了实现切片功能，必须在NFV架构基础上引入SDN，那么上图ETSI的NFV架构就演进成上图最右边部分，在NFVO层面引入SDNO，实现对SDNC的全局编排管理，而在VIM层面引入SDNC（主要是在OpenStack的neutron组件下挂接各厂家SDN控制器实现），实现对底层虚拟网元资源和物理网络资源的统一管理。由SDNO去编排SDNC，从而实现全网网络资源的灵活编排和部署，并实现各切片网络需求的网络隔离性。</p><h2 id="SBA服务化网络架构"><a href="#SBA服务化网络架构" class="headerlink" title="SBA服务化网络架构"></a>SBA服务化网络架构</h2><p>因此，从网络架构实现角度来说，5G网络架构相比传统2G/3G/4G架构而言，是一种颠覆性的的变化。但是，从网络功能来说，5G网络相比传统2G/3G/4G还是由一定集成性。因为，通信网络的各代演进从来都不是一种割裂的状态，而是一种螺旋式递进的规律，每一代通信网络都会继承前一代一些特征，并演变出一些的新的特征。</p><p>比如：2G到2.5G，无线接入网没有变化，核心网在电路域的基础上引入分区域，实现GPRS功能呢。到了3G，无线接入网变为NodeB+RNC的架构模式，核心网在电路域和分组域的基础上引入了IMS域。而到了4G网络时代，无线接入网少了RNC这一层，NodeB变胖了变成了eNodeB，集成了原NodeB、RNC和SGSN的部分功能，实现接入控制、移动性管理、路由控制和无线资源控制，同时各个eNodeB之间多了个X2接口，因此4G网络时代，无线接入网的变化可以总结为“少一层、多一口、胖基站”。而核心网层面电路域和IMS域没有变化，但是分组域为了学习IMS域业务-控制-转发分离的架构，也拆分为MME、S-GW和P-GW，同时引入QoS的集中控制功能PCRF。但是4G网络在分组域的这种变化并不彻底，S-GW和P-GW上还存在部分控制功能。</p><p><img src="https://i.loli.net/2019/06/07/5cf99fdd3086281355.jpg"></p><p>为了实现5G网络的功能模块化、无状态设计和C/U分离需求，3GPP在网络功能进行从新定义，将传统2G/3G/4G的网元重新定位网络功能模块，就是上图右边各种命名以F结尾的功能块。与4G网络功能相比，主要有9点变化，前6点针对4G网络功能拆分和整合，7,8,9三点是5G网络功能的新增。</p><p>AMF就相当于4G网络的MME，但是只负责用户接入控制和移动性管理，而MME的会话管理部分被拆分整合进SMF，此外SMF还整合原S-GW/P-GW的路由控制，地址分配和合法监听等控制功能。PCF与4G的PCRF相比没有多大变化，仍然负责QoS的控制，但是5G网络的QoS的是基于流的，这一点与4G网络基于承载的QoS相比，变化还是较大的。UDM与传统HSS网元类似，负责用户数据和签约信息的存储，但是UDM不具备传统HSS/HLR的鉴权功能，这一部分被拆分至AUSF功能中，其目的是为了便于互联网业务鉴权的整合，实现5G网络的统一鉴权功能。UPF就是4G网络的S-GW/P-GW的纯转发面，也就是用户面功能。</p><p>至于第七点变化的NEF功能，属于能力开放功能，便于与各类互联网和物联网应用对接和扩展。其实，在4G网络中也有能力开放，目前在杭研集中一点实现，主要实现了PCC和智能网的能力开放。而NSSF功能主要是5G网络中为了实现切片管理新增的功能，而<strong>NRF功能用于5G网络各种网元功能的注册管理，这也是实现SBA服务网络架构的关键。</strong></p><p><strong>基于SBA服务网络架构是中国移动提出的一种架构，借鉴了IT领域生产者和消费者的概念</strong>。所谓生产者就是只产生网络服务能力，至于谁去使用并不关心。而消费者就是使用网路服务能力去实现网络服务，至于网络服务能力由谁产生并不关心。</p><p>通过这种理念引进，整个网络功能实现松耦合，各个网络功能挂载一条总线上（上图右边画圈部分），各个网络功能既是生产者，也是消费者，不仅自身产生网络服务能力，同时也消费其他网络功能的能力，这种角色的转变就是通过NRF网络功能实现。各个网络功能都向NRF注册，将自己IP地址、域名和能力集等信息注册到NRF上，NRF统一监控总线上各类网络功能的变更，并及时更新已变更的信息。当某个网络功能需要与另一个网络功能通信时，会去NRF上查询对端的信息，NRF会将对端的IP、域名和能力集进行反馈。然后，该网络功能就能通过总线寻址到对端，从而完成通信。因此，这种服务化的架构不仅实现了网络功能松耦合，而且将原来架构中的固定转发路径变为点到点转发。<strong>NRF的作用类似现网DNS，VoLTE的ENS，用作全网路由寻址。</strong></p><p>还有一点变化就是在5G网络架构中，接入网是一种固移融合的组网形式，统一由AMF完成接入控制。同时，在5G网络用户的移动性附着与承载会话建立是全解耦的，也就是说用户在5G网络附着后，无需建立PDU会话承载，当有业务需求时再按需发起PDU会话承载的建立。而不像4G网络，用户附着网络必须建立一条默认PDN会话承载。这种改变一定程度上减少了全网资源消耗。</p><p><img src="https://i.loli.net/2019/06/07/5cf9a079f0d9e24978.jpg"></p><p>这种服务化架构在云原生的应用就如上图所示，每一朵云都是基于这种SBA的组网形式，不仅云内各服务松耦合，云与云之间也是一种松耦合状态，通过全网统一的编排和管理实现对接。目前，<strong>中国移动内部规划通过OSS4.0实现全网资源的统一编排和管理。</strong></p><h2 id="中国移动电信云-amp-5GC资源池部署演进策略"><a href="#中国移动电信云-amp-5GC资源池部署演进策略" class="headerlink" title="中国移动电信云&amp;5GC资源池部署演进策略"></a>中国移动电信云&amp;5GC资源池部署演进策略</h2><p>前面提到，5G网络架构的实现基于NFV/SDN的一种云化网络架构。因此，各大运营商在部署5G之前必须对传统网络进行一种云化改造。中国移动在2015年底就启动了NFV云化试点工作，经过4年多省市连续试点，目前已经解决大网云化改造中80%的问题。因此，在2019年中国移动启动电信云资源池的建设，全国分为8个大区（西北、西南、华北、华南、东北、华东1、华东2、中部），按照“三步走，两级演进”的策略，最终全网核心控制云8大区共部署10万台服务器，仅西北大区就需部署20000+服务器。如下图所示：</p><p><img src="https://i.loli.net/2019/06/07/5cf9b67d1101088422.jpg"></p><p>如上图所示，三步走策略中目前已基本完成第一步8大区电信云资源池的建设，预计在今年底8大区电信云进入商用阶段。在目前第一阶段中，首先完成大网核心网的云化改造，涉及IMS/EPC两大专业，并在今年完成各省市大网业务的上云割接，同时随着资源池规模的逐步加大，最终预计在2022年完成全网业务云化搬迁。</p><p>同时，在今年9月同步开启二阶段资源池建设，并且在一阶段电信云资源池的基础上引入5GC资源池，同步引入SDN技术。由于SDN的引入需要对已有资源池从虚层开始完成改造和重部署，考虑到已割接上云业务安全和网络稳定性，因此5GC资源池与一阶段电信云资源池采用<strong>“共址不共池”</strong>策略部署。</p><p>后续，在明年年中同步启用三阶段资源池扩容部署，主要目的是满足全网用户资源规模需求。并在明年年中同步开始一阶段4G云化核心网向5GC升级演进。</p><p>而所谓的“二级架构”是根据业务需求来逐步部署，总体原则是先“先核心后边缘”。在上述二阶段5GC资源池引入后，如果有30ms~10ms的业务需求，在各省省会城市部署边缘云资源池，主要完成CDN下沉、vBRAS改造、MEC部署以及试点CU池部署。如果有低于10ms业务需求，可考虑在地市/区县层面部署计算/存储一体化的站点计算域资源池，主要涉及C-RAN的CU/DU就近接入改造，从而满足低时延业务需求。该层面的资源池不部署管理节点，有省端边缘云统一纳管调度，同时各站点资源池之间不涉及跨池迁移特性。接入站点资源池可考虑三种部署模式“AZ、Cell和轻量化资源池”，目前在接入站点不考虑MEC需求时，统一采用AZ方式部署，如果后续部署MEC，考虑到MEC可能有运营管理需求，可考虑轻量化资源池方式部署。（AZ和Cell是OpenStack NOVA组件中的概念，因此需要大家具备OpenStack的一些基本知识，也可看我公众号的系列文章，公众号ID：CloudSupermanLLB）</p><p>而在8大区核心控制云资源池中，目前采用“二层解耦、逐步演进到三层解耦”的策略部署。如下图示所示：</p><p><img src="https://i.loli.net/2019/06/07/5cf9b6a1a3f9833513.jpg"></p><p>所谓二层解耦，就是硬件和软件解耦，目前8大区电信云资源池硬件采用中兴/浪潮服务器、华为/中兴TOR和EOR、迈普的管理TOR，西北大区除服务器为浪潮外，其余硬件配置与其他大区一致。除了硬件外，虚层VIM、虚拟网元层VNF/VNFM、编排层NFVO都是同厂商部署，由于软件层面目前因未开始正式集采招标，因此厂商还未确定。但是可以确定的是—<strong>“每个大区两个软件厂商”</strong>，由于电信云采用分布式存储，因此存储的招标与软件一起完成，目前也未确定。</p><p>二层解耦部署策略主要考虑集成交付快、不涉及不同厂商软件兼容性问题，因此问题较少，但是同样也会带来资源利用率不高的缺点。比如：厂商A的池内资源不够，即使厂商B池内由空闲资源也无法提供给厂商A使用，因为这里涉及虚层VIM、虚拟网元层VNF/VNFM、编排层NFVO和分布式存储等软件重新部署，网络改动量大，不利于大网稳定。同时，由于软件采用双厂商部署，也会导致运维人员必须掌握两个厂家软件机制、部署流程和运维管理流程等，对运维人员的要求较高。</p><p>而边缘资源池和接入站点也是采用精简化的两级部署架构，<strong>主要以业务需求来驱动。</strong>如下图所示：</p><p><img src="https://i.loli.net/2019/06/07/5cf9b6c3cca4469289.jpg"></p><p>VIM的集中管理部署在省端，采用3节点高可用部署策略，每个节点资源池可纳管不多余256个接入站点资源池，如果后续省内业务规模较大导致接入站点资源池扩容，省端的VIM集中管理节点可考虑按照3、5、7。。。等节点规划扩容（必须为奇数才能实现高可用策略）。同时，在接入站点资源池除MEC部署需求外，统一部署计算/存储一体化资源池，实现<strong>“VIM集中、AZ拉远”</strong>的部署要求，不仅利于节约投资，也方便划分省/地两端的维护界面。在接入站点，由于采用计算/存储一体化部署策略，因此存储层面必须采用分布式存储，每个计算服务器部署存储软件“机头”，每个资源池内所有服务器的硬盘（除系统盘外）构成OSD存储池，整体的管理调度由省端VIM层面部署集群控制软件来完成。详见本站分布式存储介绍文章。</p><p>按照这种“两级部署”策略，后续随着5G业务的发展，整体维护界面较清晰，同时也利于省端维护人员转型积极性的提升。整体5G电信云资源池的维护界面如下图所示：</p><p><img src="https://i.loli.net/2019/06/07/5cf9b6e4bdf5067337.jpg"></p><p>核心网控制云维护由大区负责，涉及从动环到NFVO各层面维护职责，主要全网资源监控、业务质量的分析，以及自动化手段的开发部署。随着业务需求发展，省/地两端负责边缘云资源池和接入站点的维护，同样涉及从动环到NFVO各层面维护职责。整体界面划分清晰。</p><p>在对运维人员转型要求方面主要按照NFV云化网络各层来进行专业划分，传统烟囱式的专业划分逐渐淡化和模糊化。基础设施运维层面主要负责硬件故障处理、Host OS和Hypervisor的运维管理，因此运维人员必须具备IT硬件及Linux运维实践的知识技能。业务层面的维护主要涉及传统CT网元运维管理、云管平台的运维管理，因此运维人员除掌握传统CT技能外，还需掌握云计算和虚拟化相关知识技能，并具备一定运维经验。在业务编排层面，除了要掌握各类开发语言，如：Linux shell、Python、yaml和xml等，还需具备下两层的运维经验，具备云化网络端到端拉通能力，否则开发的脚本在部署加载会引发各类不可预知的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;IMT-2020推进组提出的“三朵云“架构&quot;&gt;&lt;a href=&quot;#IMT-2020推进组提出的“三朵云“架构&quot; class=&quot;headerlink&quot; title=&quot;IMT-2020推进组提出的“三朵云“架构&quot;&gt;&lt;/a&gt;IMT-2020推进组提出的“三朵云“架构&lt;/h2&gt;&lt;p&gt;前文提出了5G网络架构是一种颠覆性的变化，之所以这么说，是从5G网络架构的实现方式来说的。由于“三朵云”概念的提出，导致5G的网络功能软件模块化实现，也就是我们常说的云化网络架构，其特征就是控制集中化、功能模块化和接口软件化。如下图所示：
    
    </summary>
    
      <category term="5G网络架构" scheme="https://kkutysllb.cn/categories/5G%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="5G" scheme="https://kkutysllb.cn/tags/5G/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-07-2G~5G网络架构的演进</title>
    <link href="https://kkutysllb.cn/2019/06/07/2019-06-07-2G-5G%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E8%BF%9B/"/>
    <id>https://kkutysllb.cn/2019/06/07/2019-06-07-2G-5G网络架构的演进/</id>
    <published>2019-06-06T23:01:09.000Z</published>
    <updated>2019-06-06T23:14:30.813Z</updated>
    
    <content type="html"><![CDATA[<p>5G将渗透到未来社会的各个领域，以用户为中心构建全方位的信息生态系统。面对极致的体验、效率和性能要求，以及“万物互联” 的愿景，5G的网络架构设计将面临极大挑战。相较于2G/3G/4G时代，5G的网络结构会发生颠覆性的变化。<a id="more"></a></p><p><img src="https://i.loli.net/2019/06/07/5cf99b7eacb7156407.jpg"></p><p>移动通信网络架构的演进包括两个方面，即<strong>无线接入网（RAN，Radio Access Network）</strong>的演进和<strong>核心网（CN，Core Network）</strong>的演进。</p><h2 id="2G-3G-4G网络架构的演变"><a href="#2G-3G-4G网络架构的演变" class="headerlink" title="2G/3G/4G网络架构的演变"></a>2G/3G/4G网络架构的演变</h2><p><strong>从GSM网络（2G）演进到GPRS网络（2.5G），最主要的变化是引入了分组交换业务。</strong>原有的GSM网络是基于电路交换技术，不具备支持分组交换业务的功能。因此，为了支持分组业务，在原有GSM 网络结构上增加 了几个功能实体，相当于在原有网络基础上叠加了一个小型网络，共同构成GPRS网络。</p><p><img src="https://i.loli.net/2019/06/07/5cf99bb2e030d13262.jpg"></p><p>在接入网方面，在<strong>BSC上增加了分组控制单元（PCU，Packet Control Unit）</strong>，用以提供分组交换通道；<strong>在核心网方面，增加了服务型GPRS支持节点（SGSN，Service GPRS Supported Node）</strong>和<strong>网关型GPRS支持节点（GGSN，Gateway GPRS Supported Node）</strong>，功能方面与MSC和GMSC一致，区别在于处理的是分组业务，外部网络接入IP网；从GPRS叠加网络结构开始，引入了两个概念。<strong>一个是电路交换域，一个是分组交换域，即CS域与PS域。</strong></p><p>到了3G时代，在速率方面有了质的飞跃，而在网络结构上，同样发生巨大变化。首先，就是空中接口的改变，以往网络结构中的<strong>Um接口变成Uu接口</strong>，而接入网和核心网的接口也换成了<strong>Iu接口</strong>，不再是A接口。在接入网方面，不再包含BTS和BSC，取而代之的是<strong>基站NodeB与无线网络控制器（RNC，Radio Network Controller）</strong>，功能方面与以往保持一致，<strong>核心网方面基本与原有网络共用，无太大区别。</strong></p><p><img src="https://i.loli.net/2019/06/07/5cf99bd204c3734559.jpg"></p><blockquote><p><strong>NodeB的功能：主要完成射频处理和基带处理两大类工作。</strong>射频处理主要包括发送或接收高频无线号，以及高频无线信号和基带信号的相互转换功能；基带处理主要包括信道编/译码、复用/解复用、扩频调制及解扩/解调功能。</p><p><strong>RNC的功能：主要负责控制和协调基站间配合工作，完成系统接入控制、承载控制、移动性管理、宏分集合并、无线资源管理等控制工作。</strong></p></blockquote><p>到4G时代，整个LTE网络从接 入网和核心网方面分为E-UTRAN和EPC。在接入网方面，网络扁平化，不再包含两种功能实体，整个网络只有一种基站eNodeB，它包含整个NodeB和部分RNC的功能，演进过程可以概括为<strong>“少一层，多一口，胖基站”</strong>。这样做降低了<strong>呼叫建立时延</strong>和<strong>用户数据传输时延</strong>，并且随着网络逻辑节点的减少，可以降低建设和维护成本，满足低时延、低复杂度和低成本的求。</p><p><img src="https://i.loli.net/2019/06/07/5cf99c5a308af45797.jpg"></p><blockquote><p><strong>“少一层”：</strong>4层组网架构变为3层，去掉了RNC，减少了基站和核心网之间信息交互的多节点开销，用户平面时延大大降低，系统复杂性降低。 </p><p><strong>“多一口”：</strong>以往无线基站之间是没有连接的，而eNodeB直接通过X2接口有线连接，实现无线侧IP化传输，使基站网元之间可以协调工作。eNodeB互连后，形成类似于“Mesh”的网络，避免某个基站成为孤点，这增强了网络的健壮性。 </p><p><strong>“胖基站”：</strong>eNodeB的功能由3G阶段的NodeB、RNC、SGSN、GGSN的部分功能演化而来，新增了系统接入控制、承载控制、移动性管理、无线资源管理、路由选择等。</p></blockquote><p>核心网侧也发生了重大变革。在GPRS/UMTS中，服务GPRS支持节点（SGSN）主要负责鉴权、移动性管理和路由选择，而网关GPRS支持节点（GGSN）负责IP地址分配、数据转发和计费。到了LTE时代，EPC（Evolved Packet Core）对之前的网络结构能够保持前向兼容，但自身结构方面不再有3G时的各种实体部分，主要由<strong>移动管理实体（MME，Mobile Management Entity）、服务网关S-GW和分组数据网关（P-GW）构成，外部网络只接入IP网。</strong>其中，<strong>MME主要负责移动性管理，包括承载的建立和释放、用户位置更新、鉴权、加密等</strong>，这些笼统地被称为控制面功能，而<strong>S-GW和P-GW更主要的是处理用户面的数据转发，但还保留内容过滤、数据监控与计费、接入控制以及合法监听等控制面功能。</strong>可以看到，从GPRS到EPC的演进中，有着相似的体系架构和接口，并朝着控制与转发分离的趋势演进，但这种分离并不彻底。比如MME相当于SGSN的控制面功能，S-GW则相当于SGSN的用户面。</p><p>此外， LTE核心网新增了一个网元<strong>PCRF</strong>（图中未画出），即<strong>策略与计费执行功能单元</strong>，可以实现对用户和业务态服务质量（QoS）进行控制，为用户提供差异化的服务，并且能为用户提供业务流承载资源保障以及流计费策略。</p><h2 id="5G架构设计需求分析"><a href="#5G架构设计需求分析" class="headerlink" title="5G架构设计需求分析"></a>5G架构设计需求分析</h2><p>5G的架构设计主要需要满足<strong>关键性能需求</strong>和<strong>网络运营需求</strong>。3GPP定义了5G应用的三大场景：<strong>eMBB、mMTC</strong>和<strong>uRLLC。</strong></p><p><img src="https://i.loli.net/2019/06/07/5cf99ca6bdf6996202.jpg"></p><p><strong>对于eMBB场景，又可进一步细分为连续广覆盖场景</strong>和<strong>热点高容量场景。</strong>在连续广覆盖场景下，要随时随地提供100Mbps ~1Gbps的高体验速率，并支持在高速移动如500km/h过程中的基本服务能力和业务的连续性。在LTE网络结构中，基站间虽然可以通过X2接口实现南北向数据交互，但无线资源管理、移动性管理和站间协同能力较弱。 另外，4G主要是通过核心网实现多种无线接入的统一控制，不同的接入技术在无线侧控制面各异，无法统一，互操作复杂。在热点高容量场景下，核心网网关部署的实际位置较高，且数据转发模式单一，回传网络的容量压力较大。</p><p><strong>对于mMTC场景，</strong>当海量的5G差异化的物联网终端接入时，<strong>由于LTE采用的是与移动互联网场景相同的单一移动性和连接管理机制，承载物联网少量数据仍需消耗较大的基于隧道的GTP-C报头开销，</strong> 不仅效率低，还极有可能造成信令拥堵。</p><p><strong>对于uRLLC场景，现有网络架构的控制面功能逻辑上分布在MME/SAE-GW等多个网元中，</strong>无法实现集中控制，同一网络控制功能可能需要多个网元通过接口协议协商完成，<strong>端到端通信时需经历较长的传输时延，且可能由于某些原因存在路由迂回现象。</strong>这既无法满足5G高可靠性前提下的低时延要求（现网端到端时延与 5G的时延要求约存在两个数量级的差距），又无法满足特定业务如车联网的安全性要求。</p><p>而网络运营方面的需求主要是运营商在部署新网络时，一般都会考虑建设和运营的可行性，便利性。因此，对5G网络提出了<strong>灵活部署、覆盖与容量兼容、精细化控制、能力开放</strong>和<strong>异构兼容</strong>等需求，这些需求在当前LTE网络只能实现很少一部分。因此， 5G网络架构的设计需要满足<strong>转控分离、集中控制、分布式部署、资源池化</strong>和<strong>服务模块化</strong>等要求。</p><p><img src="https://i.loli.net/2019/06/07/5cf99ccb8141160843.jpg"></p><p><strong>综上，对于5G接入网，要设计一个满足多场景的以用户为中心的多层异构网络，以支持宏微结合，统一容纳多种接入技术，提升小区边缘协同处理效率，提高无线和回传资源利用率。对于5G核心网的设计，一方面要将转发功能进一步简化和下沉，将业务存储和计算能力从网络中心下移至网络边缘，以支持高流量和低时延业务要求，以及灵活均衡的流量负载调度功能；另一方面通过能力开放、资源编排、集中控制等更高效地支持差异化的业务需求。</strong></p><h2 id="5G网络架构的提出"><a href="#5G网络架构的提出" class="headerlink" title="5G网络架构的提出"></a><strong>5G网络架构的提出</strong></h2><p>在5G逻辑架构的设计上，业界遵循<strong>先继承、后创新</strong>的思路，参照现有成熟的LTE网络架构，<strong>引入SDN和NFV</strong>等关键技术对网络功能进行解析和重构，以逐步适应5G架构的演进需要。</p><p><img src="https://i.loli.net/2019/06/07/5cf99d251f94e28923.jpg"></p><p>为了便于大家理解，我们从LTE的网络结构分析入手，逐步引出5G的网络逻辑架构演进思路。我们将LTE非漫游网络架构从逻辑上划分为3个部分，如下图所示。 第1部分是LTE为了满足网络的后向兼容性所引入的，在此不做赘述。第2部分是接入网，接入网（空口）的演进几乎是历代移动通信网络架构演进中最为关键的部分。第3部分是核心网，是一个并不彻底的转控分离架构，且是5G时代优先需要重构的重点。因此，我们优先聚焦该部分的重构。</p><p><img src="https://i.loli.net/2019/06/07/5cf99d4d0f27a27468.jpg"></p><p><strong>Step1：</strong>为了解决控制与转发分离不彻底的问题，需要首先对兼具控制和转发功能的网关进行解耦。LTE架构中S-GW和P-GW实际上是物理网元功能合一的，在逻辑上我们可以将其视为统一的SAE-GW，然后引入SDN技术进行网络功能解耦，<strong>用户面功能由新定义的网元UPF承载</strong>，<strong>控制面功能则交由新网元SMF进行统一管理</strong>。相应地，将原本已是纯控制面网元的<strong>MME、HSS和PCRF分别定义为AMF、UDM和PCF</strong>，但对网元的实际功能只做微小的变更或整合。核心网第一步重构后的网络逻辑架构如下图所示。</p><p><img src="https://i.loli.net/2019/06/07/5cf99d6c1fb2681576.jpg"></p><p><strong>Step2：</strong>为了满足网络资源充分灵活共享的需求，实现基于实际业务需求的网络自动部署、弹性伸缩、故障隔离和自愈等，需要引入NFV对网络功能进行虚拟化。因此，我们<strong>定义新网元NF以适应新的需要。考虑到NF面向的是用户差异化的服务，我们将其置于网元PCF，并定义新的接口以便NF能够按需获取PCF的策略控制等参数。</strong>核心网第二步重构后的网络逻辑架构如下图所示。</p><p><img src="https://i.loli.net/2019/06/07/5cf99dcd9eff240196.jpg"></p><p><strong>Step3：需要将业务平台下沉到网络边缘，为用户就近提供业务计算和数据缓存能力，实现网络从接入管道向信息化服务使能平台的关键跨越。</strong>因此，需要<strong>增强UDM的功能，以承担业务平台下沉后相应的数据管理工作。</strong>同时，<strong>引入网元AUSF承担数据访问的鉴权和授权工作。</strong>此外，考虑到5G面向的是极端差异化的业务场景，传统的“竖井式”单一网络体系架构无法满足多种业务的不同QoS保障需求，还需<strong>引入网元NSSF以实现网络切片选择的功能</strong>，使网络本身具备弹性和灵活扩展的能力。核心网第三步重构后的网路逻辑架构如下图所示。</p><p><img src="https://i.loli.net/2019/06/07/5cf99de61cb1061772.jpg"></p><p><strong>Step4：接入网侧的网元编排。</strong>从1G到4G，无线通信系统经历了迅猛的发展，现实网络逐步形成了包含多种无线制式、频谱利用和覆盖范围的复杂现状。在5G时代，同一运营商将面临多张不同制式网络长期共存的局面。因此，多网络融合也将成为5G网络架构设计不可规避的因素。对此，需要改变原有网络单一的eNodeB接入形式，对接入网侧做进一步的优化和增强。<strong>需要重新定义新的网元为（R）AN，以表示接入侧不再是单一的无线接入，而是固移融合。</strong>第四步重构后的网络逻辑架构图如下图所示。</p><p><img src="https://i.loli.net/2019/06/07/5cf99e04f205e50100.jpg"></p><p><strong>Step5：</strong>完成各个网元之间逻辑接口的定义，终端UE与AMF实体之间的<strong>N1接口</strong>是新定义的空口，使得低时延、高可靠、超密连接等愿景具备落地的可能。同时，原来的NF被定义为运营商授信、自行部署的<strong>应用功能单元AF</strong>。</p><p><img src="https://i.loli.net/2019/06/07/5cf99e1c243c042206.jpg"></p><p>通过五步重构，我们就得到了3GPP所确定的5G网络架构，这是一个点到点网络架构，主要包含以下网元功能：</p><p><strong>AMF：接入和移动性管理功能单元，负责控制面的注册和连接管理、移动性管理、信令合法监听以及上下文的安全性管理。</strong>相比4G的MME，AMF将<strong>漫游控制、承载管理</strong>以及<strong>网关选择</strong>等功能剥离，是一个“瘦身版”的MME。</p><p><strong>SMF：会话管理功能单元，相当于是4G的MME和SAE-GW控制面整合后集中控制单元，主要负责会话管理，包括会话建立、变更和释放，以及AN节点和UPF间的承载维持。</strong>SMF同时也继承了4G MME的<strong>漫游控制功能、UPF选择和控制功能</strong>和P-GW的<strong>UE-IP地址分配功能。</strong></p><p><strong>UPF：数据转发功能单元，保留了4G SAE-GW的数据转发功能，包括本地移动性锚点、包路由和转发、上下行传输级包标记、包过滤和用户面策略控制功能执行，</strong>相当于SDN架构中纯转发面，无自主控制权，只能执行来自SMF的控制指令。</p><p><strong>PCF：策略控制功能单元，负责制定统一的策略框架来管理网络行为。一方面结合自定义信息作出决策并强制控制面执行，另一方面也为前端提供连接用户数据库获取用户订阅信息的渠道。</strong>PCF与4G的PCRF功能几乎相同。</p><p><strong>UDM：统一数据管理单元，类似4G的HSS分为FE和BE两个功能实体，UDM同样也包含两个功能实体UDM和UDR。</strong>UDM类似FE属于应用前端接口单元，负责鉴权、授权、位置管理和订阅管理。UDR类似BE属于用户数据库单元，负责用户数据的存储、包括订阅表示、安全认证、移动性数据、会话数据等。与HSS相比，是一个增强版的HSS。</p><p><strong>AUSF：认证服务器功能单元，负责业务层面的认证和授权，相当于将4G时代业务服务器AS中对访问进行鉴权和授权的功能单独剥离出来。</strong>作为网络准入的裁决者，AUSF联合UDM对通过AMF来访的UE进行准入授权，认证通过的UE可以凭借AUSF授权的专用秘钥token实现数据的访问和获取。</p><p><strong>NSSF：网络切片选择功能单元，主要根据网络配置，为合法的UE选择可提供特定服务的网络切片示例。</strong>其机制是通过切片需求辅助信息的匹配，为UE选择一个或一组特定的AMF提供网络服务。需要注意一点，<strong>NSSF只是完成核心网层面的切片选择，至于端到端的切片建立，是通过NSSF选择的AMF来实现接入网切片的确认。</strong></p><p><strong>AF：应用功能单元，通过与核心网交互对外提供专用服务。</strong>AF是运营商自行部署的授信的应用，可以直接访问网络的相关应用功能，<strong>无需经过其它外部接口。</strong></p><p>为契合IMT-2020推进组提出的“三朵云”5G网络架构，我们将上述重构后的网络架构进行平面的切割，如下图所示。</p><p><img src="https://i.loli.net/2019/06/07/5cf99e405847a72210.jpg"></p><p><strong>“三朵云”5G网络是一个可依业务场景灵活部署的融合网络。</strong> <strong>控制云</strong>完成全局的策略控制、会话管理、移动性管理、策略管理、信息管理等，并支持面向业务的网络能力开放，实现定制网络与服务， 满足不同新业务的差异化需求，扩展新的网络服务能力。<strong>接入云</strong>将支持用户在多种应用场景和业务需求下的智能无线接入，并实现多种无线接入技术的高效融合，无线组网可基于不同部署条件要求，进行灵活组网，并提供边缘计算能力。<strong>转发云</strong>配合接入云和控制云，实现业务汇聚转发功能，基于不同新业务的带宽和时延等需求，转发云在控制云的路径管理与资源调度下，实现eMBB、uRLLC和mMTC等不同业务数据流的高效转发与传输，保证业务端到端质量要求。“三朵云” 不可分割，协同配合，并且是基于SDN/NFV技术实现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;5G将渗透到未来社会的各个领域，以用户为中心构建全方位的信息生态系统。面对极致的体验、效率和性能要求，以及“万物互联” 的愿景，5G的网络架构设计将面临极大挑战。相较于2G/3G/4G时代，5G的网络结构会发生颠覆性的变化。
    
    </summary>
    
      <category term="5G网络架构" scheme="https://kkutysllb.cn/categories/5G%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="5G" scheme="https://kkutysllb.cn/tags/5G/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-07-华为分布式存储FusionStorage原理详解</title>
    <link href="https://kkutysllb.cn/2019/06/07/2019-06-07-%E5%8D%8E%E4%B8%BA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8FusionStorage%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/"/>
    <id>https://kkutysllb.cn/2019/06/07/2019-06-07-华为分布式存储FusionStorage原理详解/</id>
    <published>2019-06-06T22:46:10.000Z</published>
    <updated>2019-06-06T22:59:37.477Z</updated>
    
    <content type="html"><![CDATA[<h2 id="华为FusionStorage中的基础概念"><a href="#华为FusionStorage中的基础概念" class="headerlink" title="华为FusionStorage中的基础概念"></a>华为FusionStorage中的基础概念</h2><p>FusionStorage采用全分布式DHT架构，将<strong>所有元数据按规则分布在各存储节点，不存在跨节点的元数据访问</strong>，彻底避免了元数据瓶颈。该核心架构设计保证了FusionStorage系统相比分布式文件系统具备更大规模的线性扩展能力。<a id="more"></a></p><p><strong>所谓DHT架构，就是分布式哈希表，Distributed Hash Table</strong>。如下图所示，它是FusionStorage中指数据路由算法。</p><p><img src="https://i.loli.net/2019/06/07/5cf998494d5e771062.jpg"></p><p>DHT机制可以保证上层应用对数据的IO操作会均匀分布在不同服务器的不同硬盘上，不会出现局部的热点，实现全局复负载均衡。DHT采用的是开源对象存储swift中一致性哈希算法（详见OpenStack的swift介绍），每个存储节点负责存储一小部分数据，基于DHT实现整个系统的<strong>寻址</strong>和<strong>存储</strong>。</p><p>上图中，<strong>Partition</strong>代表了一块数据分区，在DHT环上映射为固定Hash段代表的数据区Pn（n=1,2,3…)。<strong>Key-Value键值对</strong>表示底层磁盘上的数据组织索引，每个Value代表一个块存储空间。当我们要查找某一个数据的存放位置时，通过key值计算散列值（哈希值），根据计算的散列值找到DHT换上某一个Partition，然后根据Partition与disk的映射关系找到对应的磁盘，再通过Value值查找磁盘上的具体数据。具体如下所示</p><p><img src="https://i.loli.net/2019/06/07/5cf99877a3e4e24277.jpg"></p><p><strong>Volume</strong>代表应用卷，也就是虚拟机能看见并挂载的虚拟磁盘，它本质上代表了应用看到的一个LBA连续编址。而Volume的LBA连续编址就是上述用于数据查找的key值。</p><p>在华为FusionStorage中还有一个基础概念就是<strong>资源池，它是由上述一组分区Partition构成的存储池。</strong>如下图所示，<strong>每个资源池对应一个DHT环。</strong></p><p><img src="https://i.loli.net/2019/06/07/5cf9989fd248f84381.jpg"></p><p>在上图中，由两个资源池，分别对应两个DHT环，左边三个硬盘对应DHT1的三个Partition分区（P1、P2和Px），右边三块硬盘对应DHT2的三个Partition分区（P1、P2和Py）。当虚拟机挂载上面的Volume1/2/3时，将对应的LBA散列后，计算的结果分别对用DHT1的P1/P2/Px；当虚拟机挂载上面的Volume10/11时，将对应的LBA散列后，计算的结果分别对应DHT2的P1/P2/Py。</p><p>华为FusionStorage的资源池类似于SAN存储中的RAID组概念，但是<strong>与RAID相比，其优点是主要有三点：一是条带宽度大</strong>，最大支持96块盘（2份拷贝），提供超大存储空间，而且系统自动将每个卷的数据块打散存储在不同服务器的不同硬盘上，冷热不均的数据会均匀分布在不同的服务器上，不会出现集中的热点，<strong>避免高I/O应用导致热点瓶颈；二是具有动态热备功能</strong>，资源池内所有硬盘都可用作资源池的热备盘；<strong>三是结构简单</strong>，通过资源池和Volume二层结构，使服务器直接看到Volume，消除了中间的LUN逻辑存储，提升I/O读写性能。</p><h2 id="FusionStorage的数据路由原理"><a href="#FusionStorage的数据路由原理" class="headerlink" title="FusionStorage的数据路由原理"></a>FusionStorage的数据路由原理</h2><p>如下所示，<strong>FusionStorage数据路由采取分层处理方式：1）VBS通过计算确定数据存放在哪个服务器的哪块硬盘上。2）OSD通过计算确定数据存放在硬盘的具体位置。</strong></p><p><img src="https://i.loli.net/2019/06/07/5cf998fe60e0e62718.jpg"></p><p><strong>Step1：</strong>系统初始化时，FusionStorage将哈希空间（0~2^32）划分为N等份，每一等份是1个分区（Partition），这N等份按照硬盘数量取整除并进行均分。例如：<strong>二副本场景下，系统N默认为3600</strong>，假设当前系统有36块硬盘，则每块硬盘承载100个分区。<strong>上述“分区-硬盘”的映射关系在系统初始化时会分配好，后续会随着系统中硬盘数量的变化会进行调整</strong>。该映射表所需要的空间很小，FusionStorage系统中的节点会在内存中保存该映射关系，用于进行快速路由。</p><p><strong>Step2：</strong>FusionStorage<strong>会对每个LUN在逻辑上按照1MB大小进行切片</strong>，例如1GB的LUN则会被切成1024*1MB分片。当应用侧访问FusionStorage时候，在SCSI命令中会带上LUN ID和LBA ID以及读写的数据内容，OS转发该消息到本节点VBS模块，<strong>VBS根据LUN ID和LBA ID组成一个key，该key会包含LBA ID对1MB的取整计算信息</strong>。通过DHT Hash计算出一个整数(范围在0~2^32内)，并落在指定Partition中；根据内存中记录的“<strong>分区-硬盘</strong>”映射关系确定具体硬盘，VBS将IO操作转发到该硬盘所属的OSD模块。</p><p><strong>Step3：</strong>每个OSD会管理一个硬盘，<strong>系统初始化时，OSD会按照1MB为单位对硬盘进行分片管理，并在硬盘的元数据管理区域记录每个1MB分片的分配信息</strong>。<strong>OSD接收到VBS发送的I/O操作后，根据key查找该数据在硬盘上的具体分片信息，获取数据后返回给VBS</strong>。从而完成整个数据路由过程。</p><p>比如：应用需要访问LUN1+LBA1地址起始的4KB长度的数据，首先构造key=LUN1+LBA1/1M，对该key进行HASH计算得到哈希值，并对N取模，得到partition号，根据内存中记录的“分区-硬盘“映射表可得知数据归属的硬盘。</p><p><strong>总结：</strong>VBS将操作系统的SCSI命令中的key提取出来（Key值的计算方式：Key=LUN ID+LBA ID/1MB），通过哈希运算，确定访问数据内容落到DHT环上的哪块Partition上，根据Partition和Disk的对应关系（系统初始化时形成），确定数据存放在那个服务器上的哪块盘上，再通过OSD通过计算确定数据存放在硬盘的具体位置并将获取的数据返回给VBS。</p><h2 id="FusionStorage的内部组件功能和相互关系"><a href="#FusionStorage的内部组件功能和相互关系" class="headerlink" title="FusionStorage的内部组件功能和相互关系"></a>FusionStorage的内部组件功能和相互关系</h2><h3 id="FusionStorage-VBS模块及处理流程"><a href="#FusionStorage-VBS模块及处理流程" class="headerlink" title="FusionStorage VBS模块及处理流程"></a>FusionStorage VBS模块及处理流程</h3><p>VBS模块作为FusionStorage系统存储功能的接入侧，负责完成两大类业务：<strong>一是卷和快照的管理功能；二是I/O的接入和处理。</strong>VBS模块内部如下图所示，主要由：<strong>VBM、VBP、CLIENT、DATANET、SCSI协议控制（SCSI Initiator和SCSI Target）</strong>和<strong>心跳控制模块HeartBeat</strong>组成。</p><p><img src="https://i.loli.net/2019/06/07/5cf9992d3f1b431081.jpg"></p><p><strong>VBM模块是VBS中的控制管理模块，主要负责完成卷和快照的管理功能。</strong>比如：创建卷、挂载卷、卸载卷、查询卷、删除卷、创建快照、删除快照、基于快照创建卷等。</p><p><strong>I/O数据流在VBS进程中需要经过三个模块的处理，分别为**</strong>SCSI<strong><strong>、</strong></strong>VBP<strong><strong>、</strong></strong>CLIENT<strong>**：</strong></p><p><strong>Step1：</strong>SCSI启动器模块SCSI Initiator负责从内核（VSC.KO）中将I/O引入VBS进程，SCSI目标模块SCSI Target接收到的I/O数据包是<strong>标准SCSI协议格式的I/O请求</strong>，通过<strong>SCSI协议的四元组</strong>（host_id/channel_id/target_id/lun_id）和该I/O数据包在块设备上的偏移地址<strong>offset</strong>，<strong>读写的数据长度len共三个参数标识符，唯一标识一个I/O数据包</strong>，SCSI目标模块SCSI Target将收到的I/O信息交给VBP(Virtual Block Process)模块。</p><p><strong>Step2：</strong>VBP内部将通用块格式的I/O数据包<strong>转换为FusionStorage内部Key-Value</strong>格式的I/O数据包下发给client，其中<strong>KEY的组成为</strong>：<strong>tree_id（4Byte）</strong>+<strong>block_no（4Byte）</strong>+ <strong>branch_id（2Byte）</strong>+<strong>snap_id（2Byte）</strong>，<strong>tree_id/branch_id/snap_id是FusionStorage内部对卷、快照的唯一标识</strong>；<strong>block_no是将卷按照1M的块划分，本次I/O落在哪一个1M块上的编号。（**</strong>block_no对1M取整后与tree_id/branch_id/snap_id求和就是key值，对1M取余后得到的余数就是offset<strong>**）</strong></p><p><strong>Step3：</strong>I/O数据包请求到达client模块后，<strong>client根据KEY中的tree_id/branch_id进行hash计算</strong>，<strong>确定本次I/O发给哪一个OSD进程处理</strong>，确定后将I/O通过DATANET模块发给对应的OSD处理。</p><p><strong>Step4：</strong>OSD完成I/O数据收取后在物理硬盘上执行真正的读写I/O操作，然后将操作结果逐层返回返回给内核VSC.KO模块。</p><p><strong><em>备注：卷和快照的通用属性信息（如卷大小、卷名等）及卷和快照在DSware系统内部的一些私有属性（如用于定位卷和快照数据在系统中存储位置的tree_id/branch_id/snap_id）保存在DSware内部的一个私有逻辑卷中，该卷我们就称为元数据卷。</em></strong></p><h3 id="FusionStorage-OSD模块及处理流程"><a href="#FusionStorage-OSD模块及处理流程" class="headerlink" title="FusionStorage OSD模块及处理流程"></a><strong>FusionStorage OSD模块及处理流程</strong></h3><p>FusionStorage存储池管理的每个物理磁盘对应一个OSD进程，OSD进程作为FusionStorage中读写磁盘I/O的执行进程，主要实现3大功能：一是磁盘的管理；二是I/O数据流的复制；三是I/O数据流的Cache处理。</p><p><img src="https://i.loli.net/2019/06/07/5cf99972273ce79632.jpg"></p><p>如上图所示，OSD进程是一种主备方式部署的进程，由MDC模块实时监控主备OSD进程的状态。当指定Partition所在的主OSD故障时，存储服务会实时自动切换到备OSD，保证了业务的连续性。OSD进程内部主要分为RSM、SNAP、CACHE、AIO和SMIO等子进程。各类子进程的作用的如下：</p><ul><li><strong>RSM：</strong>采用复制协议实现I/O数据流的复制。</li><li><strong>SNAP：</strong>实现卷与快照的I/O功能、磁盘空间的管理。</li><li><strong>CACHE：</strong>实现cache功能。</li><li><strong>AIO：</strong>实现异步I/O数据流下发到底层SMIO模块，并且通过调用SMIO接口来监控介质故障。</li><li><strong>SMIO：**</strong>下发I/O数据流到实际的物理介质、监控物理介质故障、获取磁盘信息。**</li></ul><p>每个OSD会管理一个硬盘，系统初始化时，OSD会按照1MB为单位对硬盘进行分片管理，并在硬盘的元数据管理区域记录每个1MB分片的分配信息。OSD接收到VBS发送的I/O数据流操作后，根据key查找该数据在硬盘上的具体分片信息，获取数据后返回给VBS，从而完成整个数据路由过程。</p><p>对于写请求，OSD根据<strong>分区-主磁盘-备磁盘1-备磁盘2</strong>映射表，通知各个备磁盘的OSD进行写操作，主与备OSD进程都完成写后返回VBS。<strong>（在多副本场景下，备OSD数据由主OSD数据同步写入，避免调用VBS进程占用计算节点资源）</strong></p><p><strong>在OSD内核部分还有有一个VDB子进程（上图中VDL，新版本已改为VDB），也就是Key-Value DB数据库。如下图所示：</strong></p><p><img src="https://i.loli.net/2019/06/07/5cf9999da7c9d70499.jpg"></p><p>磁盘的每一个1M空间都固定的分配给一个key，一定数量连续的key组成一个chunk。所谓的Chunk就是组成Partition的基本单位，一个Partition的存储空间由1个或多个Chunk构成。</p><h3 id="FusionStorage-MDC模块功能"><a href="#FusionStorage-MDC模块功能" class="headerlink" title="FusionStorage MDC模块功能"></a><strong>FusionStorage MDC模块功能</strong></h3><p><strong>MDC（Metadata Controller）是一个高可靠集群，通过HA(High Availability)机制保证整个系统的高可用性和高可靠性，如下图所示：</strong></p><p><img src="https://i.loli.net/2019/06/07/5cf999beef1c284728.jpg"></p><p>MDC模块部署在Zookeeper集群中，通过ZooKeeper集群，实现元数据（如Topology、OSD View、Partition View、VBS View等）的可靠保存。ZK盘的配置原则为：在32台服务器内，默认选择3个独立的ZK盘（HDD）；在32~128台服务器内，默认选择5个独立ZK盘（HDD）；在多于128台服务器，默认选择5个独立ZK盘（SSD）；当使用SSD卡做主存时，选择3个ZK分区。</p><p>### </p><p>MDC模块内部通过Partition分配算法，实现数据多份副本的RAID可靠性。并且，作为FusionStorage的主控制模块，通过与OSD、VBS间的消息交互，实现对OSD、VBS节点的状态变化的获取与通知。</p><p>通过与Agent间的消息交互，MDC实现系统的扩减容、状态查询、维护等。通过心跳检测机制，MDC模块完成对OSD、VBS的状态监控。</p><p><strong>Zookeeper(简称ZK) 分布式服务框架主要用来解决分布式应用中经常遇到的，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等，ZK主要工作包括三项：</strong></p><ul><li><strong>MDC主备管理：</strong> MDC采用一主两备部署模式；在MDC模块进程启动后，各个MDC进程会向ZK注册选主，先注册的为主MDC；运行过程中，ZK记录MDC主备信息，并通过心跳机制监控MDC主备健康状况，一旦主MDC进程故障，会触发MDC重新选主。</li><li><strong>数据存储：</strong>在MDC运行过程中，会生成各种控制视图信息，包括目标视图、中间视图、IO视图信息等，<strong>这些信息的保存、更新、查询、删除操作都通过ZK提供的接口实现</strong>。</li><li><strong>数据同步：</strong>数据更新到主ZK，由主ZK自动同步到两个备ZK，保证主备ZK数据实时同步。一旦ZK发生主备切换，业务不受影响</li></ul><p><strong>在FusionStorage典型部署中，为了保证系统可靠性，ZK采用一主两备部署模式，每个管理节点部署一个ZK进程，ZK主备管理由ZK内部机制保证。</strong></p><p><strong>MDC进程与ZK进程采用C/S模式，通过TCP协议通信</strong>；MDC可以连接到任意一个ZK服务器，并且维持TCP连接。如果这个TCP连接中断，MDC能够顺利切换到另一个ZK服务器。</p><h3 id="FusionStorage中的视图"><a href="#FusionStorage中的视图" class="headerlink" title="FusionStorage中的视图"></a>FusionStorage中的视图</h3><p>如下图所示，FusionStorage中有三种视图最为关键，分别是：<strong>OSD View、IO View和Partition View。</strong></p><p><img src="https://i.loli.net/2019/06/07/5cf999f73062d18561.jpg"></p><p><strong>其中，OSD View</strong>主要记录OSD进程的ID和状态；而<strong>IO View</strong>中记录了主Partition分区ID和对应的OSD节点之间的映射关系；<strong>Partition View</strong>中记录主备Partition与OSD的对应关系。<strong>IO View是Partition View的子集。</strong></p><p><strong>MDC通过心跳感知OSD的状态，</strong>OSD每秒上报给MDC特定的消息（比如：OSD容量等），当MDC连续在特定的时间内（比如：当前系统为5s）没有接收到OSD的心跳信息，则MDC认为该OSD已经出故障（<strong>比如：OSD进程消失或OSD跟MDC间网络中断等</strong>），MDC则会发送消息告知该OSD需要退出，MDC更新系统的OSD视图信息，并给每台OSD发送视图变更通知，OSD根据新收到的视图，来决定后续的操作对象。</p><p><strong>多副本复制取决于MDC的视图</strong>；两副本情况下，当client发送一个写请求到达该OSD的时候，<strong>该OSD将根据Partition视图的信息，将该写请求复制一份到该Partition的备OSD</strong>。多副本情况下，则会复制发送多个写请求到多个备OSD上。</p><h3 id="FusionStorage-主要模块交互关系"><a href="#FusionStorage-主要模块交互关系" class="headerlink" title="FusionStorage 主要模块交互关系"></a>FusionStorage 主要模块交互关系</h3><p><img src="https://i.loli.net/2019/06/07/5cf99a1ac589f26534.jpg"></p><p><strong>Step1：**</strong>系统启动时，MDC与ZK互动决定主MDC**。主MDC与其它MDC相互监控心跳，主MDC决定某MDC故障后接替者。其它MDC发现主MDC故障又与ZK互动升任主MDC。</p><p><strong>Step2：**</strong>OSD启动时向主MDC查询归属MDC,向归属MDC报告状态，归属MDC把状态变化发送给VBS。<strong>当归属MDC故障，主MDC指定一个MDC接管，</strong>最多两个池归属同一个MDC。**</p><p><strong>Step3：**</strong>VBS启动时查询主MDC，向主MDC注册（主MDC维护了一个活动VBS的列表，主MDC同步VBS列表到其它MDC，以便MDC能将OSD的状态变化通知到VBS），向MDC确认自己是否为leader<strong>；</strong>VBS从主MDC获取IO View<strong>，</strong>主VBS向OSD获取元数据，其它VBS向主VBS获取元数据**。</p><p>FusionStorage系统中会存在多个VBS进程，如果多个VBS同时操作元数据卷，会引起数据被写坏等问题。<strong>为避免该问题，FusionStorage系统对VBS引入了主备机制，只有主VBS可操作元数据卷，所有的备VBS不允许操作元数据卷</strong>，<strong>一套FusionStorage系统中只存在一个主VBS</strong>；VBS的主备角色由MDC进程确定，所有VBS通过和MDC间的心跳机制保证系统中不会出现双主的情况。</p><p><strong>只有主VBS能够操作元数据，所以备VBS收到的卷和快照管理类命令需要转发到主VBS处理，对于挂载、卸载等流程，主VBS完成元数据的操作后，还需要将命令转到目标VBS实现卷的挂载、卸载等操作。</strong></p><h2 id="FusionStorage的I-O读写流程"><a href="#FusionStorage的I-O读写流程" class="headerlink" title="FusionStorage的I/O读写流程"></a><strong>FusionStorage的I/O读写流程</strong></h2><h3 id="FusionStorage-Cache读机制"><a href="#FusionStorage-Cache读机制" class="headerlink" title="FusionStorage Cache读机制"></a><strong>FusionStorage Cache读机制</strong></h3><p>FusionStorage的<strong>读缓存采用分层机制</strong>，第一层为内存cache，内存cache采用LRU机制缓存数据。第二层为SSD cache，SSD cache采用热点读机制，系统会统计每个读取的数据，并统计热点访问子，当达到阈值时，系统会自动缓存数据到SSD中，同时会将长时间未被访问的数据移出SSD。FusionStorage预读机制，统计读数据的相关性，读取某块数据时自动将相关性高的块读出并缓存到SSD中。</p><p><img src="https://i.loli.net/2019/06/07/5cf99a60c14b049375.jpg"></p><p><strong>如上图所示，OSD收到VBS发送的读I/O操作的步骤处理：</strong></p><p><strong>Step 1：</strong>从<strong>“内存读cache”</strong>中查找是否存在所需I/O数据，存在则直接返回，并调整该I/O数据到“<strong>读cache”LRU队首</strong>，否则执行Step 2；</p><p><strong>Step 2：</strong>从<strong>“SSD的读cache”</strong>中查找是否存在所需I/O数据，存在则直接返回，并增加该I/O数据的热点访问因子，否则执行Step 3；</p><p><strong>Step 3：</strong>从<strong>“SSD的写cache”</strong>中查找是否存在所需I/O数据，存在则直接返回，并增加该I/O数据的热点访问因子；如果热点访问因子达到阈值，则会被缓存在<strong>“SSD的读cache”</strong>中。如果不存在，执行Step 4；</p><p><strong>Step 4：</strong>从硬盘中查找到所需I/O数据并返回，同时增加该I/O数据的热点访问因子，如果热点访问因子达到阈值，则会被缓存在<strong>“SSD的读cache”</strong>中；</p><p><strong>读修复：</strong>在读数据失败时，系统会判断错误类型，如果是磁盘扇区读取错误，系统会自动从其他节点保存的副本读取数据，然后重新写入该副本数据到硬盘扇区错误的节点，从而保证数据副本总数不减少和副本间的数据一致性。</p><h3 id="FusionStorage-Cache写机制"><a href="#FusionStorage-Cache写机制" class="headerlink" title="FusionStorage Cache写机制"></a>FusionStorage Cache写机制</h3><p>OSD在收到VBS发送的写I/O操作时，会将写I/O缓存在SSD cache后完成本节点写操作。OSD会周期性地将缓存在SSD cache中的写I/O数据批量写入到硬盘，写Cache有一个水位值，未到刷盘周期超过设定水位值也会将Cache中数据写入到硬盘中。</p><p><img src="https://i.loli.net/2019/06/07/5cf99a8a5198479125.jpg"></p><p><strong>FusionStorage支持将服务器部分内存用作读缓存，NVDIMM和SSD用作写缓存。并且支持大块直通，按缺省配置大于256KB的块直接落盘不写Cache，这个配置可以人为修改。</strong></p><h3 id="FusionStorage-读I-O流程"><a href="#FusionStorage-读I-O流程" class="headerlink" title="FusionStorage 读I/O流程"></a><strong>FusionStorage 读I/O流程</strong></h3><p><img src="https://i.loli.net/2019/06/07/5cf99aa4c463820797.jpg"></p><p><strong>Step1：</strong>APP下发读IO请求到OS，OS转发该IO请求到本服务器的VBS模块；VBS根据读I/O信息中的LUN和LBA信息，通过数据路由机制确定数据所在的Primary OSD；如果此时Primary OSD故障，VBS会选择secondary OSD读取所需数据。</p><p><strong>Step2：</strong>Primary OSD接收到读I/O请求后，按照Cache机制中的“<strong>Read cache机制</strong>”获取到读I/O所需数据，并返回读I/O成功给VBS。</p><h3 id="FusionStorage-写I-O流程"><a href="#FusionStorage-写I-O流程" class="headerlink" title="FusionStorage 写I/O流程"></a><strong>FusionStorage 写I/O流程</strong></h3><p><img src="https://i.loli.net/2019/06/07/5cf99aca30dfa92476.jpg"></p><p><strong>Step1：</strong>APP下发写I/O请求到OS，OS转发该I/O请求到本服务器的VBS模块；VBS根据写I/O信息中的LUN和LBA信息，通过数据路由机制确定数据所在的Primary OSD。</p><p><strong>Step2：</strong>Primary OSD接收到写I/O请求后，同时以同步方式写入到本服务器SSD cache以及数据副本所在其他服务器的secondary OSD，secondary OSD也会同步写入本服务器SSD cache。Primary OSD接收到两个都写成功后，返回写I/O成功给VBS；同时，SSD cache中的数据会异步刷入到硬盘。</p><p><strong>Step3：</strong>VBS返回写I/O成功，如果是3副本场景，primary OSD会同时同步写I/O操作到secondary OSD和third OSD。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;华为FusionStorage中的基础概念&quot;&gt;&lt;a href=&quot;#华为FusionStorage中的基础概念&quot; class=&quot;headerlink&quot; title=&quot;华为FusionStorage中的基础概念&quot;&gt;&lt;/a&gt;华为FusionStorage中的基础概念&lt;/h2&gt;&lt;p&gt;FusionStorage采用全分布式DHT架构，将&lt;strong&gt;所有元数据按规则分布在各存储节点，不存在跨节点的元数据访问&lt;/strong&gt;，彻底避免了元数据瓶颈。该核心架构设计保证了FusionStorage系统相比分布式文件系统具备更大规模的线性扩展能力。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-03-华为分布式存储FusionStorage概述</title>
    <link href="https://kkutysllb.cn/2019/06/03/2019-06-03-%E5%8D%8E%E4%B8%BA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8FusionStorage%E6%A6%82%E8%BF%B0/"/>
    <id>https://kkutysllb.cn/2019/06/03/2019-06-03-华为分布式存储FusionStorage概述/</id>
    <published>2019-06-03T02:54:41.000Z</published>
    <updated>2019-06-03T03:13:46.998Z</updated>
    
    <content type="html"><![CDATA[<p>分布式存储基于通用的x86服务器，通过分布式的存储软件来构建存储系统，开放丰富灵活的软件义 策略和接口，允许管理员和租户进行自动化的系统管理和资源调度发放。业界的分布式存储，有多种形态，包括<strong>分布式块存储、分布式文件存储、分布式对象存储</strong>。各大存储厂商都由自己的解决方案，而开源的代表的就是ceph。考虑到电信云NFV领域的应用特性和相关设备商分布，我们这里主要介绍华为的分布式存储系统FusionStorage。<a id="more"></a></p><h2 id="传统SAN架构"><a href="#传统SAN架构" class="headerlink" title="传统SAN架构"></a>传统SAN架构</h2><p>在传统的IT行业和电信云早期时候，计算和存储是分离的，计算由各种x86服务器组成计算集群，存储由各种SAN/NAS组成存储集群，这种分离架构虽然通过多个计算实例共享相同存储，在计算节点发生故障后，无须耗时的外存 数据迁移，即可快速在其他计算节点上恢复故障应用。但是，<strong>这种传统存储资源缺乏共享。</strong>因为，传统存储设备和资源往往由不同厂家提供，之间无法进行资源共享，数据中心看到的是一个个孤立的存储资源。如下图所示。</p><p><img src="https://i.loli.net/2019/06/03/5cf48c6cc124549810.jpg"></p><p>而且，SAN机头成为制约系统扩展的单点瓶颈。因为，随着计算集群规模的不断扩展，由于存储资源池集中式控制机头的存在，使得共享存储系统的<strong>扩展性、性能加速、可靠性</strong>受到制约。比如：采用多个SAN系统，单个SAN系统的机头最多由双控扩展到16控，不同的SAN系统独立管理无法组成集群。各计算节点只能依赖于集中式机头内的缓存实现I/O加速，但由于各节点cache有限，且不共享导致cache的I/O缓存加速只能达到GB级别。尽管SAN控制机头自身具有主备机制，但依然存在异常条下主备同时故障的可能性。</p><p><strong>在传统SAN存储系统中，一般采用集中式元数据管理方式</strong>，元数据中会记录所有LUN中不同偏移量的数据在硬盘中的分布，例如LUN1+LBA1地址起始的4KB长度的数据分布在第32块硬盘的LBA2上。<strong>每次I/O操作都需要去查询元数据服务，随着系统规模逐渐变大，元数据的容量也会越来越大，系统所能提供的并发操作能力将受限于元数据服务所在服务器的能力，元数据服务将会成为系统的性能瓶颈。</strong></p><h2 id="什么是Server-SAN"><a href="#什么是Server-SAN" class="headerlink" title="什么是Server SAN?"></a><strong>什么是Server SAN?</strong></h2><p>为了解决上述问题，业界针对大多数企业事务型IT应用而言，提出信息数据的“ 即时处理” 理念，通过计算和存储融合，<strong>引入scale-out存储机制</strong>，也就出现了<strong>Server SAN</strong>的概念。<strong>所谓Server SAN就是由多个独立服务器自带的存储组成一个存储资源池，同时融合了计算和存储资源</strong>。scale-out的存储架构示意图如下：</p><p><img src="https://i.loli.net/2019/06/03/5cf48c994721e73916.jpg"></p><p><strong>以数据中心级资源共享为例，</strong>一个数据中心内可以构建一个很大的存储资源池，如下图所示，满足数据中心内各类应用对存储容量，性能和可靠性的需求，实现资源共享和统一管理。</p><p><img src="https://i.loli.net/2019/06/03/5cf48cae5829479983.jpg"></p><p>通过在各个计算节点引入分布式控制器VBS，这是一种软件控制机头，线性扩展能力最大到4096个节点。而且，利用各个计算节点的cache组成分布式cache集群，满配情况下可达到TB级别的cache缓存能力。同时，各计算节点存储平面的存储网卡可采用专用IB接口卡，可以实现P2P的无阻赛传输网络，可达到100Gbps的数据传输能力。</p><p><strong>最重要的是，采用分布式软件存储方式属于软件定义存储SDS的范畴，通过将控制面数据抽象和解耦，可与厂商的专有存储硬件解耦，底层的专有存储设备变为通用设备，实现简单统一管理和线性扩展。</strong>以华为FusionStorage为例，由于其采用的DHT算法（分布式哈希表结构，后面会详细讲），不仅数据能够尽可能分布到所有的节点中，可以使得所有节点负载均衡，现有节点上的数据不需要做很大调整实现<strong>数据均衡性</strong>，而且当有新节点加入系统中，系统会重新做数据分配，数据迁移仅涉及新增节点，从而实现数据修改的<strong>单调性</strong>。</p><h2 id="分布式存储池的概念"><a href="#分布式存储池的概念" class="headerlink" title="分布式存储池的概念"></a>分布式存储池的概念</h2><p>分布式存储系统把所有服务器的本地硬盘组织成若干个资源池，基于资源池提供创建/删除应用卷（Volume）、创建/删除快照等接口，为上层软件提供卷设备功能。分布式存储系统资源池示意图如下，具有以下特点。</p><p><img src="https://i.loli.net/2019/06/03/5cf48cd17f44718707.jpg"></p><p>每块硬盘分为若干个数据分片<strong>（Partition）</strong>，每个Partition只属于一个资源池，Partition是数据多副本的基本单位，也就是说多个数据副本指的是多个Partition，如上图中的Pn色块。 系统自动保证多个数据副本尽可能分布在不同的服务器上（服务器数大于数据副本数时）。系统自动保证多个数据副本间 的数据强一致性（所谓强一致性是指无论读还是写I/O，都要进行数据一致性检查）。Partition中的数据以Key-Value的方式存储。对上层应用呈现并提供卷设备（Volume），没有LUN的概念，使用简单。系统自动保证每个硬盘上的主用Partition和备用Partition数量相当，避免出现集中的热点。所有硬盘都可以用做资源池的热备盘，单个资源池最大支持数百上千块硬盘。</p><p>分布式存储系统采用<strong>分布式集群控制技术</strong>和<strong>分布式Hash数据路由</strong>技术，提供分布式存储功能特性。这里的概念比价抽象，所以下面结合华为FusionStorage的具体情况进行介绍。</p><h2 id="华为FusionStorage分布式存储概述"><a href="#华为FusionStorage分布式存储概述" class="headerlink" title="华为FusionStorage分布式存储概述"></a>华为FusionStorage分布式存储概述</h2><p>FusionStorage通过创新的架构把分散的、低速的SATA/SAS机械硬盘组织成一个高效的类SAN存储池设备，提供比SAN设备更高的I/O，如下图所示：</p><p><img src="https://i.loli.net/2019/06/03/5cf48ced3ac0154311.jpg"></p><p>集群内各服务器节点的硬盘使用独立的I/O带宽，不存在独立存储系统中大量磁盘共享计算设备和存储设备之间有限带宽的问题。其将服务器部分内存用做读缓存，NVDIMM用做写缓存，数据缓存均匀分布到各个节点 上，所有服务器的缓存总容量远大于 采用外置独立存储的方案。即使采用大容量低成本的SATA硬盘，分布式存储系统仍然可以发挥很高的I/O性能，整体性能提升1～3倍，同时提供更大的有效容量。</p><p>由于其采用<strong>无状态的分布式软件机头</strong>，机头部署在各个服务器上，无集中式机头的性能瓶颈。单个服务器上软件机头只占用较少的CPU资源，提供比集中式机头更高的IOPS和吞吐量。例如：假设系统中有20台服务器需要访问FusionStorage提供的存储资源，每台服务器提供给存储平面的带宽为2<em>10Gb，我们在每台服务器中部署1个VBS模块（相当于在每台服务器中部署1个存储机头），20台服务器意味着可部署20个存储机头，所能获取到的总吞吐量最高可达20</em>2*10Gb=400Gb，随着集群规模的不断扩大，<strong>可以线性增加的存储机头，突破了传统的双控或者多控存储系统集中式机头的性能瓶颈。</strong></p><p>华为FusionStorage是一种软件定义存储SDS技术，将通用X86服务器的本地HDD、SSD等介质通过分布式技术组织成大规模存储资源池。同时，以开放API的方式对非虚拟化环境的上层应用和虚拟机提供工业界标准的SCSI和iSCSI接口。关键一点是，华为官方自己吹牛逼的的口号—-<strong>FusionStorage是目前唯一商用支持PB级数据吞吐/存储能力的Server SAN产品。</strong></p><p>华为FusionStorage主要应用于两种场景，如下图所示，而中国移动的电信云NFV领域的应用属于第一种场景。</p><p><img src="https://i.loli.net/2019/06/03/5cf48d0aa74ae86247.jpg"></p><p><strong>场景一：与主流云平台的集成，构建云资源池中的存储资源池。</strong>可以和各种云平台集成，如华为FusionSphere、VMware、开源Openstack等，按需分配存储资源。</p><p><strong>场景二：提供传统的存储应用，</strong>如各种业务应用(如SQL、Oracle RAC、Web、行业应用等等)。</p><p>因此，华为FusionStorage是一种<strong>开发兼容</strong>的系统，可以兼容各种主流数据库、各种主流虚拟化平台以及各种主流服务器。同时，<strong>支持虚拟化平台和数据库资源池融合部署</strong>，即共用一个数据中心内的FusionStorage存储资源池。<strong>华为FusionStorage的软硬件兼容性总结如下：</strong></p><p><img src="https://i.loli.net/2019/06/03/5cf48d26bece826299.jpg"></p><p><strong>硬件兼容性主要包括：</strong>服务器、各种HDD盘，SSD盘，PCIE SSD卡/盘以及各种RAID卡，以太网卡， Infiniband卡等。</p><p><strong>软件兼容性主要包括：</strong>各种虚拟化平台、各种操作系统以及各种数据库软件。</p><p>而且，FusionStorage支持使用SSD替代HDD作为高速存储设备，支持使用<strong>Infiniband网络</strong>替代GE/10GE网络提供更高的带宽，为对性能要求极高的大数据量实时处理场景提供完美的支持<strong>千万级IOPS</strong>。InfiniBand（直译为“无限带宽”技术，缩写为IB）是一个用于高性能计算的计算机网络通信标准，它具有极高的吞吐量和极低的延迟，用于计算机与计算机之间的数据互连。InfiniBand也用作服务器与存储系统之间的直接或交换互连，以及存储系统之间的互连。硬件接口图如下：</p><p><img src="https://i.loli.net/2019/06/03/5cf48d3c621c526479.jpg"></p><p><strong>inifiniband接口可以实现链路可聚合：</strong>大多数系统使用一个4X聚合。12X链路通常用于计算机集群和超级计算机互连，以及用于内部网络交换器连接。而且，<strong>InfiniBand也提供远程直接内存访问（RDMA）能力以降低CPU负载。除了板式连接，它还支持有源和无源铜缆（最多30米）和光缆（最多10公里）。通过Inifiniband Association可以指定CXP铜连接器系统，</strong>用于通过铜缆或有源光缆达到高达传输速率<strong>120Gbps</strong>的能力。</p><p><strong>FusionStorage的技术规格参数表如下：</strong></p><table><thead><tr><th><strong>集群指标</strong></th><th><strong>规格</strong></th><th><strong>卷规格指标</strong></th><th><strong>规格</strong></th></tr></thead><tbody><tr><td>单集群存储服务器数量</td><td>4,096个</td><td>集群最大卷数量</td><td>1,280,000个</td></tr><tr><td>单集群硬盘数量</td><td>49,152个</td><td>单资源池最大卷数量</td><td>65,000个</td></tr><tr><td>单集群支持的计算节点数量</td><td>10,240个</td><td>卷容量</td><td>64MB～256TB</td></tr><tr><td>单集群最大资源池数量</td><td>128个</td><td>卷最大共享主机数量</td><td>128个</td></tr><tr><td><strong>资源池规格指标</strong></td><td><strong>规格</strong></td><td>每个主机最大挂载卷数量</td><td>512个</td></tr><tr><td>单资源池的硬盘数量</td><td>两副本（HDD或SSD）：12个～96个三副本（HDD或SSD）：12个～2,048个</td><td>共享卷最大数量</td><td>20,000个</td></tr><tr><td>单资源池的存储服务器数量</td><td>两副本（HDD或SSD）：3个～16个三副本（HDD或SSD）：4个～256个</td><td>单个卷最大快照数量</td><td>无限制，快照总数不超过1,280,000个</td></tr><tr><td>单资源池的机柜数量</td><td>非跨机柜数据安全：1个～12个跨机柜数据安全：3个～12个</td><td>单个卷最大链接克隆数量</td><td>2,048个</td></tr><tr><td>单个服务器最多可划分的资源池数量</td><td>3个</td><td>同步复制卷的最大数量</td><td>4,096个</td></tr><tr><td><strong>iSCSI接口协议指标</strong></td><td><strong>规格</strong></td><td>同步复制卷的最大容量</td><td>2TB</td></tr><tr><td>iSCSI CHAP用户最大数量</td><td>1,024个</td><td>每个主机支持的同步复制卷的总容量</td><td>64TB</td></tr><tr><td>iSCSI卷最大扩容容量</td><td>256TB</td><td></td></tr></tbody></table><h2 id="FusionStorage逻辑架构"><a href="#FusionStorage逻辑架构" class="headerlink" title="FusionStorage逻辑架构"></a>FusionStorage逻辑架构</h2><p>在中国移动电信云NFV中，<strong>主要使用分布式块存储</strong>，华为FusionStorage Block的解决方案将通用X86存储服务器池化，建立大规模块存储资源池，提供标准的块存储数据访问接口（SCSI和iSCSI等）。支持各种虚拟化Hypervisor平台和各种业务应用(如SQL、Web、行业应用等等)，可以和各种云平台集成，如华为FusionSphere、VMware、开源Openstack等，按需分配存储资源。如下图所示：</p><p><img src="https://i.loli.net/2019/06/03/5cf48d675d71079948.jpg"></p><p>同时，FusionStorage Block通过及SSD做Cache，构建内存—缓存（SSD)—主存（HDD/SAS/SATA)三级存储等关键技术，将存储系统的性能和可靠性得到极大的提高。</p><p>如下图所示，华为FusionStorage的逻辑架构中，按照管理角色和代理角色分为FSM和FSA两类进程，在代理进程内部按照数据管理、I/O控制和数据读写进一步细分为MDC、VBS和OSD三个子进程，各类软件逻辑模块的功能解释如下：</p><p><img src="https://i.loli.net/2019/06/03/5cf48d7c334b591527.jpg"></p><p><strong>FSM（FusionStorage Manager）：</strong>FusionStorage管理模块，<strong>提供告警、监控、日志、配置</strong>等操作维护功能。一般情况下<strong>FSM通过管理虚机主备节点部署</strong>，不考虑资源利用率的场景下，也可以使用物理服务器主备部署。</p><p><strong>FSA（FusionStorage Agent）：</strong>代理进程，部署在各节点上，实现各节点与FSM通信。FSA包含<strong>MDC、VBS</strong>和<strong>OSD</strong>三类不同的子进程。根据系统不同配置要求，分别在不同的节点上启用不同的进程组合来完成特定的功能。比如：单纯存储节点只需要部署OSD子进程，用于构建虚拟存储池。计算和存储融合节点，不仅需要部署OSD进程，还需部署VBS进程，用于控制数据读写I/O。</p><p><strong>MDC（MetaData Controller）：</strong>负责整个分布式存储集群的的元数据控制，实现对<strong>分布式集群的状态控制</strong>，以及<strong>控制数据分布式规则</strong>、<strong>数据重建规则</strong>等。 MDC是整个分布式存储集群的核心，通过MDC模块才能找到数据的具体存储位置，知悉存储副本的数量，分布和大小。因此，MDC子进程必须采用高可用HA的方式部署，默认情况下，是部署在3个节点的<strong>ZK(Zookeeper)盘上</strong>，形成MDC集群。</p><p><strong>VBS（Virtual Block System）：</strong>虚拟块存储管理组件，负责<strong>卷元数据的管理</strong>，也就是我们俗称的”软件机头“。VBS子进程提供分布式集群接入点服务，使计算资源能够通过VBS访问分布式存储资源。VBS上存储的是虚拟机虚拟磁盘卷的元数据，用于描述虚拟卷在OSD的位置，分布和大小等。凡是融合计算资源的节点，<strong>每个节点上默认部署一个VBS进程，形成VBS集群</strong>，由于VBS从OSD读取数据时可以并发，所以节点上也可以通过部署多个VBS来提升IO性能。</p><p><strong>OSD（Object Storage Device）：</strong>对象存储设备服务子进程，执行具体的I/O操作。<strong>在每个存储节点服务器上可以部署多个OSD进程。一般情况下，存储服务器上的一块本地磁盘默认对应部署一个OSD进程</strong>。如果使用大容量SSD卡作主存，为了充分发挥SSD卡的性能，可以在1张SSD卡上部署多个OSD进程进行管理，例如2.4TB的SSD卡可以部署6个OSD进程，每个OSD进程负责管理400GB存储空间。</p><h2 id="FusionStorage部署方式"><a href="#FusionStorage部署方式" class="headerlink" title="FusionStorage部署方式"></a><strong>FusionStorage部署方式</strong></h2><p>在中国移动电信云NFV解决方案中，无论软件由哪家厂商提供，虚拟化Hypervisor都是采用KVM来实现，区别只是不同厂商在开源KVM中私有增强或加固。华为FusionStorage Block支持XEN/KVM之类的Linux开放体系的Hypervisor场景，包括华为基于XEN/KVM增强的FusionSphere UVP虚拟化平台，和非华为的XEN/KVM虚拟化平台。在XEN/KVM的虚拟化场景下，FusionStorage Block既支持计算存储融合的部署模式，又支持计算存储分离的部署模式。</p><p><strong>所谓融合部署，</strong>就是指计算与存储的融合部署方式，也就是说将应用或虚拟机与存储在集群范围内部署在同一台服务器上。如下图所示：</p><p><img src="https://i.loli.net/2019/06/03/5cf48dabce4eb19183.jpg"></p><p>融合部署对应到FusionStorage的逻辑架构中，就是指的是<strong>将VBS和OSD部署在同一台服务器中</strong>。这种部署方式提升了资源利用率，虽然一定意义上增加了计算节点服务器的CPU开销，但是通过分布式存储软件策略，可以满足数据就近读写，提升I/O转发性能，同时数据副本异地存放，满足数据存储可靠性。在电信云NFV中，<strong>对时延较敏感的业务控制数据应用虚机推荐采用融合部署的方式部署。</strong></p><p><strong>所谓分离部署，顾名思义就是将</strong>计算与存储的分开部署，也就是将业务虚拟机与存储部署在不同的服务器上。如下图所示：</p><p><img src="https://i.loli.net/2019/06/03/5cf48dc0b8e4d39512.jpg"></p><p>分离部署对应到FusionStorage的逻辑架构中，就是将VBS和OSD分别部署在不同的服务器中，其中存储节点服务器只部署OSD子进程，计算节点服务器只部署VBS子进程。计算节点通过存储网络完成存储节点上数据的I/O读写，<strong>一般高性能数据库应用因为需要对数据进行计算，推荐采用分离部署的方式</strong>。但是，这种部署方式在I/O转发性能略差于融合部署。采用分离部署时，<strong>存储节点服务器Host OS可以采用通用的Linux OS操作系统，这就为后续分布式存储解耦提供了基础。</strong></p><h2 id="FusionStorage数据可靠性"><a href="#FusionStorage数据可靠性" class="headerlink" title="FusionStorage数据可靠性"></a>FusionStorage数据可靠性</h2><h3 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a><strong>集群管理</strong></h3><p>华为分布式存储FusionStorage的分布式存储软件采用集群管理方式，规避单点故障，一个节点或者一块硬盘故障自动从集群内隔离出来，不影响整个系统业务的使用。集群内选举进程Leader，Leader负责数据存储逻辑的处理，当Leader出现故障，系统自动选举其他进程成为新的Leader。</p><p><img src="https://i.loli.net/2019/06/03/5cf48de4e92a242186.jpg"></p><h3 id="多数据副本"><a href="#多数据副本" class="headerlink" title="多数据副本"></a><strong>多数据副本</strong></h3><p>华为分布式存储FusionStorage中没有使用传统的RAID模式来保证数据的可靠性（实际采用的RAID2.0+技术），而是采用了多副本备份机制，即<strong>同一份数据可以复制保存多个副本</strong>。<strong>在数据存储前，对数据打散进行分片，分片后的数据按照一定的规则保存集群节点上。</strong>如下图所，FusionStorage采用数据多副本备份机制来保证数据的可靠性，即同一份数据可以复制保存为2~3个副本。</p><p><img src="https://i.loli.net/2019/06/03/5cf48dfbc9ce645690.jpg"></p><p>FusionStorage针对系统中的每个卷，默认按照<strong>1MB</strong>进行分片，分片后的数据按照DHT算法保存集群节点上。如上图所示，对于服务器Server1的 磁盘Disk1上的数据块P1，它的数据备份为服务器Server2的磁盘Disk2上P1’，P1和P1’构成了同一个数据块的两个副本。当P1所在的硬盘故障时，P1’可以继续提供存储服务。</p><p><strong>数据分片分配算法保证了主用副本和备用副本在不同服务器和不同硬盘上的均匀分布，换句话说，每块硬盘上的主用副本和备副本数量是均匀的。扩容节点或者故障减容节点时，数据恢复重建算法保证了重建后系统中各节点负载的均衡性。</strong></p><p>### </p><h3 id="数据一致性和弹性扩展"><a href="#数据一致性和弹性扩展" class="headerlink" title="数据一致性和弹性扩展"></a><strong>数据一致性和弹性扩展</strong></h3><p><strong>数据一致性的要求是：当应用程序成功写入一份数据时，后端的几个数据副本必然是一致的，当应用程序再次读取时，无论在哪个副本上读取，都是之前写入的数据，这种方式也是绝大部分应用程序希望的。保证多个数据副本之间的数据一致性是分布式存储系统的重要技术点。</strong>华为分布式存储FusionStorage采用<strong>强一致性复制技术确保各个数据副本的一致性，一个副本写入，多个副本读取。</strong> 如下图所示，</p><p><img src="https://i.loli.net/2019/06/03/5cf48e16e330d10346.jpg"></p><p>存储资源池支持多种安全级别，采用强一致性复制协议，冗余策略按需灵活配置、部署。当采用机柜级冗余时，能同时容忍2或3个机柜失效；当采用节点级冗余时，能同时容忍2或3个节点失效。</p><p>FusionStorage还支持Read Repair机制。Read Repair机制是指在读数据失败时，会判断错误类型，如果是磁盘扇区读取错误，可以通过从其他副本读取数据，然后重新写入该副本的方法进行恢复，从而保证数据副本总数不减少。</p><p>同时，FusionStorage的分布式架构具有良好的可扩展性，支持超大容量的存储。如下图所示，<strong>扩容存储节点后不需要做大量的数据搬迁，系统可快速达到负载均衡状态。</strong></p><p><img src="https://i.loli.net/2019/06/03/5cf48e2d58d2b67146.jpg"></p><p>FusionStorage支持灵活扩容方式，不仅支持计算节点、硬盘、存储节点单类性扩容，而且支持同时进行扩容。由于软件机头、存储带宽和Cache都均匀分布到各个节点上，系统IOPS、吞吐量和Cache随着节点扩容线性增加，理论上计算节点和存储节点越多，FusionStorage的性能越强。</p><h3 id="快速数据重建"><a href="#快速数据重建" class="headerlink" title="快速数据重建"></a><strong>快速数据重建</strong></h3><p>分布式存储系统内部需要具备强大的数据保护机制。数据存储时被分片打散到多个节点上，这些分片数据支持分布在不同的存储节点、不同的机柜之间，同时数据存储时采用多副本技术，数据会自动存多份，每一个分片的不同副本也被分散保存到不同的存储节点上。在硬件发生故障导致数据不一致时，分布式存储系统通过内部的自检机制，通过比较不同节点上的副本分片，自动发现数据故障。发现故障后启动数据修复机制，在后台修复数据。如下图所示为华为FusionStorage的数据重建过程示意图。</p><p><img src="https://i.loli.net/2019/06/03/5cf48e4357cce64262.jpg"></p><p>FusionStorage中的<strong>每个硬盘都保存了多个DHT分区（Partition）</strong>，这些分区的副本按照策略分散在系统中的其他节点。当FusionStorage检测到硬盘或者节点硬件发生故障时，自动在后台启动数据修复。</p><p>由于分区的副本被分散到多个不同的存储节点上，数据修复时，<strong>将会在不同的节点上同时启动数据重建，每个节点上只需重建一小部分数据，多个节点并行工作，有效避免单个节点重建大量数据所产生的性能瓶颈，对上层业务的影响做到最小化。</strong></p><p>### </p><h3 id="掉电保护"><a href="#掉电保护" class="headerlink" title="掉电保护"></a><strong>掉电保护</strong></h3><p>分布式存储系统运行过程中可能会出现服务器突然下电的情况，此时在内存中的元数据和写缓存数据会随着掉电而丢失，需要使用非易失存储介质来保存和恢复元数据和缓存数据。 华为FusionStorage支持的保电介质为NVDIMM内存条或SSD。如下图所示，程序运行过程中会把元数据和缓存数据写入保电介质中，节点异常掉电并重启后，系统自动恢复保电介质中的元数据和缓存数据。</p><p><img src="https://i.loli.net/2019/06/03/5cf48e5a8ca0983577.jpg"></p><p>部署FusionStorage时，要求每一台服务器配备NVDIMM内存条或SSD盘，服务器掉电时会把元数据和缓存数据写入NVDIMM的Flash或SSD盘中，上电后又会把Flash中的数据还原到内存中。FusionStorage能够自动识别出系统中的NVDIMM内存，并把需要保护的数据按照内部规则存放在NVDIMM中，用于提供掉电保护功能。</p><h2 id="FusionStorage的特性"><a href="#FusionStorage的特性" class="headerlink" title="FusionStorage的特性"></a>FusionStorage的特性</h2><p>FusionStorage分布式存储软件总体框架如下所示，由<strong>存储管理模块、存储接口层、存储服务层</strong>和<strong>存储引擎层</strong>4类组成。</p><p><img src="https://i.loli.net/2019/06/03/5cf48e853b2db26201.jpg"></p><h3 id="FusionStorage块存储功能-SCSI-iSCSI块接口"><a href="#FusionStorage块存储功能-SCSI-iSCSI块接口" class="headerlink" title="FusionStorage块存储功能 - SCSI/iSCSI块接口"></a>FusionStorage块存储功能 - SCSI/iSCSI块接口</h3><p>FusionStorage通过VBS以SCSI或iSCSI方式提供块接口。当采用SCSI方式时，安装VBS的物理服务器、FusionSphere或KVM等采用SCSI接口（带内方式）；当采用iSCSI方式时，安装VBS以外的虚拟机或主机提供存储访问，VMware、MS SQL Server集群采用iSCSI模式（带外方式）。</p><p><img src="https://i.loli.net/2019/06/03/5cf48e9f11eee84034.jpg"></p><p>使用SCSI存储接口时，支持<strong>快照、快照备份、链接克隆</strong>功能，iSCSI暂不支持前述特性。对于iSCSI协议的支持是通过VBS提供iSCSI Target，块存储使用方通过本机的Initiator与iSCSI Target联接来访问存储。FusionStorage支持以下两种安全访问的标准：<strong>CHAP身份验证</strong>和<strong>LUN MASKING给Host对Lun的访问进行授权。</strong></p><h3 id="FusionStorage精简配置功能"><a href="#FusionStorage精简配置功能" class="headerlink" title="FusionStorage精简配置功能"></a><strong>FusionStorage精简配置功能</strong></h3><p>相比传统方式分配物理存储资源，精简配置可显著提高存储空间利用率。FusionStorage天然支持自动精简配置，和传统SAN相比不会带来性能下降。</p><p><img src="https://i.loli.net/2019/06/03/5cf48eb9d887d74962.jpg"></p><p>当用户对卷进行写操作时，系统才分配实际物理空间，FusionStorage Block仅处理虚拟卷空间和实际物理空间之前的映射关系，对性能无影响。</p><h3 id="FusionStorage快照功能"><a href="#FusionStorage快照功能" class="headerlink" title="FusionStorage快照功能"></a>FusionStorage快照功能</h3><p>FusionStorage快照机制，将用户卷数据在某个时间点的状态保存下来，可用作导出数据、恢复数据之用。FusionStorage Block快照数据基于DHT机制，数据在存储时采用ROW（Redirect-On-Write）机制，快照不会引起原卷性能下降。比如：针对一块容量为2TB的硬盘，完全在内存中构建索引需要24MB空间，通过一次Hash查找即可判断有没有做过快照，以及最新快照的存储位置，效率很高。</p><p><img src="https://i.loli.net/2019/06/03/5cf48ed84814c85259.jpg"></p><ul><li><strong>无限次快照：</strong>快照元数据分布式存储，水平扩展，无集中式瓶颈，理论上可支持无限次快照。</li><li><strong>卷恢复速度快：</strong>无需数据搬迁，从快照恢复卷1S内完成（传统SAN在几小时级别）。</li></ul><h3 id="FusionStorage链接克隆功能"><a href="#FusionStorage链接克隆功能" class="headerlink" title="FusionStorage链接克隆功能"></a><strong>FusionStorage链接克隆功能</strong></h3><p>FusionStorage支持一个卷快照创建多个克隆卷，对克隆卷修改不影响原始快照和其它克隆卷。克隆卷继承普通卷所有功能，即克隆卷可支持创建快照、从快照恢复及作为母卷再次克隆操作。</p><p><img src="https://i.loli.net/2019/06/03/5cf48f00b449b43091.jpg"></p><ul><li><strong>支持批量进行虚拟机卷部署，在1秒时间内创建上百个虚拟机卷。</strong></li><li><strong>支持1:2048的链接克隆比，提升存储空间利用率。</strong></li></ul><h3 id="FusionStorage-Block卷迁移"><a href="#FusionStorage-Block卷迁移" class="headerlink" title="FusionStorage Block卷迁移"></a><strong>FusionStorage Block卷迁移</strong></h3><p>为了存储池之间的容量平衡，FusionStorage支持将容量满的池迁移到一个空闲的池。或者，为了改变卷的性能，FusionStorage支持卷在不同性能的池之间的迁移，从低性能的池向高性能的池迁移。这就是卷迁移使用的场景，在FusionStorage中，由于迁移过程中，源卷不能有写数据的操作，所以这种迁移属于<strong>冷迁移。</strong></p><p><img src="https://i.loli.net/2019/06/03/5cf48f259c14a69801.jpg"></p><p><strong>步骤：</strong></p><p>【创建目标卷】&gt;&gt;&gt;【卷数据复制】&gt;&gt;&gt;【删除源卷】&gt;&gt;&gt;【目标卷改名】&gt;&gt;&gt;【完成】</p><h3 id="FusionStorage-Block双活"><a href="#FusionStorage-Block双活" class="headerlink" title="FusionStorage Block双活"></a><strong>FusionStorage Block双活</strong></h3><p><img src="https://i.loli.net/2019/06/03/5cf48f45d4d4965748.jpg"></p><p>基于AB两个数据中心的两套FusionStorage Block集群构建双活容灾关系，基于两套FusionStorage的卷虚拟出一个双活卷，两数据中心业务的主机能同时进行读写服务。任意数据中心故障，数据零丢失，业务能迅速切换到另外一个站点运行，保证业务连续型。</p><p>在原有基础服务基础上，引入复制集群，按服务化的架构提供双活业务。支持物理部署和虚拟机部署，可以做到独立安装和升级，独立扩展，按卷粒度提供双活服务。且支持优先站点仲裁和第三方仲裁的双仲裁模式，故障自动倒换，无需人工介入。同时，能跟上层Oracle RAC、VMWare等应用形成端到端双活容灾解决方案。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分布式存储基于通用的x86服务器，通过分布式的存储软件来构建存储系统，开放丰富灵活的软件义 策略和接口，允许管理员和租户进行自动化的系统管理和资源调度发放。业界的分布式存储，有多种形态，包括&lt;strong&gt;分布式块存储、分布式文件存储、分布式对象存储&lt;/strong&gt;。各大存储厂商都由自己的解决方案，而开源的代表的就是ceph。考虑到电信云NFV领域的应用特性和相关设备商分布，我们这里主要介绍华为的分布式存储系统FusionStorage。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-05-31-PXE批量部署原理及kickstart工具实践</title>
    <link href="https://kkutysllb.cn/2019/05/31/2019-05-31-PXE%E6%89%B9%E9%87%8F%E9%83%A8%E7%BD%B2%E5%8E%9F%E7%90%86%E5%8F%8Akickstart%E5%B7%A5%E5%85%B7%E5%AE%9E%E8%B7%B5/"/>
    <id>https://kkutysllb.cn/2019/05/31/2019-05-31-PXE批量部署原理及kickstart工具实践/</id>
    <published>2019-05-31T02:10:17.000Z</published>
    <updated>2019-06-03T02:56:02.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PXE服务器简介"><a href="#PXE服务器简介" class="headerlink" title="PXE服务器简介"></a>PXE服务器简介</h2><p>PXE(preboot execute environment)是由Intel公司开发的最新技术，工作于C/S的网络模式，支持工作站通过网络从远端服务器下载映像，并由此支持来自网络的操作系统的启动过程，其启动过程中，终端要求服务器分配IP地址，再用TFTP（trivial file transfer protocol）或MTFTP(multicast trivial file transfer protocol)协议下载一个启动软件包到本机内存中并执行，由这个启动软件包完成终端基本软件设置，从而引导预先安装在服务器中的终端操作系统。<a id="more"></a></p><p>在Linux中操作系统有多种的安装方式：HDD、USB、CDROM、PXE及远程管理卡等。在系统运维中，经常要批量安装操作系统，一般的企业服务器数量都在几十、几百、几千、甚至上万台。这么多的机器，如果人工的一台一台去安装，那运维人员可能要把大部分时间都花费在了安装系统上，所以，<strong>一般都会建立一个PXE服务器，通过网络来批量部署系统。PXE部署的逻辑图如下图所示：</strong></p><p><img src="https://i.loli.net/2019/05/31/5cf08e302c1a824105.jpg"></p><h2 id="无人值守部署系统流程"><a href="#无人值守部署系统流程" class="headerlink" title="无人值守部署系统流程"></a>无人值守部署系统流程</h2><p><strong>Step1：部署PXE需要的环境。</strong>首先在pxe服务器端需要有一个DHCP服务器，需要有tftp服务器和一个文件服务器，其中文件服务器可以是ftp，http，nfs等文件服务器，如果服务器性能好或者流量不是太大，这些服务器完全可以放在一台服务器上面。pxe启动需要网卡支持这样的功能，现在的绝大部分的网卡已经支持。</p><p><strong>Step2：pex功能的客户端在主机开机启动项为网络启动</strong>，一般默认都此选项，如果没有可自行设置bios启动项。</p><p><strong>Step3：</strong>客户端开机之后进入网络启动，此时客户端没有IP地址需要发送广播报文（<strong>pxe网卡内置dhcp客户端程序</strong>），dhcp服务器响应客户端请求，分配给客户端相应的IP地址与掩码等信息。</p><p><strong>Step4：</strong> 客户端得到IP地址之后，与tftp通信，<strong>下载pxelinux.0，default文件</strong>，<strong>根据default指定的vmlinuz，initrd.img启动系统内核，并下载指定的ks.cfg文件。</strong></p><p><strong>Step5：根据ks.cfg文件去文件共享服务器（http/ftp/nfs）上面下载RPM包开始安装系统</strong>，注意此时的文件服务器是提供yum服务器的功能的。</p><h2 id="部署各功能服务器（整合在一套服务器上）"><a href="#部署各功能服务器（整合在一套服务器上）" class="headerlink" title="部署各功能服务器（整合在一套服务器上）"></a>部署各功能服务器（整合在一套服务器上）</h2><p>由于我们实验环境验证10台以内服务器批量部署，网络接口的流量不是很大，因此将所有功能服务器部署在一台虚拟机上。如果是实际生产环境，应根据部署规模估算网络接口流量，根据流量大小将上述各功能服务器部署在不同的物理服务器上，采用分布式批量部署的方式。</p><h3 id="tftp服务的安装"><a href="#tftp服务的安装" class="headerlink" title="tftp服务的安装"></a>tftp服务的安装</h3><p>tftp的服务器需要安装tftp-server包，<strong>tftp工作在udp 69号端口。</strong>启动服务稍有不同，<strong>在CentOS7需要启动tftpd.socket</strong> ，<strong>而在CentOS6的版本中需要保证服务开机启用，并且重新启动xinetd</strong>。因为，在CentOS7是将所有进程托管给systemd进程，只需启动tftp.socket，打开监听的端口套接字即可，而在CentOS6中则是将不常用的服务统一托管给了xinetd进程，由xinetd进程统一进行管理，所以重启xinetd即可。</p><p><strong>tftp-server默认没有配置文件，直接启用服务，就可以使用</strong>（当然可以手动建立，但是没有必要）。</p><p><strong>Step1：安装tftp-server服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum install -y tftp-server tftp xinetd</span></span><br></pre></td></tr></table></figure><p><strong>Step2：配置xinetd.conf文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/xinetd.d/tftp</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># description: The tftp server serves files using the trivial file transfer \</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># protocol.  The tftp protocol is often used to boot diskless \</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># workstations, download configuration files to network-aware printers, \</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># and to start the installation process for some operating systems.</span></span><br><span class="line"></span><br><span class="line">service tftp</span><br><span class="line">&#123;</span><br><span class="line">    socket_type     = dgram</span><br><span class="line">    protocol        = udp</span><br><span class="line">    <span class="built_in">wait</span>            = yes</span><br><span class="line">    user            = root</span><br><span class="line">    server          = /usr/sbin/in.tftpd</span><br><span class="line">    server_args     = -s /var/lib/tftpboot</span><br><span class="line">    <span class="built_in">disable</span>         = no     <span class="comment">#这里默认是yes，改成no                                                                                                                                                                </span></span><br><span class="line">    per_source      = 11</span><br><span class="line">    cps         = 100 2</span><br><span class="line">    flags           = IPv4</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Step3：启用tftp、tftp-server服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># systemctl enable tftp.socket &amp;&amp; systemctl start tftp.socket</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># systemctl enable tftp.service &amp;&amp; systemctl start tftp.service</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># systemctl enable xinetd &amp;&amp; systemctl start xinetd</span></span><br></pre></td></tr></table></figure><p>启动完成后，进行服务启动检验，提示active就表示服务启用成功.</p><p><strong>Step4：tftp客户端测试</strong></p><p>由于本机同时是Server端，也是Client端（参见上面第一步安装的软件包tftp），所以本机不需要再次安装tftp软件包，直接测试即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入tftp服务的根目录</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># cd /var/lib/tftpboot/</span></span><br><span class="line">[root@C7-Server01 tftpboot]<span class="comment"># pwd</span></span><br><span class="line">/var/lib/tftpboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个tftp的测试文件</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 tftpboot]<span class="comment"># touch tftp.test</span></span><br><span class="line">[root@C7-Server01 tftpboot]<span class="comment"># ls -l tftp.test </span></span><br><span class="line">-rw-r--r-- 1 root root 0 May 30 12:07 tftp.test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换到root用户家目录</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 tftpboot]<span class="comment"># cd</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># pwd</span></span><br><span class="line">/root</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始测试</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># tftp 192.168.101.3</span></span><br><span class="line">tftp&gt; get tftp.test</span><br><span class="line">tftp&gt; quit</span><br><span class="line"></span><br><span class="line"><span class="comment"># 校验文件是否下载成功</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ll tft*</span></span><br><span class="line">-rw-r--r-- 1 root root 0 May 30 12:09 tftp.test</span><br></pre></td></tr></table></figure><h3 id="DHCP服务器的安装"><a href="#DHCP服务器的安装" class="headerlink" title="DHCP服务器的安装"></a>DHCP服务器的安装</h3><p>DHCP使用udp的67号端口，使用ss -unl 可以查看到监听的67号端口。</p><p><strong>Step1：安装DHCP服务器包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum install -y dhcp</span></span><br></pre></td></tr></table></figure><p><strong>Step2：因为DHCP默认配置文件为空，如下图，官方建议复制配置示例文件进行替换。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># cp /usr/share/doc/dhcp-4.2.5/dhcpd.conf.example /etc/dhcp/dhcpd.conf</span></span><br></pre></td></tr></table></figure><p><strong>Step3：编辑配置文件</strong>，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim  /etc/dhcp/dhcpd.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#---------可用最简配置-------------------</span></span><br><span class="line">next-server 192.168.101.3;     <span class="comment">#tftp服务器地址</span></span><br><span class="line">filename <span class="string">"pxelinux.0"</span>;           <span class="comment">#启动文件</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">subnet 192.168.101.0 netmask 255.255.255.0 &#123;</span><br><span class="line">range 192.168.101.110 192.168.101.131;  <span class="comment">#ip地址池</span></span><br><span class="line">option routers 192.168.101.2;        <span class="comment">#网关</span></span><br><span class="line">option domain-name-servers 192.168.101.2; <span class="comment">#DNS                                                           </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Step4：启用dhcp服务，查询监听端口状态。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 etc]<span class="comment"># systemctl enable dhcpd &amp;&amp; systemctl start dhcpd</span></span><br><span class="line">[root@C7-Server01 etc]<span class="comment"># ss -nlu|grep 67</span></span><br><span class="line">UNCONN     0      0            *:67                       *:*</span><br></pre></td></tr></table></figure><p><strong>需要注意一点：如果是多网卡，默认监听eth0，指定DHCP监听eth1网卡方法如下</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/sysconfig/dhcpd  在这个文件中，不是/etc/dhcp/dhcpd.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Command line options here</span></span><br><span class="line"></span><br><span class="line">DHCPDARGS=eth1  <span class="comment"># 指定监听网卡</span></span><br></pre></td></tr></table></figure><h3 id="安装并配置HTTP服务器"><a href="#安装并配置HTTP服务器" class="headerlink" title="安装并配置HTTP服务器"></a><strong>安装并配置HTTP服务器</strong></h3><p>我们通过http远程安装，同样也可以配置为ftp、NFS方式安装。</p><p><strong>Step1：安装httpd软件包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS7 ~]<span class="comment"># yum -y install httpd</span></span><br></pre></td></tr></table></figure><p><strong>Step2：启动httpd服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS7 ~]<span class="comment"># systemctl enable httpd &amp;&amp; systemctl start httpd</span></span><br></pre></td></tr></table></figure><h3 id="ftp服务器安装配置"><a href="#ftp服务器安装配置" class="headerlink" title="ftp服务器安装配置"></a>ftp服务器安装配置</h3><p>ftp可以进行许多安全方面的配置，但是在做一个内网的服务没有必要做许多安全方面的配置，只需要保证能正常使用即可，ftp的默认文件共享路径为：/var/ftp/pub/，需要共享文件只需放在该目录即可，安装系统可以直接将光盘挂载至该共享目录的一个子目录即可。</p><p><strong>Step1：安装软件包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum install -y vsftpd</span></span><br></pre></td></tr></table></figure><p><strong>Step2：创建挂载目录</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># mkdir -p /var/ftp/pub/centos7/</span></span><br></pre></td></tr></table></figure><p><strong>Step3：启用ftp服务</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]# systemctl enable vsftpd &amp;&amp; systemctl start vsftpd</span><br></pre></td></tr></table></figure><h2 id="配置ks-cfg文件"><a href="#配置ks-cfg文件" class="headerlink" title="配置ks.cfg文件"></a>配置ks.cfg文件</h2><p>Anaconda是RedHat、CentOS、Fedora等Linux的安装管理程序。它可以提供文本、图形等安装管理方式，并支持Kickstart等脚本提供自动安装的功能。该程序的功能是把位于光盘或其他源上的数据包，根据设置安装到主机上。为实现该定制安装，它提供一个定制界面，可以实现交互式界面供用户选择配置（如选择语言，键盘，时区等信息）。</p><p>Anaconda支持的管理模式： <strong>Kickstart提供的自动化安装、 对一个RedHat实施upgrade和Rescuse模式对不能启动的系统进行故障排除。</strong>要进入安装步骤，需要先有一个引导程序引导启动一个特殊的Linux安装环境系统。引导有多种方式： （可用的安装方式：本地CDROM、硬盘驱动器、网络方式（NFS、FTP、HTTP）等等）</p><ol><li>基于网络方式的小型引导镜像，需要提供小型的引导镜像； </li><li>U盘引导，通过可引导存储介质中的小型引导镜像启动安装过程；  </li><li>基于PXE的网络安装方式，要提供PXE的完整安装环境； </li><li>其他bootloder引导（如GRUB）。</li></ol><p><strong>Step1：拷贝ks.cfg模板文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS7 ~]<span class="comment"># cp /root/anaconda-ks.cfg  /var/www/html/ks.cfg</span></span><br></pre></td></tr></table></figure><p><strong>Step2：编辑ks.cfg文件（以下是我自己常用的设置）</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS7 ~]<span class="comment"># cat /var/www/html/ks.cfg </span></span><br><span class="line"><span class="comment">#version=DEVEL</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># System authorization information</span></span><br><span class="line"></span><br><span class="line">auth --enableshadow --passalgo=sha512</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use Network installation media</span></span><br><span class="line"></span><br><span class="line">url --url=http://192.168.101.3/centos7/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Firewall configuration</span></span><br><span class="line"></span><br><span class="line">firewall --<span class="built_in">disable</span></span><br><span class="line">selinux --<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use graphical install</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># graphical</span></span><br><span class="line"></span><br><span class="line">text</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the Setup Agent on first boot</span></span><br><span class="line"></span><br><span class="line">firstboot --<span class="built_in">disable</span></span><br><span class="line">ignoredisk --only-use=sda</span><br><span class="line"></span><br><span class="line"><span class="comment"># Keyboard layouts</span></span><br><span class="line"></span><br><span class="line">keyboard --vckeymap=us --xlayouts=<span class="string">'us'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># System language</span></span><br><span class="line"></span><br><span class="line">lang en_US.UTF-8</span><br><span class="line"></span><br><span class="line"><span class="comment"># Network information</span></span><br><span class="line"></span><br><span class="line">network  --bootproto=dhcp --device=eth0 --onboot=yes</span><br><span class="line">network  --bootproto=dhcp --device=eth1 --onboot=no</span><br><span class="line">network  --bootproto=dhcp --device=eth2 --onboot=no</span><br><span class="line">network  --hostname=CentOS7</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reboot after installation</span></span><br><span class="line"></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># Root password</span></span><br><span class="line"></span><br><span class="line">rootpw --iscrypted <span class="variable">$6</span><span class="variable">$j</span>/GfiKGObTLP91Om<span class="variable">$02u</span>.tAD0W7dPLs1PIDWebq8AiIHCCZ16h.3unIzeKp75io2PKln.g3T.zlra.Jzd.1wAZ2xmwqnUk7kcbsbh.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># System services</span></span><br><span class="line"></span><br><span class="line">services --enabled=<span class="string">"chronyd"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># System timezone</span></span><br><span class="line"></span><br><span class="line">timezone Asia/Shanghai --isUtc</span><br><span class="line"></span><br><span class="line"><span class="comment"># System bootloader configuration</span></span><br><span class="line"></span><br><span class="line">bootloader --location=mbr --boot-drive=sda</span><br><span class="line"></span><br><span class="line"><span class="comment"># Partition clearing information</span></span><br><span class="line"></span><br><span class="line">clearpart --none --initlabel</span><br><span class="line"></span><br><span class="line"><span class="comment"># Disk partitioning information</span></span><br><span class="line"></span><br><span class="line">part /boot --fstype=<span class="string">"ext4"</span> --ondisk=sda --size=400</span><br><span class="line">part swap --fstype=<span class="string">"swap"</span> --ondisk=sda --size=8192</span><br><span class="line">part / --fstype=<span class="string">"ext4"</span> --ondisk=sda --size=1 --grow</span><br><span class="line"></span><br><span class="line">%packages</span><br><span class="line">@base</span><br><span class="line">@compat-libraries</span><br><span class="line">@debugging</span><br><span class="line">@development</span><br><span class="line">gcc</span><br><span class="line">glibc</span><br><span class="line">gcc-c++</span><br><span class="line">openssl-devel</span><br><span class="line">openssh</span><br><span class="line">tree</span><br><span class="line">nmap</span><br><span class="line">sysstat</span><br><span class="line">lrzsz</span><br><span class="line">vim</span><br><span class="line">ntpdate</span><br><span class="line">net-tools</span><br><span class="line">wget</span><br><span class="line">libffi-devel</span><br><span class="line">git</span><br><span class="line">chrony</span><br><span class="line">bridge-utils</span><br><span class="line">qemu-kvm</span><br><span class="line">libvirt</span><br><span class="line">virt-install</span><br><span class="line"></span><br><span class="line">%end</span><br><span class="line"></span><br><span class="line">%addon com_redhat_kdump --<span class="built_in">disable</span> --reserve-mb=<span class="string">'auto'</span></span><br><span class="line"></span><br><span class="line">%end</span><br></pre></td></tr></table></figure><p><strong>Step3：更改ks.cfg权限</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS7 ~]<span class="comment"># chmod +r /var/www/html/ks.cfg</span></span><br></pre></td></tr></table></figure><p><strong>最后生成的配置文件如下：</strong>其中以#开头的行是注释行，其它部分开头是%开头，%end结尾。%packages是系统要安装的包，@开头是软件包组，@^是环境包组开头，以-开头是排除在外的包名或组名，除非必须的依赖性包则会安装，否则不会安装。%pre，%post是脚本，%pre是在任何磁盘分区之前进行，%post是在系统安装之后进行的系统配置。</p><h2 id="复制内核文件"><a href="#复制内核文件" class="headerlink" title="复制内核文件"></a>复制内核文件</h2><p><img src="https://i.loli.net/2019/05/31/5cf090dae198e42736.jpg"></p><p>内核文件、虚拟根文件以及菜单文件，都是通过tftp服务来提供的，由于系统及版本的不同，对于一个比较复制集群来说，需要准备不同系统，不同版本的内核文件，initrd.img文件。菜单文件只需要一份即可。/var/lib/tftpboot/目录规划如下：</p><p><strong>Step1：挂载光驱，虚拟机方式一定要先在连接光驱打上钩</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS7 ~]<span class="comment"># ls -l /dev|grep cdrom</span></span><br><span class="line">lrwxrwxrwx. 1 root root           3 May 31 00:53 cdrom -&gt; sr0</span><br><span class="line">crw-rw----. 1 root cdrom    21,   1 May 31 00:53 sg1</span><br><span class="line">brw-rw----. 1 root cdrom    11,   0 May 31 00:53 sr0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建挂载目录</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># mkdir /var/www/html/centos7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于我的镜像文件已上传到linux中，所以通过以下方式挂载</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># mount -o loop /mydata/img/CentOS-7-x86_64-DVD-1804.iso /var/www/html/centos7/</span></span><br></pre></td></tr></table></figure><p><strong>Step2：将镜像中的启动文件COPY到tftp server的根目录中</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS7 ~]<span class="comment"># cp /var/www/html/centos7/images/pxeboot/&#123;vmlinuz,initrd.img&#125; /var/lib/tftpboot/</span></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># cp /usr/share/syslinux/menu.c32  /var/lib/tftpboot/</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>vmlinuz</strong>是可引导的、压缩的内核文件，是可执行的Linux内核。</p><p><strong>initrd</strong>是“initial ram disk”的简写，用来临时的引导硬件被vmlinuz接管并继续引导。</p></blockquote><p><strong>Step3：复制pxelinux.0文件到tftp目录下</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先安装syslinux，因为pxelinux.0文件由syslinux包提供</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># yum install -y syslinux</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/</span></span><br></pre></td></tr></table></figure><p><strong>Step4：编辑pxelinux.cfg菜单文件,即isolinux.cfg</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建pxelinux.cfg目录</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># mkdir /var/lib/tftpboot/pxelinux.cfg</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制菜单文件</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># cp /var/www/html/centos7/isolinux/isolinux.cfg /var/lib/tftpboot/pxelinux.cfg/default</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制启动信息文件，可以编辑</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># cp /var/www/html/centos7/isolinux/boot.msg /var/lib/tftpboot</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编辑default文件</span></span><br><span class="line"></span><br><span class="line">[root@CentOS7 ~]<span class="comment"># cat /var/lib/tftpboot/pxelinux.cfg/default </span></span><br><span class="line">default menu.c32</span><br><span class="line">timeout 600</span><br><span class="line"></span><br><span class="line">menu title CentOS Linux PXE Install</span><br><span class="line">label centos7</span><br><span class="line">    menu label ^Auto Install CentOS Linux 7</span><br><span class="line">    kernel vmlinuz</span><br><span class="line">    append initrd=initrd.img inst.repo=http://192.168.101.3/centos7 ks=http://192.168.101.3/ks.cfg net.ifnames=0 biosdevname=0 ksdevice=eth0</span><br></pre></td></tr></table></figure><h2 id="测试安装"><a href="#测试安装" class="headerlink" title="测试安装"></a>测试安装</h2><p><strong>在Vmware中建立一个空的虚拟机，设置开机启动为网卡启动，启动后出现如下图示，表示pxe安装配置成功。ps：我的系统装了两个版本的CentOS批量安装，大家可以参照上述方法自行添加第二、第三系统的配置。下一篇我们讲述cobbler自动化批量部署系统，与kickstart相比，配置稍简单点儿。但是cobbler是kickstart的升级版，所以理解并掌握cobblerl前必须掌握kickstart。</strong></p><p><img src="https://i.loli.net/2019/05/31/5cf0917d67e7f90362.jpg"></p><p><img src="https://i.loli.net/2019/05/31/5cf0918dd7dfa78629.jpg"></p><p><img src="https://i.loli.net/2019/05/31/5cf091a2367ac55875.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;PXE服务器简介&quot;&gt;&lt;a href=&quot;#PXE服务器简介&quot; class=&quot;headerlink&quot; title=&quot;PXE服务器简介&quot;&gt;&lt;/a&gt;PXE服务器简介&lt;/h2&gt;&lt;p&gt;PXE(preboot execute environment)是由Intel公司开发的最新技术，工作于C/S的网络模式，支持工作站通过网络从远端服务器下载映像，并由此支持来自网络的操作系统的启动过程，其启动过程中，终端要求服务器分配IP地址，再用TFTP（trivial file transfer protocol）或MTFTP(multicast trivial file transfer protocol)协议下载一个启动软件包到本机内存中并执行，由这个启动软件包完成终端基本软件设置，从而引导预先安装在服务器中的终端操作系统。
    
    </summary>
    
      <category term="自动化运维" scheme="https://kkutysllb.cn/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/"/>
    
    
      <category term="Linux" scheme="https://kkutysllb.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>2019-05-29-华为存储虚拟化解决方案</title>
    <link href="https://kkutysllb.cn/2019/05/29/2019-05-29-%E5%8D%8E%E4%B8%BA%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://kkutysllb.cn/2019/05/29/2019-05-29-华为存储虚拟化解决方案/</id>
    <published>2019-05-29T14:53:34.000Z</published>
    <updated>2019-05-29T15:16:11.145Z</updated>
    
    <content type="html"><![CDATA[<p>在前文《存储虚拟化概述》中，我们提到存储虚拟化与软件定义存储SDS的区别就是其控制平面数据和存储平面数据是一种紧耦合的关系，因此各厂商的存储虚拟化解决方案是一种“私有”方案，不仅不同厂商之间不能兼容，甚至通常不同产品之间有可能不兼容。因此，考虑到电信云NFV的应用场景，以及设备商的分布情况，我们这里主要讨论<strong>华为的虚拟化存储解决方案</strong>和<strong>华为的分布式存储Fusion Storage</strong>的情况。<a id="more"></a></p><h2 id="华为存储虚拟化的存储模型"><a href="#华为存储虚拟化的存储模型" class="headerlink" title="华为存储虚拟化的存储模型"></a>华为存储虚拟化的存储模型</h2><p>华为对存储虚拟化的定义：<strong>存储虚拟化是将存储设备抽象为数据存储，虚拟机在数据存储中作为一组文件存储在自己的目录中。而数据存储是逻辑容器，类似于文件系统，它将各个存储设备的特性隐藏起来，并提供一个统一的模型来存储虚拟机文件。同时，存储虚拟化技术可以更好的管理虚拟基础架构的存储资源，使系统大幅提升存储资源利用率和灵活性，提高应用的正常运行时间。</strong></p><p><img src="https://i.loli.net/2019/05/29/5cee9d428946a52708.jpg"></p><p><strong>能够封装为数据存储的存储单元包括：</strong></p><ul><li>SAN存储（包括iSCSI或光纤通道的SAN存储）上划分的LUN；</li><li>NAS存储上划分的文件系统共享目录；</li><li>华为分布式存储FusionStorage上的存储池；</li><li>主机的本地硬盘以及主机的本地内存盘；</li></ul><p><strong>数据存储可支持文件系统格式：</strong></p><ul><li>华为私有的虚拟镜像管理系统（VIMS），为存储虚拟机而优化的高性能文件系统，主机可以将虚拟镜像管理系统数据存储部署在任何基于SCSI的本地或联网存储设备上，包括光纤通道、以太网光纤通道和iSCSI SAN设备。</li><li>网络文件系统（NFS），NAS设备上的文件系统。FusionSphere支持NFS V3协议，可以访问位于NFS服务器上制定的NFS磁盘，挂载该磁盘并满足任何存储需求。</li><li>EXT4，FusionSphere支持服务器的本地磁盘虚拟化。</li></ul><p>从上面华为存储虚拟化的定义可以看出，主要涉及三个概念：<strong>存储资源、存储设备</strong>和<strong>数据存储。</strong>华为的这些基本概念定义的比较奇葩，从我个人理解来看是一种反人类的定义，如下图所示。按照正常人的理解，存储设备指的是真实物理存储，存储资源指的是抽象化后逻辑存储，但是它的定义正好是相反的。</p><p><img src="https://i.loli.net/2019/05/29/5cee9d89ef79033740.jpg"></p><p><strong>华为存储虚拟化模型中，存储概念的定义如下：</strong></p><p><strong>存储资源：</strong>表示物理存储设备，例如IPSAN、Advanced SAN、NAS等，其中，Advance SAN是不支持虚拟化的。一个存储资源可以包含多个物理存储。<strong>主机访问存储资源时，先需要添加存储资源，再选定主机关联存储资源。</strong>如下所示：</p><p><img src="https://i.loli.net/2019/05/29/5cee9dc14243863228.jpg"></p><p><strong>存储设备：</strong>表示存储资源中的管理单元，包括本地磁盘、LUN、 Advanced SAN存储池、Fusion Storage存储池、NAS共享目录等。多个存储设备可以归属在一个存储资源中，且<strong>存储设备必须被添加为数据存储才能使用</strong>。其中，<strong>LUN在使用前需要在存储侧或者交换机侧进行配置</strong>，该配置根据不同的厂家会不一样，具体需要参照相关厂家的技术文档。<strong>存储设备需要通过主机探测的方式进行扫描来发现：</strong>主机需要链接存储资源后才能扫描存储资源所包含的存储设备，而且每个主机都能发现各自的存储设备，也能发现共享的存储设备。如下所示：</p><p><img src="https://i.loli.net/2019/05/29/5cee9dfea5d6515965.jpg"></p><p><strong>数据存储：</strong>表示在存储设备上创建的逻辑管理单元，需要创建在指定的存储设备上，且<strong>一个存储设备只能创建一个数据存储，数据存储和存储设备一一对应，且数据存储大小依赖于存储设备的大小</strong>。数据存储和主机关联，为主机提供资源，数据存储可以关联到多个主机，一个主机也可以使用多个数据存储。数据存储承载了具体的虚拟机业务，例如创建磁盘等。对于SAN存储上的LUN，也可以作为数据存储直接供虚拟机使用，而不再创建虚拟磁盘，此过程称为<strong>裸设备映射</strong>，目前仅支持部分操作系统的虚拟机使用，用于搭建数据库服务器等对磁盘空间要求较大的场景。如果使用裸设备部署应用集群服务（如 Oracle RAC等），建议不要使用虚拟机的快照、快照恢复功能，快照恢复后，会导致应用集群服务异常。数据存储映射的示意图如下：</p><p><img src="https://i.loli.net/2019/05/29/5cee9e1f26a4b48820.jpg"></p><p>比如：裸设备映射方式可以将一个SAN设备【<strong>存储资源</strong>】分配的一个LUN【<strong>存储设备</strong>】接入到FusionCompute环境成为一个数据存储【<strong>数据存储</strong>】，可以在该数据存储上创建运行业务的虚拟机，对外提供服务。</p><p>在华为的FusionCompute中配置数据存储时，首先需要完成存储接口的设置，然后可以通过以下流程接入和使用存储资源，如下图所示：</p><p><img src="https://i.loli.net/2019/05/29/5cee9e3b7dc7412851.jpg"></p><ol><li>在FusionCompute界面上首先添加存储资源（如：IPSAN等等），并在存储设备上进行主机启动器的配置。</li><li>主机关联存储资源后，执行“<strong>扫描存储设备</strong>”动作，将IPSAN上的LUN扫描到主机。</li><li>主机选择存储设备，执行“<strong>添加数据存储</strong>”动作，并选择“<strong>虚拟化</strong>”，存储配置完成。</li><li>随后，可以在数据存储上可以进行创建卷、创建快照等行为。</li></ol><blockquote><p><strong>所谓存储接口，是指主机与存储设备连接所用的端口。</strong>可以将主机上的一个物理网卡，或者多个物理网卡的绑定设置为存储接口。当使用iSCSI存储时，一般使用主机上两个物理网卡与存储设备多个存储网卡相连，组成存储多路径，此时不需要绑定主机存储平面的物理网卡；当使用NAS存储时，为保证可靠性，建议将<strong>主机的存储平面网卡以主备模式进行绑定设置为存储接口</strong>与NAS设备连接。</p><p>同时，在存储接口设置还有个<strong>存储多路径</strong>的概念，主要指存储设备通过多条链路与主机一个或多个网卡连接，<strong>通过存储设备的控制器控制数据流的路径，实现数据流的负荷分担</strong>，保证存储设备与主机连接的可靠性。一般情况下，iSCSI存储和光纤通道存储（如IP SAN存储设备、FC SAN存储设备、OceanStor 18000系列存储）均支持存储多路径。而且在华为的解决方案中，存储多路径包含<strong>华为多路径</strong>与<strong>通用多路径</strong>两种模式，<strong>通用多路径下虚拟机采用裸设备映射的磁盘时，不支持Windows Server操作系统的虚拟机搭建MSCS集群。</strong></p></blockquote><h2 id="华为虚拟化存储的链接"><a href="#华为虚拟化存储的链接" class="headerlink" title="华为虚拟化存储的链接"></a>华为虚拟化存储的链接</h2><p><strong>FC-SAN：</strong>存储区域网络(Storage Area Networks，SAN)是一个用在服务器和存储资源之间的、专用的、高性能的网络体系。 <strong>SAN是独立于LAN的服务器后端存储专用网络</strong>。 SAN采用<strong>可扩展的网络拓扑结构</strong>连接服务器和存储设备，每个存储设备不隶属于任何一台服务器，所有的存储设备都可以在全部的网络服务器之间作为对等资源共享。SAN主要利用Fibre Channel protocol（光纤通道协议），通过FC交换机建立起与服务器和存储设备之间的直接连接，因此我们通常也称这种利用FC连接建立起来的SAN为FC-SAN。如下图所示，FC特别适合这项应用，原因在于一方面它可以传输大块数据，另一方面它能够实现较远距离传输。SAN主要应用在对于性能、冗余度和数据的可获得性都有很高的要求高端、企业级存储应用上。</p><p><img src="https://i.loli.net/2019/05/29/5cee9e85176dd28733.jpg"></p><p><strong>SAN架构中常用的三种协议：</strong></p><ul><li><strong>FC 协议 (Fibre Channel)</strong> ，使用该种协议的SAN架构，称为FC-SAN。</li><li><strong>iSCSI 协议 (Internet SCSI)，</strong>使用该种协议的SAN架构，称为IP-SAN。</li><li><strong>FCoE 协议(Fibre Channel over Ethernet)，</strong>FC 协议通常和iSCSI协议用于现代的SAN架构中，而FCoE协议在服务器需要融合SAN和LAN业务时，也是用得越来越多。</li></ul><p><strong>IP-SAN：</strong>以TCP/IP协议为底层传输协议，采用以太网作为承载介质构建起来的存储区域网络架构。</p><p>实现IP-SAN的典型协议是iSCSI，它定义了<strong>SCSI指令集在IP中传输的封装</strong>方式。IP-SAN把SCSI指令集封装在了TCP/IP上。类似于不管我们是选择哪家快递公司，最终都是把我们想要发送的东西发送至目的地，都是由我们发起寄送请求，快递公司进行响应，差别只在于快递公司不同而已。<strong>iSCSI是全新建立在TCP/IP和SCSI指令集的基础上的标准协议，所以其开放性和扩展性更好。</strong>如下图所示，IP-SAN具备很好的扩展性、灵活的互通性，并能够突破传输距离的限制，具有明显的成本优势和管理维护容易等特点。</p><p><img src="https://i.loli.net/2019/05/29/5cee9ea90c8d696515.jpg"></p><p><strong>IP-SAN典型组网方式有：</strong></p><ul><li><strong>直连：</strong>主机与存储之间直接通过以太网卡、TOE卡或iSCSI HBA卡连接，这种组网方式简单、经济，但较多的主机分享存储资源比较困难；</li><li><strong>单交换：</strong>主机与存储之间由一台以太网交换机，同时主机安装以太网卡或TOE卡或iSCSI HBA卡实现连接。这种组网结构使多台主机能共同分享同一台存储设备，扩展性强，但交换机处是一个故障关注点；</li><li><strong>双交换：</strong>同一台主机到存储阵列端可由多条路径连接，扩展性强，避免了在以太网交换机处形成单点故障。</li></ul><p>IP-SAN是基于IP网络来实现数据块传输的网络存储形态，与传统FC SAN的最大区别在于传输协议和传输介质的不同。目前常见的IP-SAN协议有iSCSI、FCIP、iFCP等，其中iSCSI是发展最快的协议标准，大多时候我们所说的IP-SAN就是指基于iSCSI实现的SAN。</p><p><strong>NAS：</strong>网络附加存储，<strong>是一种将分布、独立的数据进行整合，集中化管理，以便于对不同主机和应用服务器进行访问的技术</strong>。如下图所示，NAS和SAN最大的区别就在于<strong>NAS有文件操作和管理系统</strong>，而SAN却没有这样的系统功能，其功能仅仅停留在文件管理的下一层，即数据管理。</p><p><img src="https://i.loli.net/2019/05/29/5cee9ec9bb0ac50848.jpg"></p><p>SAN和NAS并不是相互冲突的，是可以共存于一个系统网络中的，但NAS通过一个公共的接口实现空间的管理和资源共享，SAN仅仅是为服务器存储数据提供一个专门的快速后方存储通道。NAS文件共享功能有点儿类似FTP文件服务，但是两者是完全不同的。FTP只能将文件传输到本地的目录之后才能执行，而网络文件系统NAS可以允许直接访问源端的文件，不需要将数据复制到本地再访问。</p><h2 id="华为存储虚拟化的原理"><a href="#华为存储虚拟化的原理" class="headerlink" title="华为存储虚拟化的原理"></a>华为存储虚拟化的原理</h2><p>存储虚拟化技术可以将不同存储设备进行格式化，屏蔽存储设备的能力、接口协议等差异性，将各种存储资源转化为统一管理的数据存储资源，可以用来存储虚拟机磁盘、虚拟机配置信息、快照等信息，使得用户对存储的管理更加同质化。华为的虚拟化存储栈示意图如下所示，其实现存储虚拟化的关键的就是<strong>中间的文件系统</strong>那一层。</p><p><img src="https://i.loli.net/2019/05/29/5cee9ef00c5b983694.jpg"></p><p><strong>华为虚拟化存储栈中文件系统作用就是提供文件操作接口，屏蔽底层存储设备的差异，并且为虚拟化卷文件提供存放空间。</strong>当前FusionCompute所支持的文件系统格式有：VIMS、EXT4、NFS。它们在应用场景，虚拟化特性支持方面的差异如下：</p><table><thead><tr><th></th><th><strong>所需存储**</strong>设备**</th><th><strong>是否创建**</strong>文件系统**</th><th><strong>是否支持**</strong>共享**</th><th><strong>是否支持**</strong>延迟置零卷**</th></tr></thead><tbody><tr><td>VIMS</td><td>LUN</td><td>是</td><td>是</td><td>是</td></tr><tr><td>EXT4</td><td>本地磁盘</td><td>是</td><td>否</td><td>是</td></tr><tr><td>NFS</td><td>共享目录</td><td>否</td><td>是</td><td>否</td></tr></tbody></table><p>其中，NFS不支持创建文件系统是因为NFS本身就具备文件系统功能，而且NFS是以共享目录的方式提供数据存储，如果采用延迟置零卷的分配方式，容易造成数据丢失。EXT4主要用于服务器本地硬盘，如果要支持共享，需要在存储I/O通道上通过锁机制来避免竞争，目前方案不支持。上述的三个文件系统中，VIMS是华为自研的文件系统，其余两个都是标准文件系统类型。</p><p><strong>VIMS (Virtual Image Manage System)虚拟镜像管理系统</strong>，属于华为自研的一种文件系统，是一种高性能集群文件系统。在华为存储虚拟化解决方案中，它是实现是<strong>自动精简置备磁盘、快照、存储迁移等高级特性的技术基础。</strong>如下图所示，VIMS使虚拟化技术的应用超出了单个存储系统的限制，其设计、构建和优化针对虚拟服务器环境，可让多个虚拟机共同访问一个整合的集群式存储池，从而显著提高了资源利用率。</p><p><img src="https://i.loli.net/2019/05/29/5cee9f18639fe69372.jpg"></p><p>VIMS兼容FC SAN、IPSAN、NAS、本地磁盘，支持建立<strong>固定空间磁盘、动态空间磁盘、差分磁盘</strong>等。主要应用于需要<strong>存储迁移、快照、链接克隆</strong>等高级存储特性虚拟机。</p><h2 id="虚拟磁盘的类型"><a href="#虚拟磁盘的类型" class="headerlink" title="虚拟磁盘的类型"></a>虚拟磁盘的类型</h2><p>一个虚拟机在虚拟化计算节点文件系统上表现为一个或多个虚拟磁盘文件，也就是说，一个虚拟机磁盘体现为一个或多个虚拟磁盘文件。在华为存储虚拟化解决方案中，虚拟磁盘的格式为VHD镜像格式。它是FusionCompute实现精简卷、快照等功能的基本载体，实现了FusionCompute虚拟机镜像数据的基本储存功能。如下所示，虚拟机的一个虚拟磁盘对应一个VHD文件。</p><p><img src="https://i.loli.net/2019/05/29/5cee9f3d9238019790.jpg"></p><p>在华为存储虚拟化解决方案中，虚拟机的虚拟磁盘类型分为三种：<strong>固定空间磁盘、动态空间磁盘</strong>和<strong>差分磁盘。</strong></p><p><strong>固定空间磁盘：</strong>创建时需要将磁盘文件对应的存储块空间全部进行初始化成”0”，创建速度慢，但是IO性能最佳，适用于对IOPS要求较高的场景。如下图所示，磁盘大小恒定，<strong>创建后使用空间和预留空间相等。</strong></p><p><img src="https://i.loli.net/2019/05/29/5cee9f61b2d8415270.jpg"></p><p>数据区主要用来存放虚拟机业务数据，未写满的时候内部空间包含大量0，数据冗余度很高。最后一个扇区用来存放磁盘的元数据，也就是虚拟磁盘的大小、块的个数，位于物理存储的位置等数据，固定空间的磁盘主要应用于系统中的普通卷。</p><p><strong>动态空间磁盘：</strong>创建时只需写头和结束块，<strong>创建速度块，IO性能较差，适用于应用于精简磁盘和普通延迟置零磁盘。</strong>如下图所示，磁盘大小会随着用户写入数据而增长，但不会随着用户删除数据而缩减，只能通过磁盘空间回收来手动缩减应用在系统中的精简磁盘空间。</p><p><img src="https://i.loli.net/2019/05/29/5cee9f8543c1a89829.jpg"></p><p>前面和后面共m+1个扇区用来存放虚拟磁盘的元数据信息，其余扇区用来存放虚拟机业务数据。当用来建立精简磁盘时，首次创建只建立头和尾共m+1扇区的元数据区，后续随着虚拟机业务数据的增加逐渐增加数据区的大小，因此在一些IOPS要求不高的场景可以节省物理存储空间。动态空间磁盘通过工具可以和固定空间磁盘互相转换，例如，可以用一个精简磁盘的模板部署一个普通磁盘的虚拟机。</p><p><strong>差分磁盘：</strong>差分磁盘的结构和动态磁盘一模一样，只是<strong>文件头中会记录它的父文件路径，因此差分卷不能独立存在，必须能够访问到父文件才能正常工作</strong>，如下图所示，主要用于链接克隆场景。</p><p><img src="https://i.loli.net/2019/05/29/5cee9fa68710a98959.jpg"></p><p>前m个扇区和最后一个扇区存放虚拟磁盘自身的元数据信息，从m+1到k扇区之间存储父文件的元数据信息，数据区存放相对于父文件数据的增量业务数据，所以被称为差分磁盘。差分卷也可以成为父文件，在此场景下，差分卷不仅集成自身父文件的数据，还要向其子卷提供数据，类似祖孙三代中父亲的角色。</p><p>差分磁盘的特性和动态盘类似，但是很多业务有限制。差分磁盘以块为单位记录相对于父文件的修改。配合快照、非持久化磁盘、链接克隆等功能被使用，起到保护源盘不被修改，并可以跟踪虚拟机磁盘差异数据的作用。</p><h2 id="华为存储虚拟化特性"><a href="#华为存储虚拟化特性" class="headerlink" title="华为存储虚拟化特性"></a>华为存储虚拟化特性</h2><p>华为存储虚拟化提供基于磁盘的RAID 2.0+、基于虚拟磁盘的精简置备和空间回收、快照、连接克隆、存储热迁移等特性，适用于各类不同业务场景下的虚拟机。</p><h3 id="RAID-2-0"><a href="#RAID-2-0" class="headerlink" title="RAID 2.0+"></a>RAID 2.0+</h3><p>我们在了解华为RAID2.0+技术原理之前，可以先了解一下我们的传统RAID技术。我们传统的RAID技术是一种<strong>盘级虚拟化</strong>的技术，有RAID0 RAID1 RAID5 RAID10等常见的RAID技术，如下图所示，是一个RAID 50的组合。</p><p><img src="https://i.loli.net/2019/05/29/5cee9fd00b97698132.jpg"></p><p>传统RAID的状态主要有几种：<strong>创建RAID 、RAID组正常工作 、RAID组降级、 RAID组失效</strong> 。传统RAID技术热备方式主要是通过<strong>固定的盘</strong>来进行数据的恢复。</p><p>在传统的RAID技术中，是将几块小容量廉价的磁盘组合成一个大的逻辑磁盘给大型机使用。后来硬盘的容量不断增大，组建RAID的初衷不再是构建一个大容量的磁盘，而是利用RAID技术实现数据的可靠性和安全性，以及提升存储性能。如下图所示，由于单个容量硬盘都已经较大了，数据硬盘组建的RAID容量更大，然后再把RAID划分成一个一个的LUN映射给服务器使用。</p><p><img src="https://i.loli.net/2019/05/29/5cee9fefc02e591366.jpg"></p><p>随着硬盘技术的发展，单块硬盘的容量已经达到数T，传统RAID技术在重建的过程中需要的时间越来越长，也增加了在重构过程中其它硬盘再坏掉对数据丢失造成的风险，为了解决这一问题，块虚拟化技术应运而生，将以前以单块硬盘为成员盘的RAID技术再细化，将硬盘划分成若干的小块，再以这些小块为成员盘的方式构建RAID,也就是现在业界所说的RAID2.0+技术。</p><p><img src="https://i.loli.net/2019/05/29/5ceea008f34e382625.jpg"></p><p><strong>RAID2.0+是一种块级的虚拟化技术</strong>，如下图所示。</p><p><img src="https://i.loli.net/2019/05/29/5ceea025ef0ec28574.jpg"></p><p>它由不同类型的硬盘组成硬盘域，把硬盘域内<strong>每个硬盘切分为固定64MB的块（CK）</strong>，硬盘域内同种类型的硬盘被划分为一个个的<strong>Disk Group（DG）</strong>，从同一个DG上随机选择多个硬盘，每个硬盘选取CK按照RAID算法组成<strong>Chunk Group（CKG）</strong>，CKG被划分为固定大小的<strong>Extent</strong>，Thick LUN以Extent为单位映射到LUN（上图上半部分），也可以采用Grain在Extent的基础上进行更细粒度的划分，Thin LUN以Grain 为单位映射到LUN（上图下半部分）。</p><p>为了进一步说明RAID 2.0+各类逻辑概念以及相互之间的关系，通过下图RAID 2.0+的软件逻辑架构进行说明。</p><p><img src="https://i.loli.net/2019/05/29/5ceea048c4eaa30278.jpg"></p><p><strong>Disk Domain（磁盘域）：</strong>一个硬盘域上可以创建多个存储池（Storage Pool）一个硬盘域的硬盘可以选择SSD、SAS、NL-SAS中的一种或者多种，不同硬盘域之间是完全隔离的，包括故障域、性能和存储资源等。　</p><p><strong>Storage Pool（存储池）&amp; Tier：</strong>一个存储池基于指定的一个硬盘域创建，可以从该硬盘域上动态的分配Chunk（CK）资源，并按照<strong>每个存储层级（Tier）</strong>的“<strong>RAID策略</strong>”组成Chunk Group（CKG）向应用提供具有RAID保护的存储资源。</p><p><strong>Disk Group（DG）：</strong>由硬盘域内相同类型的多个硬盘组成的集合，硬盘类型包括SSD、SAS和NL-SAS三种。</p><p><strong>LD（逻辑磁盘）：</strong>是被存储系统所管理的硬盘，和物理硬盘一一对应。</p><p><strong>Chunk（CK）：</strong>是存储池内的硬盘空间切分成若干固定大小的物理空间，每块物理空间的大小为64MB，是组成RAID的基本单位。</p><p><strong>Chunk Group（CKG）：</strong>是由来自于同一个DG内不同硬盘的CK按照RAID算法组成的逻辑存储单元，是存储池从硬盘域上分配资源的最小单位。</p><p><strong>Extent：</strong>是在CKG基础上划分的固定大小的逻辑存储空间，大小可调，是热点数据统计和迁移的最小单元（数据迁移粒度），也是存储池中申请空间、释放空间的最小单位。</p><p><strong>Grain：</strong>在Thin LUN模式下，Extent按照固定大小被进一步划分为更细粒度的块，这些块称之为Grain。</p><p><strong>Volume &amp; LUN：</strong>Volume即卷，是存储系统内部管理对象；LUN是可以直接映射给主机读写的存储单元，是Volume对象的对外体现。</p><p>RAID2.0+ 优秀性能主要体现在<strong>负载均衡优、数据重构时间短</strong>和<strong>提升单卷（LUN）的读写性能</strong>等几个方面。</p><p><strong>1）RAID2.0+的负载均衡更优，</strong>如下图所示，数据在存储池中硬盘上的自动均衡分布，避免了硬盘的冷热不均，从而降低了存储系统整体的故障率。</p><p><img src="https://i.loli.net/2019/05/29/5ceea074399ec47928.jpg"></p><p>RAID 2.0+的负载均衡方式主要有两种：<strong>第一种是根据crush算法</strong>，在创建CKG的时候选择CK，保证硬盘被选中的概率与硬盘剩余容量成正比；<strong>第二种就是smartmotion</strong>，主要用于数据迁移发生在DG层次。当有新盘加入硬盘域的时候，会触发smartmotion，查出待均衡的原CKG，然后给该CKG分配一个目标CKG，该CKG包含来源于新盘的CK，如果原CKG和目标CKG中对应位置的CK落在不同的盘，就会触发实现均衡。那么有数据的原CKG迁移到目标CKG中，没有数据的只需要改变CKG的映射关系即可。</p><p><strong>2）RAID2.0+的数据重构时间短</strong>，如下图所示，相较传统RAID重构数据流串行写入单一热备盘的方式，RAID2.0+采用多对多的重构，重构数据流并行写入多块磁盘，极大缩短数据重构时间，1TB数据仅需30分钟。</p><p><img src="https://i.loli.net/2019/05/29/5ceea09166f4349338.jpg"></p><p>RAID2.0+的数据重构方式主要有三种：<strong>第一种是全盘重构</strong>，就是当一块盘故障或者被拔出之后进行数据恢复；<strong>第二种是局部重构</strong>，就是硬盘上出现坏块，通过RAID算法将上面的数据重构到热备CK中；<strong>第三种是恢复重构</strong>，就是在某块硬盘正常访问期间的写操作无法完成，只能处于降级写状态，会在系统上记录相关的日志并且更新校验值，待硬盘恢复后，将故障期间的数据根据RAID算法计算之后更新数据。</p><p><strong>3）提升单卷（LUN）的读写性能</strong>，如下图所示，RAID2.0+的热备采用的是空间的形式，每个盘上面的CK都可以作为磁盘热点，这种条块化的分割极大提升单卷（LUN）的读写IOPS。</p><p><img src="https://i.loli.net/2019/05/29/5ceea0b1be3f817902.jpg"></p><p>传统存储的RAID通常是以单个磁盘为粒度来建立RAID，RAID被限制在有限的几个磁盘上，所以当主机对一个较小的卷进行密集访问时，只能访问到有限的几个磁盘，这就造成磁盘访问瓶颈，导致磁盘热点。 而RAID2.0+技术基于Chunk而非物理磁盘构成RAID。一个物理磁盘上的不同CK可以用于构成不同RAID类型的卷。对于HVS阵列而言，即使是很小的卷也可以通过CK的方式分布到很多磁盘上。宽条带化技术使得小的卷不再需要额外的大容量即可获得足够的高性能，且避免了磁盘热点。物理磁盘上剩余的CK还可以用于其它的卷。</p><p>在使用RAID 2.0+ 技术时，首先创建硬盘域，指定该硬盘域使用的硬盘类型和每种类型硬盘的数量；另外还要指定针对不同类型的硬盘，使用其上的CK创建CKG时采取的RAID算法，不同类型的硬盘可以使用不同的RAID算法，比如针对故障率高的SATA硬盘采用可靠性更高的RAID 6算法等。接下来创建存储池，一个存储池是基于一个硬盘域的，创建存储池时可以设置该存储池所使用的Extent大小，设置后不能更改。然后在存储池内创建LUN，只需要指定LUN的容量大小即可，系统会根据事先定义的规则（数据分层、自动迁移等）选择适当的Extent完成创建。最后将创建好的LUN映射给需要的主机，主机在使用LUN时RAID 2.0+ 技术就会在后台发挥作用。</p><p><strong>由此可以看出，当一块硬盘损坏时，只会影响该硬盘域上的存储池中的CKG，对于其它硬盘域上的CKG无影响，因此可以实现故障隔离。</strong></p><h2 id="精简磁盘和空间回收"><a href="#精简磁盘和空间回收" class="headerlink" title="精简磁盘和空间回收"></a>精简磁盘和空间回收</h2><p>华为存储虚拟化支持创建精简磁盘，可以随着用户使用而自动分配空间。但是，后续膨胀的精简磁盘不会随着用户删除数据而缩小，必须使用空间回收工具可以将用户删除的数据空间释放到数据存储。如下图所示，创建虚拟机可以选择精简磁盘模式，提高磁盘使用率，增加虚拟机部署密度。</p><p><img src="https://i.loli.net/2019/05/29/5ceea0daed68148460.jpg"></p><p><strong>精简磁盘可应用于局点运行初期，用户磁盘使用率低的情况。</strong>能够降低初始存储投资及维护成本，存储设备只保存有效数据，不保存预留空间，可以提高存储资源利用率。</p><p>在FusionCompute中，选择“存储池”。 进入“存储池”页面。在左侧导航树，选择“站点名称 &gt; 数据存储名称”。 显示“入门”页签。单击“磁盘”。 显示磁盘信息列表。单击“创建磁盘”。 弹出“创建磁盘”对话框，如图所示。</p><p><img src="https://i.loli.net/2019/05/29/5ceea0f29ec5f50184.jpg"></p><p><strong>配置模式：</strong></p><ul><li><strong>普通：</strong>根据磁盘容量为磁盘分配空间，在创建过程中会将物理设备上保留的数据置零。这种格式的磁盘性能要优于其他两种磁盘格式，但创建这种格式的磁盘所需的时间可能会比创建其他类型的磁盘长，且预留空间和实际占用空间相等，建议系统盘使用该模式。</li><li><strong>精简：</strong>该模式下，系统首次仅分配磁盘容量配置值的部分容量，后续根据使用情况，逐步进行分配，直到分配总量达到磁盘容量配置值为止。数据存储类型为“<strong>FusionStorage</strong>”或“<strong>本地内存盘</strong>”时，只支持该模式；数据存储类型为“<strong>本地硬盘</strong>”或“<strong>SAN存储</strong>”时，不支持该模式。</li><li><strong>普通延迟置零：</strong>根据磁盘容量为磁盘分配空间，创建时不会擦除物理设备上保留的任何数据，但后续从虚拟机首次执行写操作时会按需要将其置零。<strong>创建速度比“普通”模式快；IO性能介于“普通”和“精简”两种模式之间。</strong>只有数据存储类型为“<strong>虚拟化本地硬盘</strong>”、“<strong>虚拟化SAN存储</strong>”或版本号为<strong>V3的“Advanced SAN存储</strong>”时，支持该模式。</li></ul><p><strong>磁盘模式：</strong></p><ul><li><strong>从属：</strong>快照中包含该从属磁盘，默认选项。</li><li><strong>独立-持久：</strong>更改将立即并永久写入磁盘，持久磁盘不受快照影响。即对虚拟机创建快照时，不对该磁盘的数据进行快照。使用快照还原虚拟机时，不对该磁盘的数据进行还原。</li><li><strong>独立-非持久：</strong>关闭电源或恢复快照后，丢弃对该磁盘的更改。</li></ul><h3 id="快照和快照链"><a href="#快照和快照链" class="headerlink" title="快照和快照链"></a>快照和快照链</h3><p>虚拟机快照记录了虚拟机在某一时间点的内容和状态，通过恢复虚拟机快照使虚拟机多次快速恢复到这一时间点，比如我们初次手动安装OpenStack服务时，每安装一个服务都会创建一个快照，这样当后续出现错误且无法恢复时，可以通过前面快照恢复这一步安装前的状态，而不用从头再来。虚拟机快照包含磁盘内容、虚拟机配置信息、内存数据，多次快照之间保存差量数据，节约存储空间。主要适用于虚拟机用户在执行一些重大、高危操作前，例如系统补丁，升级，破坏性测试前执行快照，可以用于故障时的快速还原。</p><p>虚拟机快照的创建、恢复和删除都是由用户手动触发的，系统并不会自动执行。如下图所示，创建快照时会生成一个新的差分卷，创建的方式一般包括COW（写时拷贝）、ROW（写时重定向）和WA（随机写），一般都是写时重定向ROW方式创建。</p><p><img src="https://i.loli.net/2019/05/29/5ceea11807a6c40312.jpg"></p><p>当创建快照采用了ROW方式时，快照虚拟机会挂载这个差分卷，快照创建后的写操作会进行重定向，所有的写IO都被重定向到新卷中，所有旧数据均保留在只读的源卷中。</p><p>当用户对一个虚拟机进行多次快照操作，可以形成快照链，如下图所示，一个虚拟机生成快照的数量不能超过<strong>32个</strong>，也是快照链的最大长度。</p><p><img src="https://i.loli.net/2019/05/29/5ceea13a9c1d752467.jpg"></p><p>SNAP1是基于源卷的第一次差分卷，SNAP2是基于SNAP2的第二次差分卷，且虚拟机源卷始终挂载在快照链的最末端。用户可以将虚拟机从当前状态恢复到快照链中的某个状态，且快照链中任意一个快照都可以删除而不影响其余快照。</p><h2 id="链接克隆"><a href="#链接克隆" class="headerlink" title="链接克隆"></a>链接克隆</h2><p>链接克隆在桌面云解决方案里面有重要的地位，在电信云NFV领域目前暂时没有应用，因此只需要了解链接克隆特性以及与快照的区别即可。</p><p>链接克隆技术是一种通过将源卷和差分卷组合映射为一个链接克隆卷，提供给虚拟机使用的技术。如下图所示，一个链接克隆模板可以创建多个链接克隆差分卷，对应创建多个链接克隆虚拟机。</p><p><img src="https://i.loli.net/2019/05/29/5ceea190153f728001.jpg"></p><p>上图中黄色部分为虚拟机源卷，VM1和VM2两个虚拟机是基于源卷+各自差分卷创建出来的链接克隆虚拟机。链接克隆虚拟机新创建的差分卷初始占用空间很小，随着虚拟机的使用，空间会逐渐膨胀。</p><p><strong>与快照相同的是，链接克隆虚拟机的写IO操作也只会更新到差分卷中，且创建数量和创建时间不限。不同的是，链接克隆虚拟机可以和源虚拟机同时运行，且能同时处于同一网络，但是快照与源虚拟机不能同时运行，自然也就不能处于同一网路。而且，快照主要用于记录源虚拟机某一时间的状态，链接克隆虚拟机主要用于同质业务虚拟机多拷贝分发。虚拟机快照可以在源虚拟机运行时创建，但是链接克隆虚拟机必须在源虚拟机关闭时才能创建。</strong></p><h2 id="存储热迁移"><a href="#存储热迁移" class="headerlink" title="存储热迁移"></a>存储热迁移</h2><p>华为存储虚拟化解决方案支持将虚拟机的磁盘从一个数据存储迁移到另一个数据存储。当需要对数据存储空间进行减容时，这时我们需要将源数据存储上的虚拟机磁盘进行迁移，如下如所示，可以将虚拟机的所有磁盘整体迁移，也可以单个磁盘分别迁移。</p><p><img src="https://i.loli.net/2019/05/29/5ceea204ed9dd83008.jpg"></p><p>在迁移虚拟机虚拟磁盘文件时，虚拟机的快照可以一起迁移（<strong>但只支持关机状态下冷迁移</strong>），且无论虚拟机是开启或者关闭状态，都可以迁移。当虚拟机为关机状态时，这种数据存储间的迁移称为冷迁移，数据存储冷迁移前后性能对比如下：</p><table><thead><tr><th><strong>源存储类型（源配置模式）</strong></th><th><strong>目的存储类型</strong></th><th><strong>配置模式是否变化</strong></th><th><strong>迁移后模式</strong></th><th><strong>是否支持带**</strong>快照迁移**</th></tr></thead><tbody><tr><td>虚拟化存储（普通，延迟置零，精简）</td><td>虚拟化存储（非NAS）</td><td>否</td><td>保持不变</td><td>是</td></tr><tr><td>虚拟化存储（延迟置零）</td><td>虚拟化存储（NAS）</td><td>是</td><td>精简</td><td>是</td></tr><tr><td>虚拟化存储（延迟置零，精简）</td><td>块存储</td><td>是</td><td>普通</td><td>否</td></tr><tr><td>虚拟化存储（普通）</td><td>块存储</td><td>否</td><td>普通</td><td>否</td></tr><tr><td>块存储</td><td>虚拟化存储</td><td>否</td><td>保持不变</td><td>否</td></tr><tr><td>块存储</td><td>块存储</td><td>否</td><td>保持不变</td><td>否</td></tr></tbody></table><p>当虚拟机为开机状态时，这种迁移就称为<strong>存储热迁移</strong>。数据存储冷迁移前后性能对比如下：</p><table><thead><tr><th><strong>源存储类型（源配置模式）</strong></th><th><strong>目的存储类型</strong></th><th><strong>配置模式是否变化</strong></th><th><strong>迁移后模式</strong></th></tr></thead><tbody><tr><td>块存储</td><td>虚拟化存储</td><td>是</td><td>迁移时可以选择为普通延迟置零（NAS不支持）或者精简</td></tr><tr><td>虚拟化存储（普通卷）</td><td>虚拟化存储</td><td>是</td><td>迁移时可以选择为普通延迟置零（NAS不支持）或者精简</td></tr><tr><td>虚拟化存储（延迟置零卷）</td><td>虚拟化存储（非NAS）</td><td>否</td><td>保持不变</td></tr><tr><td>虚拟化存储（延迟置零卷）</td><td>虚拟化存储（NAS）</td><td>是</td><td>精简</td></tr><tr><td>虚拟化存储（精简卷）</td><td>虚拟化存储</td><td>否</td><td>保持不变</td></tr></tbody></table><p><strong>当发生存储热迁移时，同时需迁移虚拟机磁盘镜像和系统内存状态，也就说存储热迁移一般和虚拟机热迁移同步进行。</strong>存储热迁移的示意图如下所示：</p><p><img src="https://i.loli.net/2019/05/29/5ceea2289543282397.jpg"></p><p>在存储热迁移场景下，华为的解决方案并不完美，仍然受以下条件限制：<strong>不支持迁移已挂载为“共享”类型的磁盘</strong>和<strong>链接克隆虚拟机的磁盘</strong>。当虚拟机为“运行中”时，不支持<strong>非持久化磁盘、带快照虚拟机磁盘</strong>和<strong>开启iCache功能虚拟机磁盘</strong>的迁移，可将虚拟机关闭后迁移；当虚拟机为“已停止”时，<strong>如果目标数据存储为块存储，不支持非持久化磁盘、带快照虚拟机磁盘的迁移。</strong></p><p>根据上图，华为存储虚拟化解决方案中存储热迁移的步骤如下：</p><p><strong>Step1：</strong>在目的存储上创建一个与源相同的空镜像文件。</p><p><strong>Step2：</strong>将目的存储的镜像文件设置为源镜像文件的mirror，使虚拟机的I/O写也能落盘在目的存储上，保证了脏块数据的同步。</p><p><strong>Step3：</strong>通过迭代迁移的技术，将源镜像的数据迁移到目的镜像中，保证了基线数据的同步。</p><p><strong>Step4：</strong>在基线数据同步完成后，短暂的时间内暂停虚拟机的I/O请求，将虚拟机的存储文件从源镜像切换到目的镜像上，这样就完成了存储的迁移。</p><p><strong>而且，在华为存储虚拟化解决方案中，可以通过界面设置3种不同热迁移速率，应对不同的业务场景：</strong></p><ul><li><strong>适中</strong>  （迁移速率不高于20M/s，用于存储IO压力较大场景，缓解迁移操作对用户虚拟机的影响）</li><li><strong>快速</strong>   （迁移速率不高于30M/s，用于存储IO压力正常场景，在保证迁移速度的同时可以适当减少对用户虚拟机的影响）</li><li><strong>不限</strong>   （迁移速率不高于1024M/s，用于用户虚拟机业务优先级很低的场景）</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前文《存储虚拟化概述》中，我们提到存储虚拟化与软件定义存储SDS的区别就是其控制平面数据和存储平面数据是一种紧耦合的关系，因此各厂商的存储虚拟化解决方案是一种“私有”方案，不仅不同厂商之间不能兼容，甚至通常不同产品之间有可能不兼容。因此，考虑到电信云NFV的应用场景，以及设备商的分布情况，我们这里主要讨论&lt;strong&gt;华为的虚拟化存储解决方案&lt;/strong&gt;和&lt;strong&gt;华为的分布式存储Fusion Storage&lt;/strong&gt;的情况。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-05-27-5G：看起来很美，任重而道远</title>
    <link href="https://kkutysllb.cn/2019/05/27/2019-05-27-5G%EF%BC%9A%E7%9C%8B%E8%B5%B7%E6%9D%A5%E5%BE%88%E7%BE%8E%EF%BC%8C%E4%BB%BB%E9%87%8D%E8%80%8C%E9%81%93%E8%BF%9C/"/>
    <id>https://kkutysllb.cn/2019/05/27/2019-05-27-5G：看起来很美，任重而道远/</id>
    <published>2019-05-27T14:57:50.000Z</published>
    <updated>2019-05-27T15:11:37.756Z</updated>
    
    <content type="html"><![CDATA[<p><strong>5G：第五代移动通信技术的简称</strong> ，4G技术的延伸，弥补了4G技术的不足，在吞吐率、时延、连接数量、能耗等方面进一步提升系统性能。 5G致力于应对2020年后多样化差异化业务的巨大挑战，满足超高速率、超低时延、高速移动、高能效和超高流量与连接数密度等多维能力指标。 <a id="more"></a></p><h2 id="5G的总体愿景"><a href="#5G的总体愿景" class="headerlink" title="5G的总体愿景"></a>5G的总体愿景</h2><p>5G网络就是第五代移动通信网络。 5G将通信的作用从人与人之间的连接扩展到各行各业、万事万物之间的互相连接，形成崭新的数字化社会、物联网世界新格局。5G将渗透到未来社会的各个领域，以用户为中心构建全方位的信息生态系统。5G将使信息突破时空限制，提供极佳的交互体验，为用户带来身临其境的信息盛宴；5G将拉近万物的距离，通过无缝融合的方式，便捷地实现人与万物的智能互联。5G将为用户提供光纤般的接入速率，“零”时延的使用体验，千亿设备的连接能力，超高流量密度、超高连接数密度和超高移动性等多场景的一致服务，业务及用户感知的智能优化，同时将为网络带来超百倍的能效提升和超百倍的比特成本降低，最终实现“<strong>信息随心至，万物触手及</strong>”的总体愿景。</p><p><img src="https://i.loli.net/2019/05/27/5cebfb9ad46f316989.jpg"></p><p>在最近几年里，普通人了解到 的5G特征往往是速度快。 当前，4G LTE网络服务传输速率实际仅为75Mbps。 而在5G网络中，最早三星电子利用64个天线单元的自适应阵列传输技术，成功地在28GHz波段下达到1Gbps的传输速率，实现了新的突破。 未来5G网络的传输速率可达到10Gbps，这意味着手机用户在不到1s时间内即可下载一部高清电影。</p><p>在通信业内人士眼里，<strong>5G网络的主要目标是让终端用户始终处于联网状态</strong>。5G网络将来支持的设备远远不止是智能手机和平板电脑，它还要承载个人智能通信工具、可穿戴设备等。5G网络将是4G网络的颠覆性升级版，它的基本要求并不仅仅体现在无线网络上，还有为实现5G功能而搭建的核心网、承载网以及接入网。</p><h2 id="5G概念提出的背景"><a href="#5G概念提出的背景" class="headerlink" title="5G概念提出的背景"></a>5G概念提出的背景</h2><p><strong>移动互联网</strong>和<strong>物联网</strong>是推进未来移动通信网络发展的两大驱动力。移动互联网颠覆了传统移动通信业务模式，为用户提供前所未有的、多样化的使用体验，深刻影响着人们工作生活的方方面面。物联网扩展了移动通信的服务范围，从人与人通信延伸到物与物、人与物智能互联，使移动通信技术渗透至更加广阔的行业和领域。2014年到2019年全球IMT流量将进一步快速增长，总的流量上涨倍数达到几十到100倍。中国国内由国家技术创新委员会、工信部和发改委组织的IMT-2020（5G推进组）于2014年发布的《 5G愿景与需求白皮书》中预计，2010~2020年全球移动数据流量增长将超过200倍，2010~2030年 将增长近20000倍。中国的移动数据流量增速高于全球平均水平，预计2010~2020年将增长300倍上，2010~2030年将增长超过40000倍。发达城市及热点地区的移动数据流量增速更快，2010~2020 年上海移动数据流量的增长率可达原来的600倍，北京热点区域移动数据流量的增长率可达原来的1000 倍。</p><p><img src="https://i.loli.net/2019/05/27/5cebfbc6bfc7272260.jpg"></p><p>另外，<strong>我国“互联网+”国家战略需求中明确指出：未来电信基础设施和信息服务要在国民经济中下沉，满足农业、医疗、金融、交通、流通、制造、教育、生活服务、公共服务、教育和能源等垂直行业的信息化需求，改变传统行业，促生跨界创新</strong>。因此，未来5G网络不仅需要继续面对移动互联网业务带来的挑战，例如：频谱效率和用户体验速率的提升，时延的减少，移动性的增强等，同时还需要满足物联网多样化的业务需求。</p><p>从信息交互对象不同的角度出发，目前5G应用分为三大类场景：<strong>增强移动宽带（eMBB）、海量机器类通信（mMTC）</strong>和<strong>超可靠低时延通信（uRLLC）</strong>。eMBB 场景是指在现有移动宽带业务场景的基础上， 对于用户体验等性能的进一步提升，主要还是追求人与人之间极致的通信体验。mMTC 和 uRLLC都是物联网的应用场景，但各自侧重点不同。mMTC 主要是人与物之间的信息交互，而 uRLLC 主要体现物与物之间的通信需求。未来全球移动通信网络连接的设备总量将达到千亿规模。预计到2020年，全球移动终端（不含物联网设备）数量将超过100 亿，其中中国将超过20亿。全球物联网设备连接数也将快速增长， 2020年将接近全球人口规模达到70亿，其中中国将接近15亿。</p><p><img src="https://i.loli.net/2019/05/27/5cebfbe93d9b616173.jpg"></p><h2 id="5G的三大业务模式"><a href="#5G的三大业务模式" class="headerlink" title="5G的三大业务模式"></a>5G的三大业务模式</h2><p>为了更好的面向数字化的世界，服务数字化的社会，全球范围内的运营商都在进行数字化转型。<strong>运营商数字化转型的目标在于为企业客户、消费者提供 ROADS (Real- time, On-Demand, All online, DIY, Social) 体验</strong>，这需要通过端到端协同整体架构才能够实现，需要在各个环节都实现敏捷，自动化和智能化。<strong>运营商的网络、运营系统、业务的全面云化是必要条件和实现手段。</strong></p><p>5G 时代将以一张物理的基础网络支撑多种不同的商业需求，<strong>云化的端到端网络架构</strong>通过以下几个方面实现上述需求：</p><p><img src="https://i.loli.net/2019/05/27/5cebfc0d0c78552089.jpg"></p><ul><li>在同一套物理基础设上基于不同的业务需求生成逻辑隔离的独立运行的网络切片，通过基于数据中心的云化架构支撑多种应用场景。</li><li>利用CloudRAN对无线接入网络进行重构，满足5G时代多技术连接以及RAN功能按需部署的需求。</li><li>通过控制面和用户面（C/U）分离，功能模块化以及统一的数据库管理技术简化核心网络架构，实现网络功能的按需配置。</li><li>基于应用驱动来自动的生成，维护，终止网络切片服务， 利用敏捷的网络运维降低运营商的运营成本。</li></ul><p>5G时代新的通信需求对现有网络提出了包括<strong>技术上的，商业模式上</strong>的种种挑战，需要下一代移动网络来满足。ITU将5G时代的主要移动网络业务划分为三 类：<strong>eMBB（Enhanced Mobile Broadband）, uRLLC(Ultra-reliable and Low-latency Communications)</strong> 以 及 <strong>mMTC（Massive Machine Type Communications）</strong>。</p><p><img src="https://i.loli.net/2019/05/27/5cebfc3b4bb4399954.jpg"></p><p><strong>eMBB 聚焦对带宽有极高需求的业务</strong>，例如高清视频，虚拟现实/增强现实等等，满足人们对于数字化生活的需求。比如：移动的环境中，网联无人机对大带宽、低时延的需求，将会引爆众多高价值创新行业应用。初期可以采用4G拓展应用，后续随着5G引入业务体验更好、创新应用更多。</p><p><img src="https://i.loli.net/2019/05/27/5cebfc611a93087287.jpg"></p><p><strong>uRLLC 聚焦对时延极其敏感的业务</strong>，例如自动驾驶/辅助驾驶，远程控制等，满足人们对于数字化工业的需求。比如：5G技术可满足车联网低时延、高速、高可靠性的业务需求，可以重新定义汽车安全，促使车联网创新应用成为现实。</p><p><img src="https://i.loli.net/2019/05/27/5cebfc83729d444773.jpg"></p><p><strong>mMTC 则覆盖对于联接密度要求较高的场景</strong>，例如智慧城市，智能农业，满足人们对于数字化社会的需求。比如：奠基国家工业互联网，助力中国制造2025，采用授权频率，使用4G/5G及有线网络等技术，基于运营商的泛在网络，为工业互联网工厂内外数据应用提供连接服务，主要包含<strong>采集类、控制类、监测类</strong>等应用。</p><p><img src="https://i.loli.net/2019/05/27/5cebfc9f0e9bf32000.jpg"></p><h2 id="5G的技术定义"><a href="#5G的技术定义" class="headerlink" title="5G的技术定义"></a>5G的技术定义</h2><p>5G（第五代移动通信）是IMT（国际移动通信）的下一阶段，ITU（国际电信联盟）将其正式命名为IMT-2020。目前，ITU正在对IMT-2020进行初步的规划。此外，端到端系统的大多数其他变革（既包括核心网络内的，又包括无线接入网络内的）也将会成为未来5G系统的一部分。在移动通信市场中，IMT-Advanced（包括LTE-Advanced与WMAN-Advanced）系统之后的系统即为“<strong>5G</strong>”。</p><p>在大力研发5G潜在“候选技术”的同时，全球移动通信行业对于5G技术研发驱动的理解也逐步达成了共识。<strong>ITU-R（国际电信联盟无线电通信局）确定未来的5G具有以下三大主要的应用场景：1）增强型移动宽带；2）超高可靠与低延迟的通信；3）大规模机器类通信。</strong>具体包括吉比特每秒移动宽带数据接入、智慧家庭、智能建筑、语音通话、智慧城市、三维立体视频、超高清晰度视频、云工作、云娱乐、增强现实、行业自动化、紧急任务应用、自动驾驶汽车等。</p><p>一项技术创新可以分为<strong>渐进式创新、模块创新、架构创新</strong>和<strong>彻底创新</strong>4类。从2G到4G是频谱效率和安全性等逐步提升的渐进式创新，也是在维持集中式网络构架下的模块式创新，还有从网络构架向扁平化和分离化演进的架构创新。但到了5G时代，除了网络能力以外，还必须面向各种新的行业服务，提供随时需要的、高质量的连接服务，这也要求5G网络的建设是多方位的、彻底的创新。</p><p>移动网络架构架主要包括<strong>核心网</strong>和<strong>无线接入网</strong>，到了5G时代，移动网络按循序渐进的方式引入5G网元设备。 </p><p><img src="https://i.loli.net/2019/05/27/5cebfcd038b5b25267.jpg"></p><blockquote><p><strong>Step1：</strong>5G NR（新无线）先行， 5G基站（gNodeB）与4G基站（eNodeB）以双连接的方式共同接入4G核心 网。 </p><p><strong>Step2：</strong>5G基站独立接入5G核心网（NGCN，下一代核心网）。</p><p><strong>Step3：</strong>5G基站和4G基站统统接入5G核心网，4G核心网退出历史舞台。</p></blockquote><p>以上5G网络架构演进看似整体一致，实际上，我们把核心网和无线接入网分开来看，其内部架构发生了颠覆性的改变。核心网的网元由4G时代的MME/SAE-GW变为5G时代的AMF/UPF（AMF/UPF是由中国移动牵头提出的SBA 5G核心网基础架构）。</p><ul><li><strong>AMF（Mobility Management Function）</strong>负责控制面的移动性和接入管理，代替了MME的功能。 </li><li><strong>UPF（User Plane Function）</strong> 负责用户面，它代替了原来4G中执行路由和转发功能的SGW和PGW。 </li></ul><p>另外一个概念是<strong>5G系统服务架构</strong>，这<strong>是一个基于云原生设计原则的架构，不仅要对传统4G核心网网元NFV虚拟化，网络功能还将进一步软件模块化，实现从驻留于云到充分利用云的跨越</strong>，以实现未来以软件化、模块化的方式灵活、快速地组装和部署业务应用。</p><p><img src="https://i.loli.net/2019/05/27/5cebfd065352515880.jpg"></p><p>为了灵活应对智慧城市、车联网、物联网等多样化的服务，使能网络切片，核心网基于云原生构架设计，面临毫秒级时延、海量数据存储与计算等挑战，云化的C-RAN构架和实时的移动边缘计算（MEC）应运而生。 从核心网到接入网，未来5G网络将分布式部署巨量的计算和存储于云基础设施之中，<strong>核心数据中心和分布式云数据中心构成网络拓扑的关键节点</strong>。这是一场由海量数据引发的从量变到质变的数据革命，是一场由技术创新去推动社会进步的革命，因此，5G需广泛地与各行业深入合作，共同激发创新，从而持续为社会创造价值。</p><p>无线接入网发生的主要改变是<strong>分离</strong>，首先是控制面和用户面的分离，其次是基站被分离为AAU、DU和CU这3个部分。</p><p><img src="https://i.loli.net/2019/05/27/5cebfd26baf2c95934.jpg"></p><p>5G无线关键技术有<strong>微基站（Small Cell）</strong>和<strong>Massive MIMO</strong>。 5G的容量需求是4G的1000倍，峰值速率10～20Gbps，要提升容量和速率无非就是<strong>增大频谱带宽、提升频谱效率</strong>和<strong>增加小区数量</strong>“三板斧”操作。 首先是频谱带宽，高频段的频率资源丰富，同时，目前小于3GHz的低频段基本被2G/3G/4G占用（中国移动的5G目前规划为2.6G整频段，电联需要退出目前4G的占用），所以，5G必然要向高频段3.5～30GHz（甚至更高）扩展。那么如何解决频段越高，穿透能力越差，覆盖范围越小的问题就引出了 5G的两大关键技术—<strong>Massive MIMO</strong>和<strong>微基站</strong>。 </p><p><strong>微基站已成为未来解决网络覆盖和容量的关键。</strong>未来城市路灯、广告牌、电杆等各种街道设施都将成为微基站挂靠的地方。<strong>Massive MIMO就是在基站侧配置远多于现有系统的大规模天线阵列的MU-MIMO，来同时服务多个用户。</strong>它可以大幅提升无线频谱效率，增强网络覆盖和系统容量，简言之， 就是通过分集技术提升传输可靠性、空间复用提升数据速率、波束赋形提 覆盖范围。 MU-MIMO将多个终端联合起来空间复用，同时使用多个终端的天线，这样一来，大量的基站天线和终端天线形成一个大规模的、虚拟的MIMO信道系统。这是从整个网络的角度更宏观地去思考提升系统容量。</p><p><strong>波束赋形</strong>是指大规模多天线系统可以控制每一个天线单元的发射（或接收）信号的相位和信号幅度，产生具有指向性的波束，消除来自四面八方的干扰，增强波束方向的信号。它可以补偿无线传播损耗。</p><p>国家政策驱动下，<strong>“企业上云”</strong>的进程将进一步加快。云业务的发展，对网络需求自内生而建，使两者从独立走向融合。5G超高速上网和万物互联将产生呈指数级上升的海量数据，这些数据需要云存储和云计算，并通过大数据分析和人工智能产出价值。与此同时，为了面向未来多样化和差异化的5G服务，<strong>一场基于虚拟化、云化的ICT融合技术革命正在推动着网络重构与转型。</strong></p><p><img src="https://i.loli.net/2019/05/27/5cebfd5335d1f73825.jpg"></p><p>引入新的组件： <strong>编排器和网络控制器</strong>，地市城域网实现互连、地市PTN网络实现互连、新增云专线网关，最终实现“云+专线” 融合业务的自动化发放。</p><h2 id="关于5G的标准"><a href="#关于5G的标准" class="headerlink" title="关于5G的标准"></a>关于5G的标准</h2><h3 id="ITU和3GPP"><a href="#ITU和3GPP" class="headerlink" title="ITU和3GPP"></a>ITU和3GPP</h3><p><strong>5G最重要的标准化组织有ITU和3GPP</strong>。其中，<strong>ITU是联合国负责国际电信事务的专业机构</strong>，其下分为电信标准化部门（ITU-T）、无线电通信部门（ITU-R）和电信发展部门（ITU-D），每个部门下设多个研究组，每个研究组下设多个工作组，<strong>5G的相关标准化工作是在ITU-R WPSD 下进行的</strong>。ITU-R WPSD是专门研究和制订移动通信标准IMT（包括IMT-2000和IMT-Advanced）的组织，根据ITU的工作流程，<strong>每一代移动通信技术国际标准的制订过程包括业务需求、频率规划和技术方案3个部分</strong>，当前对5G的时间表已经确定了3个阶段：</p><ul><li>第一个阶段截至2015年底，完成IMT-2020国际标准前期研究，重点是完成5G宏观描述，包括5G的愿景、5G的技术趋势和ITU的相关决议，并在2015年世界无线电大会上获得必要的频率资源；</li><li>第二个阶段是2016~2017年底，主要完成5G性能需求、评估方法研究等内容；</li><li>第三个阶段是收集5G的候选方案。 </li></ul><p><strong>而3GPP是一个产业联盟，其目标是根据ITU的相关需求，制订更加详细的技术规范与产业标准，规范产业行为。</strong>3GPP（the 3rd Generation Partnership Project）是领先的3G技术规范机构，是由欧洲的ETSI、日本的ARIB和TTC、韩国的TTA、美国的T1和中国的无线通信组CWTS共6个标准化组织伙伴组成。3GPP的会员包括组织伙伴、市场代表伙伴和个体会员3类。3GPP市场代表伙伴不是官方的标准化组织，它们是向3GPP提供市场建议和统一意见的机构组织。</p><h3 id="5G的几个3GPP阶段性标准"><a href="#5G的几个3GPP阶段性标准" class="headerlink" title="5G的几个3GPP阶段性标准"></a>5G的几个3GPP阶段性标准</h3><p>根据3GPP此前公布的5G网络标准制订过程，5G整个网络标准分几个阶段完成，如下图所示。</p><p><img src="https://i.loli.net/2019/05/27/5cebfd8e8fe7f16722.jpg"></p><p><strong>2017年12月21日，在国际电信标准组织3GPP RAN第78次全体会议上，5G NR（New Radio）首发版本正式发布，这是全球第一个可商用部署的5G标准。</strong> 非独立组网的NSA 5G标准被冻结，但这只是一种过渡方案，仍然依托4G基站和网络，只是空口采用5G，算不上真正的5G标准。 非独立组网标准的确立，可以让一些运营商在已有的4G网络上进行改造，在不进行大规模设备替换的前提下，将移动 网速提升到5G网络，即1Gbps速率。</p><p><strong>R15阶段重点满足增强移动宽带（eMBB）和低时延高可靠（uRLLC）应用需求</strong>，该阶段又分为两个子阶段：<strong>第一个子阶段，5G NR非独立组网特性已于2017年12月完成，2018年3月冻结</strong>；<strong>第二个子阶段，5G NR独立组网标准于2018年6月14日冻结。</strong>2018年6月，已经完成了5G独立组网（SA）标准，支持增强移动宽带和低时延高可靠物联网，完成了网络接口协议定义。现在的R15 5G标准只能算是第一阶段，重点满足增强移动宽带（eMBB）和低时延高可靠（uRLLC）应用需求，可用于设计制造专业5G设备以及网络建设，单独建立一张全新的5G网络，可以满足超高视频、VR直播等对移动带宽需求大的业务，而无人驾驶、工业自动化等需要高可靠连接的业务也有了网络保证。</p><p>5G第二个标准版本R16计划于2019年12月完成，2020年3月冻结，全面满足eMBB、uRLLC、大连接低功耗场景mMTC 等各种场景的需求。可以说，预计2020年3月形成的5G标准才是完整的5G标准。<strong>5G技术标准由3GPP确定之后，还需要经过ITU认定。</strong></p><h3 id="解读3GPP-R15"><a href="#解读3GPP-R15" class="headerlink" title="解读3GPP R15"></a>解读3GPP R15</h3><p><strong>2018年6 月14日， 3GPP 全会批准了首个5G独立组 网（SA）标准，这意味着3GPP首个完整的5G标准R15正式落地，5G产业链进入商用阶段。</strong>3GPP正式最终确定5G第二阶段标准（R16）的15个研究方向。</p><table><thead><tr><th>序号</th><th>研究方向</th><th>研究内容</th></tr></thead><tbody><tr><td>1</td><td>MIMO的进一步演进</td><td>多用户MU-MIMO、Mutil-TRP和波束管理增强。</td></tr><tr><td>2</td><td>52.6GHz以上的新空口</td><td>将对5G系统使用 52. 6GHz 以上的频谱资源进行研究。</td></tr><tr><td>3</td><td>5G NR双链接完善</td><td>新增异步NR-NR双链接方案研究。</td></tr><tr><td>4</td><td>无线接入/无线回传一体化</td><td>3GPP将在R16阶段继续研究并考虑无线接入/无线回传一体化设计。</td></tr><tr><td>5</td><td>工业物联网</td><td>5G第二阶段标准（R16）将进一步研究URLLC（超高可靠与低时延信）增强来满足诸如“工业制造”“电力控制” 等更多的5G工业物联网应用场景。</td></tr><tr><td>6</td><td>5G新空口移动性增强</td><td>包括提高移动过程的可靠性、缩短由移动导致的中断时间。</td></tr><tr><td>7</td><td>基于5G新空口的V2X</td><td>研究基于5G新空口的V2X技术，使得其满足由SA1定义的“高级自动驾驶”应用场景，与LTE V2X形成“互补”。</td></tr><tr><td>8</td><td>5G新空口的新型定位方式</td><td>研究更精确的定位技术，包括“RAT-dependent” 以及混合定位技术。</td></tr><tr><td>9</td><td>非正交多址接入NOMA</td><td>面向5G的NOMA有多种候选技术。而R16将研究潜在的技术方案并完成标准化工作。</td></tr><tr><td>10</td><td>5G NR-U</td><td>在5G第二阶段标准（R16）中，5G NR-U需可利用非授权频谱提升5G系统容量。</td></tr><tr><td>11</td><td>非地面5G网络</td><td>研究面向“非地面5G网络”的物理层的控制机制、随机接入和HARQ切换、系统架构等。</td></tr><tr><td>12</td><td>远程干扰管理+交叉链路干扰抑制</td><td>5G第二阶段标准（R16）将研究如何识别造成强干扰的远端5G基站，以及如何进行干扰抑制。</td></tr><tr><td>13</td><td>5G终端能力</td><td>5G第二阶段标准（R16）将研究5G终端上报“终端能力”并降低5G终端上报信令开销的方法。</td></tr><tr><td>14</td><td>5G新空口以无线接入网为中心的数据收集与利用</td><td>5G第二阶段标准（R16）将研究SON、MDT等技术。</td></tr><tr><td>15</td><td>5G新空口终端功耗</td><td>5G第二阶段标准（R16）将研究5G终端工作在“CONNECTED”模式下如何降低功耗。</td></tr></tbody></table><p>2018年6月发布的SA标准完成了5G核心网架构，实现了5G独立组网。<strong>此次独立组网标准的冻结，让5G确定了全新的网络架构和核心网组网方式，让网络向IT化、互联网化、极简化、服务化转变。</strong></p><p><strong>在IT化方面，全软件化的核心网实现了统一的IT基础设施和调度。功能软件化、计算和数据分离是代表性的技术。</strong>传统“网元”重构为5G的“网络功能”，以“软件”的形式部署，充分发挥云化、虚拟化技术的 优势。将处理逻辑和数据存储分离，更便于提升系统的可靠性、动态性、大数据分析的能力。</p><p><strong>在互联网化方面，从固定网元、固定连接的刚性网络到动态调整的柔性网络。服务化架构（SBA，Service- based Architecture）、新一代核心网协议体系（基于HTTP2. 0/JSON）是其代表性技术。</strong>SBA的设计是由模块化、可独立管理的“服务”来构建的。服务可灵活调用、灰度发布，实现网络能力的按需编排和快速升级。传统电信特有的接口协议代之以互联网化的API调用，使得5G网络更加开放、灵活。</p><p><strong>在极简化方面，极简的转发面提高性能，集中灵活的控制面提升效率。C/U分离（控制面和用户面离）、新型移动性及会话管理是其代表性技术。</strong>通过C/U分离，一方面实现控制面集中部署、集中管控、集中优化，另一方面实现用户面功能简化，实现高效、低成本、大流量的数据转发。移动性管理和会话管理解耦，使得终端可以按需建立会话连接，节省了网络地址和存储资源。同时，针对不同的终端类型定义了多种类型的移动性管理，简化了终端和网络的状态。</p><p><strong>在服务化方面，从通用化服务到个性化、定制化服务。网络切片、边缘计算是其代表性技术。</strong>网络切片提供定制化、逻辑隔离、专用的端到端虚拟移动络（包括接入网、核心网），是5G面向垂直行业、实现服务可保障的基本技术形式。而边缘计算将网络的功能应用靠近用户部署，使得极致的低时延、本地特色应用成为可能，是5G满足如智能工厂等垂直行业业务需求的重要基础。</p><p>同时，在无线侧，5G NR为<strong>设计、架构、频段、天线</strong>4个方面带来新变化。</p><p><strong>在设计上，与以往通信系统不同，通信行业和垂直行业的跨界融合是5G发展的关键之一</strong>。为满足垂直行业的各种差异性需求，并应对部署场景的多样性与复杂性，5G在帧结构等方面提出了全新的设计。与4G相比，5G提供了更多可选择的帧结构参数，可根据5G基础通信业务、物联网和车联网等多样化应用场景，以及宏基站、小基站等不同网络部署需求灵活地配置，通过“软件定义空口”的设计理念使无线信号“量体裁衣”，通过同一个空口技术来满足5G多样化的业务需求，大幅提升5G网络部署的效率。</p><p><strong>在架构上，为了使组网方式更加灵活并提升网络效率，5G引入了接入网CU/DU分离的无线接入网架构，</strong>可将基站的功能分成实时处理的DU部分和非实时处理的CU部分，从而使得中心单元CU可以部署到集中的物理平台，以承载更多的小区和用户，提升了小区间协作和切换的效率。</p><p><strong>在频段上，5G系统需要不同频段来共同满足其覆盖、容量、连接数密度等关键性能指标要求。因此，与4G不同的是，5G通过灵活的参数设计（子载波间隔和CP长度等），可支持更大范围的频率部署，</strong>包括6GHz以下以及6GHz以上的毫米波频段。其中，6GHz以下频段主要用于实现5G系统的连续广域覆盖，保证高移动性场景下的用户体验以及海量设备的连接；而6GHz以上频段能够提供连续较大宽，可满足城市热点、郊区热点与室内场景极高的用户体验速率和极高容量需求。</p><p><strong>在天线上，5G支持大规模天线大幅度提升系统效率。</strong>大规模天线实现三维的波束赋形，形成能量更集中、覆盖更立体、方向更精准的波束。在大规模天线的架构下，波束扫描与波束管理等多个5G先进技术成为可能，网络覆盖及用户体验的顽健性可得到进一步的提升，实现更好的控制信道和业务信道的覆盖平衡。</p><h2 id="我国提出的5G目标"><a href="#我国提出的5G目标" class="headerlink" title="我国提出的5G目标"></a>我国提出的5G目标</h2><p>随着IMT-2020（5G）推进组发布5G试验第三阶段规范，5G预商用开始进入倒计时。此前在2017年12月1日，3GPP的5G第一个标准冻结，打响了全球5G市场竞赛的发令枪。在全球产业链的共同推动下，5G商用时间点不断被提前。</p><p>我们移动通信领域在经历了“<strong>2G追赶，3G突破，4G并进</strong>”的进阶之后，在即将到来的5G时代，我国通信业正在酝酿一出更加精彩的大戏—“<strong>5G引领</strong>”，这是中国移动通信产业提出的新目标。要实现5G引领，从2017年年底到今年年初各方面的一系列动作看，我国正在政策引导、频率规划和技术创新等多方面协同发力。</p><p><img src="https://i.loli.net/2019/05/27/5cebfddf65c7e50019.jpg"></p><p>当前，全球多国正积极筹备5G试商用。日前，美国运营商AT＆T已经明确宣布：“2018年将会在十余个美国城市首先推出5G服务，其部署的5G将是3GPP不久前刚刚批准的5G标准。”亚洲其他国家也已经宣布5G商用时间表，其中，韩国将于2018年平昌冬奥会期间实现5G预商用，而日本预计将于2020年为东京奥运会提供5G商用服务。</p><p>目前，国内布局5G的步伐还在不断加快，中国5G第三阶段试验大幕已经拉开。<strong>第三阶段的重点是面向5G商用前的产品研发、验证和产业协同，开展商用前的设备单站、组网、互操作，及系统、芯片、仪表等产业链上下游的互联互通测试，全面推进产业链主要环节基本达到预商用水平。</strong>目前，根据国内三大运营商规划，2018年已经开始陆续在主要城市进行5G试验，2019年则进行规模试商用，2020年正式开始商用部署。中国移动前期预计在若干城市建设每城20个基站的预商用试验网，中国电信表示将在2018年之前完成原型无线组网的验证阶段，目前在广东深圳、成都、兰州、江苏苏州、上海、河北雄安六地启动中国电信5G示范网试验。中国联通目前正在加快推进相关研究工作，计划2018年在多个城市启动5G外场试验工作，2019年进一步扩大试验规模。</p><p><strong>作为5G发展的基础性资源，频谱对5G商用进展有至关重要的作用。</strong>2017年11月，我国率先发布了5G系统在中频段频谱使用规划，明确将3300-3400MHz（原则上限室内使用）、3400-3600MHz和4800-5000MHz频段作为5G系统的工作频段。与之前2G、3G、4G相比，5G具备远超以往的带宽、更高的速率，且同时支持千亿级物联网设备的连接，5G所需频谱数量也远超之前几代移动通信之和。与此同时，为了实现移动宽带、低时延、超大规模组网三大应用场景，5G系统在规划之初就确定了“<strong>全频段”，需要从高频、中频、低频统筹规划。</strong></p><p>在低频段大多为现有2G、3G和4G占用的情况下，在中频段上，3.5GHz频段因为有利于信号覆盖，被全球多个国家视为5G网络的先锋频段。目前，<strong>我国已为IMT分配522MHz，低频段频谱需求808-1078MHz，频谱缺口300-500MHz。</strong></p><p>在中国IMT-2020（5G）推进组的领导下，以中国移动为代表的中国企业发挥了重要的作用，贡献的文稿数占整个项目文稿数的半壁江山。<strong>5G系统架构（5GS）项目由中国移动担任报告人主导完成，并得到全球超过67家合作伙伴的大力支持，是中国人首次牵头设计新一代移动网络的系统架构。</strong>在全球运营商中，中国移动的文稿贡献数和通过数都排在第一位。</p><p><img src="https://i.loli.net/2019/05/27/5cebfe022ec7763508.jpg"></p><p>还有，华为在5G核心技术上作出了与其市场体量相匹配的创新贡献，在5G编码技术、多址技术、空口技术、天线技术、网络架构、物联网接入、用户体验保证上都有原创型技术，这些原创技术在3GPP前一阶段的5G关键技术“选美”中获选，随着标准的冻结，固化为国际标准。</p><p>中兴近年来也一直致力于对包括Massive MIMO、MUSA（多用户共享接入）、FB-OFDM（滤波器组OFDM）、虚拟和网络分片等在内的核心5G技术进行研发，并携手产业链合作伙伴，共同推动5G研发进程。</p><p><strong>从2G到5G，中国实现了从追赶走向引领，在通信领域前所未有地接近世界大格局的中央。5G引领，符合国家“强国战略，中华民族伟大复兴”的战略目标，需要政府、运营商、设备商以及社会各行各业发生共振效应，共同努力才能实现。整个过程任重而道远，需要我们不忘初心，砥砺前行。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;5G：第五代移动通信技术的简称&lt;/strong&gt; ，4G技术的延伸，弥补了4G技术的不足，在吞吐率、时延、连接数量、能耗等方面进一步提升系统性能。 5G致力于应对2020年后多样化差异化业务的巨大挑战，满足超高速率、超低时延、高速移动、高能效和超高流量与连接数密度等多维能力指标。
    
    </summary>
    
      <category term="5G解决方案" scheme="https://kkutysllb.cn/categories/5G%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    
    
      <category term="5G" scheme="https://kkutysllb.cn/tags/5G/"/>
    
  </entry>
  
</feed>
