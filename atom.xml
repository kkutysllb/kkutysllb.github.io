<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一花一菩提，一云一世界</title>
  
  <subtitle>佛系ICT人士技术博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://kkutysllb.cn/"/>
  <updated>2020-03-03T15:22:31.153Z</updated>
  <id>https://kkutysllb.cn/</id>
  
  <author>
    <name>kkutysllb</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-03-03-计量和监控管理服务Telemetry</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E8%AE%A1%E9%87%8F%E5%92%8C%E7%9B%91%E6%8E%A7%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Telemetry/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-计量和监控管理服务Telemetry/</id>
    <published>2020-03-03T15:14:16.000Z</published>
    <updated>2020-03-03T15:22:31.153Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Telemetry是OpenStack的计量与监控服务，用来获取和保存计量与监控的各种测量值，并根据测量值进行报警。同时这些保存下来的测量值也可以被第三方系统获取，用来做更进一步的分析、处理或展示。早期OpenStack的计量和监控服务由Ceilometer独自完成，将原始计量数据存储在MongoDB中，性能方面可以说是极烂，社区的活跃度也低，基本到了快被放弃的边缘。随着Gnocchi的出现，Gnocchi的出现使计量服务不论是在性能还是空间消耗上都有了质的飞跃，功不可没，从而挽救了Ceilometer这个项目。后续，OpenStack社区受自身架构的启发，针对计量和监控这块对Ceilometer项目做了拆分，Ceilometer专门为为OpenStack环境提供一个获取和保存各种测量值的统一框架，明确目标，同时将其告警功能拆分到aodh项目，采集数据存储到gnocchi时间序列数据库，事件相关的服务拆分到panko项目，相关数据仍存储在MongoDB中。因此，现在OpenStack的计量和监控项目就是一个“小帐篷”（“大帐篷”就是OpenStack自身），包括了：Ceilometer、Aodh、Gnocchi和Panko，统称为Telemetry。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ExSJFEjaLoCi.png?imageslim" alt="mark"></p><p>Gnocchi是Telemetry项目的中的“灵魂”，是一个多租户时间序列，计量和资源数据库。提供了REST API接口来创建和操作数据，用于超大规模计量数据的存储，同时向操作者和用户提供对度量和资源信息的访问。Gnocchi需要与Ceilometer对接传递计量数据，而Ceilometer作为一个框架，处于核心位置，它像一个漏斗一样，能把OpenStack内部发生的几乎所有的事件都收集起来，为计费和监控以及其它服务提供数据支撑。Ceilometer服务及其剥离出来的服务由四个组件构成，四个组件分工明确，各司其职，使得计量系统结构清晰明了。</p><ul><li><strong>Gnocchi：</strong>时间序列数据库，保存计量数据。</li><li><strong>Panko：</strong>事件数据库，保存事件数据。</li><li><strong>Ceilometer：</strong>数据采集服务，采集资源使用量相关数据，并推送到Gnocchi，采集操作事件数据，并推送到Panko。</li><li><strong>Aodh：</strong>告警服务，基于计量和事件数据提供告警通知功能。</li></ul><h2 id="Ceilometer架构"><a href="#Ceilometer架构" class="headerlink" title="Ceilometer架构"></a><strong>Ceilometer架构</strong></h2><p>Ceilometer框架整体采用了高度可扩展性的设计思想，其逻辑架构如下图所示，Ceilometer通过通知代理和轮询代理获取测量值，经流水线的发布者发布给收集器或外部系统。收集器继而将收到的测量值保存到数据库中，同时，外部系统也可以通过Ceilometer API将测量值送达Ceilometer数据库。Ceilometer告警由告警评估器触发，发送给告警通知器，同时调用Ceilometer API，最终将告警保存到Ceilometer数据库中。此外，告警通知器可以将告警发送给外部系统，外部系统也可以通过Ceilometer API新建告警发送给Ceilometer。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/jWEFdXpeTBnU.png?imageslim" alt="mark"></p><p>Ceilometer可以通过以下3种方式获取测量值数据，其数据采集机制有通知、轮询和REST ful API方式3种：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/v7PgO9DgRgeP.png?imageslim" alt="mark"></p><ul><li><strong>通知：</strong>所有的OpenStack服务都会在执行了某种操作或者状态变化时发送通知消息到oslo-messaging（OpenStack整体的消息队列框架），一些消息中包含了计量所需要的数据，这部分消息会被Ceilometer的ceilometer-agent-notification服务组件处理，并转化为samples。通知数据采集方式是被动地采集计量数据。</li><li><strong>轮询：</strong>Ceilometer中的服务组件根据配置定期主动地通过OpenStack服务的API或者其他辅助工具（如Hypervisors）去远端或本地的不同服务实体中获取所需要的计量数据；Ceilometer的轮询机制通过3种类型的代理实现，即ceilometer-agent-central、ceilometer-agent-compute和ceilometer-agent-ipmi服务组件。每种代理使用不同的轮询插件（pollster）从不同的命名空间来收集数据。</li><li><strong>REST ful API：</strong>用户可以通过调用RESTful API直接把利用其他方式获取的任意测量数据送达给Ceilometer。</li></ul><p>Ceilometer通过以上3种方法获取到测量值数据后，会把它转化为符合某种标准格式的数据采样（Sample）通过内部总线发送给Notification agent。然后Notification Agent根据用户定义的Pipeline来对数据采样进行转换（Transform）和发布（Publish）。如果根据Pipeline的定义，这个数据采样（Sample）最后被发布给Collector的话，Collector会把这个数据采样保存在数据库中。Ceilometer的计量数据经过数据采集（agent）、数据处理（流水线数据转换及发布）、数据存储（collector）几个步骤，各个步骤有着各自的处理流程。</p><h2 id="Ceilometer计量数据采集和转换发布机制"><a href="#Ceilometer计量数据采集和转换发布机制" class="headerlink" title="Ceilometer计量数据采集和转换发布机制"></a><strong>Ceilometer计量数据采集和转换发布机制</strong></h2><p>Ceilometer的各个服务中，负责计量数据采集的服务组件有4个，分别是agent-notification、agent-central、agent-compute、agent-ipmi。其采集数据的方式各有不同，主要分为Poll和Push两种方式，如下图所示，Poll即轮询方式，主动采集数据，Push即通知方式，被动获取数据。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/c8nGYYMQ1Fez.png?imageslim" alt="mark"></p><ul><li><p><strong>agent-notification</strong>服务采用Push方式被动获取数据，即通过监听OpenStack通知总线（Notification Bus）上的通知消息来获取数据。OpenStack中各个模块在执行了某种操作或者状态变化时都会推送通知信息到oslo-messaging消息框架。agent-notification通过访问这个消息队列服务框架，获取相关通知信息，并进一步转化为采样数据的格式。agent-notification服务的运行流程如下：</p><p><strong>Step1：</strong>解析Pipeline配置文件得到Pipeline定义。</p><p><strong>Step2：</strong>调用stevedore库，载入所有的notification listener插件。</p><p><strong>Step3：</strong>对每一个notification listener插件通过oslo.messaging库构造其对应的oslo.messaging库的notification listener对象，并且启动此对象监听通知消息。</p><p><strong>Step4：</strong>当通知总线上有某个notification listener插件所感兴趣的通知到达时，所对应的notification listener插件就会被agent-notification调用，根据此通知消息构造出采样值Sample，然后根据Pipeline中的定义将此采样值转换和发布出去。</p></li><li><p><strong>agent-central</strong>服务可以部署在任何节点上，通常部署在控制节点上，它用来和远程的各种不同的实体和服务进行通信，获取不同的测量值。agent-central主要通过调用各OpenStack服务的REST API来获取OpenStack服务的各种信息，以及通过SNMP来获取Hardware资源的信息，通过kwapi来收集设备能耗数据。</p></li><li><p><strong>agent-compute</strong>服务需要部署在运行nova-compute服务的计算节点上，主要用来收集计算节点上的虚拟机实例的计量数据，在每一个计算节点上都要运行这个服务组件。agent-compute主要是用来和Hypervisor进行通信，通过调用Hypervisor的API来获取相关测量值，需要定期Poll轮询收集信息。</p></li><li><p><strong>agent-ipmi</strong>服务需要部署在支持IPMI的计算节点上，该计算节点需要安装ipmitool工具。agent-ipmi服务通过ipmitool收集本机IPMI传感器（Sensor）的数据，以及Intel Node Manager的数据。</p></li><li><p>agent-central、agent-compute、agent-ipmi都属于Polling代理，Polling代理的作用是根据Pipeline的定义，周期性地调用不同的Pollster插件去轮询获得Pipeline中定义的测量值，再根据Pipeline的定义，对这些采样值进行转换和发布。各种不同的Polling代理的运行流程基本类似，如下：</p><p><strong>Step1：</strong>调用stevedore库，获取属于本Agent的所有Pollster插件。</p><p><strong>Step2：</strong>创建PartitionCoordinator类实例对象，加入一个partition group，并创建一个定时器用来周期性地发送心跳消息。</p><p><strong>Step3：</strong>解析Pipeline配置文件得到Pipeline的定义，并根据解析的结果创建一个或多个不同的PollingTask和所对应的定时器。由于所有采样频率相同的Pipeline都会在同一个PollingTask里处理，所以每个PollingTask都由某个特定频率的定时器驱动，在某一个线程中被执行。</p><p><strong>Step4：</strong>由定时器驱动的PollingTask会周期性地调用其所包含的各种Pollster，由这些Pollster获取测量数据，然后根据Pipeline的定义，把获取的测量取样值交给Transformer转换后，再由Publisher发布。</p></li></ul><p>Ceilometer的计量数据处理采用了Pipeline机制。Ceilometer适用于不同应用场景下的测量数据，采样频率可能有不同的要求，例如：用于计费的数据采用频率比较低，一般为10~30min，而用来监控的数据采用的频率就会比较高，一般会达到1~10s。除此之外，对于测量数据的发布方式，不同应用场景也有不同的要求，对于计费数据要求数据采样值不能丢失，而对于用来监控的数据则要求不那么严格。为此，Ceilometer引入了Pipeline的概念来解决采样频率和发布方式的问题。Pipeline由源（Source）和目标（Sink）两部分组成。源中定义了需要测量哪些数据、数据的采样频率、在哪些端点上进行数据采样，以及这些数据的目标。目标中定义了获得的数据要经过哪些Transformer进行数据转换，并且最终交由哪些Publisher发布。Ceilometer中同时允许有多个Pipeline，每个Pipeline都有自己的源和目标，这就解决了不同采样频率、不同发布方式的问题。Ceilometer的数据处理流程如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/FG6h2hBRB0sl.png?imageslim" alt="mark"></p><p>Transformer可以针对一个或者多个同一种类的数据采样值进行各种不同的操作，例如改变单位、聚合计算等，最终转换成一个或者多个其他不同种类的测量数据。通过Transformer转换后的数据最终会交由Pipeline定义中的Publisher进行数据发布，不同的Publisher发布的数据可以有不同的数据接收者，可以是Ceilometer的collector服务，也可以是外部系统。</p><p>Ceilometer获得的测量数据通过Pipeline发布后，需要有一个数据接收者获得这些数据并且保存下来，以便对数据进行进一步的处理。Ceilometer的collector服务就是用来接收这些测量数据的，并最终持久化到存储介质中。collector服务可以配置一个或多个dispatcher，对于每一个collector所接收到的采样数据，collector会调用所有配置的dispatcher，由这些dispatcher来决定如何处理数据。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/PcS51xRBiQVB.png?imageslim" alt="mark"></p><p>Ceilometer支持4种不同的数据库后台：MongoDB、MySQL、PostgreSQL和Hbase。不同的数据库后台，所支持的功能和性能有所不同，比如采用MongoDB作为后台数据库，除了一般的最大值、最小值、平均值、累加总和、总数之外，还支持求标准方差和求基数两个操作。虽然Ceilometer的默认后台数据库是MySQL，但是历史上一般建议在实际生产环境中采用MongoDB为后台数据库。由于性能等方面的原因，目前Telemetry社区建议使用Gnocchi代替上面所说的数据库来保存采样数据。</p><h2 id="aodh、panko和gnocchi"><a href="#aodh、panko和gnocchi" class="headerlink" title="aodh、panko和gnocchi"></a><strong>aodh、panko和gnocchi</strong></h2><p>在OpenStack Liberty版本中，为了适应更灵活的部署方案，Telemetry社区把Ceilometer项目中和报警相关的功能剥离出来，成立了一个新的项目Aodh，主要是提供基于Ceilometer所获取的测量值或者Event事件进行报警的功能。Aodh的基本体系结构主要由以下几种服务构成，每种服务都是可水平扩展（scale out）的。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/NjWsVRQjxbko.png?imageslim" alt="mark"></p><ul><li><strong>API：</strong>主要提供面向用户的RESTful API接口服务。</li><li><strong>Alarm Evaluator：</strong>用来周期性的检查除了event类型之外其他警告器（Alarm）相关的告警条件是否满足。</li><li><strong>Alarm Listener：</strong>根据消息总线（Notification Bus）上面的Event事件消息，来检查相对应的event类型的警告器（Alarm）告警条件是否满足。</li><li><strong>Alarm Notifier：</strong>当警告器的告警条件满足时，执行用户定义的动作。</li></ul><p>对于每一种警告器的状态，在新建或者修改警告器的时候，都可以为其设置不同的报警动作。当Alarm Evaluator周期性检查警告器状态时，或者当Alarm Listener接收到相关的Event事件并进行检查后，如果发现当前警告器状态有对应的报警动作，那么它会通过Alarm Notifier服务来调用相应的报警动作。</p><p>在OpenStack Newton版本中，Telemetry社区把Ceilometer关于Event事件部分的API移到了Panko项目中。目前，Event事件的产生和保存仍旧是由Ceilometer负责，但是对于从后台数据库中读取Event事件的API service 被移到了Panko项目中（Ceilometer API中与Event相关的API目前还保留着）。Panko主要接收来自Ceilometer的事件数据，并提供查询接口。其主要组件只有一个：panko-api:，用于提供事件数据的插入和查询接口。</p><p>Gnocchi接收来自Ceilometer的原始计量数据，进行聚合运算后保存到持久化后端。Gnocchi中保存与资源使用量相关的计量数据，为了提高检索效率，额外将计量数据的元数据信息单独存储。另外由于从原始输入数据到最终存储的聚合数据，需要进行大量计算，为了缓冲输入与处理间的速率，引入缓存后端。因此Gnocchi中涉及三种存储后端：</p><ul><li><strong>索引后端：</strong>存储计量对象和采集项的基础属性，比如对象类型（虚拟机、硬盘、网络）、原始资源uuid等。索引数据量不大，一半用MySQL。</li><li><strong>聚合数据后端：</strong>存储经过聚合计算的计量数据，比如cpu使用率的平均值、最大值、最小值等。推荐用Ceph，可以支持多实例共享数据。</li><li><strong>传入数据后端：</strong>保存来自Ceilometer的原始计量数据。默认与聚合后端一致，推荐使用Redis。</li></ul><p>gnocchi的主要组件如下：</p><ul><li><strong>gnocchi-api：</strong>提供数据传入接口，接收原始计量数据，并将它们保存到传入数据后端。同时提供聚合计量数据的查询接口，从聚合数据后端读取计量数据返回给用户。</li><li><strong>gnocchi-metricd：</strong>从传入数据后端读取原始计量数据，进行聚合计算，然后将聚合数据保存到聚合数据后端。</li></ul><h2 id="Telemetry操作实战"><a href="#Telemetry操作实战" class="headerlink" title="Telemetry操作实战"></a><strong>Telemetry操作实战</strong></h2><p><strong>步骤1：</strong>执行以下指令，查看归档策略</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack metric archive-policy list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HcPBilyMX68F.png?imageslim" alt="mark"></p><p>归档策略指定计量数据的聚合计算方式，包括聚合方法（mean、min、max、sum、std、count），和计量数据的统计时间粒度、数据点数。根据时间粒度和统计点数可以确定计量的时间跨度，根据一个数据点的大小在0.05 bytes~8.04 bytes，就可以确定一个metric需要使用的存储空间大小。Gnocchi内置了bool、low、medium、high四种归档策略。</p><p><strong>步骤2：</strong>执行以下指令，查看归档策略规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack metric archive-policy-rule list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/L6FTRP7nM3Jd.png?imageslim" alt="mark"></p><p>归档策略的作用单位是计量项（metric），上述默认规则将所有metric关联到low策略，metric使用通配符匹配。</p><p><strong>步骤3：</strong>执行以下指令，查看资源列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack metric resource list --<span class="built_in">limit</span> 10</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5ievLhgX3Yzo.png?imageslim" alt="mark"></p><p>资源就是对于OpenStack各个项目中的逻辑资源，比如实例、端口、镜像、卷等。</p><p><strong>步骤4：</strong>执行以下指令，查看metric列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack metric metric list --<span class="built_in">limit</span> 10</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/VRC3MQoxq4sc.png?imageslim" alt="mark"></p><p>Metric是资源统计的基本单位，一个资源会有多个metrics，比如实例资源有cpu_util、memory.usage、disk.root.size等。</p><p><strong>步骤5：</strong>执行以下指令，查看metric列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack metric measures show 00b09a5c-3c7f-445e-b885-872db5d80b2b</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/o9KswPsjLACk.png?imageslim" alt="mark"></p><p>Measures就是Gnocchi中最终保存的计量数据，即每个metric的数据点。</p><p><strong>步骤6：</strong>执行以下指令，创建告警：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack alarm create --name cpu_high --<span class="built_in">type</span> gnocchi_resources_threshold --description <span class="string">'Instance Running HOT'</span> --metric cpu_util --threshold 65 --comparison-operator ge --aggregation-method mean --granularity 300 --resource-id 07b49533-64fd-4e53-a6c7-9824c03e043a --resource-type instance --alarm-action <span class="string">'log://'</span> --ok-action <span class="string">'log://'</span></span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5nskViw0CuuB.png?imageslim" alt="mark"></p><p>这里设置触发器状态变为alarm和ok时都执行log动作，即记录到aodh-notifier日志中。可以将log://替换为外部告警接口，触发邮件、短信等通知，或者heat的扩容接口，实现服务自动扩容。</p><p>—————————————————————– 结束 ——————————————————————————</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Telemetry是OpenStack的计量与监控服务，用来获取和保存计量与监控的各种测量值，并根据测量值进行报警。同时这些保存下来的测量值也可以被第三方系统获取，用来做更进一步的分析、处理或展示。早期OpenStack的计量和监控服务由Ceilometer独自完成，将原始计量数据存储在MongoDB中，性能方面可以说是极烂，社区的活跃度也低，基本到了快被放弃的边缘。随着Gnocchi的出现，Gnocchi的出现使计量服务不论是在性能还是空间消耗上都有了质的飞跃，功不可没，从而挽救了Ceilometer这个项目。后续，OpenStack社区受自身架构的启发，针对计量和监控这块对Ceilometer项目做了拆分，Ceilometer专门为为OpenStack环境提供一个获取和保存各种测量值的统一框架，明确目标，同时将其告警功能拆分到aodh项目，采集数据存储到gnocchi时间序列数据库，事件相关的服务拆分到panko项目，相关数据仍存储在MongoDB中。因此，现在OpenStack的计量和监控项目就是一个“小帐篷”（“大帐篷”就是OpenStack自身），包括了：Ceilometer、Aodh、Gnocchi和Panko，统称为Telemetry。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-资源编排管理服务Heat</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E8%B5%84%E6%BA%90%E7%BC%96%E6%8E%92%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Heat/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-资源编排管理服务Heat/</id>
    <published>2020-03-03T15:00:36.000Z</published>
    <updated>2020-03-03T15:13:50.959Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Heat是OpenStack项目中实现资源编排的子项目，它的目的是帮助用户实现在OpenStack云环境下手工资源创建、配置、应用部署、弹性伸缩等过程的自动化，从而提高OpenStack应用的效率，比如华为云Stack6.5.1版本的Deploy就是通过Heat的编排模版完成FusionSphere OpenStack的部署，华为的FusionSphere OpenStack部署和配置的复杂度只要实践过都知道，那么如果不依靠编排去自动部署而是手动部署，那种酸爽可想而知。因此，Heat这种编排类的系统或软件出现，就解决了这种复杂系统重复部署效率低下的问题。由于OpenStack缘起于AWZ Cloud，因此Heat至今仍保留着 AWZ Cloud的编排REST API请求接口，并在功能上继承AWZ的Formation（现在甚至逐渐在超越）。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6WoaqU55Giqv.png?imageslim" alt="mark"></p><p>对于Heat的功能和实现，简单来说就是用户可以预先定义一个规定格式的任务模版，任务模版中定义了一连串的相关任务（例如用什么配置开几台虚拟机，然后在其中一台中安装一个mysql服务，设定相关数据库属性，然后再配置几台虚拟机安装web服务群集等等），然后将模版交由Heat执行，就会按一定的顺序执行heat模版中定义的一连串任务。因此，Heat向开发人员和系统管理员提供了一种简便地创建和管理一批相关资源的方法，并通过有序且可预测的方式对其进行资源配置和更新。我们介绍Heat服务也是重点介绍如何使用Heat的标准示例模板或自己创建模板来设定业务资源以及业务程序运行时所需的所有相关依赖项或运行时参数。</p><h2 id="Heat的架构"><a href="#Heat的架构" class="headerlink" title="Heat的架构"></a><strong>Heat的架构</strong></h2><p>Heat项目总体上可以看作由Heat模板文件和Heat执行引擎两部分。其中Heat模板定义了资源部署流程（资源可以是虚拟机实例，也可以是网络、IP、镜像、用户），而Heat执行引擎通过OpenStack标准的API与其他组件交互协作，不仅实现了云环境中资源依赖关系处理、资源的初始化、资源自动部署等针对资源的基本操作，还能实现弹性伸缩、负载均衡等服务配置操作。下图就是Heat的逻辑架构，Heat的逻辑架构主要由Heat-Client、Heat-API、Heat-Engine以及Service-Client（本质上是Plugin）组成。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/eUbr9RSKjDHw.png?imageslim" alt="mark"></p><ul><li><strong>Heat-Client：</strong>是Heat的命令行工具，提供Heat-API的命令行访问模式，安装Heat的Dashboard后，Horizon也可以作为Heat-Client，但是支持的功能较少。</li><li><strong>Heat-api：</strong>为Heat-engine提供OpenStack风格的Rest查询接口，通过消息队列与Heat-engine交互。同时，Heat也支持AWS Cloud的REST API请求，通过Heat-api-cfn接收。</li><li><strong>Heat-engine：</strong>Heat的核心，实现自动化部署的主要工作，包括定义模板、解析模板、资源依赖分析、资源部署调用等，以及提供相应的回调接口。</li><li><strong>Service-Client：</strong>严格来说不属于Heat的组件，主要在Heat-Engine中通过插件方式实现OpenStack的其他服务的Client，来调用其他服务创建资源。</li></ul><p>这里需要重点提一下Heat-api和Heat-Engine两个组件。Heat API包括Heat-api和Heat-api-cfn，Heat-api主要接收OpenStack自身的对HOT（Heat Orchestration Template）模版的API编排请求，为Heat-Engine提供操作OpenStack自身的资源类型的接口，如OS:: XX:: XXXX格式的资源。Heat-api-cfn主要接收AWS对Formation模版的API编排请求，为Heat-Engine提供操作AWS CLoud的资源类型接口，如AWS:XX:XXXX格式的资源。两种云OS的操作类型，在OpenStack的Heat中均支持，且能进行相关查询，如下图示。<strong>Heat API的调用授权管理遵循OpenStack中的标准，采用Keystone进行统一管理。</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/KXaETNnfMEpq.png?imageslim" alt="mark"></p><p>Heat-Engine作为Heat服务的核心，实现模版的定义、解析、资源依赖分析和部署调用，是整个Heat服务的“灵魂”。整体上如下图所示，Heat-Engine分为三层：<strong>模版解析层、资源依赖分析层</strong>和<strong>资源实现层</strong>。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/0qluqOkqj1r7.png?imageslim" alt="mark"></p><ul><li><strong>模版解析层：</strong>最上面那层，处理Heat-api层面的请求，就是根据模板和输入参数来创建Stack，这里的Stack其实就是各种模版实现逻辑以及模版之间的嵌套关系，包含计算、存储、网络、镜像、裸机等包含各种资源的集合和约束关系。Heat-Engine首要工作就是是完成模板格式校验，此功能由Heat-Engine的cfn和hot组件完成，分别对应AWS和HOT类型的两种模板格式。</li><li><strong>资源依赖分析层：</strong>解析Stack里各种资源的依赖关系以及模版和嵌套模版的关系，通过Heat-Engine的Resource模块中的stack组件完成的，具体为检测模板定义的ref和deponds_on属性的有效性和关系型，将所有资源形成一个有效的资源关系图。</li><li><strong>资源实现层：</strong>根据解析出来的资源的依赖关系，依次调用各种服务客户端来创建各种资源。Heat模板中的每一种资源都会对应一个有效的资源类型，引擎通过一个唯一的资源类型名称找到对应的资源实现，这个映射关系由资源的接入机制实现，具体为：<strong>扫描指定目录下的所有类，如果发现其中包含了资源类型实现映射函数resource_mapping（），并且指向一个有效的实现，则建立相应的映射关系，为后续的栈调度执行打下基础。</strong></li></ul><p>Heat-Engine除了上面分析的三层逻辑，还有内部重要的功能子组件，那就是<strong>调度引擎</strong>。它为模板的执行建立了一套完整的流程引擎，包括资源创建状态监听，资源创建状态回调通知等。采用Stack的方式存储流程定义，提供与其他组件的同异步交互控制机制，并负责Stack生命周期状态的管理，在出现异常时，还能进行相应的回滚操作，并控制资源的依赖关系，保证回滚操作的完成。那么既然调度引擎负责各种资源调度，必然少不了与OpenStack的其他服务进行交互，它们之间就是通过Service-Client进行交互。但是，一个服务或业务的编排各类资源的实现并不简单的串行流程，资源实现根据模版中的定义不仅有依赖关系，甚至可以并行完成。比如，A资源的实现需要依赖B资源创建完成（虚拟机实例的拉起需要网络资源首先完成虚拟交换机创建），或者A资源和B资源可以同步进行（某虚拟机实例需要两个网卡，那么网络资源编排时可以同时拉起两个虚拟交换机），而这些工作就是由调度引擎利用内部的OpenStack其他服务模拟Client通过消息队列AMQP向Nova、Cinder、Neutron等各个服务发起的，他们之间是一种RPC.CAST调用机制。既然是异步调用，那么Heat-Engine内部还需要有接收这些异步回调结果的模块来确认结果，并根据资源创建结果以及模版的定义决定下一步操作流程。</p><h2 id="Heat中的Stack"><a href="#Heat中的Stack" class="headerlink" title="Heat中的Stack"></a><strong>Heat中的Stack</strong></h2><p>资源编排项目Heat的主要目标，就是充分利用OpenStack中强大的自动化资源使用功能。通过Stack这一概念的引入，终端用户通过很简单的方式就可简单明了地组织自己的应用，而无须经历痛苦折磨的学习过程和各种“救火”经历。无论使用哪种格式的模板栈，终端用户所关心的，只是如何部署应用，并将整个部署过程看成一个独立单元。Heat使用的是由YAML语言定义的HOT（Heat Orchestration Template）格式模板，允许用户使用代码对OpenStack资源进行自动分配。HOT文件中的文本化描述语言充当了用户与应用环境之间的桥梁，OpenStack中资源栈的构建过程，就是从HOT文件中置备完整应用环境的过程。典型的HOT文件代码结构如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/tfpQYT41z60O.png?imageslim" alt="mark"></p><ul><li><strong>heat_template_version：</strong>这里指定了HOT文件所使用的模板语法版本，HOT模版起始标准版本是2013-05-23，也就是说HOT模版诞生在2013-05-23这个时间点，在这前都是AWS的版本编号。通过Heat的命令行工具或Dashboad可以查看当前Heat支持的版本类型，如下图为Dashboard上的查询结果，不同版本的模版虽然语法上保持向前兼容，但是功能上有些许变化，各版本支持的具体功能可以查看官网<a href="https://docs.openstack.org/heat/latest/template_guide/hot_spec.html#hot-spec。" target="_blank" rel="noopener">https://docs.openstack.org/heat/latest/template_guide/hot_spec.html#hot-spec。</a></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GyomFBlnD55Y.png?imageslim" alt="mark"></p><ul><li><strong>description：</strong>此处是关于该模板的详细描述，可选参数，可以不填写任何值，但是必须保留该字段，并且字段最后冒号”: “后面有一个空格（YAML语言格式）。</li><li><strong>parameters：</strong>此处声明输入列表，每一个参数都有给定的名称、类型、描述和默认值，且默认值是可选的。参数部分可以包括任何信息，例如一个特定的镜像或者用户指定的网络ID。parameters的使用示例如下，定义两个参数image_id和flavor，都是String类型，各自的标签（label）分别为Image ID和Instance Type，这里的标签（label）在resources模块中，可以通过内部函数get_param()直接引用。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HuWch2NCt7rH.png?imageslim" alt="mark"></p><ul><li><strong>resources：</strong>这里的资源可以看成是Heat需要创建或者在其操作中需要修改的对象，HOT文件中的resources代码段就是定义不同组件的地方，例如，resource_name为virtual_web的资源具有OS::Nova::Server的属性，这就指定了资源类型为Nova计算实例。资源还可被子属性列表扩展限定，例如，可以为virtual_web实例资源指定所使用的镜像、资源模板和私有网络等资源。resources的使用示例如下，资源名称为my_instance，资源类型为OS: :Nova: :Server，资源使用的道具有镜像image和虚拟机实例类型flavor。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/uV9DwL1834pB.png?imageslim" alt="mark"></p><ul><li><strong>outputs：</strong>在将资源栈部署到Heat engine后，可以将其属性全部导出。outputs的使用示例如下，输出虚拟机实例的ip地址instance_ip，输出值为通过内置函数get_attr()获取虚拟机实例的第一个IP地址。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GIrNNmLVzz7y.png?imageslim" alt="mark"></p><p>经过以上的分析，我们可以写出如下的批量拉起虚拟机实例的简单模板。该模板用于批量拉起三个虚拟机实例instance，定义了三个输入参数，分别是：image、flavor和internal_network，在create stack时通过–parameter Image ID=XXX，–parameter Instance Type=XXX和– parameter Internal_net=XXX指定。同时，定义了三个输出参数，分别输出三个虚拟机实例的首个网卡地址（在我们的模板中就定义一个虚拟机网卡，也可以定义多个），资源类型只使用了OS: :Nova: :Server。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">heat_template_version:</span> <span class="number">2018</span><span class="bullet">-08</span><span class="bullet">-31</span></span><br><span class="line"></span><br><span class="line"><span class="attr">description:</span> <span class="string">Simple</span> <span class="string">template</span> <span class="string">to</span> <span class="string">deploy</span> <span class="string">multiple</span> <span class="string">compute</span> <span class="string">instance</span></span><br><span class="line"></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> image_id:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  type:</span> <span class="string">string</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  label:</span> <span class="string">Image</span> <span class="string">ID</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  description:</span> <span class="string">Image</span> <span class="string">to</span> <span class="string">be</span> <span class="string">used</span> <span class="string">for</span> <span class="string">compute</span> <span class="string">instance</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> flavor:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  type:</span> <span class="string">string</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  label:</span> <span class="string">Instance</span> <span class="string">Type</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  description:</span> <span class="string">Type</span> <span class="string">of</span> <span class="string">instance</span> <span class="string">(flavor)</span> <span class="string">to</span> <span class="string">be</span> <span class="string">used</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> in_net:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  type:</span> <span class="string">string</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  label:</span> <span class="string">Internal_net</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  description:</span> <span class="string">IP</span> <span class="string">address</span> <span class="string">of</span> <span class="string">instance</span> <span class="string">(Internal_net)</span> <span class="string">to</span> <span class="string">be</span> <span class="string">used</span></span><br><span class="line"></span><br><span class="line"><span class="attr">resources:</span> </span><br><span class="line"></span><br><span class="line"><span class="attr"> instance01:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  type:</span> <span class="attr">OS::Nova::Server</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  properties:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   image:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">image_id</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   flavor:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">flavor</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   networks:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">in_net</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> instance02:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  type:</span> <span class="attr">OS::Nova::Server</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  properties:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   image:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">image_id</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   flavor:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">flavor</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   networks:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">in_net</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> instance03:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  type:</span> <span class="attr">OS::Nova::Server</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  properties:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   image:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">image_id</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   flavor:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">flavor</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   networks:</span> <span class="string">&#123;</span> <span class="attr">get_param:</span> <span class="string">in_net</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">outputs:</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> instance01_ip:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  description:</span> <span class="string">The</span> <span class="string">IP</span> <span class="string">address</span> <span class="string">of</span> <span class="string">the</span> <span class="string">deployed</span> <span class="string">instance</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  value:</span> <span class="string">&#123;</span> <span class="attr">get_attr:</span> <span class="string">[instance01,</span> <span class="string">first_address]</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> instance02_ip:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  description:</span> <span class="string">The</span> <span class="string">IP</span> <span class="string">address</span> <span class="string">of</span> <span class="string">the</span> <span class="string">deployed</span> <span class="string">instance</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  value:</span> <span class="string">&#123;</span> <span class="attr">get_attr:</span> <span class="string">[instance02,</span> <span class="string">first_address]</span> <span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> instance03_ip:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  description:</span> <span class="string">The</span> <span class="string">IP</span> <span class="string">address</span> <span class="string">of</span> <span class="string">the</span> <span class="string">deployed</span> <span class="string">instance</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  value:</span> <span class="string">&#123;</span> <span class="attr">get_attr:</span> <span class="string">[instance03,</span> <span class="string">first_address]</span> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>除了上面单一模版自动化编排的示例外，在复杂编排的场景下，模板也可以实现嵌套，类似软件模块化开发的思想。可以先定义一个父模版Stack，作为进入其余子模板Stack的入口文件。应用部署所需的全部资源均在子模板Stack文件中描述，子栈之间的参数传递可由父栈进行管理，父栈将某个子栈创建后的输出作为另一个子栈的输入，例如将网络栈创建后的输出作为应用栈的输入。</p><h2 id="Heat的编排场景"><a href="#Heat的编排场景" class="headerlink" title="Heat的编排场景"></a><strong>Heat的编排场景</strong></h2><p>Heat 采用了业界流行使用的模板方式来设计或者定义编排，用户只需要打开文本编辑器，编写一段基于 Key-Value 的模板，就能够方便地得到想要的编排。为了方便用户的使用，Heat 提供了大量的模板例子，可以参考<a href="https://docs.openstack.org/heat/latest/template_guide/hot_guide.html。大多数时候用户只需要选择想要的编排，通过拷贝-粘贴的方式来完成模板的编写。" target="_blank" rel="noopener">https://docs.openstack.org/heat/latest/template_guide/hot_guide.html。大多数时候用户只需要选择想要的编排，通过拷贝-粘贴的方式来完成模板的编写。</a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/aCUE2T9ctmhV.png?imageslim" alt="mark"></p><p>如上图，Heat 从四个方面来支持编排。</p><h3 id="基础架构资源编排"><a href="#基础架构资源编排" class="headerlink" title="基础架构资源编排"></a><strong>基础架构资源编排</strong></h3><p>也就是OpenStack自己提供的基础架构资源，包括计算，网络和存储等资源。通过编排这些资源，用户就可以得到最基本的VM。在编排VM的过程中，用户可以提供一些简单的脚本，以便对VM做一些简单的配置，如下示例图中红框部分。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/hJWuOReYw2I1.png?imageslim" alt="mark"></p><h3 id="应用资源的编排"><a href="#应用资源的编排" class="headerlink" title="应用资源的编排"></a><strong>应用资源的编排</strong></h3><p>也就是用户可以通过Heat提供的Software Configuration和Software Deployment等对VM进行复杂的配置，比如安装软件、配置软件。Heat 提供了多种资源类型来支持对于软件配置和部署的编排，如下所列：</p><ul><li>OS::Heat::CloudConfig： VM引导程序启动时的配置，由 OS::Nova::Server 引用</li><li>OS::Heat::SoftwareConfig：描述软件配置</li><li>OS::Heat::SoftwareDeployment：执行软件部署</li><li>OS::Heat::SoftwareDeploymentGroup：对一组VM执行软件部署</li><li>OS::Heat::SoftwareComponent：针对软件的不同生命周期部分，对应描述软件配置</li><li>OS::Heat::StructuredConfig：和OS::Heat::SoftwareConfig类似，但是用Map来表述配置</li><li>OS::Heat::StructuredDeployment：执行OS::Heat::StructuredConfig对应的配置</li><li>OS::Heat::StructuredDeploymentsGroup：对一组VM执行 OS::Heat::StructuredConfig对应的配置</li></ul><p>其中最常用的是<strong>OS::Heat::SoftwareConfig</strong>和<strong>OS::Heat::SoftwareDeployment</strong>，编排流程如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/2K6mQH3gp8nW.png?imageslim" alt="mark"></p><h3 id="高级功能编排"><a href="#高级功能编排" class="headerlink" title="高级功能编排"></a><strong>高级功能编排</strong></h3><p>也就是如果用户有一些高级的功能需求，比如需要一组能够根据负荷自动伸缩的VM组，或者需要一组负载均衡的VM，Heat提供了AutoScaling 和Load Balance等进行支持。如果要用户自己单独编程来完成这些功能，所花费的时间和编写的代码都是不菲的。现在通过Heat，只需要一段长度的Template，就可以实现这些复杂的应用。Heat提供自动伸缩组OS::Heat::AutoScalingGroup和伸缩策略OS::Heat::ScalingPolicy，结合基于Ceilometer的OS::Ceilometer::Alarm实现了可以根据各种条件，比如负载，进行资源自动伸缩的功能。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/8Mi1ovoPsdNJ.png?imageslim" alt="mark"></p><p>Heat对负载均衡的编排是由一组不同的资源类型来实现的。资源类型包括：</p><ul><li>OS::Neutron::Pool：定义资源池，一般可以由VM组成。</li><li>OS::Neutron::PoolMember：定义资源池的成员。</li><li>OS::Neutron::HealthMonitor：定义健康监视器，根据自定的协议，比如TCP来监控资源的状态，并提供给OS::Neutron::Pool来调整请求分发等。</li><li>OS::Neutron::LoadBalancer：关联资源池以定义整个负载均衡。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rcWKAszeh17h.png?imageslim" alt="mark"></p><h3 id="第三方工具继承编排"><a href="#第三方工具继承编排" class="headerlink" title="第三方工具继承编排"></a><strong>第三方工具继承编排</strong></h3><p>也就是如果用户的应用足够复杂，或者说用户的应用已经有了一些基于流行配置管理工具的部署，比如说已经基于Chef有了Cookbook，那么可以通过集成Chef来复用这些Cookbook，这样就能够节省大量的开发时间或者是迁移时间。Heat在基OS::Heat::SoftwareConfig和OS::Heat::SoftwareDeployment的协同使用上，提供了对Chef、Puppet、Ansible等流行配置管理工具的支持。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/IhX3evYhCTSR.png?imageslim" alt="mark"></p><h2 id="Heat的编排实战"><a href="#Heat的编排实战" class="headerlink" title="Heat的编排实战"></a><strong>Heat的编排实战</strong></h2><p>Heat的编排实战涉及面较广，由于篇幅所限且也无法大量凭空臆造业务场景举例，因此这里着重从不同种类HOT模版编写入手通过一些简单实例熟悉Heat的基础功能用法和编排效果验证。Heat要想用好，必须有一定编程基础，建议大家首先熟悉HOT模板的语法格式和内置函数，可以参考OpenStack官方网站手册，里面有大量的模版实例可供参考。</p><p><strong>步骤1：</strong>执行以下命令，安装python-heatclient，等待安装完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install python-heatclient</span><br></pre></td></tr></table></figure><p><strong>步骤2：</strong>执行以下命令，查看当前可用的镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/WcYlLzi8L1tb.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，查看当前可用的规格</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/TlUiPC7ww1PA.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，查看当前可用的密钥对</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack keypair list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6Y7QWcgiP0Bb.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>执行以下命令，创建“demo-template.yaml”，作为HOT模板。（注：每个“：”后都要有空格，即使“：”后没有字符也要有空格）</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/wBth7vhjCkFa.png?imageslim" alt="mark"></p><p>其中，resources.server中的image、flavor、key_name为环境中可用的镜像、规格和密钥对。</p><p><strong>步骤9：</strong>执行以下命令，查看当前可用网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack network list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/t807DPicYdAv.png?imageslim" alt="mark"></p><p><strong>步骤10：</strong>执行以下命令，使用HOT模板“demo-template.yaml”，创建堆栈“Stack_demo”，网络即为上面的网络ID，过几秒后有如下输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack stack create -t demo-template.yaml --parameter <span class="string">"NetID=ce3368e7-cd00-4395-91f6-b83b6f38bc9f"</span> Stack-Demo</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/WxKHySEvOhyw.png?imageslim" alt="mark"></p><p><strong>步骤11：</strong>等待几分钟后，执行以下命令，查看堆栈的创建过程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack stack event list Stack-Demo</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5E6NT9NMtuo5.png?imageslim" alt="mark"></p><p><strong>步骤12：</strong>执行以下命令，查看堆栈列表，堆栈“Stack_demo”的状态变为“CREATE_COMPLETE”表示堆栈创建完成。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/TqItl3Se9dPI.png?imageslim" alt="mark"></p><p><strong>步骤13：</strong>执行以下命令，查看堆栈的详细信息，图中表明已创建一个虚拟机实例，虚拟机server_name=Stack-Demo-server-o2fur3jgvsis，ip=10.0.11.105</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/s8s5bAWjAHo9.png?imageslim" alt="mark"></p><p>也可以执行以下命令，查看堆栈创建完成后输出的虚拟机实例名称和IP地址。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rOwzde3IjAmk.png?imageslim" alt="mark"></p><p><strong>步骤14：</strong>执行以下命令，查看虚拟机实例列表进行确认，并查看虚拟机实例的状态是否为“Active”</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pke1XPVW4aqs.png?imageslim" alt="mark"></p><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、Heat能解决什么问题？</strong></p><p><strong>2、Heat中有哪些常用概念？</strong></p><p><strong>3、如何查看Heat中的支持的版本号，内置函数以及资源类型？</strong></p><p><strong>4、Heat能应用到哪些场景？每一种场景需要的资源类型有哪些？</strong></p><p><strong>5、HOT模版中parameter的参数用于什么场景？在创建Stack时，通过什么方式导入？</strong></p><p><strong>6、当前OpenStack Rocky版本中HOT模版中resources模块共有多少种？请列举出常用的resources模块（不少于5种），并说明其资源类型。</strong></p><p><strong>7、Heat中的stack本质是什么？起什么作用？</strong></p><p><strong>8、删除stack后，所创建的虚拟机实例是否会被删除？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Heat是OpenStack项目中实现资源编排的子项目，它的目的是帮助用户实现在OpenStack云环境下手工资源创建、配置、应用部署、弹性伸缩等过程的自动化，从而提高OpenStack应用的效率，比如华为云Stack6.5.1版本的Deploy就是通过Heat的编排模版完成FusionSphere OpenStack的部署，华为的FusionSphere OpenStack部署和配置的复杂度只要实践过都知道，那么如果不依靠编排去自动部署而是手动部署，那种酸爽可想而知。因此，Heat这种编排类的系统或软件出现，就解决了这种复杂系统重复部署效率低下的问题。由于OpenStack缘起于AWZ Cloud，因此Heat至今仍保留着 AWZ Cloud的编排REST API请求接口，并在功能上继承AWZ的Formation（现在甚至逐渐在超越）。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-对象存储管理服务Swift</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Swift/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-对象存储管理服务Swift/</id>
    <published>2020-03-03T14:33:12.000Z</published>
    <updated>2020-03-03T15:00:09.786Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Swift 最初是由Rackspace公司开发的高可用分布式对象存储服务，并于2010年贡献给OpenStack开源社区作为其最初的核心子项目之一，为其Nova子项目提供虚机镜像存储服务。Swift构筑在比较便宜的标准硬件存储基础设施之上，无需采用 RAID（磁盘冗余阵列），通过在软件层面引入一致性散列技术和数据冗余性，牺牲一定程度的数据一致性来达到高可用性和可伸缩性，支持多租户模式、容器和对象读写操作，适合解决互联网的应用场景下<strong>非结构化数据存储</strong>问题。Swift在OpenStack系统中不依赖于任何服务，可以独立部署为其他系统提供分布式对象存储服务。而在OpenStack的应用中，其Proxy Server往往由Keystone节点兼任，由Keystone来完成服务访问的安全认证。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/eXgvoqetXXi3.png?imageslim" alt="mark"></p><p>Swift是业务提供时，使用普通的服务器来构建冗余的、可扩展的分布式对象存储集群，存储容量可达PB级。通过统一REST API进行友好访问，不仅易于扩展，且无中心数据库，避免单点故障或单点性能瓶颈。Swift主要通过<strong>Account、Container</strong>和<strong>Object</strong>三个表单结构来完成存储对象的存储、查询、获取和上传等功能，通过数据存储的多副本机制实现数据的高可用。其中，Object是最小存储表单，代表实际存储的数据，三个表单的对应关系为为：<strong>Account 1:m Container 1:m Object</strong>。</p><h2 id="Swift的架构和组件"><a href="#Swift的架构和组件" class="headerlink" title="Swift的架构和组件"></a><strong>Swift的架构和组件</strong></h2><p>Swift的架构是一种完全对称、面向资源的分布式系统架构，所有组件都可扩展，避免因单点失效而影响整个系统运转。通信方式采用非阻塞式 I/O 模式，提高了系统吞吐和响应能力，如下图所示。系统架构整体上采用分层的理念设计，共分为两层：<strong>访问层</strong>和<strong>存储层</strong>。Controller的Ring以上部分属于访问层，接收外部REST API的访问，实现负载均衡和访问安全验证，并定位对象数据的存储的位置。后端的Server部分属于存储层，用来分别存储不同的对象数据：Account、Container和Object。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/r4LhC3i8QYrX.png?imageslim" alt="mark"></p><p>上图Swift的架构中的一些关键组件服务需要说明，包括：</p><ul><li><strong>代理服务（Proxy Server）：</strong>对外提供对象服务 API，会根据环（Ring）的信息来查找服务地址并转发用户请求至相应的账户、容器或者对象服务；由于采用无状态的 REST请求协议，可以进行横向扩展来均衡负载。</li><li><strong>认证服务（Authentication Server）：</strong>验证访问用户的身份信息，并获得一个对象访问令牌（Token），在一定的时间内会一直有效。同时验证访问令牌的有效性并缓存下来，直到令牌过期为止。</li><li><strong>缓存服务（Cache Server）：</strong>缓存的内容包括对象服务令牌，账户和容器的存在信息，但不会缓存对象本身的数据；缓存服务可采用Memcached集群，Swift 会使用一致性哈希算法来分配缓存地址。</li><li><strong>账户服务（Account Server）：</strong>提供账户元数据和统计信息，并维护所含容器列表的服务，每个账户的信息被存储在一个 SQLite 数据库中。</li><li><strong>容器服务（Container Server）：</strong>提供容器元数据和统计信息，并维护所含对象列表的服务，每个容器的信息也存储在一个 SQLite 数据库中。</li><li><strong>对象服务（Object Server）：</strong>提供对象元数据和内容服务，每个对象的内容会以文件的形式存储在文件系统中，元数据会作为文件属性来存储，建议采用支持扩展属性的 XFS 文件系统。</li><li><strong>复制服务（Replicator）：</strong>会检测本地分区副本和远程副本是否一致，具体是通过对比哈希文件和高级水印来完成，发现不一致时会采用推式（Push）更新远程副本，例如对象复制服务会使用远程文件拷贝工具 rsync 来同步，另外一个任务是确保被标记删除的对象从文件系统中移除。</li><li><strong>更新服务（Updater）：</strong>当对象由于高负载的原因而无法立即更新时，任务将会被序列化到在本地文件系统中进行排队，以便服务恢复后进行异步更新。例如成功创建对象后容器服务器没有及时更新对象列表，这个时候容器的更新操作就会进入排队中，更新服务会在系统恢复正常后扫描队列并进行相应的更新处理。</li><li><strong>审计服务（Auditor）：</strong>检查对象，容器和账户的完整性，如果发现<strong>比特级</strong>的错误，文件将被隔离，并复制其他的副本以覆盖本地损坏的副本，其他类型的错误会被记录到日志中。</li><li><strong>账户清理服务（Account Reaper）：</strong>移除被标记为删除的账户，删除其所包含的所有容器和对象。</li></ul><p>Swift中对象的存储URL示例格式为：<a href="https://swift.example.com/v1/account/container/object，整个存储URL有两个基本部分：**集群位置**和**存储位置**。比如示例URL中的swift.example.com/v1/表示集群位置，" target="_blank" rel="noopener">https://swift.example.com/v1/account/container/object，整个存储URL有两个基本部分：**集群位置**和**存储位置**。比如示例URL中的swift.example.com/v1/表示集群位置，</a> /account/container/object表示存储位置。而对象的存储位置中主要包括三层：</p><ul><li><strong>/account：</strong>帐户存储位置，唯一命名的存储区域，其中包含帐户本身的元数据（描述性信息）以及帐户中的容器列表。必须说明一点，Swift中的帐户不是用户身份，而是一个存储区域。</li><li><strong>/account/container：</strong>容器存储位置，是帐户内的用户自定义的存储区域，类似我们PC操作系统中的文件夹，其中包含容器本身和容器中的对象列表的元数据。</li><li><strong>/account/container/object：</strong>对象存储位置，存储了数据对象及其元数据的位置。</li></ul><h2 id="Swift中的数据结构原理"><a href="#Swift中的数据结构原理" class="headerlink" title="Swift中的数据结构原理"></a><strong>Swift中的数据结构原理</strong></h2><h3 id="一致性哈希算法"><a href="#一致性哈希算法" class="headerlink" title="一致性哈希算法"></a><strong>一致性哈希算法</strong></h3><p>Swift中最重要的算法就是一致性哈希（Consistent Hashing），它是Swift实现海量数据存储，并能实现数据均衡度和可扩展性兼容的保证，可以不过分的认为一致性哈希算法是所有分布式存储的灵魂，不仅Swift中有它的影子，在开源分布式存储Ceph以及华为分布式存储FusionStorage中同样有它的影子。在面对海量级别的对象，需要存放在成千上万台服务器和硬盘设备上，首先要解决寻址问题，即如何将对象均匀地分布到这些设备地址上。这也是Swift中的一致性哈希首先要解决的问题，算法的基本思路是：<strong>通过计算可将对象均匀分布到虚拟空间的虚拟节点上，在增加或删除节点时可大大减少需移动的数据量；虚拟空间大小通常采用 2 的 n 次幂，便于进行高效的移位操作；然后通过独特的数据结构 Ring（环）再将虚拟节点映射到实际的物理存储设备上，完成寻址过程。</strong></p><p>一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环（其实就是一个线性队列），如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希空间环如下图（1），整个空间按顺时针方向组织，0和2^32-1在零点中方向重合。下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将4台服务器使用ip地址哈希后在环空间的位置如下图（2）。接下来定位数据访问到相应服务器的算法：将数据key使用相同的函数Hash计算出哈希值，并映射此数据在环上的位置，从此位置沿环顺时针旋转，第一台遇到的服务器就是其应该存储到的服务器。例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下图（3），根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。现假设Node C不幸宕机，从图（3）中可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。如果在系统中增加一台服务器Node X，如下图（4）所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/TiO0sCArTDgc.png?imageslim" alt="mark"></p><p>此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向旋转遇到的第一台服务器）之间数据，其它数据也不会受到影响。因此，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。在实际生产部署时，往往通过虚拟节点代替物理节点，并增加虚拟节点到物理节点的映射来确保即使服务器数量很少也能做到数据均匀分布存储。即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为<strong>虚拟节点</strong>。具体做法可以在服务器ip或主机名的后面增加编号来实现。这样即使在只有2台物理节点的场景下，可以为每台服务器计算3个虚拟节点，结果分别为“Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点。同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上，这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，比如华为的FusionStorage的虚拟节点数为固定值3600。</p><h3 id="数据一致性模型（Consistency-Model）"><a href="#数据一致性模型（Consistency-Model）" class="headerlink" title="数据一致性模型（Consistency Model）"></a><strong>数据一致性模型（Consistency Model）</strong></h3><p>Swift中数据一致性策略放弃严格一致性，而采用最终一致性模型（Eventual Consistency），来达到高可用性和无限水平扩展能力。为了实现这一目标，Swift采用 Quorum仲裁协议(Quorum 有法定投票人数的含义)，定义如下：</p><ul><li><strong>N：</strong>数据的副本总数；W：写操作被确认接受的副本数量；R：读操作的副本数量；</li><li><strong>强一致性：R+W&gt;N</strong>，以保证对副本的读写操作会产生交集，从而保证可以读取到最新版本；如果 W=N，R=1，则需要全部更新，适合大量读少量写操作场景下的强一致性；如果 R=N，W=1，则只更新一个副本，通过读取全部副本来得到最新版本，适合大量写少量读场景下的强一致性。</li><li><strong>弱一致性：R+W&lt;=N</strong>，如果读写操作的副本集合不产生交集，就可能会读到脏数据，适合对一致性要求比较低的场景。</li></ul><p>Swift提供的对象存储服务面向的都是读写都比较频繁的场景，所以采用了比较折中的策略，即<strong>写操作需要满足至少一半以上成功 W &gt;N/2，再保证读操作与写操作的副本集合至少产生一个交集，即 R+W&gt;N</strong>。<strong>Swift默认配置是N=3，W=2&gt;N/2，R=1或2，即每个对象会存在 3 个副本，这些副本会尽量被存储在不同区域的节点上，W=2 表示至少需要更新 2 个副本才算写成功；当 R=1 时意味着某一个读操作成功便立刻返回，此种情况下可能会读取到旧版本（弱一致性模型）；当 R=2 时，需要通过在读操作请求头中增加 x-newest=true 参数来同时读取 2 个副本的元数据信息，然后比较时间戳来确定哪个是最新版本（强一致性模型）；如果数据出现了不一致，后台服务进程会在一定时间窗口内通过检测和复制协议来完成数据同步，从而保证达到最终一致性。</strong></p><h3 id="环（Ring）的数据结构"><a href="#环（Ring）的数据结构" class="headerlink" title="环（Ring）的数据结构"></a><strong>环（Ring）的数据结构</strong></h3><p>环（Ring）是为了将虚拟节点（分区）映射到一组物理存储设备上，并提供一定的冗余度而设计的，其数据结构由以下信息组成：</p><ul><li><strong>存储设备列表、设备信息：</strong>包括唯一标识号（id）、区域号（zone）、权重（weight）、IP 地址（ip）、端口（port）、设备名称（device）、元数据（meta）。</li><li><strong>分区到设备映射关系（replica2part2dev_id数组)</strong></li><li><strong>计算分区号的位移(part_shift整数)</strong></li></ul><p>在Swift中主要有三种环（Ring）：<strong>Account Ring、Container Ring 、Object Ring</strong>，<strong>其分别对应Swift的三种数据模型：Account、Container和Object。</strong>三种数据模型的层次关系如下图，<strong>共设三层逻辑结构：Account/Container/Object（即账户/容器/对象)，每层节点数均没有限制，可以任意扩展。</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Yw6HfKYVog2M.png?imageslim" alt="mark"></p><p><strong>环（Ring）的数据结构实现原理如下图所示：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/sFOetcv9tT1w.png?imageslim" alt="mark"></p><p>层次结构 account/container/object 作为键，使用 MD5 散列算法得到一个散列值，对该散列值的前 4 个字节进行右移操作得到分区索引号，移动位数由上面的 part_shift设置指定；按照分区索引号在分区到设备映射表（replica2part2dev_id）里查找该对象所在分区的对应的所有设备编号，这些设备会被尽量选择部署在不同区域（zone）内，区域只是个抽象概念，它可以是某台机器，某个机架，甚至数据中心的机群，以提供最高级别的冗余性，建议至少部署 5 个区域；权重参数是个相对值，可以来根据磁盘的大小来调节，权重越大表示可分配的空间越多，可部署更多的分区。</p><h2 id="Swift中存储的实现"><a href="#Swift中存储的实现" class="headerlink" title="Swift中存储的实现"></a><strong>Swift中存储的实现</strong></h2><h3 id="Swift中的数据存储过程"><a href="#Swift中的数据存储过程" class="headerlink" title="Swift中的数据存储过程"></a>Swift中的数据存储过程</h3><p>当收到一个需要保存的object的PUT请求时，Proxy server 会执行如下的操作流程：</p><p>1）首先根据其完整的对象路径（/account[/container[/object]]）计算其哈希值，哈希值的长度取决于集群中分区的总数（虚拟节点的总数）。</p><p>2）将哈希值的开头N个字符映射为数目同replica值的若干partition ID。</p><p>3）根据 partition ID 确定某个数据服务的 IP 和 port。</p><p>4）依次尝试连接这些服务的端口，如果有一半的服务无法连接，则拒绝该请求。</p><p>5）尝试创建对象，存储服务会将对象以文件形式保存到某个磁盘上。（Object server 在完成文件存储后会异步地调用 container service 去更新container数据库）</p><p>6）在3份副本拷贝中有两份被成功写入后， Proxy Server就会向客户端返回成功。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/8r0V0t3EB3Xa.png?imageslim" alt="mark"></p><p>当Proxy server收到一个获取object的GET请求时，它所执行的操作前四步同前面的PUT请求，主要功能就是确定存放所有replica的所有磁盘，而在第五步时，Proxy Server会排序这些存储节点，尝试连接第一个，如果成功，则将二进制数据返回客户端；如果不成功，则尝试下一个，直到成功或者都失败。</p><p>总体来说该过程简单直接，这也符合Swift的总体设计风格。在上面的数据结构原理部分，我们介绍了Swift中数据的映射机制和具体操作。那么在集群中的每一台存储节点上，Swift是如何实现Account、Container、Object的具体存储呢？</p><h3 id="Swift中存储实现分析"><a href="#Swift中存储实现分析" class="headerlink" title="Swift中存储实现分析"></a><strong>Swift中存储实现分析</strong></h3><p>在Storage node上运行着Linux系统并使用了XFS文件系统，逻辑上使用一致性哈希算法将固定总数的partition映射到每个Storage node上，每个Data也使用同样的哈希算法映射到Partition上，其层次结构如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/oMMVuQIpkSSj.png?imageslim" alt="mark"></p><p>以我们的一台storage node computer1为例，该device的文件路径挂载到/srv/node/sdc，目录结构如下所示：</p><p>root@computer1:/srv/node/sdc# ls</p><p>accounts async_pending containers objects quarantined tmp</p><p>其中<strong>accounts、containers、objects分别是账号、容器、对象的存储目录，async_pending是异步待更新目录，quarantined是隔离目录，tmp是临时目录</strong>。</p><p>在objects目录下存放的是各个partition目录，其中每个partition目录是由若干个suffix_path名的目录和一个hashes.pkl文件组成，suffix_path目录下是由object的hash_path名构成的目录，在hash_path目录下存放了关于object的数据和元数据，object存储目录的层次结构如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yW2KU5KbUmyw.png?imageslim" alt="mark"></p><p>其中，hashes.pkl是存放在每个partition中的一个2进制pickle化文件。例如：</p><p>root@computer1:/srv/node/sdc/objects/100000# ls</p><p>8bd hashes.pkl</p><p>通过ipython读取这个pkl文件返回结果如下红色字体所示，其中‘8bd’是suffix_dir，而9e99c8eedaa3197a63f685dd92a5b4b8则是该partition下数据的md5哈希值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">with</span> open(<span class="string">'hashes.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fp:</span><br><span class="line"></span><br><span class="line">  ...:   <span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">  ...:   hashes = pickle.load(fp)</span><br><span class="line"></span><br><span class="line">  ...:  </span><br><span class="line"></span><br><span class="line">  ...:  </span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: hashes</span><br><span class="line"></span><br><span class="line">Out[<span class="number">2</span>]: &#123;<span class="string">'8bd'</span>: <span class="string">'9e99c8eedaa3197a63f685dd92a5b4b8'</span>&#125;</span><br></pre></td></tr></table></figure><p>object的存储路径由object server进程内部称为DiskFile类初始化时产生，过程如下：</p><p>1）由文件所属的account、container和object名称产生’/account/container/object’格式的字符串，和HASH_PATH_SUFFIX组成新的字符串，调用hash_path函数，生成md5 hash值name_hash。其中HASH_PATH_SUFFIX作为salt来增加安全性，HASH_PATH_SUFFIX值存放在/etc/swift/swift.conf中。</p><p>2）调用storage_directory函数，传入DATADIR, partition, hash_path参数生成DATADIR/partition/name_path[-3:]/name_path格式字符串。</p><p>3）连结path/devcie/storage_directory(DATADIR, partition,name_ hash)生成数据存储路径datadir。</p><p>4）调用normalize_timestamp函数生成“16位.5位”的时间戳+扩展名的格式生成对象名称。</p><p>例如，某object的存储路径为：/srv/node/sdc/objects/19892/ab1/136d0ab88371e25e16663fbd2ef42ab1/1320050752.09979.data，其中每个目录分别表示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/CGulS0if1iCQ.png?imageslim" alt="mark"></p><p>Object的数据存放在后缀为.data的文件中，它的metadata存放在以后缀为.meta的文件中，将被删除的Object以一个0字节后缀为.ts的文件存放。</p><p>在accounts目录下存放的是各个partition，而每个partition目录是由若干个suffix_path目录组成，suffix_path目录下是由account的hash名构成的目录，在hash目录下存放了关于account的sqlite db，account存储目录的层次结构如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kuSw0d1Tspqp.png?imageslim" alt="mark"></p><p>account使用AccountController类来生成path，其过程与object类似，唯一的不同之处在于，account的db命名调用hash_path(account)来生成，而不是使用时间戳的形式。例如，某account的db存储路径为：/srv/node/sdc/accounts/20443/ac8/c7a5e0f94b23b79345b6036209f9cac8/ c7a5e0f94b23b79345b6036209f9cac8.db，其中每个目录分别表示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HmpQQgeMPJ2u.png?imageslim" alt="mark"></p><p>在account的db文件中，包含了account_stat、container、incoming_sync 、outgoing_sync 4张表。</p><ul><li><strong>表account_stat：</strong>记录关于account的信息，如名称、创建时间、container数统计等等。比如，其中account表示account名称，created_at表示创建时间，put_timestamp表示put request的时间戳，delete_timestamp表示delete request的时间戳，container_count为countainer的计数，object_count为object的计数，bytes_used表示已使用的字节数，hash表示db文件的hash值，id表示统一标识符，status表示account是否被标记为删除，status_changed_at表示状态修改时间，metadata表示account的元数据。</li><li><strong>表container：</strong>记录关于container的信息等等。比如，其中ROWID字段表示自增的主键，name字段表示container的名称，put_timestamp和delete_timestamp分别表示container的put和delete的时间戳，object_count表示container内的object数， bytes_used 表示已使用的空间，deleted表示container是否标记为删除。</li><li><strong>表incoming_sync：</strong>记录到来的同步数据项等等。比如，其中remote_id字段表示远程节点的id，sync_point字段表示上一次更新所在的行位置，updated_at字段表示更新时间。</li><li><strong>表outgoing_sync：</strong>表示推送出的同步数据项等等。比如，其中，remote_id字段表示远程节点的id，sync_point字段表示上一次更新所在的行位置，updated_at字段表示更新时间。</li></ul><p><strong>Container目录</strong>结构和生成过程与Account类似，Container的db中共有5张表，其中incoming_sync和outgoing_sync的schema与Account中的相同。其他3张表分别为container_stat、object、sqlite_sequence，结构与account中的表类似，具体不再赘述。</p><p><strong>tmp目录</strong>作为account/container/object server向partition目录内写入数据前的临时目录。例如，在client向服务端上传某一文件，object server调用DiskFile类的mkstemp方法在创建路径为path/device/tmp的目录。在数据上传完成之后，调用put()方法，将数据移动到相应路径。</p><p><strong>async_pending目录</strong>是本地server在与remote server建立http连接或者发送数据时超时导致更新失败时，将把文件放入该目录。这种情况经常发生在系统故障或者是高负荷的情况下。如果更新失败，本次更新被加入队列，然后由Updater继续处理这些失败的更新工作。例如，假设一个container server处于高负荷下，此时一个新的对象被加入到系统。当Proxy成功地响应Client的请求时，这个对象将变为直接可访问的。但是container服务器并没有更新对象列表，本次更新将进入队列等待延后的更新。所以，container列表不可能马上就包含这个新对象。随后Updater使用object_sweep扫描device上的async pendings目录，遍历每一个prefix目录并执行升级。一旦完成升级，则移除pending目录下的文件(实际上，是通过调用renamer函数将文件移动到object相应的目录下)。account和container的db pending文件并不会独立地存在于async_pending目录下，它们的pending文件会与其db文件在一个目录下存放。但是，account与container的db与object两者的pending文件处理方式不同：<strong>db的pending文件在更新完其中的一项数据之后，删除pending文件中的相应的数据项，而object的数据在更新完成之后，移动pending文件到目标目录。</strong></p><p><strong>quarantined目录</strong>是Auditor进程会在本地服务器上每隔一段时间就扫面一次磁盘来检测account、container、object的完整性。一旦发现不完整的数据，该文件就会被隔离，该目录就称为quarantined目录。为了限制Auditor消耗过多的系统资源，其默认扫描间隔是30秒，每秒最大的扫描文件数为20，最高速率为10Mb/s。obj auditor使用AuditorWorker类的object_audit方法来检查文件的完整性，该方法封装了obj server的DiskFile类，该类有一个_handle_close_quarantine方法，用来检测文件是否需要被隔离，如果发现损坏，则直接将文件移动到隔离目录下。随后replicator从其他replica那拷贝新的文件来替换，最后Server计算文件的hash值是否正确。而account使用AccountAuditor类的account_audit方法，container使用ContainerAuditor类的container_audit方法对目录下的db文件进行检查。</p><h2 id="Swift存储服务部署和实战"><a href="#Swift存储服务部署和实战" class="headerlink" title="Swift存储服务部署和实战"></a><strong>Swift存储服务部署和实战</strong></h2><p>正常来说，Swift服务部署至少需要3节点，但是我们实验环境首先机器配置原因，因此在控制节点新增两块硬盘/dev/sdc、/dev/sdd作为对象存储的后端存储设备，也就不考虑对象存储的性能和平衡性了。同时，Swift服务部署成功后，需要将Glance的镜像存储后端由本地文件系统改为对象存储，同时新增cinder-backup服务，其后端也对接对象存储Swift，以此来模拟实际生产环境。</p><h3 id="控制节点部署预置环境"><a href="#控制节点部署预置环境" class="headerlink" title="控制节点部署预置环境"></a><strong>控制节点部署预置环境</strong></h3><p>1）创建swift用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt swift</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/3PGUd1RFHpOU.png?imageslim" alt="mark"></p><p>2）给swift用户添加admin角色</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role add --project service --user swift admin</span><br></pre></td></tr></table></figure><p>3）创建swift服务实体</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack service create --name swift --description <span class="string">"OpenStack Object Storage"</span> object-store</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/QNCeAtELmw2Y.png?imageslim" alt="mark"></p><p>4）创建对象存储服务API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne object-store public http://rocky-controller:8080/v1/AUTH_%\(project_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne object-store internal http://rocky-controller:8080/v1/AUTH_%\(project_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne object-store admin http://rocky-controller:8080/v1</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/P34KqkClv3es.png?imageslim" alt="mark"></p><p>5）安装软件包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openstack-swift-proxy python-swiftclient python-keystoneclient python-keystonemiddleware memcached -y</span><br></pre></td></tr></table></figure><p>6）从对象存储源仓库获取代理服务配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o /etc/swift/proxy-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/proxy-server.conf-sample</span><br></pre></td></tr></table></figure><p>7）编辑/etc/swift/proxy-server.conf文件并完成以下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置绑定端口，用户和配置目录：</span></span><br><span class="line">[DEFAULT]</span><br><span class="line">bind_port = 8080</span><br><span class="line">user = swift</span><br><span class="line">swift_dir = /etc/swift</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[pipeline:main]部分中，删除tempurl和 tempauth模块并添加authtoken和keystoneauth 模块</span></span><br><span class="line">[pipeline:main]</span><br><span class="line">pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[app:proxy-server]部分中，启用自动帐户创建：</span></span><br><span class="line">[app:proxy-server]</span><br><span class="line">use = egg:swift<span class="comment">#proxy</span></span><br><span class="line">account_autocreate = True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[filter:keystoneauth]部分中，配置操作员角色：</span></span><br><span class="line">[filter:keystoneauth]</span><br><span class="line">use = egg:swift<span class="comment">#keystoneauth</span></span><br><span class="line">operator_roles = admin,user</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="comment">#在该[filter:authtoken]部分中，配置身份服务访问(注释掉或删除该[filter:authtoken] 部分中的任何其他选项)：</span></span><br><span class="line">[filter:authtoken]</span><br><span class="line">paste.filter_factory = keystonemiddleware.auth_token:filter_factory</span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_id = default</span><br><span class="line">user_domain_id = default</span><br><span class="line">project_name = service</span><br><span class="line">username = swift</span><br><span class="line">password = swift</span><br><span class="line">delay_auth_decision = True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[filter:cache]部分中，配置memcached位置：</span></span><br><span class="line">[filter:cache]</span><br><span class="line">use = egg:swift<span class="comment">#memcache</span></span><br><span class="line">memcache_servers = rocky-controller:11211</span><br></pre></td></tr></table></figure><h3 id="存储节点部署预置环境（控制节点兼任）"><a href="#存储节点部署预置环境（控制节点兼任）" class="headerlink" title="存储节点部署预置环境（控制节点兼任）"></a><strong>存储节点部署预置环境（控制节点兼任）</strong></h3><p>1）安装支持实用程序包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install xfsprogs rsync</span><br></pre></td></tr></table></figure><p>2）格式化/dev/sdc和/dev/sdd设备为XFS文件系统：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/romKwLeVMCMH.png?imageslim" alt="mark"></p><p>3）创建挂载点目录结构：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /srv/node/sdc</span><br><span class="line">mkdir -p /srv/node/sdd</span><br></pre></td></tr></table></figure><p>4）编辑/etc/fstab文件并将以下内容添加到其中，设置开机自动挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"/dev/sdc /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2"</span> &gt;&gt; /etc/fstab</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"/dev/sdd /srv/node/sdd xfs noatime,nodiratime,nobarrier,logbufs=8 0 2"</span> &gt;&gt; /etc/fstab</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/uBrqfqxxnNtQ.png?imageslim" alt="mark"></p><p>5）挂载安装设备</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount -a</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Amlz3fwX3Miz.png?imageslim" alt="mark"></p><p>6）创建或编辑/etc/rsyncd.conf文件以包含以下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">uid = swift</span><br><span class="line">gid = swift</span><br><span class="line"></span><br><span class="line"><span class="built_in">log</span> file = /var/<span class="built_in">log</span>/rsyncd.log</span><br><span class="line">pid file = /var/run/rsyncd.pid</span><br><span class="line"></span><br><span class="line">address = 10.28.101.81 <span class="comment">#存储节点上管理网络的IP地址</span></span><br><span class="line"></span><br><span class="line">[account]</span><br><span class="line">max connections = 2</span><br><span class="line">path = /srv/node/</span><br><span class="line"><span class="built_in">read</span> only = False</span><br><span class="line">lock file = /var/lock/account.lock</span><br><span class="line"></span><br><span class="line">[container]</span><br><span class="line">max connections = 2</span><br><span class="line">path = /srv/node/</span><br><span class="line"><span class="built_in">read</span> only = False</span><br><span class="line">lock file = /var/lock/container.lock</span><br><span class="line"> </span><br><span class="line">[object]</span><br><span class="line">max connections = 2</span><br><span class="line">path = /srv/node/</span><br><span class="line"><span class="built_in">read</span> only = False</span><br><span class="line">lock file = /var/lock/object.lock</span><br></pre></td></tr></table></figure><p>7）启动rsyncd服务并将其配置为在系统引导时启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> rsyncd.service &amp;&amp; systemctl start rsyncd.service &amp;&amp; systemctl status rsyncd.service</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/zidN0sjSTiHH.png?imageslim" alt="mark"></p><p>8）安装软件包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openstack-swift-account openstack-swift-container openstack-swift-object</span><br></pre></td></tr></table></figure><p>9）从对象存储源仓库获取记帐，容器和对象服务配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -o /etc/swift/account-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/account-server.conf-sample</span><br><span class="line"></span><br><span class="line">curl -o /etc/swift/container-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/container-server.conf-sample</span><br><span class="line"></span><br><span class="line">curl -o /etc/swift/object-server.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/object-server.conf-sample</span><br></pre></td></tr></table></figure><p>10）编辑/etc/swift/account-server.conf 文件并完成以下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在[DEFAULT]节中，配置绑定IP地址，绑定端口，用户，配置目录和挂载点目录：</span></span><br><span class="line">[DEFAULT]</span><br><span class="line">bind_ip = 10.28.101.81   <span class="comment">#&lt;===为存储节点上管理网络的IP地址</span></span><br><span class="line">bind_port = 6202</span><br><span class="line">user = swift</span><br><span class="line">swift_dir = /etc/swift</span><br><span class="line">devices = /srv/node</span><br><span class="line">mount_check = True</span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[pipeline:main]部分中，启用相应的模块：</span></span><br><span class="line">[pipeline:main]</span><br><span class="line">pipeline = healthcheck recon account-server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在该[filter:recon]部分中，配置recon（米）缓存目录：</span></span><br><span class="line">[filter:recon]</span><br><span class="line">use = egg:swift<span class="comment">#recon</span></span><br><span class="line">recon_cache_path = /var/cache/swift</span><br></pre></td></tr></table></figure><p>11）/etc/swift/container-server.conf文件并完成以下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在[DEFAULT]节中，配置绑定IP地址，绑定端口，用户，配置目录和挂载点目录：</span></span><br><span class="line">[DEFAULT]</span><br><span class="line">bind_ip = 10.28.101.81  <span class="comment">#&lt;===为存储节点上管理网络的IP地址</span></span><br><span class="line">bind_port = 6201</span><br><span class="line">user = swift</span><br><span class="line">swift_dir = /etc/swift</span><br><span class="line">devices = /srv/node</span><br><span class="line">mount_check = True</span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[pipeline:main]部分中，启用相应的模块：</span></span><br><span class="line">[pipeline:main]</span><br><span class="line">pipeline = healthcheck recon container-server </span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[filter:recon]部分中，配置recon（米）缓存目录：</span></span><br><span class="line">[filter:recon]</span><br><span class="line">use = egg:swift<span class="comment">#recon</span></span><br><span class="line">recon_cache_path = /var/cache/swift</span><br></pre></td></tr></table></figure><p>12）编辑/etc/swift/object-server.conf文件并完成以下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在[DEFAULT]节中，配置绑定IP地址，绑定端口，用户，配置目录和挂载点目录：</span></span><br><span class="line">[DEFAULT]</span><br><span class="line">bind_ip = 10.28.101.81  <span class="comment">#&lt;===为存储节点上管理网络的IP地址</span></span><br><span class="line">bind_port = 6200</span><br><span class="line">user = swift</span><br><span class="line">swift_dir = /etc/swift</span><br><span class="line">devices = /srv/node</span><br><span class="line">mount_check = True</span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[pipeline:main]部分中，启用相应的模块：</span></span><br><span class="line">[pipeline:main]</span><br><span class="line">pipeline = healthcheck recon object-serve</span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[filter:recon]部分中，配置recon（米）缓存和锁定目录：</span></span><br><span class="line">[filter:recon]</span><br><span class="line">use = egg:swift<span class="comment">#recon</span></span><br><span class="line">recon_cache_path = /var/cache/swift</span><br><span class="line">recon_lock_path = /var/lock</span><br></pre></td></tr></table></figure><p>13）确保安装点目录结构的正确所有权：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chown -R swift:swift /srv/node</span><br><span class="line">sudo restorecon -R /srv</span><br></pre></td></tr></table></figure><p>14） 创建recon目录并确保其正确拥有：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/cache/swift</span><br><span class="line">chown -R root:swift /var/cache/swift</span><br><span class="line">chmod -R 775 /var/cache/swift</span><br></pre></td></tr></table></figure><h3 id="控制节点上创建Ring"><a href="#控制节点上创建Ring" class="headerlink" title="控制节点上创建Ring"></a><strong>控制节点上创建Ring</strong></h3><p>1）切换到/etc/swift目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/swift/</span><br></pre></td></tr></table></figure><p>2）创建基本account.builder，container.builder和object.builder文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">swift-ring-builder account.builder create 10 2 1</span><br><span class="line">swift-ring-builder container.builder create 10 2 1</span><br><span class="line">swift-ring-builder object.builder create 10 2 1</span><br></pre></td></tr></table></figure><p>3）将每个存储节点添加到环中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">swift-ring-builder account.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6202 --device sdc --weight 100</span><br><span class="line">swift-ring-builder account.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6202 --device sdd --weight 100</span><br><span class="line">swift-ring-builder container.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6201 --device sdc --weight 100</span><br><span class="line">swift-ring-builder container.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6201 --device sdd --weight 100</span><br><span class="line">swift-ring-builder object.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6200 --device sdc --weight 100</span><br><span class="line">swift-ring-builder object.builder add --region 1 --zone 1 --ip 10.28.101.81 --port 6200 --device sdd --weight 100</span><br></pre></td></tr></table></figure><p>4）验证环内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">swift-ring-builder account.builder</span><br><span class="line">swift-ring-builder container.builder</span><br><span class="line">swift-ring-builder object.builder</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/MAnrmpV6CoIq.png?imageslim" alt="mark"></p><p>5）重新平衡环</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">swift-ring-builder account.builder rebalance</span><br><span class="line">swift-ring-builder container.builder rebalance</span><br><span class="line">swift-ring-builder object.builder rebalance</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/49M8VA8AdiJc.png?imageslim" alt="mark"></p><p>6）分配环的配置文件到每个swift的存储节点，将生成的副本account.ring.gz，container.ring.gz以及 object.ring.gz文件复制到/etc/swift每个存储节点和运行代理服务的任何其他节点上目录。（我们环境控制节点兼任swift存储节点）</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/feUzea4oDl5F.png?imageslim" alt="mark"></p><p>7）从Object Storage源仓库获取swift.conf配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -o /etc/swift/swift.conf https://opendev.org/openstack/swift/raw/branch/stable/stein/etc/swift.conf-sample</span><br></pre></td></tr></table></figure><p>8）编辑/etc/swift/swift.conf 文件并完成以下操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在该[swift-hash]部分中，为您的环境配置哈希路径前缀和后缀。</span></span><br><span class="line">[swift-hash]</span><br><span class="line">swift_hash_path_suffix = PASSWORD</span><br><span class="line">swift_hash_path_prefix = PASSWORD  <span class="comment">#&lt;===此处PASSWORD可以换成其他唯一值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在该[storage-policy:0]部分中，配置默认存储策略：</span></span><br><span class="line">[storage-policy:0]</span><br><span class="line">name = Policy-0</span><br><span class="line">default = yes</span><br></pre></td></tr></table></figure><p>9） 将swift.conf文件复制到/etc/swift每个存储节点上的目录以及运行代理服务的任何其他节点。</p><p>10）在所有节点上（控制节点和存储节点），确保配置目录的正确权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R root:swift /etc/swift</span><br></pre></td></tr></table></figure><p>11）在控制器节点和运行代理服务的任何其他节点上，启动对象存储代理服务（包括其依赖项）并将其配置为在系统引导时启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-swift-proxy.service memcached.service &amp;&amp; systemctl start openstack-swift-proxy.service memcached.service</span><br></pre></td></tr></table></figure><p>12）在存储节点上，启动对象存储服务并将其配置为在系统引导时启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.service</span><br><span class="line"></span><br><span class="line">systemctl status openstack-swift-account.service openstack-swift-account-auditor.service openstack-swift-account-reaper.service openstack-swift-account-replicator.service</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.service</span><br><span class="line"></span><br><span class="line">systemctl status openstack-swift-container.service openstack-swift-container-auditor.service openstack-swift-container-replicator.service openstack-swift-container-updater.service</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.service</span><br><span class="line"></span><br><span class="line">systemctl status openstack-swift-object.service openstack-swift-object-auditor.service openstack-swift-object-replicator.service openstack-swift-object-updater.service</span><br></pre></td></tr></table></figure><h3 id="glance后端配置为swift"><a href="#glance后端配置为swift" class="headerlink" title="glance后端配置为swift"></a><strong>glance后端配置为swift</strong></h3><p>1）修改glance-api.conf的[glance_store]部分如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5H0rNjTpAznt.png?imageslim" alt="mark"></p><p>2）重启glancea服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-glance-*</span><br></pre></td></tr></table></figure><p>3）验证：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传镜像cirros</span></span><br><span class="line">openstack image create --public --disk-format qcow2 --container-format bare --file cirros-0.4.0-x86_64-disk.img cirros_swift</span><br></pre></td></tr></table></figure><p># 查看swift的容器，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/2bIEz65umR1j.png?imageslim" alt="mark"></p><p># 查看glance-img容器下对象数据</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/bnU9G5veXO2l.png?imageslim" alt="mark"></p><p># 与新创建的cirros_img的id进行对比</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/oWjg9i8SILoE.png?imageslim" alt="mark"></p><p># 在Horizon中验证</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/sP4X2kuk2fxY.png?imageslim" alt="mark"></p><h3 id="配置cinder-backup使用swift作为后端"><a href="#配置cinder-backup使用swift作为后端" class="headerlink" title="配置cinder-backup使用swift作为后端"></a><strong>配置cinder-backup使用swift作为后端</strong></h3><p>1）安装cinder-backup组件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-cinder -y</span><br></pre></td></tr></table></figure><p>2）编辑/etc/cinder/cinder.conf文件，完成如下配置：（注释掉的是NFS的backup后端）</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/8KmcOV53I1BB.png?imageslim" alt="mark"></p><p>3）重启cinder相关服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-cinder-*</span><br></pre></td></tr></table></figure><p>4）验证：</p><p># 查看当前cinder服务</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/E8oCJLMk2PGG.png?imageslim" alt="mark"></p><p># 创建一个卷swift_test_volume01</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/F0Kr0heXiiNX.png?imageslim" alt="mark"></p><p># 创建该卷的备份卷swift_test_backup_volume01</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/53Qz67SiG7pV.png?imageslim" alt="mark"></p><p># 查看当前swift容器</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/y6j8kJKjjHTF.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/e2OCCWOGQIDv.png?imageslim" alt="mark"></p><p># 删除源卷，从备份卷恢复</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/wMdosQBOx1Ln.png?imageslim" alt="mark"></p><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、swift中ring对应华为分布式存储FusionStorage原理中哪个算法？其作用是什么？开源ceph实现同样功能是否也采用一致性hash算法？</strong></p><p><strong>2、swift中的zone对应分布式存储的哪个进程？作用是什么？</strong></p><p><strong>3、一致性hash中的vNode在华为FusionStorage和中兴Ceph中分别取值为多少？其所起的作用是什么？</strong></p><p><strong>4、swift中强一致性和弱一致性的区别是什么？华为FusionStorage和中兴Ceph分别是采用哪种数据一致性规则？</strong></p><p><strong>5、华为FusionStorage是否源自开源Ceph？请从软件架构的角度分析，并说出理由？</strong></p><p><strong>6、一致性hash与余数hash的区别是什么？为什么在对象存储和分布式存储中普遍采用一致性hash？</strong></p><p><strong>7、请分别说出对象存储中account、container和object的作用和各个角色之间关系？以及数据在对象存储中的存取路径？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Swift 最初是由Rackspace公司开发的高可用分布式对象存储服务，并于2010年贡献给OpenStack开源社区作为其最初的核心子项目之一，为其Nova子项目提供虚机镜像存储服务。Swift构筑在比较便宜的标准硬件存储基础设施之上，无需采用 RAID（磁盘冗余阵列），通过在软件层面引入一致性散列技术和数据冗余性，牺牲一定程度的数据一致性来达到高可用性和可伸缩性，支持多租户模式、容器和对象读写操作，适合解决互联网的应用场景下&lt;strong&gt;非结构化数据存储&lt;/strong&gt;问题。Swift在OpenStack系统中不依赖于任何服务，可以独立部署为其他系统提供分布式对象存储服务。而在OpenStack的应用中，其Proxy Server往往由Keystone节点兼任，由Keystone来完成服务访问的安全认证。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-块存储管理服务Cinder</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E5%9D%97%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Cinder/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-块存储管理服务Cinder/</id>
    <published>2020-03-03T14:16:40.000Z</published>
    <updated>2020-03-03T14:32:51.302Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>OpenStack早期版本是使用nova-volume为云平台提供持久性块存储服务器的。从Folsom版本后，就把作为Nova组成部分的nova-volume分离了出来，形成了独立的Cinder组件。Cinder本身并不直接提供块存储设备实际的管理和服务，而是在虚拟机和具体的存储设备之间引入一个抽象的“逻辑存储卷”。Cinder与Neutron类似也是通过Plugins-Agent的方式通过添加了不同厂家的DRIVE来整合多种厂家的后端存储设备，并通过提供统一的API接口的方式为云平台提供持久性的块设备存储服务，类似于Amazon的EBS（Elastic Block Storage）。Cinder服务的实现在OpenStack众多服务中，只依赖Keystone服务提供认证。可能有些人觉得Cinder提供的Volume作为云主机的云磁盘，因此Cinder与Nova也有依赖关系。其实，这是一种错误的映像，Cinder-Volume创建的“逻辑存储卷”不仅可以用于云主机的云磁盘，也可以用于其他场景，其创建卷的过程与Nova创建的云主机的状态并没有直接关联。或者换个角度来看，Nova创建的云主机也可以不用挂载Cinder创建的Volume而正常运行。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rdbsUqaYc6WV.png?imageslim" alt="mark"></p><p>在OpenStack系统中，共有4中存储类型，分别是：<strong>临时存储、块存储、对象存储</strong>和<strong>文件系统存储</strong>，负责实现其功能服务的也不相同。下表就是OpenStack系统中，各种存储类型在访问方式、客户端、管理服务、生命周期、容量和典型应用场景方面的对比。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/LHLyAXWvBSq1.png?imageslim" alt="mark"></p><p>上面的4种存储类型总体上分为<strong>临时性存储</strong>和<strong>持久性存储</strong>两大类，除了Nova管理的临时存储外，其余都是持久性存储。所谓<strong>临时存储（Ephemeral Storage）</strong>，就是指如果只部署了Nova服务，默认分配给虚拟机实例instance的系统磁盘为计算节点的本地磁盘空间，以文件形式存放在本地磁盘中，随着虚拟机实例instance终止，该存储空间也会被释放，虚拟机实例instance的数据无法持久性保存。而<strong>持久性存储（Persistent Storage）</strong>，是指持久化存储设备的生命周期独立于任何其他系统资源和设备，无论虚拟机实例instance的运行状态如何，是否被终止，其上存储的数据一直可用。</p><p>在OpenStack支持的持久化存储中，块存储操作对象是磁盘或卷（Volume），将其直接挂在到主机，一般用于主机的直接存储空间和数据库应用，DAS和SAN设备都可以提供块存储功能，甚至NFS作为Cinder存储后端时，也可以当做块存储设备来使用（OpenStack的文件存储管理服务是Manila，Manila也可以使用NFS作为存储后端，这时NFS被当做文件系统存储使用，但是NFS挂载为Cinder的后端存储时，只能当做块存储使用）。总结起来而言：<strong>块存储就是通过SAN或iSCSI等存储协议将存储设备端的卷（Volume）挂载到虚拟机上并进行分区和格式化，然后挂载到本地文件系统使用的存储实现方式。</strong></p><h2 id="Cinder的架构"><a href="#Cinder的架构" class="headerlink" title="Cinder的架构"></a><strong>Cinder的架构</strong></h2><p>在OpenStack中，块存储服务Cinder为Nova项目所实现的虚拟机实例提供了数据持久性的存储服务。此外，块存储还提供了Volumes管理的基础架构，同时还负责Volumes的快照和类型管理。从功能层面来看，Cinder以插件架构的形式为各种存储后端提供了统一API访问接口的抽象层实现，使得存储客户端可以通过统一的API访问不同的存储资源，而不用担心底层各式各样的存储驱动。Cinder提供的块存储通常以存储卷的形式挂载到虚拟机后才能使用，目前一个Volume同时只能挂载到一个虚拟机，但是不同的时刻可以挂载到不同的虚拟机，因此Cinder块存储与AWS的EBS不同，不能像EBS一样提供共享存储解决方案。除了挂载到虚拟机作为块存储使用外，用户还可以将系统镜像写入块存储并从加载有镜像的Volume启动系统（SAN BOOT）。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/eOFThv3VQROR.png?imageslim" alt="mark"></p><p>Cinder的逻辑架构上图，除了cinder-client外，其余均是Cinder服务的核心组件，共有4个：<strong>cinder-api、cinder-scheduler、cinder-volume</strong>和<strong>cinder-backup</strong>。而cinder-client其实就是封装了Cinder提供的REST接口的CLI工具。</p><ul><li><strong>cinder-api：</strong>对外提供REST API，对操作需求进行解析，对API进行路由并寻找相应的处理方法，包含卷的CRUD，快照的CRUD、备份、Volume Type的管理、卷的挂载/卸载等操作。Cinder API本质上其实是一个WSGI App，启动Cinder API服务相当于启动一个名为osapi_volume的WSGI服务去监听Client的HTTP请求。OpenStack定义了两种类型的Cinder资源，包括<strong>核心资源（Core Resoure）</strong>和<strong>扩展资源（Extension Resoure）</strong>。而核心资源及其API路由器分为V1及V2两个版本，分别放在V1和V2两个目录下，其中API路由器（目录中的router.py）负责把HTTP请求分发到其管理的核心资源中去；V1的核心资源则包括卷（Volume）、卷类型（Volume Type）、快照（Snapshot）的操作管理，比如创建和删除一个卷，或为某个卷做一个快照等；V2的核心资源增加了Qos、limit及备份的操作管理（H版本以后）。</li><li><strong>cinder-scheduler：</strong>负责收集后端存储Backends上报的容量、能力信息。根据指定的算法完成卷到cinder-volume的映射。在Folsom版本中，cinder-scheduler服务只是实现了<strong>简单调度（Simple Scheduler）算法</strong>和<strong>随机调度（Chancer Scheduler）算法</strong>。简单调度算法就是获取活动的卷服务节点，按剩余容量从小到大排列，选择剩余容量最多的Host节点；随机调度算法就是在满足条件的节点中随机挑选出一个Host节点。在G版本后有了类似nova-scheduler的基于过滤和权重的新调度策略FilterScheduler。</li><li><strong>cinder-volume：</strong>多节点部署，使用不同的配置文件、接入不同的backend设备，由各存储厂商插入driver代码与设备交互完成设备容量和能力信息收集、卷操作。每个存储节点都会运行一个cinder-volume服务，若干个这样的存储节点联合起来可以构成一个存储资源池。</li><li><strong>cinder-backup：</strong>实现卷的数据备份到其他介质。目前支持的有以Ceph、Swift和IBM TSM（Tivoli Storage Manager）为后端存储的备份存储系统，其中默认的是采用Swift备份的存储系统。与Cinder-volume类似，Cinder-backup也通过Driver插件架构的形式与不同的存储备份后端交互。</li><li><strong>cinder-db：</strong>提供存储卷、快照、备份、service等数据，支持Mysql、PG、MSSQL等SQL数据库。</li></ul><p>Cinder服务除了上面的逻辑架构外，还有部署架构。在实际生产中，为了实现数据持久化存储的高可用功能，避免数据丢失风险，往往要对Cider服务的关键组件以及相关数据库、消息队列进行HA。基本思路如下图：<strong>Cinder服务的关键组件可以部署在一个节点也可以分布式部署在多个节点，icinder-api，cinder-scheduler和cinder-volume均采用采用active/1active模式部署。利用HAProxy实现的负载均衡器实现API请求到多个cinder-api的分发，避免单个cinder-api故障影响块存储服务提供；cinder-scheduler利用RabbitMQ实现负载均衡模式，向多个cinder-scheduler节点分发任务，同时利用RabbitMQ收取cinder-volume上报的能力信息，调度时，scheduler通过在DB中预留资源从而保证数据一致性。多个cinder-volumen同时上报同一个backend的容量和能力信息，并同时接受请求进行处理。而数据库DB往往采用主备或集群部署，实现数据库的HA。</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dE9nSkIpfObP.png?imageslim" alt="mark"></p><h2 id="Cinder-Scheduler的过滤和加权机制"><a href="#Cinder-Scheduler的过滤和加权机制" class="headerlink" title="Cinder-Scheduler的过滤和加权机制"></a><strong>Cinder-Scheduler的过滤和加权机制</strong></h2><p>在G版本后cinder-scheduler与nova-scheduler类似，有了基于过滤和权重的新调度策略FilterScheduler，其工作原理如下图所示，首先使用一个指定的过滤器过滤后得到符合条件的cinder-volume节点，然后对列表中cinder-volume的节点计算权重并进行排序，获得最佳的一个Host节点。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5a2oHkCNo6vN.png?imageslim" alt="mark"></p><p>上图中，scheduler负责收集后端上报的容量、能力信息，根据设定的算法完成卷到指定cinder-volume的调度。其中关键环节就是第二步根据后端的能力进行筛选，其筛选的依据主要来自三个方面：</p><ul><li>根据Driver定期上报后端的能力和状态进行筛选；</li><li>根据管理员admin创建的卷类型进行筛选；</li><li>创建卷时，根据用户指定的卷类型进行筛选；</li></ul><p><strong>cinder-scheduler服务提供了调度重试机制，在通过排序挑选出最优的Host节点后，将创建卷的消息发送给该节点，并由该节点的cinder-volume服务来处理。如果该节点的cinder-volume服务在处理过程中由于某些条件不满足而导致返回错误结果，cinder-scheduler将选择次优Host节点重试。最大可重试次数默认是3次，并且可以设置。</strong></p><p>cinder-scheduler实现了多种Filter和Weighter的算法。所有的Filter实现都在cinder/scheduler/filters目录中，所有的Weighter的实现在cinder/scheduler/weighters目录中。主要的过滤器功能如下：</p><ul><li><strong>Availability Zone Filter：</strong>在指定的Zone中选择Host。</li><li><strong>Retry Filter：</strong>在没有重试过的Host中重新进行调度。</li><li><strong>JsonFilter：</strong>允许应用以Json的形式写出比较复杂的查询Host的表达式。</li><li><strong>CapabilitiesFilter：</strong>在一个由各种类型存储组合（LVM、Ceph等）的存储系统中选择某种存储类型。</li><li><strong>CapacityFilter：</strong>容量过滤，只选择那些剩余容量大于要创建的卷的大小的Host。</li></ul><h2 id="Cinder-Volume的插件类型"><a href="#Cinder-Volume的插件类型" class="headerlink" title="Cinder-Volume的插件类型"></a><strong>Cinder-Volume的插件类型</strong></h2><p>Cinder-volume服务通过插件式（Plugins）的Driver与各种后端Volume Providers进行交互，从而屏蔽了后端各种开源或商业的存储实现方式，并对外提供统一的Cinder API接口。Cinder项目在架构上与Neutron类似，均是采用可插拔的插件来兼容各个厂商的存储设备和驱动，从而使得各个存储厂商均可参与Cinder项目并提供自己的存储插件供用户使用，而用户也可以自主选择适合自己的存储产品，并只需在Cinder的配置文件中进行适当的存储Driver设置即可。在Cinder中，Cinder-volume为不同的Volume后端（Volume Providers，通常为不同厂商的存储设备）<strong>提供了统一的Driver接口</strong>，而Volume Provider只需实现这些接口即可。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/LGgguzi6SUV3.png?imageslim" alt="mark"></p><p>从实现上来看，Cinder的插件可以分为<strong>基于软件实现的插件</strong>和<strong>基于硬件的插件</strong>。基于软件实现的插件又可以分为<strong>基于文件系统的插件（开源Gluster FS、NFS、Ceph RBD和IBM的GPFS等插件）和基于块存储的插件（LVM插件）</strong>。<strong>基于硬件实现的插件主要是各个厂商针对自己的存储设备所实现的插件</strong>，这些插件主要是基于FC（Fiber Channel）和iSCSI协议来实现的，如IBM的XIV、V7000、DS8000和System Flash等存储设备插件，以及EMC的VNX、VMAX等存储设备插件，当然还有很多如Dell、Netapp、HPE和华为等存储厂商所提供的插件。对于用户而言，部署Cinder块存储服务的首要步骤便是选择使用哪种Cinder存储后端插件，是采用完全基于开源软件实现的插件还是采用存储设备厂商提供的特定插件，需要根据用户自己的预算和特定的存储环境进行选择。通常LVM插件适用于各种厂商存储设备，但是其I/O性能相对较差，因此比较适合轻负载应用系统。而针对特定厂商存储设备的FC插件通常具有更好的I/O性能和较低的I/O延迟，但是需要采购特定厂商的存储设备，因此比较适合I/O要求较高的生产环境系统。除了基于本地LVM的Cinder插件和Vendors厂商插件外，日立（Hitachi）的Mitsuhiro团队还提出了一种集合LVM和FC的共享LVM插件。在我们的实验环境中，主要使用了LVM和NFS两种后端存储，因此这里主要介绍这两种开源的软件插件，其他插件请参考服务商提供的产品手册。而后续我们会将实验环境的后端块存储改造为ceph-rbd，至于Cinder对接Ceph实战会在OpenStack运维实践中专门介绍，这里不再赘述。</p><h3 id="LVM插件实现"><a href="#LVM插件实现" class="headerlink" title="LVM插件实现"></a><strong>LVM插件实现</strong></h3><p>基于软件实现的Cinder插件主要以开源软件为主，其中使用最为普遍的是LVM插件、NFS插件和Ceph RBD插件。下图中，在LVM插件架构中，Volumes以VG的LV形式被创建，即用户在配置Cinder块存储和创建Volumes之前，需要在存储节点上创建卷组VG，并在配置文件中指定使用此VG进行Volumes创建源。此外，计算节点与存储节点之间的存储网络通过iSCSI协议实现，运行虚拟机的计算节点作为iSCSI的发起端（iSCSI Initiator），而运行Cinder-volume的存储节点作为iSCSI的目标端（iSCSI Target），用户通过Cinder-API创建的每个Volume对应的都是存储节点中VG上的一个逻辑卷LV，当用户发起Attach（或Detach）操作时，存储节点上的Volumes通过iSCSI协议自动挂载到计算节点虚拟机上（或从计算节点虚拟机上卸载）。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/NyWEQGaewpJL.png?imageslim" alt="mark"></p><p>在基于LVM的Cinder创建架构中，Volume操作完全由LVM来实现，此外，全部计算节点虚拟机上的I/O通过软件iSCSI协议集中传输到存储节点，因此采用LVM插件实现的Cinder块存储比较适合轻负载I/O应用场景，而且由于全部虚拟机的读写I/O通过软件iSCSI协议汇聚至存储节点，这很容易造成存储节点的I/O性能瓶颈。不过使用LVM插件的优势在于无须特定存储设备和对应插件的支持，任何存储设备都可以映射到存储节点Linux系统中，并通过LVM插件架构实现Cinder块存储服务。</p><h3 id="NFS插件实现"><a href="#NFS插件实现" class="headerlink" title="NFS插件实现"></a><strong>NFS插件实现</strong></h3><p>Cinder默认使用的存储后端驱动是LVM，而LVM是一种基于块存储实现的Cinder插件，在基于软件实现的插件中，除了LVM外，还有很多基于文件系统的插件，如Gluster FS、GPFS和NFS等插件。其中，NFS是一种开源实现的文件系统，其配置实现和使用非常简单，这里主要介绍基于NFS插件实现的Cinder块存储服务，至于NFS服务端和客户端如何配置，请参见博客《<a href="http://note.youdao.com/noteshare?id=068c1fb7c996dc282bedf724ca2efb24&amp;sub=EB4FF085B9CE4FC4A50B976F80B82D91" target="_blank" rel="noopener">Linux开源网络文件系统NFS</a>》。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7GJSTM2fXmBD.png?imageslim" alt="mark"></p><p>从本质上而言，<strong>基于NFS的Cinder块存储服务并不允许虚拟机像LVM驱动一样访问块级别的存储设备。相反，NFS驱动在NFS共享目录上创建对应Volume的文件，并通过计算节点将文件映射为块设备供虚拟机使用</strong>。基于文件系统的块存储服务通常采用控制流与数据流分离的设计，即用户通过控制节点对运行Cinder-volume的存储节点进行Volume相关的控制操作，而虚拟机对存储设备的访问并不经过存储节点，而是通过计算节点后直接进入Volume Provider，这里Volume Provider就是NFS服务器。由于Cinder屏蔽了不同存储后端的差异，并对外提供统一的API访问，因此基于NFS驱动的Cinder块存储服务与基于LVM的Cinder块存储服务在Volume操作上并无差异。</p><h3 id="Multi-Backends实现"><a href="#Multi-Backends实现" class="headerlink" title="Multi-Backends实现"></a><strong>Multi-Backends实现</strong></h3><p>对于公有云而言，多后端存储实现的优势在于可为用户提供更灵活的块存储使用选择，而对于私有云建设而言，多后端存储的实现允许采用不同的厂商存储插件，以集成不同的存储设备或存储解决方案，从而节省存储设备上再投资成本。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/iicKI6ANVVyS.png?imageslim" alt="mark"></p><p>在OpenStack中，Cinder项目也实现了多后端存储功能（Storage Multi-Backends），通过并行运行多个存储后端插件和驱动。实际使用时，可以通过指定存储类型的形式来指定Volume应该由哪个运行中的存储后端提供。通过Cinder的Multi-Backends功能，可以配置多个后端存储解决方案以供相同的OpenStack计算资源使用。针对配置的每一个存储后端，Cinder会启动一个对应的Cinder-volume服务（服务名称格式为“hostname@backend_name”）。理论上，可以自由组合不同的存储后端插件，如同时实现LVM后端和Ceph RBD后端，或者IBM存储后端和EMC存储后端。为了简单起见，我们在实验环境中实现LVM和NFS驱动的多后端组合，部署仍然采取控制节点加存储节点模式，存储多后端插件（LVM和NFS Plugins）由存储节点加载，并由控制节点统一管理。<strong>Cinder多后端存储的实现与单独存储后端的实现类似，不同之处在于，前者需要在存储节点的/etc/cinder/cinder.conf配置文件中同时配置多个存储后端的实现部分，并在控制节点中为每个存储后端创建对应的存储类型，在多后端场景中进行Volume操作时，需要通过指定存储类型来使用特定的存储后端。</strong></p><h2 id="Cinder的卷操作流程"><a href="#Cinder的卷操作流程" class="headerlink" title="Cinder的卷操作流程"></a><strong>Cinder的卷操作流程</strong></h2><p>Cinder的核心功能就是对卷的管理，可以对卷、卷的类型、卷的快照、卷备份进行处理。在Cinder提供的卷有关的上述操作中，创建、扩展、删除、Attach和Detach是最核心的功能，也是块存储工作过程中最常使用的功能。其他如快照创建和基于镜像的Volume创建也是比较重要的功能。但是，我们这里主要讨论实际生产中最常使用3个流程：卷的创建、挂载和卸载。理解了这3个流程，卷的其他操作流程的理解也就水到渠成了。</p><h3 id="创卷流程"><a href="#创卷流程" class="headerlink" title="创卷流程"></a><strong>创卷流程</strong></h3><p>在使用Cinder提供的“逻辑磁盘卷”之前，我们首先要完成卷的创建，尤其在使用多后端存储的生产环境下，创建卷时还需要特意指定卷的类型。如下图所示，就是一个典型的生产环境下，多后端存储的创卷流程。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Yxgbt1lca68d.png?imageslim" alt="mark"></p><ul><li><strong>Step1：</strong>用户下发创卷请求，请求API中携带创建卷的相关关键字段，如{“name”:”test”,”size”:”100”,”type”:”IPSAN”}；</li><li><strong>Step2：</strong>cinder-api校验用户是否具有权限，以及做一些基本校验，quota预占等操作，并通过消息队列的cast异步返回卷信息（生成卷id）。</li><li><strong>Step3：</strong>cinder-api将创卷消息投递到cinder-scheduler消息队列中；</li><li><strong>Step4：</strong>cinder-scheduler从自己的消息队列中订阅并消费创卷消息，根据各个cinder-volume定期上报的后端存储能力以及卷信息，选择一个主机进行创卷。由于在创卷请求中，指定了后端存储类型type为IPSAN，因此cinder-scheduler通过CapabilitiesFilter过滤器找出所有安装IPSAN插件的后端存储节点，再根据创卷请求中size值，筛选出符合要求的节点。</li><li><strong>Step5：</strong>cinder-scheduler调度到主机后，将消息投递到相应的cinder-volume队列中，上图中表示创卷请求消息被投递到IPSAN后端的cinder-volume队列。</li><li><strong>Step6：</strong>cinder-volume从自己的消息队列中消费创卷消息，调用driver的接口进行创卷，最后更新数据库。</li><li><strong>Step7：</strong>cinder-volume调用相应的driver插件。</li><li><strong>Step8：</strong>Drive插件r将发送相应的创卷命令到实际的存储设备。</li></ul><h3 id="挂载卷的流程"><a href="#挂载卷的流程" class="headerlink" title="挂载卷的流程"></a><strong>挂载卷的流程</strong></h3><p>挂载卷/卸载卷是通过Nova和Cinder的配合最终将远端的卷连接到虚拟机所在的Host节点上，并最终通过虚拟机管理程序映射到内部的虚拟机中。挂载卷的流程如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Dlu4PkwjmlLb.png?imageslim" alt="mark"></p><ul><li><p><strong>Step1：</strong>客户端发起虚拟机实例instance挂载卷的API请求到nova-api，nova-api发起异步调用cast，将挂卷请求通过attach_volume()函数发给nova-compute，nova-compute通过volume_bdm()函数向nova-api返回块设备的映射信息，并通过reserver_block_device_name()函数保存块设备的基础信息，然后通过REST API向cinder-api发起获取卷信息请求，利用get()函数传递主机的信息，如hostname, iSCSI initiator name, FC WWPNs。</p></li><li><p><strong>Step2：</strong>Cinder API根据传递的卷信息，利用db_volume_get()函数去cinder-db中查询该卷归属的volume详细信息，将请求发给cinder-volume。</p></li><li><strong>Step3：</strong>Cinder Volume通过创建卷时保存的host信息找到对应的Cinder Driver。</li><li><p><strong>Step4：</strong>Cinder Driver通知存储允许该主机访问该卷。</p></li><li><p><strong>Step5：</strong>Cinder-Driver返回该存储的连接信息（如iSCSI iqn，portal，FC Target WWPN，NFS path等）到nova-compute。</p></li><li><p><strong>Step6：</strong>Nova-compute调用check_volume()函数针对于不同存储类型进行主机识别磁盘的代码（ Cinder 提供了brick模块用于参考）实现识别磁盘或者文件设备，成功后调用reserve_volume()函数向cinder-api发起更新卷挂载等预占操作信息，cinder-api完成数据库卷的状态更新。</p></li><li><strong>Step7：</strong>Nova-compute向cinder-api通知进行卷的挂载，此操作分两步进行。首先，nova-compute发起卷的初始化连接信息，cinder-api向cinder-volume发起异步调用cast的初始化连接信息，cinder-volume收到请求后向cinder-driver发起初始化连接信息，完成后，cinder-driver返回初始化连接成功响应，并携带相关卷的iqn等信息给nova-compute。</li><li><strong>Step8：</strong>然后，nova-compute将卷的及主机的设备信息传递给hypervisor来实现虚拟机挂载磁盘，成功后nova-compute通知cinder-api完成卷的数据库状态更新。</li></ul><h3 id="卸载卷的流程"><a href="#卸载卷的流程" class="headerlink" title="卸载卷的流程"></a><strong>卸载卷的流程</strong></h3><p>卸载卷的流程与挂在卷类似，区别如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kmsnJ1Toa8VP.png?imageslim" alt="mark"></p><p>1）在nova-compute向cinder-api发起begin_detach_volume()请求后，nova-compute首先向hypervisor发起disconnect_volume()的请求，待hypervisor完成卷的连接删除后，nova-compute向cinder-api发起disconnect_volume()请求。</p><p>2）cinder-api将disconnect_volume()请求发给cinder-volume，cinder-volume根据卷的信息找到cinder-driver完成卷的连接删除，cinder-driver删除成功后返回none信息给nova-compute。</p><p>3）nova-compute收到cinder返回的卸载卷响应None后，nova-compute向cinder-api发起detach()请求，完成卷在cinder-db中的状态更新。</p><h2 id="Cinder的操作实战"><a href="#Cinder的操作实战" class="headerlink" title="Cinder的操作实战"></a><strong>Cinder的操作实战</strong></h2><p><strong>由于我们的实验环境采用LVM和NFS多后端方式，因此在创建卷前需要首先创建存储类型。</strong></p><p><strong>步骤1：</strong>输入以下命令，创建lvm和nfs的存储类型</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cinder <span class="built_in">type</span>-create lvm</span><br><span class="line">cinder <span class="built_in">type</span>-create nfs</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/0Yoai0g107b1.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>输入以下命令，将存储类型绑定到对应的存储后端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cinder <span class="built_in">type</span>-key lvm <span class="built_in">set</span> volume_backend_name=kklvm01</span><br><span class="line">cinder <span class="built_in">type</span>-key nfs <span class="built_in">set</span> volume_backend_name=kknfs01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/11bi37Xxx8QQ.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>输入以下命令，在lvm存储后端创建大小为2GB，名称为lvm-volume1的Volume</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cinder create --volume-type lvm --name lvm-volume1 2</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/2Brk1BB4xuyt.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>输入以下命令，在nfs存储后端创建大小为2GB，名称为nfs-volume1的Volume</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cinder create --volume-type nfs --name nfs-volume1 2</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6NxYGe1Vc7ux.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>输入以下命令，查看当前系统的卷列表信息，并在Horizon中查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cinder list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pBKYTG1jfjUU.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/wmVvJXDo8YwV.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>在NFS后端的挂载目录下，我们可以看见有一个volume-前缀的文件，后面跟的就是Volume的ID</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pCcs285eguik.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，在NFS后端创建一个启动卷boot-nfs-vol-cirros，大小1G，可用分区nova，卷类型nfs，卷来源Cirros</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cinder create --volume-type nfs --name boot-nfs-vol-cirros --image Cirros --availability-zone nova 1</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/tcVPyhE9HC13.png?imageslim" alt="mark"></p><p><strong>步骤8：</strong>执行以下命令，利用启动卷boot-nfs-vol-cirros拉起一个虚拟机实例nfs-boot-instance01</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server create --flavor Flavor_Web --volume 3c5432ca-9969-405d-9dad-d416660945f2 nfs-boot-instance01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/fcnuBhsQJ4TQ.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rcVzUyVydFHa.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/i8wenJsuanm5.png?imageslim" alt="mark"></p><p><strong>步骤9：</strong>执行以下命令，将lvm-volume1卷挂载到虚拟机实例nfs-boot-instance01上，作为一个数据盘/dev/vdb</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server add volume nfs-boot-instance01 lvm-volume1</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HOlhyxS5mRFS.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pHyy3MCc6JS6.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ap86KNGU12Ue.png?imageslim" alt="mark"></p><p><strong>步骤10：</strong>执行以下命令，将lvm-volume1卷从虚拟机实例nfs-boot-instance01上卸载掉</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server remove volume nfs-boot-instance01 lvm-volume1</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HxKpyl9P4Sah.png?imageslim" alt="mark"></p><p><strong>步骤11：</strong>执行以下命令，创建卷lvm-volume1的快照snap-lvm-volume1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cinder snapshot-create --name snap-lvm-volume1 lvm-volume1</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/MFtW0WIJdFjg.png?imageslim" alt="mark"></p><p><strong>步骤12：</strong>执行以下命令，将卷lvm-volume1的扩容到4G</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cinder extend lvm-volume1 4</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/jEaXohGXcH0H.png?imageslim" alt="mark"></p><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、OpenStack中有哪几种类型的存储？各自的管理服务和应用场景是什么？</strong></p><p><strong>2、块存储服务Cinder主要包含哪些组件？各组件的功能是什么？从创建卷的场景，通过时序图描述各组件的交互过程？</strong></p><p><strong>3、在高可用场景下，cinder-scheduler的消息分发依靠什么组件完成负载均衡功能？cinder-scheduler获取存储节点容量等信息，依靠哪种机制获取？（PUSH或PUT）</strong></p><p><strong>4、块存储服务Cinder实现多后端存储时，首先需要完成什么配置才能使用多后端存储（从LVM和NFS两种类型的后端存储进行阐述），在使用多后端时，必须依赖什么来创建卷？</strong></p><p><strong>5、已挂载的卷是否支持创建快照，若支持，应该使用什么命令？</strong></p><p><strong>6、要想从卷发放虚拟机实例，则卷必须设置哪种卷属性？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;OpenStack早期版本是使用nova-volume为云平台提供持久性块存储服务器的。从Folsom版本后，就把作为Nova组成部分的nova-volume分离了出来，形成了独立的Cinder组件。Cinder本身并不直接提供块存储设备实际的管理和服务，而是在虚拟机和具体的存储设备之间引入一个抽象的“逻辑存储卷”。Cinder与Neutron类似也是通过Plugins-Agent的方式通过添加了不同厂家的DRIVE来整合多种厂家的后端存储设备，并通过提供统一的API接口的方式为云平台提供持久性的块设备存储服务，类似于Amazon的EBS（Elastic Block Storage）。Cinder服务的实现在OpenStack众多服务中，只依赖Keystone服务提供认证。可能有些人觉得Cinder提供的Volume作为云主机的云磁盘，因此Cinder与Nova也有依赖关系。其实，这是一种错误的映像，Cinder-Volume创建的“逻辑存储卷”不仅可以用于云主机的云磁盘，也可以用于其他场景，其创建卷的过程与Nova创建的云主机的状态并没有直接关联。或者换个角度来看，Nova创建的云主机也可以不用挂载Cinder创建的Volume而正常运行。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-网络管理服务Neutron</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Neutron/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-网络管理服务Neutron/</id>
    <published>2020-03-03T12:40:36.000Z</published>
    <updated>2020-03-03T13:20:07.189Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Neutron便是OpenStack云计算中的网络服务项目，其源自早期的Nova-network网络组件。Nova-network从Nova项目中独立出来之后，社区成立了针对网络功能虚拟化的Quantum项目。但由于商标侵权的原因，在Havana版本后，Quantum项目更名为现在的Neutron项目。Neutron是OpenStack的核心项目之一，虽然是OpenStack核心项目中成熟相对较晚的项目，但是Neutron项目在社区的热度很高。直到目前，在最新发行的OpenStack版本中，Neutron是新增功能和问题修复最多的核心项目。OpenStack的网络服务由Neutron项目提供，Neutron允许OpenStack用户创建和管理网络对象，如网络、子网、端口和路由等，而这些网络对象正是其他OpenStack服务正常运行所需的网络资源。Neutron项目中实现的各种插件使得用户可以选择不同的网络设备和软件，并为OpenStack的网络架构和部署提供极大的灵活性。此外，Neutron提供了API Server以供用户进行云计算网络的定义和配置，而Neutron灵活多样的插件使得用户可以借助各种网络技术来增强自己的云计算网络能力。Neutron还提供了用以配置、管理各种网络服务的API，如L3转发、NAT、负载均衡、防火墙和VPN等高级网络服务。因此，我们大致可以画出Neutron在OpenStack上下文架构（增加个Heat云编排服务），如下图所示，Neutron已经成为OpenStack三大核心（计算、存储、网络）之一，对外提供Naas（Network as a Service）服务。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/D2PDJQIy9DDG.png?imageslim" alt="mark"></p><p>目前为止，Neutron支持的特性如上表所示，Neutron支持的这些特性，涵盖了2~7层的各种服务。除了基本的、必须支持的二层、三层服务外，4~7层支持的服务有：LBaaS（负载均衡服务）、FWaaS（防火墙服务）、VPNaaS（VPN服务）、Metering（网络计量服务）、DNSaaS（DNS服务）等。Neutron在大规模高性能层面，还支持L2POP、BGP、DVR、VRRP等特性。<strong>Neutron项目与OpenStack其他项目（Cinder除外）不同的地方在于，其基本全是通过Plugin-Agent方式实现各类网络技术的软件化呈现，虽然不如Nova项目复杂，但确是OpenStack中最灵活、最难理解的一个项目，尤其是针对没有网络基础新手，可以说直接让您懵逼也不为过。因此，要学好Nuetron的除了重点理解各类资源概念外，还需要宏观的去理解Neutron的不同表现形成。</strong></p><h2 id="Neutron的应用方式"><a href="#Neutron的应用方式" class="headerlink" title="Neutron的应用方式"></a><strong>Neutron的应用方式</strong></h2><p>Neutron的应用本质上分为两大类：<strong>基于OpenStack的应用和基于SDN的应用</strong>。前者是在云的场景下，与OpenStack其他部件配合，为用户提供基于云的服务，此时Neutron只需要提供租户网络隔离和二层转发功能，使用户的云服务器能够进行网络互联互通。后者是在SDN场景下，作为SDN Controller或与SDN Controller（如ODL、OVN)一起，为用户提供NaaS服务。</p><h3 id="基于OpenStack的应用—从Provider-Network-Service到Self-Network-Service"><a href="#基于OpenStack的应用—从Provider-Network-Service到Self-Network-Service" class="headerlink" title="基于OpenStack的应用—从Provider Network Service到Self-Network Service"></a><strong>基于OpenStack的应用—从Provider Network Service到Self-Network Service</strong></h3><p>基于OpenStack的应用，就是原生的云应用，Neutron作为OpenStack中的网络实现部件，为用户提供网络服务，其本质上是一种Provider Network Service类型。在Provider网络中，由于二层网络直接接入物理设备，二层网络之间的通信也由物理设备进行转发，因此Provider网络在实现过程中无须L3服务，控制节点只需部署API Server、ML2核心插件及DHCP和Linux bridging代理即可。计算节点中的实例通过虚拟二层交换机直接接入物理网络，因此计算节点只需部署如Open vSwitch或Linux bridging等代理软件即可。先来看一个简单的例子，如下图左边所示，是一个单平面的租户共享网络，共享网络VLAN/FLAT就是Neutron网络实现控制方式。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/qaG3cHiA0Iqn.png?imageslim" alt="mark"></p><p>上图左边的含义为：租户可以创建许多虚机，但是这些虚机只能属于一个Network。不同租户共享这同一个网络，而且这个网络只能是VLAN网络或FLAT网络，网络类型必须单一实现。同时，在这个架构中，是通过外部物理网络的路由器实现网络包的转发功能，Neutron并不提供虚拟路由转发功能。而且因为各租户属于同一网络，也不支持IP地址重叠功能。其实，这个网络实现对应物理网络实现就是如上图右边所示，由一个交换机+一个路由器来完成。</p><p>如果网络都这么简单该多好，各种标准组织、厂家等没必要为各自的网络实现方式满世界BB，维护和开发人员也没必要花大力气去学习网络，整个世界都清净了。但是世界是复杂的，网络也是复杂的。就算只是和OpenStack其他服务为用户云服务，Neutron的简单网络实现也不是上面那么简单，而是下面这个样子：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/uXnl2zDAF8Ni.png?imageslim" alt="mark"></p><ul><li><ul><li>租户A独占一个VLAN网络，地址段为10.0.0.0/24，同时租户A的虚机要求能够外部网络互联互通（包括和租户B的虚机互联互通）。</li><li>租户B占用2个隧道网络，一个VxLAN网络，地址段也为10.0.0.0/24，另一个是GRE网络，地址段为10.0.1.0/24。而且，租户B这两个网络的虚机也要求和外部网络互联互通（包括和租户A网络内的虚机互联互通）。</li><li>租户A独占一个虚拟路由器，租户B独占一个虚拟路由器，且租户A网络地址段和租户B私有网络（1）地址段重叠，也就是要实现网络资源隔离功能。</li><li>租户A和租户B与外部网络互联互通时，要支持SNAT/DNAT功能。</li></ul></li></ul><p>可以看出，即使是基于OpenStack的云服务应用，在多租户场景下，Neutron也从最初始低端交换机提供角色发展成为支持各种协议、融合交换路由转发一体的，支持多租户隔离的网络实现角色，即Self-Network Service。上面的模型对应到物理网络实现就是如下的架构：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/LSSfpRqg6AAN.png?imageslim" alt="mark"></p><p>因此，Neutron基于OpenStack的应用，网络的实现一般都是Host内的虚拟交换机、子网和端口模型，3层-7层的路由等功能可以通过物理设备实现也可通过虚拟设备实现，取决于不同网络部署类型（Provider or Self）。</p><h3 id="基于SDN的应用"><a href="#基于SDN的应用" class="headerlink" title="基于SDN的应用"></a><strong>基于SDN的应用</strong></h3><p>不同的人心中，就有不同SDN实现方式。因此，这也是目前SDN亟待完成标准制定和冻结的需求所在。即使是业内公认的北向采用RESTful协议，大家都号称自己支持并采用的就是RESTful方式，但是对接起来兼容性仍然感人。既然要谈SDN的应用，我们首先看下什么是SDN？这里我们采用设备商和运营商共同认可的方式来介绍。通过下图的左边，大家可以对SDN有个初步感性认识。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/bL9MfzCHyOdj.png?imageslim" alt="mark"></p><p>SDN主要将网络的控制平面与数据转发平面进行分离，采用集中控制替代现有的分布式控制，并通过开放的可编程的接口实现“软件定义”的网络架构。<strong>SDN是IT化的网络，是“软件主导一切”的趋势从IT产业向网络领域延伸的标志性技术</strong>，其核心就是网络的“软化”。SDN的标准架构就是俗称的“三层两接口”，其实这种架构并不是SDN所独有，在现有网络中如VoLTE网络的核心网IMS也是一个标准的“三层两接口”架构，其目的就是实现转控分离。别忘了，IMS可是转发、控制和业务三层完全分离的一个架构体系。SDN的核心特点是将实体设备作为基础资源，抽象出NOS（网络操作系统），隐藏底层物理细节并向上层提供统一的管理和编程接口。以NOS为平台，开发的应用程序可以实现通过软件定义的网络拓扑、资源分配和处理流程及机制等。SDN架构的特点如下：</p><ul><li><ul><li>分离网络的控制功能和转发功能。</li><li>使得网络变得可编程控制。</li><li>底层的设备抽象成虚拟网络设备。</li><li>南向支持OpenFlow协议。</li></ul></li></ul><p>接下来，我们来看下Neutron作为SDN应用场景下逻辑架构是什么？如上图右边所示，Neutron具备SDN网络的4个特点。在Neutron基于SDN的应用场景中，所有SDN控制器都挂接在Netron之下，这不仅是因为业界期望Neutron能够成为统一的北向接口，也是源于Neutron的可扩展能力。我们再来看看Neutron内部组件实现方式是什么？先来放一张较复杂图，大家不要被吓着，重点看我图中标注出来的（1）、（2）、（3）三个地方，这些标注出来的地方就是Neutron的可扩展能力，其余部分会在后文中逐步介绍。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/1hX7WoTmKIP8.png?imageslim" alt="mark"></p><ul><li><ol><li><strong>支持Vendor Plugins（设备商/供应商插件）扩展。</strong>这是对Core API（Network、Subnet和Port三个核心资源的业务API)的扩展。原生的Neutron，实现Core API的载体是ML2插件，这是一种Driver方式，实现多种协议的协同，支持网络多种方式实现和共存。因此，作为设备商不会去扩展Core API，因为Core API正是运营商所追求的统一API接口模式。但是，Neutron原生的ML2管理的是虚拟交换机，如果需要管理设备商自己的交换机，则需要设备商自己扩展。因此，Neutron在设计时，就保留了Vendor Plugins的位置，便于不同设备商开发自己的插件并实现对接自己的SDN控制器。</li><li><strong>支持Service Plugins扩展。</strong>与第(1)点一样，不过它所对应的其他业务API（Neutron称之为Extension API，如上图中标注位置（3）点）。它的功能是：如果设备商觉得Neutron原生的Service Plugins不合适或者功能缺失，设备商可以扩展自己的的插件，然后对接自己的SDN控制器。比如与ODL对接时，此处的LBaaS，FWaaS、VPNaaS和L3 Service均需要替换。</li><li><strong>支持Extension API扩展。</strong>这一点可以说将Neutron的可扩展能力放大到极致。所谓挂一漏万、百密一疏，Neutron也不能保证自己的原生定义的业务API就能100%满足业界需求，所以Neutron也允许其他组织（包括运营商、设备商）对它的业务API进行扩展，并同时在标注点（2）完成对应插件的扩展，这样就能满足各种业务需求。</li></ol></li></ul><p>上面列举的3个可扩展点只是为了说明Neutron具备的优势，并不是它的全部，它的全部包括但不限于上图中列举出所有功能。也就是说，厂商或运营商可以根据自己的需要对Neutron进行扩展，如果不扩展，Neutron仍然可以提供完备的网络实现方案。为了和SDN网络的“三层两接口”架构进行对应，让大家更好的理解“<strong>Neutron就是SDN”</strong>这个说法，我们再将上面的进行抽象，就得出下面SDN应用场景典型逻辑架构图。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/9ntWGD8emzjW.png?imageslim" alt="mark"></p><p>Neutron对外提供的CLI和Web UI接口，其用户主要是“人”；对外提供的REST ful API接口，其用户主要是OpenStack之内的其他服务（如Nova等）以及OpenStack之外的其他系统（如MANO等）。Neutron管理的网元，主要以“软”网元为主（即虚拟网络功能）。这些“软”网元，有三种来源：</p><ul><li><ul><li>Linux原生（内核模块）的网络功能，比如LinuxBridge，LinuxRouter等。</li><li>开源的网络功能，比如OVS等。</li><li>厂商提供的闭源商用产品。</li></ul></li></ul><p>Neutron管理的网元，也可能涉及一些“硬”网元，比如采用隧道网络进行主机之间互通时，往往各个机柜中TOR交换机（如5300\9300等）就是隧道网络的VTEP（隧道终结点），此时这些TOR交换机也在Neutron的管理范围内。再比如数据内部的DCGW，Firewall、LB这些3-7层的硬件网络设备，也是Nuetron的管理对象。这就是Neutron作为SDN应用时承担的管理角色。</p><h2 id="Neutron的逻辑架构"><a href="#Neutron的逻辑架构" class="headerlink" title="Neutron的逻辑架构"></a><strong>Neutron的逻辑架构</strong></h2><p>Neutron服务的架构和OpenStack其他服务一样，是一种分布式架构，各子服务服务通过消息队列来完成异步通信，从而对外提供网络服务。如下图所示，主要包括neutron-server、neutron-agent、neutron-metadata、消息队列MQ几部分。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/PpHzaPOfQN2H.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>Neutron Server：</strong>包含neutron-api和neutron-plugin两部分。neutron-api对外提供统一的REST ful API接口，接收和响应外部其他用户API请求。neutron-plugin接收来自neutron-api的请求，建立并维护网络的逻辑状态，同时调用响应agent来实现各种网络服务，包括core-plugin和service-plugin两部分。</li><li><strong>Nuetron-agent：</strong>处理 Plugin 的请求，负责在 network provider （linuxbridgde、openvswitch、物理网络交换机等）上真正实现各种网络功能。核心的neutron-agent包括：neutron-ml2-agent（实现2层网络的功能，比如通过虚拟交换机linuxbridge或openstackvswitch实现2层转发功能）、neutron-dhcp-agent（实现DHCP服务）、neutron-L3-agent（实现3层路由转发功能）。</li><li><strong>Neutron-metadata：</strong>上面图中未画出，在后面节点服务分布图中相应位置。用来和nova-metadata服务通信，获取虚机的网络元数据。因为VM在启动时需要访问 nova-metadata-api 服务获取 metadata 和 userdata，这些 data 是该VM的定制化信息，比如 hostname, ip， public key 等。但VM 启动时并没有 ip，那如何通过网络访问到 nova-metadata-api 服务呢？答案就是 neutron-metadata-agent 让VM 能够通过 dhcp-agent 或者 l3-agent 与 nova-metadata-api 通信。</li><li><strong>Neutron-ML2-agent：</strong>响应来自ML2-plugin的调用，实现2层网络功能。由于ML2-plugin属于Core-plugin，用来控制实现Network、Subnet、Port等核心网络功能，因此ML2-agent负责完成这些核心网络功能的具体实现，概括起来有两种实现方式，主要基于软件虚拟的linuxbridge（官方推荐）或OVS（生产环境普遍采用）和基于专有硬件的物理交换机VLAN网络或VxLAN网络。</li><li><strong>Neutron-L3-agent：</strong>L3 Agent在Provider类型的网络中并不是必须的，而在Self-Service网络中却是必须的。L3 Agent服务主要在外网访问租户网络中的虚拟机时提供L3 Route功能和DNAT功能，其也需要访问消息队列，其路由功能默认通过 IPtables 实现。</li><li><strong>Neutron-dhcp-agent：</strong>DHCP Agent服务（neutron-dhcp-agent）主要为租户网络提供DHCP服务，DHCP Agent对于Neutron支持的全部网络Plugins都是相同的，即DHCP代理并不依赖用户选用的Plugin。DHCP Agent服务也需要访问消息队列以便同网络Plugins进行交互。</li><li><strong>AMQP-Server：</strong>Neutron Server，Plugin 和 Agent 之间通过 Messaging Queue 通信和调用。</li><li><strong>Neutron-DATABASE：</strong>存放 OpenStack 的网络状态信息，包括 Network, Subnet, Port, Router 等。</li></ul></li></ul><p>在Neutron中，除了上述介绍的DHCP Agent和L3 Agent，还有很多提供高级服务的插件代理，如VPNaaS代理、FWaaS代理和LBaaS代理等，而这些代理通过基于消息队列的RPC与Neutron的API Server、Plugins进行交互。Neutron之所以能够处理各种网络技术和协议，主要因为Neutron包含了实现各种网络技术的插件。从代码层面而言，<strong>插件是“可插拔（Pluggable）”的Python类和函数</strong>，<strong>这些Python类在Neutron API响应请求时被触发，同时插件在Neutron Server服务启动时加载，加载完成后插件被当成Neutron Server的一部分而运行</strong>。在Neutron中，API是Pluggable的，因此用户可以实现自己的Plugin并将其“插入”Neutron API中使用，通常不同插件实现的Neutron API是不同的。用户对插件的实现可以是<strong>完整性（Monolithic）</strong>的也可以是<strong>模块式（Moduler）</strong>的。Monolithic意味着用户需要实现对网络进行直接或间接控制的全部核心技术，由于其实现过程较为复杂，Monolithic形式的插件正被逐步淘汰，取而代之的是Moduler插件，其中最为成功的便是Moduler Layer2，即ML2插件。<strong>ML2插件解耦了对不同网络驱动的调用，它将驱动分为两个部分，即TyperDriver和MechanismDriver</strong>。在Neutron网络中，<strong>TyperDriver代表不同的网络隔离类型（Segmentation Type）</strong>，如Flat、Local、VLAN、GRE和VxLAN，TyperDriver负责维护特定类型网络所需的状态信息、执行Provider网络验证和租户网络分配等工作，而<strong>MechanismDriver主要负责提取TyperDriver所建立的信息并确保将其正确应用到用户启用的特定网络机制（Networking Mechanism）中</strong>。尽管ML2插件是个单一整体的插件，但是ML2插件支持多种TyperDriver和MechanismDriver，并且<strong>不同的MechanismDriver所支持的TyperDriver种类不一样</strong>，如LinuxBridge和OpevSwitch两种MechanismDriver都支持Flat、VLAN、VXLAN和GRE这4种TyperDriver，而L2 population仅支持VXLAN和GRE这2种TyperDriver，这意味着在启动L2 population时，用户不能使用Flat和VLAN进行租户网络的隔离。ML2插件各类型MechanismDriver与TypeDriver的矩阵管理如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/3egBBjWDsL4E.png?imageslim" alt="mark"></p><p>在Neutron中，Neutron Server服务扮演的是网络控制中心的角色，而实际上与网络相关的命令和配置主要在网络节点和各个计算节点上执行，位于这些节点上的Agents便是与网络相关命令的实际执行者，而Agents可以划分为<strong>L2 Agents与非L2 Agents</strong>两大类，如OpenvSwitch agent、LinuxBridge Agent、SRIOV nic switch Agent和Hyper-V Agent等属于L2 Agents。L2 Agents主要负责处理OpenStack中的二层网络通信，是Neutron网络中最为核心的部分。非L2 Agents主要包括L3 Agent、DHCP Agent和Metadata Agent等。Agents所获取的消息或指令来自消息队列总线，并且由Neutron Server或Plugins发出。由于Agents负责网络的具体实现，因此Agents与特定的网络技术、Plugins密切相关，如使用OpenvSwitch Agent则意味着通过OpenvSwitch技术来实现虚拟网络，并且其对应的插件是OpenvSwitch Plugin（被ML2插件取代）。大致上Neutron实现虚拟网络的实现过程就是：Neutron Server接收到客户端的API请求后，触发ML2插件进行请求处理，这里假设ML2插件已经加载了OVS Mechanism Driver（即OpenvSwitch插件），于是ML2插件将请求转发给OVS驱动，OVS驱动收到请求后使用其中的可用信息创建RPC消息，RPC消息以Cast形式投递到计算节点上特定的OVS Agent，计算节点上的OVS Agent接收到消息后便开始配置本地OpenvSwitch交换机实例。</p><p>这里有一个业界常使用的概念—<strong>参考实现，即Mechanism Driver与L2 Agent的一种组合</strong>，如Open vSwitch这个Mechanism Driver与Open vSwitch Agent这个代理的组合被称为OpenvSwitch参考实现，用Open vSwitch&amp;Open vSwitch Agent表示。<strong>不同的参考实现支持不同的非L2 Agent</strong>，如参考实现MacVTap&amp;MacVTap Agent不支持L3 Agent和DHCP Agent，这意味着如果用户在ML2插件配置时选择了使用MacVTap这个Mechanism Driver，并在计算节点/网络节点部署了MacVTap Agent，则该Neutron网络中不能使用L3服务和DHCP服务。不同参考实现支持的非L2 Agent的对应关系如下表所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5qIfo3ddTmcf.png?imageslim" alt="mark"></p><p>在Neutron中，L3 Service是个非常关键的服务，其主要提供虚拟机不同L2子网之间的L3路由，以及虚拟机与外网之间的SNAT和基于Floating IP的DNAT功能，如果OpenStack网络中未部署L3服务，则用户只能部署基于Provider的网络，而无法实现真正的云计算Self-Service网络。L3 Service主要由L3 Service插件及其API和对应的L3 Agent组成，从OpenStack的Juno版本开始，L3 Agent也可以部署在计算节点上以实现<strong>分布式虚拟路由（Distributed Vritual Router，DVR）</strong>功能。通常情况下，租户可以通过L3 Plugin提供的API创建虚拟Router和Floating IP，并通过Nova的API将Floating IP绑定到实例上，从而实现租户内网与外网的通信。</p><p>Neutron各组件在各节点的部署方案一般有两种：<strong>网络节点和控制节点合一部署，网络节点单独部署</strong>。在我们的实验环境采用的是前者，从逻辑功能上划分，其实两者都一样，Nuetron各组件服务在不同角色的节点上实际部署如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/NqtmkrJDxHRT.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>控制节点：</strong>部署 neutron server 、消息队列MQ、数据库NuetronDB等服务。其作用就是接收网络实现请求，建立网络逻辑模型，保存网络逻辑模型到NuetronDB，然后通过消息队列MQ调度网络节点和计算节点的各种agent完成网络的具体实现。简易流程如下：</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6hxSkHzOIMPx.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>计算节点：</strong>部署 core plugin 的agent，调用network provider来完成二层网络的具体实现。agent一直监听Nuetron MQ的消息，一旦发现有来自Plugin的网络建立请求，根据请求中网络实现需求，调用本节点的Network provider来实现网络功能。比如在我们的例子中采用linuxbridge来实现网络，那么计算节点的linuxbridge-agent就会调用本节点linux OS原生的linuxbridge来建立虚拟二层交换机。简易流程如下：</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/SgPsoP56O1Yv.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>网络节点：</strong>部署的服务包括：ML2- agent 和 service plugin 的 agent。其中，ML2-agent用来在网络节点完成具体二层网络实现，用来确保各计算节点虚机VM的数据流量通过网络节点的ML2-agent来互通，即网络节点为整个网络中数据流量汇聚点。而各种Service plugin的agent用来实现各种3层服务。在我们的例子中用到了DHCP三层服务，具体由DHCP-agent调用linux操作系统的dnsmasq来具体实现DHCP服务，同时默认实现DNS三层服务，其简易流程如下：</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nlqK2kvuHxvz.png?imageslim" alt="mark"></p><p>通过上面对各节点上Neutron各组家部署方案的了解，我们应该抽象并总结出Neutron在OpenStack的各个功能节点中所处的角色和功能：控制节点上的Neutron-Server并不实现具体的网络功能，它只是对各种虚拟网元和实体网元做管理配合的工作。控制节点的Neutron-Server进程（子服务）接收REST ful API外部请求，调用内部的Plugins通过RPC与agent进行交互，Neutron-server内的Puligins进程与各个agent进行共同完成网络的管理任务；计算节点上的neutron-ml2-agent通过调用本节点操作系统原生linuxbridge或OVS完成各种2层网络的具体实现，并完成虚机的虚拟网卡绑定。即用户创建的租户网络请求信息到达计算节点后，ml2-agent通过各种Bridge来实现2层虚拟交换机，或调用物理交换机来实现2层功能，同时将Port通过tap设备绑定到VM上，提供VM的MAC和IP，即实现虚拟网卡功能；网络节点上的neutron-ml2-agent建立各种bridge（OVS也是bridge）实现一个二层网络的汇聚转发网桥，各计算节点中的虚拟机互通或访问外部网络时，数据包首先发到这个二层汇聚转发网桥上，同时这个网桥会连接到物理网络中的一个设备（如交换机、路由器），通过这个设备再到达物理网络的网关，其上的DHCP-agent，采用dnsmasq进程提供DNS、DHCP和TFTP服务，一个网络对应一个DHCP，不同网络的DHCP通过namespace实现隔离。L3-agent，利用linux操作系统内核实现虚拟router功能，同时还提供SNAT/DNAT服务。</p><h2 id="Neutron的资源模型"><a href="#Neutron的资源模型" class="headerlink" title="Neutron的资源模型"></a><strong>Neutron的资源模型</strong></h2><p>在OpenStack的官网<a href="https://developer.openstack.org/api-ref/network/v2/，对其公开的REST" target="_blank" rel="noopener">https://developer.openstack.org/api-ref/network/v2/，对其公开的REST</a> ful API做了相关说明。这些RESTful API背后就是Neutron的网络资源模型。Neutron把它管理的对象统统称为资源，比如Network、Subnet、Port等，表面看与传统网络中概念一样，但是由于Neutron管理的范围（DC内）和对象的特点（Host内部虚机VM）等原因，与传统网络的概念并不完全相同，甚至有些令人困惑。Neutron管理的核心资源包括：Network、Subent、Port和Router。</p><ul><li><ul><li><strong>Network：</strong>是一个隔离的二层广播域。Neutron支持多种类型的network，包括local，flat，VLAN，VxLAN和GRE。其中，VxLAN和GRE属于隧道网络，local、vlan和flat属于非隧道网络。<strong>从全局看，Network是Neutron资源模型的“根”，Subnet隶属于它，Port也要隶属于它。</strong>在Network中引申出两个重要概念：<strong>运营商网络(provider network)和物理网络（provider:physical_network）</strong>。运营商网络（provider network）是相对于租户（project）创建的网络，即租户网络而言。租户创建的租户网络属于内网网络，和外部网络之间是隔离的，如果该租户创建的虚机VM有访问外部网络的需求，此时neutron就必须通过一个网络来映射这个外部网络，实现这个功能的就是运营商网络（provider network），即运营商网络就是运营商的某个物理网络在OpenStack的延伸。而物理网络（provider：physical_network)主要指为了满足租户（project）创建的虚机VM访问外网的需求（这里的外网不一定是internet，也可能是企业内部其他物理网络），需要创建一个运营商网络（provider network）来对外网进行映射，而这个映射是如何实现的呢？答案就是通过物理主机Host的物理网卡来完成，通过这物理网卡实现运营商网络和外部网络的对接。这个物理网卡就是neutron管理范围内的物理网络（如果Neutron作为SDN控制器，该物理网络好包括DC内部的物理数通网络设备)，这个概念在创建运营商网络类型为VLAN和FLAT是尤其明显，因为创建这两种网络类型时，其配置文件中明确要求对应物理网卡名称。但是当创建的运营商网络为隧道网络时，却不需要指定物理网卡。这又是为什么呢？答案就要从隧道网络的报文说起，如下图所示，隧道性网络离开主机的报文，外面有一层隧道Header的，这个Header包括隧道的源IP和目的IP。只要有了目的IP，主机Host的IP协议栈就会找到合适的网卡将报文转发出去。</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nvyViAIlOxPQ.png?imageslim" alt="mark"></p><p>因此，当运营商网络类型为VxLAN或GRE等隧道网络时，是通过对应物理网卡的IP找到物理网卡转发报文，本质上还是需要占用一个物理网卡，只是配置上无需指定物理网卡名称，只需完成隧道网络对应VTEP IP配置即可。这里面还隐含一个概念，就是隧道网络的物理网卡可以复用，即当我们的创建多个隧道网络类型的运营商网络时，可以通过同一个VTEP IP对应的物理网卡转发数据包。那么VLAN网络类型和FLAT网络类型运营商网络指定了物理网卡名称，是否也可以复用？答案是，VLAN网络可以通过在物理网卡上起子接口来实现复用，但是主机Host的物理网卡要配置成Trunk模式。这个不难理解，和实际物理网络交换机的实现方式一致。但FLAT网络一个网络必须独占一个物理网卡，这也是flat网络在实际生产中应用不多的原因，因为成本太大。</p><ul><li><ul><li><strong>Subnet：</strong>是一个IPv4或者IPv6地址段，其数据模型只包含两个字段cidr和ip_version。虚拟机VM的IP从subnet中分配。每个subnet需要定义IP地址的范围和掩码。<strong>network与subnet是1对多关系，即一个subnet只能属于某 network，一个network可以有多个subnet，这些subnet可以是不同的IP段，但不能重叠。</strong>表面看，Subnet只是代表纯逻辑资源，是一批IP地址的集合，但实际上每个IP背后都代表一个实体，如VM。那么就会产生以下两个问题：虚机VM的IP地址如何分配？虚机VM的DNS是什么？因此，Subnet除了cidr和ip_version等纯逻辑资源外，还隐含了IP核心网络服务的内容，即俗称的<strong>DDI服务（DHCP\DNS\IPAM）</strong>。当创建网络时，勾选启用DHCP服务，其对应配置文件中的enabel_dhcp=true，此时Neutron会自动创建一个DHCP Server，该DHCP Server被操作系统OS创建在namespace中，通过veth pair与Bridge网桥相连。DHCP可以配置为一个IP地址池，如果没配置，就默认采用subnet的cidr字段作为标准地址池。当创建网络时，勾选启用DHCP服务，同时也默认启用DNS服务，Neutron会根据虚机的主机名和绑定Port信息完成主机名和IP地址解析功能。有了DNS、有了DHCP，这个还不够。实际的组网中，一般还有一个IPAM（IP Address Management，IP地址管理）功能，数据模型中与其相关的字段是Subnetpool_id，其与DHCP数据模型的allocation_pools重复，也就意味着IPAM就是从subnet设定的地址池中管理虚机VM的IP地址分配。</li></ul></li></ul><ul><li><ul><li><strong>Port：</strong>可以看做虚拟交换机上的一个端口。port上定义了MAC地址和IP地址，当VM的虚拟网卡VIF（Virtual Interface） 绑定到port时，port会将MAC和IP分配给VIF。其实这个port其实就是linux操作系统的一个tap设备。而tap又是什么呢？它其实是Linux原生的虚拟网络设备，在linux中所指的“设备”并不是我们实际生产或生活中常见路由器或交换机这类设备，它其实本质上往往是一个数据结构、内核模块或设备驱动这样含义。在linux中，tap和tun往往是会被并列讨论，tap位于二层，tun位于三层，详见博客《<a href="https://kkutysllb.cn/2019/06/24/2019-06-24-Linux原生网络虚拟化实践/">Linux原生网络虚拟化实践</a>》一文。</li></ul></li></ul><ul><li><ul><li><strong>Router：</strong>如果说Port是Neutron资源模型的“灵魂”，那么Router就是Neutron资源模型的“发动机“，它承担着路由转发功能。Router的资源模型可以简单抽象为三部分：<strong>端口、路由表、路由协议处理单元</strong>。如下图所示：</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/aesgt38i4ON1.png?imageslim" alt="mark"></p><p>Router并没有使用某个字段表示端口，而是提供2个API以增加/删除端口，如下所示，非常好理解。</p><ul><li><ul><li>Add interface to router /v2.0/routers/{router_id}/add_router_interface  <strong>—-增加端口</strong></li><li>Remove interface from router /v2.0/routers/{router_id}/remove_router_interface  <strong>—-删除端口</strong></li></ul></li></ul><p>理论上，Router只要有了路由表以及对应的端口信息就可以进行路由转发，但是对于外部网络（Neutron管理范围之外的网络）路由转发，尤其公网internet，Router还用了一个特殊字段external_gateway_info（外部网关信息）来表示。这又是什么意思？我们通过一个例子来理解，如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/m7BaBpW9WPip.png?imageslim" alt="mark"></p><p>位于Nuetron管理网络的内部虚机VM，IP地址为10.10.10.10，它要访问位于公网（外部网络）的网站<a href="http://www.openstack" target="_blank" rel="noopener">www.openstack</a> .org，IP地址为104.20.110.33，需要经过公网的路由器RouterB才能到达。RouterB的Port2直接与Neutron网络节点的RouterA的Port1相连（中间经过Bridge相连）。这个RouterB就是真正意义上的外部网关，RouterB的接口Port2的IP地址120.192.0.1就是Neutron网络的外部网关IP。但是RouterB根本不在Neutron的管理范围内（RouterA才属于），而且Neutron也不需要管理它。从路由转发的角度讲，它只需要在RouterA中建一个路由表项即可，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/msiA3WTJMjTU.png?imageslim" alt="mark"></p><p>不过从RouterA的角度来看，不仅仅是增加一个路由表项那么简单（后面网络云SDN实现源码讲解时会详细阐述）。于是，Neutron提出了external_gateway_info这个模型，它由network_id，enable_snat，external_fixed_ips等几个字段组成。对应上图，其数据结构如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">“external_gateway_info":&#123;</span><br><span class="line">   "enable_snat": true，</span><br><span class="line">   “external_fixed_ips": [</span><br><span class="line">     &#123;</span><br><span class="line">      <span class="attr">"ip_address"</span>: <span class="string">"120.192.0.6"</span></span><br><span class="line">      <span class="string">"subnet_id"</span>: <span class="string">"b7832312223-ceb8-40ad-8b81---a332dd999dse"</span></span><br><span class="line">      &#125;,</span><br><span class="line">],</span><br><span class="line">   "network_id": "ae3405f12-aa7d-4b87-abdd-50fccaadef453"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中，ip_address就是RouterA的Port1的地址，subnet_id中对应的subnet的gateway_ip就是RouterB中的Port2地址。所以，external_gateway_info其实隐含了Neutron的管理理念：</p><ul><li><ul><li><strong>Neutron只能管理自己的网络。</strong></li><li><strong>Neutron不需要管理外部网络，只需知道外部网络网关IP即可。而它获取外部网关IP的方式就是通过subnet_id间接获取到其gateway_ip得到。</strong></li><li><strong>当enable_snat=true时，SNAT真正生效的地方在RouterA的port1上。</strong></li></ul></li></ul><p>如上面例子，当我们创建外部网关信息（external_gateway_info)的时候，Neutron会自动在Router上增加一个相应的路由表项，这个路由称为<strong>默认静态路由</strong>。同时，当我们在Router上增加一个端口时，这个端口背后的subnet的所有进出流量都要经过这个端口转发，这其实也是一种路由转发模式，称为<strong>直连路由</strong>。但是，无论是增加外部网关信息还是增加端口，其产生的路由表项均不会在Router的路由表routers中增加相应的表项。那这个routers到底有什么用？答案是，这个routers中只体现静态路由信息，它与默认路由一样，都是通往外部网络的。不过，静态路由的外部网络一般指的是私网，而默认静态路由的外部网络一般指的是公网，同时在external_gateway_info中并没有包含目的网段，也就是说除了直连路由（由链路层协议自动发现，不体现在路由表中），静态路由（体现在路由表中，需手工配置）外，到达其他所有目的地都走这条默认静态路由转发。默认静态路由、静态路由和直连路由的路由表转发逻辑如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/VgJnPNzUF86g.png?imageslim" alt="mark"></p><h2 id="Neutron网络资源的隔离机制"><a href="#Neutron网络资源的隔离机制" class="headerlink" title="Neutron网络资源的隔离机制"></a><strong>Neutron网络资源的隔离机制</strong></h2><p>OpenStack是一个多租户的云操作系统，其中提供网络实现Neutron自然也需要提供多租户的服务。所以，多租户之间资源隔离是Neutron必须要支持的特性。在Neutron的资源模型中，老版本有一个tenant_id的字段，而从Newton版本开始，又引入了project_id，在官方文档解释中，这两个都是：The ID of the project。因此，不论新老版本，这两个字段都表示一个意思：租户资源隔离。<strong>租户隔离，顾名思义就是为了资源隔离，但其更深层次的含义是多租户资源共享！！！</strong>—-这句话怎么理解?我们慢慢往下看。</p><h3 id="Neutron下面租户资源隔离的含义"><a href="#Neutron下面租户资源隔离的含义" class="headerlink" title="Neutron下面租户资源隔离的含义"></a><strong>Neutron下面租户资源隔离的含义</strong></h3><p>从租户的视角，或者从需求的角度来看，租户资源隔离有三种含义：<strong>管理面资源隔离、数据面资源隔离</strong>和<strong>故障面资源隔离</strong>。<strong>管理面资源隔离是指管理权限的隔离</strong>，如下图所示，两个网络Network VIN 100和Network VNI 200都是Neutron的管理范围，但是VNI 100属于租户A，VNI 200属于租户B，这就意味租户A的网络对租户B而言是不可见的，租户A也就无法管理（CRUD）租户B的网络，反之亦然。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Xl4XVNu7syKG.png?imageslim" alt="mark"></p><p><strong>数据面资源隔离是指数据转发的隔离。</strong>不同租户的网络不仅在管理上面隔离，而且在数据转发层面还可以“重复”。比如，租户A建立一个私网网段10.0.10.0/24，租户B同样可以建立10.0.10.0/24这个网段，这两个网段不仅不会冲突，还可以互通（通过3层转发），因此，数据面资源隔离的目的正是为了资源共享。<strong>故障面的资源隔离就是指一个租户网络出问题了，不能影响另一个租户网络，但是这是相对的</strong>。比如上面那个图，如果租户A和租户B同属于一个计算节点host1，当租户A网络的分布式虚拟路由器DVR-A出问题了，自然不会影响租户B的网络。但是，当租户A和租户B在网路节点的互通路由器router_global出问题了，那么自然两个网络都会受影响。再比如，该云系统在都在西咸数据中心A机房部署，那么这个机房的动力、线路等出问题，部署在该机房A的所有租户业务都会中断。因此，故障面的租户资源隔离，只是针对租户本身独占的资源而言，而对于多租户共享资源是无法做到也无需刻意去做的（就算是土豪动，也不能做到每个租户所有资源都独占，那属于败家！）。因此，在多租户共享层面资源只能尽量做到容错，比如高可用集群的架构。其实，不光Neutron的故障资源隔离是这样，所有OpenStack其他服务的故障隔离都是这种特点。</p><h3 id="Neutron的租户资源隔离实现模型"><a href="#Neutron的租户资源隔离实现模型" class="headerlink" title="Neutron的租户资源隔离实现模型"></a><strong>Neutron的租户资源隔离实现模型</strong></h3><p><strong>管理面的租户资源隔离实现模型</strong>：对于Neutron而言就是控制节点资源隔离实现。OpenStack的控制节点实现模型如下图所示，对于管理面而言，租户资源隔离一般涉及几个层面：<strong>硬件/操作系统层面、应用程序层面、数据库层面</strong>。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/DsMOufsjaHvD.png?imageslim" alt="mark"></p><p><strong>Neutron在这几个层面的隔离方案如下：</strong></p><ul><li><ul><li><strong>硬件/操作系统层面：</strong>不隔离。管理面（控制节点）都部署在一个Host，多个租户共享一个Host、一个操作系统，因此无法隔离，只能依靠集群高可用实现容灾保障。</li><li><strong>应用程序层面：</strong>不隔离。原因同上，管理面的各个服务，多租户共享，因此无法隔离，也只能依靠集群高可用在做容灾保障。</li><li><strong>数据库层面：</strong>轻微隔离。OpenStack的各个服务（包括Nuetron）针对多个租户，在数据库层面采用共享数据库，共享表的方式进行资源共享，只是通过表中字段(project_id）来区分不同租户。因此是一种轻微隔离方案。除了这种方案，数据库层面还可以采用独立数据库或共享数据库，独立表的方式来做隔离。</li></ul></li></ul><p>通过上面的分析，在OpenStack这种云操作系统中，为了尽量保证服务的高可用特性，针对控制节点而言，生产环境必然也必须采用高可用集群的方式部署，其目的就是保证业务的连续性。</p><p><strong>数据面的租户资源隔离实现模型：</strong>为清楚描述，这里我们采用OVS的实现方案，LinuxBridge的实现方案由于SecurityGroups和网桥brq在一起，会呈现独占和共享一体的画面，对于新手容易混淆。Nuetron的计算节点和网络节点都涉及数据转发，所以这两个节点也都涉及数据转发的租户隔离机制。计算节点的实现模型如下，br-ethx/br-tun、br-int分别只有一个实例，这个属于：<strong>用“多租户共享”的方案，来实现多租户隔离</strong>。比如br-int、br-ethx通过VLAN来隔离来自各个租户网络的数据流量，br-tun通过相应的tunnel ID，即VNI来隔离多租户的网络流量。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/xIRAR6cbjFDa.png?imageslim" alt="mark"></p><p>qbr和VM一一对应，这个属于：<strong>“单租户独占”的方案，来实现多租户隔离。</strong>qbr由于绑定了安全组，它在原生的数据面租户隔离的基础上又多加了一层“安全层”来保证租户资源隔离。原生的数据面转发（br-tun/br-ehtx、br-int）负责“正常行为”的租户隔离，而安全技术（qbr)负责“异常行为”（非法访问）的租户隔离。Router/DVR跟租户相对应，而且每个Router/DVR运行在一个namespace中，这个属于：<strong>“单租户独占”（用namespace隔离）的方案，来实现多租户隔离的目的。</strong>这里的namespace是linux操作系统虚拟网络的一个重要概念。最早的linux系统的许多资源都是共享的，比如进程ID资源，而namespace的出现就是将这些资源进行隔离，详见博客《<a href="https://kkutysllb.cn/2019/06/24/2019-06-24-Linux原生网络虚拟化实践/">Linux原生网络虚拟化实践一文</a>》。单纯从网络角度来说，一个namespace提供了一份独立的网络协议栈（网络设备接口、IPv4、IPv6、路由、防火墙规则和套接字socket等）。一个设备（linux device）只能位于一个namespace中，不同namespace的设备可以利用veth pair进行桥接。而网络节点的实现模型如下图所示，网络节点中，br-ethx/br-tun、br-int、br-ex分别只有一个实例，这是属于：<strong>“多租户共享”的方案，实现了多租户隔离的目的</strong>。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dUxe2MoB5rhM.png?imageslim" alt="mark"></p><p>Router跟租户对应，而且每个Router运行在一个namespace中，这属于：<strong>“单租户独占”的方案，实现了多租户隔离的目的</strong>。Router/DVR不仅保证了租户间网络资源不能互相访问外，还解决了逻辑资源（IP地址）冲突的问题。</p><p><strong>故障面的租户资源隔离实现模型：</strong>通过前面的介绍，Neutron在数据面和控制面的租户资源隔离方案分为两类：资源单租户独占（比如Router等）和资源多租户共享（比如br-int等）。在故障资源隔离层面，对资源共享方案，没有任何故障层面的租户隔离能力，一旦一个部件发生故障，所有与其关联的租户都要受到影响。而对于资源独占层面，具有一定故障层面隔离能力，比如一个租户的Router发生故障，不会影响其他租户。以数据面的实现方案为例，故障面的租户隔离度与资源共享度的关系如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/EytPqzhlo4xI.png?imageslim" alt="mark"></p><p>但是，资源独占的粒度是有限的，也仅仅是在Router这样的层面才能做到租户资源独占，稍微大一点的粒度，比如主机Host，都不能保证资源独占。这不是技术问题，这是云服务的商业本质决定的。因此，高可用集群部署方案必不可少。用资源共享的方式来实现租户隔离，在正常情况下没有任何问题，但是如果发生故障，资源共享方案要想做到故障层面的租户隔离，这是不可能的。如果非要做成独占方式来隔离故障那是败家行为，土豪动也伤不起。</p><p>通过上面的介绍，就回到我们开头那句话：<strong>“租户隔离，顾名思义就是为了资源隔离，但其更深层次的含义是多租户资源共享！！！”</strong>。隔离的其实都是租户使用的逻辑资源，其目的就是为了对底层资源能够做到复用，最大限度使用底层资源，这是针对控制面和数据转发而言。而针对故障面，由于隔离的真实目的是共享这个原因，正所谓，一荣俱荣，一损俱损，此时不仅要考虑逻辑资源隔离的问题，更多的是考虑资源容错的问题，也就是我们常说的高可用集群，这其实也是资源复用的一种方式，只是复用的资源对象是集群这个整体而已。</p><h3 id="采用OVS参考实现的Neutron网络数据转发模型"><a href="#采用OVS参考实现的Neutron网络数据转发模型" class="headerlink" title="采用OVS参考实现的Neutron网络数据转发模型"></a><strong>采用OVS参考实现的Neutron网络数据转发模型</strong></h3><p>我们在前面提到过，Neutron网络管理服务的类型分为Provider Network Service和Self Network Service两种，同时在实际部署时，Neutron各组件分别部署在控制节点、网络节点和计算节点3种不同角色的逻辑功能节点上。因此，针对不同网络管理服务类型，不同的TypeDriver下的不同的参考实现方案，部署在计算节点的虚拟机实例instance的数据包转发模型并不相同。这里，我们主要分析两种网络管理服务类型下，VLAN和VxLAN两种TypeDriver下的OVS参考实现解决方案的数据包转发路径，理解了OVS参考实现解决方案，LinuxBridge就是个菜。</p><h3 id="Provider-Network-Service下的OVS参考实现数据包转发模型"><a href="#Provider-Network-Service下的OVS参考实现数据包转发模型" class="headerlink" title="Provider Network Service下的OVS参考实现数据包转发模型"></a><strong>Provider Network Service下的OVS参考实现数据包转发模型</strong></h3><p>Provider网络是一种仅实现二层网络虚拟化，不提供三层路由和更高层的VPN、LoadBlancer、Firewall等高级功能的Neutron网络类型，相对而言，Provider是一种半虚拟化的网络。在Provider中，三层以上的功能不被虚拟化，而是借助物理网络设备来实现。在Provider网络中，由于二层网络直接接入物理设备，二层网络之间的通信也由物理设备进行转发，因此Provider网络在实现过程中无须L3服务，控制节点只需部署API Server、ML2核心插件及DHCP和Linux bridging代理即可。计算节点中的实例通过虚拟二层交换机直接接入物理网络，因此计算节点只需部署如Open vSwitch或Linux bridging等代理软件即可，Provider网络的节点服务布局如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/VAt9FgWxuQqi.png?imageslim" alt="mark"></p><p>Provider网络拓扑架构很简单，只需在控制节点和计算节点中分别规划一块物理网卡，并将其接入物理网络即可。如果采用的是VLAN网络，则将交换机接口配置为Trunk模式，节点只需一块物理网卡便可通过多个VLAN ID来实现不同网络的隔离。如果采用的是Flat网络，由于Flat网络没有Tagging，如要配置多个Flat网络则需要节点提供同样数量的物理网卡。<strong>需要注意一点：Provider网络不支持VxLAN的TypeDriver。</strong>在Provider网络中，虚拟机实例instance直接接入Provider网络（外部网络，如192.168.101.0/24），因此也不存在私有网络和虚拟路由设备的概念，同时也无须Floating IP，即实例虚拟接口上获取的就是外网IP。外部网络可以直接访问位于Provider网络上的虚拟机实例，而访问控制由计算节点上的防火墙规则来实现。Provider物理和虚拟网络必须属于同一个网段（192.168.101.0/24），只是网络被分为物理实现和虚拟实现，即节点内部为虚拟网络，而节点外部为物理设备网络。相对Self-Service网络，Provider网络的拓扑架构和通信过程都很简单，在故障排查中也相对容易，并且Provider网络中节点之间和二层网络之间的通信都由物理设备负责，因此Provider的稳定性和性能比起全虚拟化实现的Self-Service网络来说要高很多也更容易被理解和实现。Provider网络的拓扑架构如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/1YqrIi7Wsa7e.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>Provider南北网络数据流分析：</strong>在Provider网络架构中，物理网络设备负责处理Provider网路（Provider Network）与外网网络（External Network）之间的路由和其他网络服务。</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nFdrKHoi7hM1.png?imageslim" alt="mark"></p><p>上图中，如果位于计算节点上的实例Instance发送一个数据包到外网中的主机上，则计算节点将产生如下的操作：（外网主机对Provider网络中的实例访问过程与实例对外网主机的访问过程正好相反）</p><ul><li><ol><li><em>实例Tap接口将数据转发到节点内部的LinuxBridge网桥qbr上，由于目标主机位于其他网络，实例instance发出的数据包中包含目标主机的MAC地址；</em></li><li><em>数据进入qbr后，其上的安全组规则对数据包进行状态跟踪和防火墙处理。之后qbr将数据转发到OpenvSwitch的聚合网桥br-int上，并为数据包添加与Provider Network对应的内部VLAN标记（Internal tag）。然后br-int将数据转发到OpenvSwitch的ProviderBridge外部网桥br-ex上，br-ex用实际的外部VLAN ID替换掉br-int添加的内部VLAN ID，然后通过计算节点的Provider网络接口将数据包转发到物理网络设备上。</em></li><li><em>数据包进入物理网络设备后，首先由交换机处理Provider Network与路由之间的外部VLAN tag操作，剥离掉外部VLAN tag，将数据包转发给路由器；</em></li><li><em>路由器查询路由表，将来自Provider Network的数据包路由到外部网络，交换机再次处理路由器与外网之间的VLAN tag操作，添加外部VLAN ID，最后交换机将数据包转发到外网。</em></li></ol></li><li></li><li><ul><li><strong>Provider东西数据流分析：</strong>Provider网络东西数据流分为两种类型，即位于相同网段中的实例通信和不同网段之间的实例通信。二者的区别在于，不同Provider网段之间的实例通信需要具备三层路由功能的物理网络设备进行路由转发，而相同Provider网络中的实例通信只需二层物理交换机进行转发。</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/1KVjrH5OHUrI.png?imageslim" alt="mark"></p><p>上图中，当Compute Node1中的实例Instance1向Compute Node2中的实例Instance2发送数据包时，Compute Node1中将会发生如下的数据传递操作：</p><ul><li><ol><li>首先Instance1的tap接口将数据包转发到LinuxBridge qbr，转发的数据包中包含了目标地址的MAC地址。</li><li>qbr中的安全组规则对数据包进行防火墙相关的操作，之后qbr将数据包转发到OpenvSwitch聚合网桥br-int，并为数据包添加属于Provider Network1的内部VLAN ID，然后br-int将数据包转发到OpenvSwitch的ProviderBridge br-ex，br-ex使用Provider Network1的外部VLAN ID替换br-int添加的内部VLAN ID，然后将数据包通过Compute Node1节点的Provider网络接口转发到物理网络设备中。</li><li>物理网络设备接收到Compute Node1转发来的数据包后，交换机处理Provider Network1与路由之间的VLAN tag操作（剥离VLAN ID），将数据包转发到路由器。</li><li>路由将来自Provider Network1的数据包转发到Provider Network2，交换机再次处理路由与Provider Network2之间的VLAN tag操作（添加VLAN ID），之后交换机将数据包转发到Compute Node2中。</li><li>Compute Node2接收到物理网络设备发送的数据包后，位于其上的Provider网络接口将数据包转发到OpenvSwitch的ProviderBridge br-ex，然后br-ex将数据包转发到OpenvSwitch的集成网桥br-int，br-int使用Provider Network2的内部VLAN ID替换掉Provider Network2的外部VLAN ID，并将数据包转发到LinuxBridge qbr，qbr使用安全组规则对网络数据包进行过滤。</li><li>最后，qbr通过tap接口将数据包转发给Instance2。至此，Compute Node1上的Instance1发出的数据包成功到达Compute Node2上的Instance2中。</li></ol></li></ul><p>当Compute Node1中的Instance1与Compute Node2中的Instance2位于相同的Provider网络时，Instance1对Instance2的访问过程与上面过程类似，但是由于相同网段通信不需路由转发，因此此时的网络设备无须具备三层路由功能，只需二层交换功能即可。</p><h3 id="Self-Network-Service下的OVS参考实现数据包转发模型"><a href="#Self-Network-Service下的OVS参考实现数据包转发模型" class="headerlink" title="Self Network Service下的OVS参考实现数据包转发模型"></a><strong>Self Network Service下的OVS参考实现数据包转发模型</strong></h3><p>Self-Service网络又称租户网络，在实现Self-Service网络之前，管理员必须已经实现Provider网络，因此在Self-Service网络中，用户也可以直接使用Provider网络，并将实例直接接入Provider物理网络而不是租户私有云网络。也可以认为Self-Service网络是对Provider网络的扩展和增强实现。与Provider网络相比，Self-Service最大的不同在于租户可以按需创建自己的私有网络，并且网络中需要提供L3服务以实现网络的东西和南北数据流。此外，从主流的Self-Service网络与Provider网络的部署模型上看，Self-Service网络的Neutron服务组件通常分布在控制节点、网络节点和计算节点上，而Provider网络并不需要网络节点。如下图所示，<strong>在最简单的三节点Self-Service网络部署中，为了Project网络既可以使用VLAN类型，也可以使用GRE/VxLAN类型，控制节点至少需要一个网络接口（管理网络），网络节点至少需要四个网络接口（管理网络、隧道网络、VLAN网络和外部网络），计算节点至少需要三个网络接口（管理网络、隧道网络和VLAN网络）。需要指出的是，计算节点和网络节点上并非同时需要VLAN网络和Tunnel网络。根据Project网络类型，用户也可以只部署VLAN网络或Tunnel网络。如Project网络为GRE/VxLAN类型，则仅需要Tunnel网络，如果Project网络为VLAN，则仅需要VLAN网络。</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GoM9Aodt9lzU.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>Self-Service网络南北数据流分析：</strong>在Self-Service网络中，南北数据通信分为两种情况，即Fixed IP形式的南北网络通信和Floating IP形式的南北网络通信。对于仅有Fixed IP而没有为其绑定Floating IP的实例，网络节点的Router负责Project网络与External网络的通信路由。</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/gGGapRm3KK1L.png?imageslim" alt="mark"></p><p>上图中，假设Compute Node1中的instance向External网络中的主机发送数据包，则<strong>该数据包在Compute Node1节点内部的处理流程如下：</strong></p><ul><li><ol><li><em>实例instance转发包含目标主机MAC地址的数据包到LinuxBridge网桥qbr。</em></li><li><em>LinuxBridge网桥qbr对数据包采用安全组规则进行过滤处理。</em></li><li><em>LinuxBridge网桥qbr将安全规则过滤后的数据转发到OpenvSwitch集成网桥br-int。</em></li><li><em>OpenvSwitch集成网桥br-int为数据包添加Project网络的内部VLAN ID。</em></li><li><em>如果Project网络是GRE/VxLAN类型，则转到步骤6。</em></li></ol></li><li><ul><li><ul><li><em>如果Project网络是VLAN类型，则OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitch VLAN网桥br-vlan。</em></li><li><em>OpenvSwitchVLAN网桥br-vlan使用Project网络的外部VALN ID替换掉br-int添加的Project网络内部VLAN ID。</em></li><li><em>OpenvSwitchVLAN网桥br-vlan通过VALN接口将数据转发到网络节点。</em></li></ul></li></ul></li><li><ol><li><em>如果Project网络是GRE/VxLAN类型，则直接进入此步骤。</em></li></ol></li><li><ul><li><ul><li><em>OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitch隧道网桥br-tun。</em></li><li><em>OpenvSwitch隧道网桥br-tun将数据封装到VxLAN/GRE隧道中，并为封装后的数据包添加用以识别不同Project网络的VNI。</em></li><li><em>OpenvSwitch隧道网桥br-tun通过隧道网络接口将数据转发到网络节点。</em></li></ul></li></ul></li></ul><p><strong>数据包到达网络节点后，在网络节点内部的处理流程如下：</strong></p><ul><li><ol><li><em>如果Project网络是GRE/VxLAN类型，则转到步骤2。</em></li></ol></li><li><ul><li><ul><li><em>网络节点的VLAN网络接口将数据包转发到OpenvSwitchVLAN网桥br-vlan。</em></li><li><em>OpenvSwitchVLAN网桥br-vlan将数据包转发到OpenvSwitch集成网桥br-int。</em></li><li><em>OpenvSwitch集成网桥br-int使用Project网络的内部VLAN ID替换掉Project的外部VLAN ID。</em></li></ul></li></ul></li><li><ol><li><em>如果Project网络是GRE/VxLAN类型，则直接进入此步骤。</em></li></ol></li><li><ul><li><ul><li><em>网络节点隧道网络接口将数据转发到OpenvSwitch隧道网桥br-tun。</em></li><li><em>OpenvSwitch隧道网桥br-tun解封经过GRE/VxLAN协议封装的数据包，并为解封后的数据添加Project网络内部tag。</em></li><li><em>OpenvSwitch隧道网桥br-tun将数据包转发到OpenvSwitch集成网桥br-int。</em></li></ul></li></ul></li><li><ol><li><em>OpenvSwitch集成网桥br-int将数据转发到qrouter命名空间的qr接口，qr接口事先配置了Project网络的网关地址。</em></li><li><em>qrouter命名空间中的iptables服务使用qg接口上的IP作为Source IP对数据包进行SNAT操作，qg接口事先配置了External网络的网关地址。</em></li><li><em>Router通过qg接口转发数据包到OpenvSwitch集成网桥br-int。</em></li><li><em>OpenvSwitch集成网桥br-int转发数据包到OpenvSwitch外部网桥br-ex。</em></li><li><em>OpenvSwitch外部网桥br-int通过网络节点上的外部网络接口将数据包转发到External网络。</em></li></ol></li></ul><ul><li><ul><li><strong>Self-Service网络东西数据流分析：</strong>Project网络内部的实例之间的通信称为东西网络通信，东西网络通信通常分为两种，即相同Project网络内部的实例通信和不同Project网络中的实例通信。此外，对于东西网络通信，实例是否绑定Floating IP并不影响通信方式。</li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nVl5UyclS30z.png?imageslim" alt="mark"></p><p>上图中，表示两个不同Project网络的东西流量模型，假设位于Compute Node1且Project网络为Network1的实例instance1要与位于Compute Node2且Project网络为Network2的实例instance1通信，则<strong>instance1发出的数据在Compute Node1上的处理过程如下：</strong></p><ul><li><ol><li><em>instance1的tap接口将包含目标MAC地址的数据转发到LinuxBridge网桥qbr中。</em></li><li><em>LinuxBridge网桥qbr按照安全组规则对数据进行过滤处理。</em></li><li><em>LinuxBridge网桥qbr中将数据转发到OpenvSwitch集成网桥br-int。</em></li><li><em>OpenvSwitch集成网桥br-int为数据包添加Project Network1的内部tag。</em></li><li><em>如果Project网络是VLAN类型，则进入此步骤，否则转到步骤6。</em></li></ol></li><li><ul><li><ul><li><em>OpenvSwitch集成网桥br-int将数据转发到OpenvSwitch VLAN网桥br-vlan。</em></li><li><em>OpenvSwitch VLAN网桥br-vlan使用Project Network1的外部VLAN ID替换掉内部tag。</em></li><li><em>OpenvSwitch VLAN网桥br-vlan通过Compute Node1的VLAN网络接口将数据转发到网络节点。</em></li></ul></li></ul></li><li><ol><li><em>如果Project网络为GRE/VxLAN类型，则直接接入此步骤。</em></li></ol></li><li><ul><li><ul><li><em>OpenvSwitch集成网桥br-int将数据转发到OpenvSwitch隧道网桥br-tun。</em></li><li><em>OpenvSwitch隧道网桥br-tun将数据封装到VxLAN/GRE隧道中，并为封装后的数据包添加用以识别不同Project网络的隧道标记tag。</em></li><li><em>OpenvSwitch隧道网桥br-tun通过Compute Node1的隧道网络接口将数据转发到网络节点。</em></li></ul></li></ul></li></ul><p><strong>数据进入网络节点之后，在网络节点中的处理过程如下：</strong></p><ul><li><ol><li><em>如果Project网络是VLAN类型，则进入此步骤，否则转到步骤2。</em></li></ol></li><li><ul><li><ul><li><em>网络节点的VLAN网络接口将数据包转发到OpenvSwitchVLAN网桥br-vlan。</em></li><li><em>OpenvSwitchVLAN网桥br-vlan将数据包转发到OpenvSwitch集成网桥br-int。</em></li><li><em>OpenvSwitch集成网桥br-int使用Project网络的内部tag替换掉Project的实际VLAN ID。</em></li></ul></li></ul></li><li><ol><li><em>如果Project网络是GRE/VxLAN类型，则直接进入此步骤。</em></li></ol></li><li><ul><li><ul><li><em>网络节点隧道网络接口将数据转发到OpenvSwitch隧道网桥br-tun。</em></li><li><em>OpenvSwitch隧道网桥br-tun解封经过GRE/VxLAN协议封装的数据包，并为解封后的数据添加Project网络内部tag。</em></li><li><em>OpenvSwitch隧道网桥br-tun将数据包转发到OpenvSwitch集成网桥br-int。</em></li></ul></li></ul></li><li><ol><li><em>OpenvSwitch集成网桥br-int将数据转发到qrouter命名空间中的qr-1接口。qr-1接口已配置有Project Network1的网关IP。</em></li><li><em>qrouter命名空间将数据包路由到qr-2接口，qr-2接口已配置有Project Network2的网关IP。</em></li><li><em>qrouter命名空间将数据包转发到OpenvSwitch集成网桥br-int。</em></li><li><em>OpenvSwitch集成网桥br-int为数据包添加Project Network2的内部tag。</em></li><li><em>如果Project网络是GRE/VxLAN类型，则转到步骤8。如果是VLAN类型，则进入此步骤。</em></li></ol></li><li><ul><li><ul><li><em>OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitchVLAN网桥br-vlan。</em></li><li><em>OpenvSwitchVLAN网桥br-vlan使用Project网络实际VLAN ID替换掉Project网络内部tag。</em></li><li><em>OpenvSwitchVLAN网桥br-vlan通过网络节点VLAN接口将数据转发到Compute Node2计算节点。</em></li></ul></li></ul></li><li><ol><li><em>如果Project网络是GRE/VxLAN类型，则进入此步骤。</em></li></ol></li><li><ul><li><ul><li><em>OpenvSwitch集成网桥br-int将数据包转发到OpenvSwitch隧道网桥br-tun。</em></li><li><em>OpenvSwitch隧道网桥br-tun将数据封装到VxLAN/GRE隧道中，并为封装后的数据包添加用以识别不同Project网络的隧道标记tag。</em></li><li><em>OpenvSwitch隧道网桥br-tun将数据包通过网络节点上的隧道网络接口转发到Compute Node2计算节点。</em></li></ul></li></ul></li></ul><p><strong>数据进入Compute Node2计算节点后，在Compute Node2中的处理过程如下：</strong></p><ul><li><ol><li><em>如果Project网络是VLAN类型，则直接进入此步骤，如果是GRE/VxLAN网络，则转到步骤2。</em></li></ol></li><li><ul><li><ul><li><em>计算节点的VLAN接口将数据包转发到OpenvSwitchVLAN网桥br-vlan。</em></li><li><em>OpenvSwitchVLAN网桥br-vlan将数据包转发到OpenvSwitch集成网桥br-int。</em></li><li><em>OpenvSwitch集成网桥br-int使用Project Network2的内部tag替换掉实际VLAN ID。</em></li></ul></li></ul></li><li><ol><li><em>如果Project网络是GRE/VxLAN类型，则直接进入此步骤。</em></li></ol></li><li><ul><li><ul><li><em>计算节点上的隧道接口将数据转发到OpenvSwitch隧道网桥br-tun。</em></li><li><em>OpenvSwitch隧道网桥br-tun解封经过GRE/VxLAN协议封装的数据包，并为解封后的数据添加Project Network2络内部tag。</em></li><li><em>OpenvSwitch隧道网桥br-tun将数据包转发到OpenvSwitch集成网桥br-int。</em></li></ul></li></ul></li><li><ol><li><em>OpenvSwitch集成网桥br-int转发数据到LinuxBridge网桥qbr。</em></li><li><em>LinuxBridge网桥qbr根据安全组规则过滤数据包。</em></li><li><em>LinuxBridge网桥qbr将数据转发到Compute Node2计算节点实例instance2的tap接口。</em></li></ol></li></ul><p>相比不同Project网络中的实例通信，相同Project网络内部实例通信要简单很多，因为相同Project网络内部实例通信无须进行路由，而是完全由OpenvSwitch Agent进行数据交换处理，因此也不需要网络节点参与。</p><h2 id="Neutron的不同网络类型实现实战"><a href="#Neutron的不同网络类型实现实战" class="headerlink" title="Neutron的不同网络类型实现实战"></a><strong>Neutron的不同网络类型实现实战</strong></h2><h3 id="Local网络"><a href="#Local网络" class="headerlink" title="Local网络"></a>Local网络</h3><p>local network的特点是不会与宿主机的任何物理网卡相连，也不关联任何的VLAN ID。对于每个local netwrok，ML2 linux-bridge会创建一个bridge，instance的tap 设备会连接到bridge。位于同一个local network的instance会连接到相同的bridge，这样instance之间就可以通信了。因为bridge没有与物理网卡连接，所以instance 无法与宿主机之外的网络通信。 同时因为每个local network有自己的bridge，bridge之间是没有连通的，所以两个local network之间也不能通信，即使它们位于同一宿主机上的同一网段。以下就是通过linuxbridge方式实现的一个local网络的拓扑逻辑图。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yyBeW1rG1FQb.png?imageslim" alt="mark"></p><p><strong>步骤1：</strong>首先通过web页面创建一个本地网络local_net1，local_net1的子网地址段cidr为10.10.10.0/24，如下图</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/1DpFcHak6RfV.png?imageslim" alt="mark"></p><p>由于创建子网subnet时，我们勾选了起用DHCP服务（默认勾选，如想手动配置地址，可不选），因此子网subnet创建成功后，默认起用了一个DHCP端口，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/jMRHUkX9yrW5.png?imageslim" alt="mark"></p><p>此时，我们看下网络节点的底层发生 了什么变化？如下图所示，在网络节点执行brctl show，如下显示</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/s6JIehWo0G45.png?imageslim" alt="mark"></p><p>我们发现，在网络节点的底层网络上，多了一个网桥brqea861547-ce，同时其上挂载了一个tap设备tapb8dca9e6-ec。通过命名我们应该可以联想到网桥brqea861547-ce就是我们创建的local_net1，后面ID就是local_network ID的前11位“短ID”，而tap设备tapb8dca9e6-ec就是DHCP Server在root namespace中接口（这是个veth pair设备，另一头连接的是DHCP Server的namespace接口），同样tap后面的字符串ID就是DHCP端口ID的前11位“短ID”。</p><p><strong>步骤2：</strong>我们再创建一个local_net2，和local_net1的创建方法一样，如下图所示。同样，我们可以设置local_net2的子网subnet的cidr为10.10.10.0/24，实现资源隔离和复用。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/RheHvcBGmE3P.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GD850Xk2Rgb8.png?imageslim" alt="mark"></p><p>同样，我们在网络节点上查看一下底层网络有什么变化？如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rV75R5W53VPz.png?imageslim" alt="mark"></p><p>我们发现网络节点的底层网络多了一个网桥brq05b09958-c0，同时其上也挂载了一个tap设备tap436a2ae5-ac，通过ID我们知道其分别对应local_net2和其上DHCP Server的veth pair接口。</p><p><strong>步骤3：</strong>此时，我们创建两个虚机VM1和VM2，其网络分配为local_net1，如下图所示</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Y5oRUbq3V4qk.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/FgQWyK2qcB4v.png?imageslim" alt="mark"></p><p>可以看出虚机VM1和VM2创建成功，DHCP分配的IP地址分别为10.10.10.13和10.10.10.16，此时，我们通过VNC或SSH登录到虚机VM2中，尝试ping虚机VM1，可以ping通。如下图，与我们预期一样。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/a4If4D9GRcrx.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/22yFotOOvExm.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>我们再创建一个虚机VM3，为其分配的网络local_net2，创建成功后DHCP为其分配的IP地址为10.10.10.11</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HW6MSWULfANc.png?imageslim" alt="mark"></p><p>同样，我们登录到虚机VM3中，尝试ping虚机VM1和VM2，如下图所示，ping不通，完全符合我们的预期。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/4dWzuDMSm3nn.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/G49jAGEvOS2n.png?imageslim" alt="mark"></p><h3 id="flat网络"><a href="#flat网络" class="headerlink" title="flat网络"></a><strong>flat网络</strong></h3><p>flat network 是不带tag的网络，就是一个大二层网络。要求宿主机的物理网卡直接虚拟网络相连，这意味着：<strong>每个flat network都会独占一个物理网卡。</strong>如下图，ens34 桥接到 brqXXX，为 instance 提供 flat 网络。如果需要创建多个 flat network，就得准备多个物理网卡。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/fauHKXyuCVz8.png?imageslim" alt="mark"></p><p><strong>步骤1：</strong>创建一个flat网络llb_flat，标签为default，子网段cidr为30.10.20.0/24，DHCP Server的IP地址为30.10.20.10.如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/F86NfnEyLaHH.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>我们看下底层网络有啥变化。如下所示</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/2STkzOl217Gv.png?imageslim" alt="mark"></p><p>对于ovs bridge “br-ex”和其上桥接的port“ens35”我们应该不会感到意外，这是前面配置的结果。然而除此之外，br-int和br-ex分别多了一个port“int-br-ex”和“phy-br-ex”，而且这两个port都是<strong>“patch”类型</strong>，<strong>同时通过“peer”指向对方</strong>。上面的配置描述了这样一个事实：br-int与br-ex这两个网桥通过int-br-ex和phy-br-ex连接在一起了。如下图所示</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/scY7Ic6BD4m6.png?imageslim" alt="mark"></p><p>此时网络节点的逻辑拓扑如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/quBL67DkldVa.png?imageslim" alt="mark"></p><p>通过前面的分析我们可知veth pair和patch port都可以连接网桥，使用的时候如何选择呢？答案是patch port 是ovs bridge自己特有的port类型，只能在ovs中使用。如果是连接两个ovs bridge，优先使用patch port，因为性能更好。</p><p><strong>步骤3：</strong>此时我们创建两个虚机FLAT_VM1和FLAT_VM2，为其配置为FLAT网络模式，如下，FLAT-VM1和FLAT_VM2通过DHCP拿到的地址分别为30.10.20.13和30.10.20.16。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rsDhqEOdeUyw.png?imageslim" alt="mark"></p><p>我们看到instance在启动过程中能够从Neutron的DHCP服务获得IP，Neutron提供DHCP服务的组件是DHCP agent。DHCP agent在网络节点运行上，通过dnsmasq实现DHCP功能。DHCP agent的配置文件位于/etc/neutron/dhcp_agent.ini。当创建network并在subnet上enable DHCP时，网络节点上的DHCP agent会启动一个dnsmasq进程为该network提供DHCP服务。dnsmasq是一个提供DHCP和DNS服务的开源软件。dnsmasq与network是一对一关系，一个dnsmasq进程可以为同一netowrk中所有enable了DHCP的subnet提供服务。DHCP agent会为每个network创建一个目录/etc/data/neutron/dhcp/，用于存放该network的dnsmasq配置文件。Neutron通过namespace为每个network提供独立的DHCP和路由服务，从而允许租户创建重叠的网络。如果没有namespace，网络就不能重叠，这样就失去了很多灵活性。在创建虚拟机实例instance时，Neutron会为其分配一个port，里面包含了MAC和IP地址信息。这些信息会同步更新到dnsmasq的host文件。同时nova-compute会设置虚机实例instance的VIF的MAC地址。一切准备就绪，虚拟机实例instance获取IP的过程如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/CujLXRlbTt3O.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>Step1：</strong>虚拟机实例instance开机启动，发出DHCPDISCOVER广播，该广播消息在整个Network中都可以被收到。</li><li><strong>Step2：</strong>广播到达veth interface的tap设备，然后传送给veth pair的另一端ns设备。dnsmasq在它上面监听，dnsmasq检查其 host 文件，发现有对应项，于是dnsmasq以DHCPOFFER消息将IP、子网掩码、地址租用期限等信息发送给虚拟机实例instance。</li><li><strong>Step3：</strong>虚拟机实例instance发送DHCPREQUEST消息确认接受此DHCPOFFER。</li><li><strong>Step4：</strong>dnsmasq发送确认消息DHCPACK，整个过程结束。</li></ul></li></ul><h3 id="vlan网络"><a href="#vlan网络" class="headerlink" title="vlan网络"></a><strong>vlan网络</strong></h3><p>vlan network是带tag的网络，是实际应用最广泛的网络类型。在Open vSwitch实现方式下，不同vlan instance的虚拟网卡都接到聚合网桥br-int上。这一点与 linux bridge非常不同，linux bridge是不同vlan接到不同的网桥上。下面，我们将采用实践部署的方式先完成网络创建，然后通过底层分析，最后画出我们的逻辑拓扑图。</p><p><strong>步骤1：</strong>首先我们创建一个VLAN网络，VLAN ID为10，如下VLAN的label是flat（在ml2.conf.ini文件中配置，名字可以随意起，也可以配置多个，之间用逗号“,”隔开），子网cidr为172.18.10.0/24，DHCP Server的端口和IP地址为172.18.10.10</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/m98xkKF8xFLM.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/hMqDuSkKnaRQ.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6o1FTMLEeNEf.png?imageslim" alt="mark"></p><p>我们在网络节点的br-int上发现多了一个tap设备，这个tap设备就是DHCP Server在root空间veth interface。此时网络节点的拓扑如下图右边所示</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/JRmDgaYjOSV5.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>现在创建一个虚拟机实例instance，为其配置网络的为上面VLAN10，同时这个虚拟机实例instance被创建在计算节点computer1上，IP地址为172.18.10.15，发现在VLAN10的网络上了多了一个端口</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/3sUOfvNmayS3.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/mzV4GvhuKBmk.png?imageslim" alt="mark"></p><p>在计算节点computer1上看下底层网络变化？发现网桥上不仅有个tap设备（虚机虚拟网卡VIF），还多个qvb设备，如下图左边上部，这其实是个veth pair设备的veth interface，同时在计算节点的br-int找到了该veth pair设备另一端接口qvo，如下图左边下部，此时计算节点computer1上的网络拓扑如下图右边。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/83veH2YUsO7p.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>再创建一个虚机VM2，该虚机被调度到了计算节点computer2上，IP地址为172.18.10.18。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/SJUzykqbuCyW.png?imageslim" alt="mark"></p><p>在计算节点computer2上查看底层网络变化，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GYjCM1akuRe9.png?imageslim" alt="mark"></p><p>发现其上创建的qbr网桥与计算节点computer1的网桥并不是同一个。这一点就是OVS网络与linuxbridge网络不同点，在linuxbridge网络中，同一个网络对应一个网桥brq，也就是说网桥brq与网络network是一一对应的关系。但是在OVS网络中，一个网桥qbr其实对应的一个虚机，而网络network对应的是br-int这个网桥，但不是一一对应，而是m:1的关系。</p><p><strong>步骤4：</strong>再创建一个vlan11的网络，方法同VLAN10，此时网络节点的拓扑如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/admWvXL4W7r7.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>再创建一个虚机VM3，为其分配VLAN11的网络，其IP地址为172.18.11.18，该虚机被调度到计算节点computer1上</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/g1cIi11htLEk.png?imageslim" alt="mark"></p><p>此时计算节点compute1的拓扑如下，VM3和VM1虽然同属于一个Host，甚至共享同一个br-int，但他们分属于不同VLAN网络，因此二层上应该是隔离的。下面我们就来分析下OVS网络下的VLAN隔离机制。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pLE2fpMVBWdm.png?imageslim" alt="mark"></p><p>与Linux Bridge driver不同，Open vSwitch driver并不通过eth1.100, eth1.101等VLAN interface来隔离不同的VLAN。所有的instance都连接到同一个网桥br-int，<strong>Open vSwitch通过 flow rule（流规则）来指定如何对进出br-int 的数据进行转发，进而实现vlan之间的隔离。具体来说：当数据进出br-int时，flow rule可以修改、添加或者剥掉数据包的VLAN tag，Neutron负责创建这些flow rule并将它们配置到br-int，br-ens34等Open vSwitch上。</strong></p><p>下面就来研究一下当前的flow rule。查看flow rule的命令是ovs-ofctl dump-flow <bridge>，首先查看计算节点computer1的br-ens34的flow rule，如下，每一个cookie就是一条flow rule，下图中br-ens34上配置了四条rule，每条rule有不少属性，其中比较重要的属性有：<strong>priority、in_port、dl_vlan和actions</strong>。</bridge></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/SrcqCsGRIKJ4.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>priority：</strong>rule 的优先级，值越大优先级越高，Open vSwitch会按照优先级从高到低应用规则。</li><li><strong>in_port：</strong>inbound端口编号，每个port 在Open vSwitch中会有一个内部的编号。可以通过命令ovs-ofctl show <bridge> 查看port编号，如下图br-ens34：ens34编号为1；phy-br-ens34编号为2。</bridge></li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/I2odGjt3tqrf.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>dl_vlan：</strong>数据包原始的 VLAN ID。</li><li><strong>actions：</strong>对数据包进行的操作。</li></ul></li></ul><p>br-ens34跟VLAN相关的flow rule比较重要的有两条，下面我们来详细分析。清晰起见，我们只保留重要的信息，如下：</p><ul><li><ul><li>priority=4,in_port=2,dl_vlan=2 actions=mod_vlan_vid:10,NORMAL</li><li>priority=4,in_port=2,dl_vlan=3 actions=mod_vlan_vid:11,NORMAL</li></ul></li></ul><p><strong>第一条的含义是：</strong>从br-ens34的端口phy-br-ens34（in_port=2）接收进来的包，如果VLAN ID是2（dl_vlan=2），那么需要将VLAN ID改为10（actions=mod_vlan_vid:10）</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/wBny2KogzxI8.png?imageslim" alt="mark"></p><p>从上面的网络结构我们可知，phy-br-ens34连接的是br-int，phy-br-ens34的inbound包实际上就是虚机VM通过br-int发送给外部网络的数据。那么怎么理解将VLAN ID 2改为VLAN ID 10呢？请看下面计算节点computer1的ovs-vsctl show的输出：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nB5PQnljyMPo.png?imageslim" alt="mark"></p><p>br-int通过tag隔离不同的port，这个tag可以看成内部的VLAN ID。从qvo2cc37622-be（对应VM1，vlan10）进入的数据包会被打上内部VLAN ID= 2的VLAN tag。因为br-int中的VLAN ID跟物理网络中的VLAN ID并不相同，所以当br-ens34接收到br-int发来的数据包时，需要对VLAN进行转换。Neutron负责维护内外VLAN ID的对应关系，并将转换规则配置在flow rule中。理解了br-ens34的flow rule，我们再来分析br-int的flow rule。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Wvs6vqLNWQyH.png?imageslim" alt="mark"></p><p>最关键的是下面两条：</p><ul><li><ul><li>priority=3,inport=2,dl_vlan=10 actions=mod_vlan_vid:2,NORMAL</li><li>priority=3,inport=2,dl_vlan=11 actions=mod_vlan_vid:3,NORMAL</li></ul></li></ul><p><strong>port 2为int-br-ens34，那么这两条规则的含义就应该是：</strong></p><ol><li><p>从物理网卡接收进来的数据包，如果 VLAN 为 10，则改为内部 VLAN 2。</p></li><li><p>从物理网卡接收进来的数据包，如果 VLAN 为 11，则将为内部 VLAN 3。</p></li></ol><p><strong>简单的说，数据包在物理网络中通过VLAN 10和VLAN 11隔离，在计算节点OVS br-int中则是通过内部VLAN 2和VLAN 3隔离。也就是说通过OVS实现的VLAN网络存在内外VLAN转换的机制，这一点同样适用于后面的VxLAN网络，同时也是和Linubridge实现的VLAN的不同点之一。OVS实现下不同VLAN之间通过router三层互通和前面linuxbridge实现基本一样，唯一区别是Router的接口不再与qbr通过veth pair相连，而改为和br-int相连。</strong></p><h3 id="VxLAN网络"><a href="#VxLAN网络" class="headerlink" title="VxLAN网络"></a><strong>VxLAN网络</strong></h3><p>前面讨论了local, flat, vlan这几类网络，OpenStack还支持vxlan和gre这两种overlay network。overlay network是指建立在其他网络之上的网络。overlay network中的节点可以看作通过虚拟（或逻辑）链路连接起来的。overlay network在底层可能由若干物理节点组成，但是不需要关心这些底层实现。例如P2P网络就是overlay network。vxlan和gre都是基于隧道技术实现的，它们也都是overlay network。linux bridge只支持vxlan，不支持gre；open vswitch两者都支持。vxlan与gre实现非常类似，而且vxlan用得较多，所以只讨论vxlan网络的Neutron实现。VxLAN即Virtual eXtensible Local Area Network，正如名字所描述的，VxLAN提供与VLAN 相同的以太网二层服务，但拥有更强的扩展性和灵活性，且支持16777216个二层网段。关于VxLAN的技术原理请参考博客《<a href="https://kkutysllb.cn/2019/06/22/2019-06-21-服务器外部交换网络虚拟化/">服务器外部交换网络虚拟化</a>》一文。Neutron服务重启后，通过ovs-vsctl show查看网络配置：可以发现br-tun和br-int上各有一个path 接口指向对方，如下图左边，此时网络节点/计算节点的拓扑如下图右边所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/eJRBmG6t816W.png?imageslim" alt="mark"></p><p><strong>步骤1：</strong>创建一个VIN=30的VxLAN网络，子网cidr为10.10.30.0/24，如下</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/J4ULOwCgvbqe.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>此时网络节点的底层网络发生如下变化，如下图左边所示，在br-int网桥上多了一个tap设备，同时该tap设备与DHCP Server的namespace ID一致，这个tap设备是个veth pair设备，一头连接在br-int网桥，一头连接namespace中DHCP Server。此时，网络节点的逻辑拓扑如下图右边所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kPmzxULVFqT6.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>在VXLAN30的网络上创建2个VM，虚机VM1被调度到计算节点computer01上，IP地址为10.10.30.15。虚机VM2被调度到计算节点computer02上，IP地址为10.10.30.13。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/IGi9PaBj6zoV.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>分别在计算节点computer01和computer02上查看下底层网络的变化，如下图左边所示，两个计算节点各多了一个qbr网桥，每个qbr网桥上挂载了虚机虚拟网卡VIF和qvb设备。此时，网络节点和计算节点的逻辑拓扑如下图右边所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ccSNVpy76Eg1.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>在computer01上有2个VxLAN端口：1个是vxlan-ac124b0c，另一个是vxlan-ac124b0e，如下图上半部分。同时，在网络节点的network01的底层网络也是存在两个VxLAN网络：vxlan-ac124b0d和vxlan-ac124b0e，如下图下半部分。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/MMsTCnAQWaQL.png?imageslim" alt="mark"></p><p>此时，网络逻辑拓扑如下，表示当计算节点computer01和computer02上的VM通过VxLAN互访时，vxlan-ac124b0c完成。当计算节点computer01和computer02上的VM要与外部网络互访时，分别通过vxlan-ac124b0d和vxlan-ac124b0e完成。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/aBcyvRtRWqa3.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>分析VxLAN网络数据包的转发规则，首先查看br-int 的 flow rule，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/M4M6uRIWUOlr.png?imageslim" alt="mark"></p><p>br-int的rule看上去虽然多，其实逻辑很简单，br-int被当作一个二层交换机，其重要的rule 是下面这条，含义为<strong>根据 vlan 和 mac 进行转发。</strong></p><ul><li><ul><li><strong>cookie=0x965a57f99ba197f2, duration=406.721s, table=0, n_packets=51, n_bytes=3570, idle_age=119, priority=0 actions=NORMAL</strong></li></ul></li></ul><p>然后，查看br-tun的flow rule，如下，这才是真正处理 VXLAN 数据包的 rule。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/LnG69KW61e29.png?imageslim" alt="mark"></p><p>其具体处理流程如下，下图各方块中的数字对应 rule 中 table 的序号。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dc67VCQfDidI.png?imageslim" alt="mark"></p><ol><li><strong>（1）编号为0的方块（table 0）对应下面4条rule：</strong></li></ol><ul><li><ul><li><ul><li>cookie=0x9455c3750d53ad75, duration=602.160s, table=0, n_packets=79, n_bytes=5500, idle_age=12, priority=1,in_port=1 actions=resubmit(,2)</li><li>cookie=0x9455c3750d53ad75, duration=420.913s, table=0, n_packets=0, n_bytes=0, idle_age=420, priority=1,in_port=2 actions=resubmit(,4)</li><li>cookie=0x9455c3750d53ad75, duration=420.908s, table=0, n_packets=0, n_bytes=0, idle_age=420, priority=1,in_port=3 actions=resubmit(,4)</li><li>cookie=0x9455c3750d53ad75, duration=602.159s, table=0, n_packets=0, n_bytes=0, idle_age=602, priority=0 actions=drop</li></ul></li><li><p>结合如下 port 编号，table 0 flow rule 的含义为：</p></li><li><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/XNh5Dd6PzGz5.png?imageslim" alt="mark"></p></li></ul></li><li><ul><li>从 port 1（patch-int）进来的包，扔给table 2 处理：actions=resubmit(,2)</li><li>从 port 2（vxlan-ac124b0e）进来的包，扔给 table 4 处理：actions=resubmit(,4)</li><li>从 port 3 （vxlan-ac124b0d）进来的包，扔给table 4处理：actions=resubmit(,4)</li><li><strong>即第一条rule处理来自内部br-int（这上面挂载着所有的网络服务，包括路由、DHCP 等）的数据；第二条、第三条rule处理来自外部VxLAN隧道的数据。</strong></li></ul></li></ul><ul><li><p><strong>（2）编号为4的方块（table 4）对应下面2条rule：</strong></p></li><li><ul><li><ul><li><strong>cookie=0x9455c3750d53ad75, duration=422.298s, table=4, n_packets=0, n_bytes=0, idle_age=422, priority=1,tun_id=0x64 actions=mod_vlan_vid:1,resubmit(,10)</strong></li><li><strong>cookie=0x9455c3750d53ad75, duration=602.155s, table=4, n_packets=0, n_bytes=0, idle_age=602, priority=0 actions=drop</strong></li></ul></li><li><p><strong>其含义为：如果数据包的 VXLAN tunnel ID 为 100（tun_id=0x64），action 是添加内部 VLAN ID 1（tag=1），然后扔给 table 10 去学习。而对于其他tun_id的数据包则直接丢弃。</strong></p></li></ul></li></ul><ul><li><p><strong>（3）编号为10的方块（table 10）对应下面1条rule：</strong></p></li><li><ul><li><ul><li>cookie=0x9455c3750d53ad75, duration=602.154s, table=10, n_packets=0, n_bytes=0, idle_age=602, priority=1actions=learn(table=20,hard_timeout=300,priority=1,cookie=0x9455c3750d53ad75,NXM_OF_VLAN_TCI[0..11],NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],load:0-&gt;NXM_OF_VLAN_TCI[],load:NXM_NX_TUN_ID[]-&gt;NXM_NX_TUN_ID[],output:OXM_OF_IN_PORT[]),output:1</li></ul></li><li><p><strong>其含义为：学习外部（从tunnel）进来的包，往 table 20 中添加对返程包的正常转发规则，然后从 port 1（patch-int）扔给 br-int。</strong></p></li></ul></li></ul><ul><li><p><strong>（4）编号为2的方块（table 2）对应下面2条rule：</strong></p></li><li><ul><li><ul><li>cookie=0x9455c3750d53ad75, duration=602.158s, table=2, n_packets=0, n_bytes=0, idle_age=602, priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)</li><li>cookie=0x9455c3750d53ad75, duration=602.157s, table=2, n_packets=79, n_bytes=5500, idle_age=12, priority=0,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,22)</li></ul></li><li><p><strong>其含义为：br-int 发过来数据如果是单播包，扔给 table 20 处理：resubmit(,20)；br-int 发过来数据如果是多播或广播包，扔 table 22 处理：resubmit(,22)。</strong></p></li></ul></li><li></li><li><p><strong>（5）编号为20的方块（table 20）对应下面3条rule：</strong></p></li><li><ul><li><ul><li>cookie=0x9455c3750d53ad75, duration=323.953s, table=20, n_packets=0, n_bytes=0, idle_age=420, priority=2,dl_vlan=1,dl_dst=fa:16:3e:36:14:b3 actions=strip_vlan,load:0x64-&gt;NXM_NX_TUN_ID[],output:2</li><li>cookie=0x9455c3750d53ad75, duration=323.950s, table=20, n_packets=0, n_bytes=0, idle_age=420, priority=2,dl_vlan=1,dl_dst=fa:16:3e:83:ec:c7 actions=strip_vlan,load:0x64-&gt;NXM_NX_TUN_ID[],output:3</li><li>cookie=0x9455c3750d53ad75, duration=602.153s, table=20, n_packets=0, n_bytes=0, idle_age=602, priority=0 actions=resubmit(,22)</li></ul></li><li><p><strong>其含义为：第一条规则就是 table 10 学习来的结果，即数据包的返程规则。内部 VLAN 号为 1（tag=1），目标 MAC 是 fa:16:3e:36:14:b3（VM1）的数据包，即发送给 VM1的包（VM1在computer01计算节点），action 是去掉 VLAN 号，添加 VXLAN tunnel ID 100(十六进制 0x64)，并从 port 2 (tunnel 端口 vxlan-ac124b0e) 发出。第二条规则也是table 10 学习来的结果，也是数据包的返程规则。内部 VLAN 号为 1（tag=1），目标 MAC 是 fa:16:3e:83:ec:c7（VM2）的数据包，即发送给 VM2的包（VM2在computer02计算节点），action 是去掉 VLAN 号，添加 VXLAN tunnel ID 100(十六进制 0x64)，并从 port 3(tunnel 端口 vxlan-ac124b0d) 发出。第三条规则是对于没学习到规则的数据包，则扔给 table 22 处理。</strong></p></li></ul></li></ul><ul><li><p><strong>（6）编号为22的方块（table 22）对应下面2条rule：</strong></p></li><li><ul><li><ul><li>cookie=0x9455c3750d53ad75, duration=323.951s, table=22, n_packets=11, n_bytes=866, idle_age=318, priority=1,dl_vlan=1 actions=strip_vlan,load:0x64-&gt;NXM_NX_TUN_ID[],output:2,output:3</li><li>cookie=0x9455c3750d53ad75, duration=602.152s, table=22, n_packets=68, n_bytes=4634, idle_age=12, priority=0 actions=drop</li></ul></li><li><p><strong>其含义为：如果数据包的内部 VLAN 号为 1（tag=1），action 是去掉 VLAN 号，添加 VXLAN tunnel ID 100(十六进制 0x64)，并从port 2 (tunnel 端口 vxlan-ac124b0e) 或port 3 (tunnel 端口 vxlan-ac124b0d) 。</strong></p></li></ul></li></ul><h3 id="SNAT、Floating-IP"><a href="#SNAT、Floating-IP" class="headerlink" title="SNAT、Floating IP"></a><strong>SNAT、Floating IP</strong></h3><p>不同VLAN网络之间是二层隔离的，如果两个不同VLAN网络要互通，就需要建立一个路由器，此时路由器会增加2个Port，分别作为这两个VLAN网络的网关，然后通过直连路由表项完成两个不同VLAN网络之间的三层互通。在数据模型部分的Router数据模型讨论中，每个虚拟Router会有3种路由表项：直连路由、静态路由，默认静态路由。而默认静态路由表项的产生，就是当我们给Router设置外部网关时，它会自动增加一个Port，而这个Port就是用来对外部网络进行映射。也就是说，在虚机角度来看，外部网络的网关就是这个Port。因此，当内部网络的数据包经过这个Port时，该Port会对数据包的源地址进行替换，也就是我们常说的SNAT（Source NAT）。而Floating IP的作用正好相反，当外部网络的主机需要访问云主机时，此时目的地址为云主机在Router上的Floating IP，当数据包到达Router时，需要将Floating IP替换为云主机的内部IP，此过程就是DNAT。floating IP 提供静态NAT功能，建立外网IP与VM租户网络IP的一对一映射，其配置在router提供网关的外网interface上的，而非VM 中。router会根据通信的方向修改数据包的源或者目的地址，即SNAT/DNAT。</p><p><strong>步骤1：</strong>建立一个外部网络ext_net1，该网络cidr为10.10.10.0/24，默认网关10.10.10.1。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/cUXCpNq1FG6s.png?imageslim" alt="mark"></p><p>同时，我们的宿主机Vnet2对应的地址段也是10.10.10.0/24，而虚拟网卡Vnet2的IP地址就是10.10.10.1，如下所示。也就是说，建立这个外部网络ext_net1就是Neutron中建立的一个物理网络的映射。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HpyDrBkEMahr.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/QjKsd5XVyQhk.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>给路由器Router设置一个外部网关，网关映射配置为ext_net1，此时ext_net1的DHCP Server会给Router的这个Port分配一个IP地址，我们这里分配的IP地址为：10.10.10.19。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rhwcXXBe3pAv.png?imageslim" alt="mark"></p><p>此时我们的底层网络网桥上会增加一个tap设备tap099f513f-e9，这是一个veth interface，用来连接namespace for router的中另一个veth interface，目的就是将Router的外部网关Port连接到外部网络ext_net1。如下，在Router的namespace中，用来连接外部网络的Port命名为：qg-xxxxx，而用来连接内部网络的Port命名为：gr-xxxx。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/e6os5ntLO6r4.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>登录到路由器Router中，查看其路由表情况发现：存在一条默认路由，网关为10.10.10.1（Vnet2地址），接口为qg_099，含义就是内网任意云服务器(VM)访问任意外部网络（0.0.0.0），数据包从qg-099转发出去，到达网关10.10.10.1。同时，存在一条直连路由，含义是当这个外部网络映射ext_net1也有云服务器时，其他任意网络云服务器访问这些云服务器的数据包从接口qg-099接口转发。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/xtvsfDKvLxLl.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>登录虚机VM进行验证，VM的IP地址为（172.18.15.12）。发现可以ping通路由器RouterA的Port1（10.10.10.19），同时也能ping通我们宿主机网卡Vnet2（10.10.10.1）。进行trace发现，路由只经过2跳就到达外部网关RouterB的Port2，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7bk9LjlzFmjQ.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>当数据包从 RouterA连接外网的接口 qg-099f513f-e9 发出的时候，会做一次 Source NAT，即将包的源地址修改为 RouterA的接口Port1的IP地址 10.10.10.19，这样就能够保证目的端能够将应答的包发回给 RouterA，然后再转发回源端 VM。可以通过 iptables 命令查看Router的 SNAT 的规则，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dc872B1l3eop.png?imageslim" alt="mark"></p><p>此时当VLAN11的VM（172.18.11.12） Ping 10.10.10.1 时，可用通过 tcpdump 分别观察 RouterA和VM的 icmp 数据包来验证 SNAT 的行为。如下：</p><ul><li><ul><li><strong>RouterA的Port0端口（qr-1b1706db-b0）的ICMP数据包：</strong></li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/FmAH1b08HLRW.png?imageslim" alt="mark"></p><ul><li><ul><li><strong>RouterA的PortA端口（qg-099f513f-e9）的ICMP数据包</strong></li></ul></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/z3U9TbJKvQQG.png?imageslim" alt="mark"></p><p><strong>因此，SNAT 让 VM 能够直接访问外网，但外网还不能直接访问VM。因为VM没有外网 IP。这里 “直接访问 VM” 是指通信连接由外网发起，例如从外网 SSH VM，这个问题可以通过 floating IP 解决。</strong></p><p><strong>步骤6：</strong>给虚机VM（172.18.15.19）绑定一个floating IP（10.10.10.12），如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/b8D8jd3u0kGi.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>查看Router的interface变化发现，在qg-099f513f-e9接口上增加了一个IP地址（10.10.10.12），如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/lYR5seRRCg43.png?imageslim" alt="mark"></p><p>查看 router 的 NAT 规则，iptables 增加了两条处理 floating IP 的规则：1）当 router 接收到从外网发来的包，如果目的地址是 floating IP 10.10.10.12，将目的地址修改为 VM的 IP 172.18.15.19。这样外网的包就能送达到 VM。2）当VM发送数据到外网，源地址 172.18.15.19 将被修改为 floating IP 10.10.10.12。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/qHyOJ8FUL4pm.png?imageslim" alt="mark"></p><p><strong>步骤8：</strong>在我的实验环境中，10.10.10.1 是宿主机网卡Vnet2的地址，现在让它PING VM。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/n4Lw0bjIkAo3.png?imageslim" alt="mark"></p><p>能够 PING 通。同时，我们任意一个主机节点ssh连接虚机VM，发现可以直接登录。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GtheCaxDAtA5.png?imageslim" alt="mark"></p><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、Nuetron的Nuetron-Server为什么与其他服务不一样，不起名为Neutron-api？</strong></p><p><strong>2、请阐述Provider与Self网络的区别，请分析网络云采用哪种网络虚拟化解决方案，并给出理由？</strong></p><p><strong>3、OVS agent与Linux Bridge agent在实现二层网络上区别是什么？为什么OVS agent实现的二层网络会包含一个Linux Bridge网桥？</strong></p><p><strong>4、请分析直连路由与静态路由的区别？并分析在Neutron的vRoute解决方案中，直连路由和静态路由的内部转发机制（结合实际虚拟机实例instance的数据包流向分析）</strong></p><p><strong>5、在网络云大区华为OVS/EVS解决方案中，一个OVS网桥内部至少包含几个二层网桥？各网桥的类型和作用是什么？</strong></p><p><strong>6、虚拟机的虚拟网卡vNIC，在Neutron的网络虚拟化解决方案中本质上是一个什么设备？它与Network上Port是否同一个概念？为什么？Network上的Port资源是否与虚拟机实例一一对应？</strong></p><p><strong>7、在网络云数据中心内各物理网络平面的网关设置在哪里？不同物理平台互通通过什么设备实现（根据管理平面下发指令到业务虚机的场景阐述）</strong></p><p><strong>8、SNAT和DNAT分别用于什么场景？NAT技术是否属于安全领域的技术之一？</strong></p><p><strong>9、在OVS的br-int网桥上不同VLAN的虚拟机实例通过什么机制进行隔离？（根据实际部署场景抓包分析）</strong></p><p><strong>10、在后续网络云资源池引入SDN控制器，其需要与Nuetron进行对接，则SDN控制作为Neutron的pulgin进行集成还是作为Neutron的agent进行对接？其API接口通过Neutron的什么API组件实现？其与neutron-server，neutron-agent是否通过消息队列进行交互？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Neutron便是OpenStack云计算中的网络服务项目，其源自早期的Nova-network网络组件。Nova-network从Nova项目中独立出来之后，社区成立了针对网络功能虚拟化的Quantum项目。但由于商标侵权的原因，在Havana版本后，Quantum项目更名为现在的Neutron项目。Neutron是OpenStack的核心项目之一，虽然是OpenStack核心项目中成熟相对较晚的项目，但是Neutron项目在社区的热度很高。直到目前，在最新发行的OpenStack版本中，Neutron是新增功能和问题修复最多的核心项目。OpenStack的网络服务由Neutron项目提供，Neutron允许OpenStack用户创建和管理网络对象，如网络、子网、端口和路由等，而这些网络对象正是其他OpenStack服务正常运行所需的网络资源。Neutron项目中实现的各种插件使得用户可以选择不同的网络设备和软件，并为OpenStack的网络架构和部署提供极大的灵活性。此外，Neutron提供了API Server以供用户进行云计算网络的定义和配置，而Neutron灵活多样的插件使得用户可以借助各种网络技术来增强自己的云计算网络能力。Neutron还提供了用以配置、管理各种网络服务的API，如L3转发、NAT、负载均衡、防火墙和VPN等高级网络服务。因此，我们大致可以画出Neutron在OpenStack上下文架构（增加个Heat云编排服务），如下图所示，Neutron已经成为OpenStack三大核心（计算、存储、网络）之一，对外提供Naas（Network as a Service）服务。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-计算管理服务Nova</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E8%AE%A1%E7%AE%97%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Nova/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-计算管理服务Nova/</id>
    <published>2020-03-03T12:05:12.000Z</published>
    <updated>2020-03-03T12:39:21.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Compute Service Nova是OpenStack最核心的服务，负责维护和管理云环境的计算资源。OpenStack作为IaaS的云操作系统，虚拟机生命周期管理就是通过Nova来实现的。因此，Nova是OpenStack云中的计算组织控制器，首次出现在OpenStack的Austin版本，也就是伴随着OpenStack的诞生就存在。提供大规模、可扩展、按需自助的计算资源服务，现在的版本同时支持管理裸机、虚拟机和容器。而在OpenStack早期的几个版本中计算、存储和网络均由Nova来提供，也就说不仅存在nova-compute组件，同时也有nova-volume和nova-network组件，后续随着项目拆分，nova-volume演变为现在Cinder服务，nova-network演变为现在Nuetron服务，而Nova自身则专注于计算服务，主要依赖Keystone提供认证服务，Glance提供镜像服务，Cinder提供块存储服务和Nuetron提供网络服务，其在OpenStack整个系统中位置如下，处于系统整体的核心地位。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/H2xnEoJVfrkR.jpeg?imageslim" alt="mark"></p><p>Nova计算管理服务的具体作用主要包括以下几个方面：</p><ul><li>支持OpenStack云中实例（VM Instances）生命周期的所有活动。</li><li>负责管理计算资源、网络、认证、所需可扩展性的平台。</li><li>Nova自身并没有提供任何虚拟化能力，它通过调用诸如libvirt之类的API和下层Hypervisors实现交互。</li><li>Nova 通过一个与Amazon Web Services（AWS）EC2 API兼容的web services API来对外提供服务。</li><li>Horizon或者其他系统可以通过调用Nova-API实现和计算服务的交互。</li><li>存在多个各司其职的服务（即守护进程）。</li></ul><h2 id="Nova的架构"><a href="#Nova的架构" class="headerlink" title="Nova的架构"></a><strong>Nova的架构</strong></h2><p>Nova项目最初的源代码由美国国家航空航天局（NASA）贡献，截至Ocata版本，Nova项目已发行了15个版本，也是社区所有项目中最为成熟和用户生产环境部署率最高的项目。在2010年OpenStack项目成立之初，Nova项目主要分为Nova-Compute、Nova-volume和Nova-network三大功能模块。在2012年9月OpenStack的Folsom版本发行时，社区才将Nova-volume和Nova-network独立出来分别构建了Cinder和Quantum项目（后因商标原因更名为Neutron项目）。在OpenStack的A至E版本中，OpenStack Nova项目的逻辑架构如下图所示，其中，除了Nova-Compute、Nova-volume和Nova-network三大功能模块之外，还有处理RESTful API请求的Nova-API模块、调度Nova-Compute的Nova-scheduler模块、用以模块信息交互的消息队列系统和配置及状态数据存储的数据库。而在早期的OpenStack版本中，仅有Nova、Swift和Glance三大项目，如果用户不准备使用对象存储Swift，则Nova和Glance项目即构成了早期的OpenStack云平台。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/byNv0abzFGQz.png?imageslim" alt="mark"></p><p>在OpenStack的Folsom版本发行后，Nova-volume和Nova-network被独立成为块存储Cinder项目和网络Neutron项目，而Nova自身的功能模块也被不断细分，除了Nova-Compute和Nova-API功能模块，以及消息队列和数据库之外，Nova项目还构建了Nova-cert、Nova-Conductor、Nova-consoleauth和nova-console等模块。块存储Cinder项目和网络服务Neutron独立后，OpenStack中三大核心功能计算、存储和网络项目之间的逻辑架构如下图所示，</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/htkaFFnbc5m8.png?imageslim" alt="mark"></p><p>Nova由负责不同功能的服务进程所构成，其对外提供的服务接口为REST API，而各个内部组件之间通过RPC消息传递机制进行通信。Nova中提供API请求处理功能的模块是Nova-API，由API服务进程来处理数据库读写请求、向其他服务组件发送RPC消息的请求和生成RPC调用应答的请求等。在Nova中，RPC消息机制通过oslo.messaging库实现，这里的oslo.messaging库是对消息队列系统的顶层抽象。Nova中的大部分服务组件，除了Nova-Compute，都能以分布式的方式运行在多台服务器上，并且各服务组件之间会使用一个Manager进程来监听RPC消息。<strong>Nova-Compute组件的特别之处在于其作为一个独立进程运行在某个Nova-Compute管理下的Hypervisor上，从RPC消息机制而言，Nova-Compute进程通过专有队列进行消息订阅，而其他Nova组件则可以通过共享队列订阅消息。</strong>关于Nova与RPC消息机制之间的更多细节，可以参考前面《2、中间件服务—消息队列RabbitMQ》的介绍。</p><p>Nova在设计过程中使用了中心化数据库思想，即各个服务组件共同使用相同的数据库。不过，为了便于用户快速升级OpenStack版本，服务组件对数据库的访问经过了一个对象层，其主要作用在于解耦数据库与Nova组件之间的强关联，使得已经升级完成的服务组件仍然可以与运行在老版本下的Nova-Compute服务通信。为了实现这一功能，<strong>Nova采用了中心化的管理组件Nova-Conductor来代理Nova-Compute对数据库的RPC请求**</strong>。**在大规模Nova-Compute部署过程中，RPC消息机制是最可能的集群瓶颈。为了解决这一问题，Nova在水平扩展时采用了一种称为Cell的部署方式。Cell有父Cell和子Cell之分，并且不同的Cell内部使用不同的消息队列系统。</p><p>在实际的OpenStack系统运行过程中，计算服务Nova通过与认证授权服务Keystone交互从而实现身份权限的识别认证过程，并通过OpenStack的镜像服务Glance为实例提供系统镜像，而用户和云管理员则通过OpenStack的控制面板服务Horizon与Nova进行交互。此外，<strong>Nova计算资源的使用限额（Qoutas）以项目（Project）为单位进行限制，而镜像资源的访问则通过项目和用户来限制。</strong>OpenStack的计算服务Nova有如下组件构成：</p><ul><li><strong>nova-api：</strong>Nova核心组件。负责接收和响应终端用户对计算资源发起的API调用请求，如WSGI APP的路由请求和授权相关请求。nova-api接收到请求后，通常将请求转发给Nova服务的其他组件，如nova-scheduler。nova-api除了支持OpenStack的API请求外，还支持Amazon的EC2 API请求，nova-api的内部逻辑如下图所示。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/4SMAjRSJKNyT.png?imageslim" alt="mark"></p><p>如上图，nova-api层提供了三种服务：支持OpenStack API调用的osapi_compute，支持Amazon API调用的ec2以及metadata元数据服务（主要提供虚拟机相关信息的获取，比如虚拟机内置了cloud-init组件后，启动时会从metadata服务获取虚拟机的名称等信息设置到虚拟机内部）。整个nova-api的架构类似洋葱结构一样，从外部到内核一层一层包裹着，中间的每层是一个filter对象，用于对消息的处理再加工（比如增加request_id）或者在消息真正被处理前做一些其他事情（比如记录access日志），最终内核就是app，真正消息的处理者。App会根据消息请求的资源和action等信息route到注册的controller执行，每个controller都处理一个资源的action，如创建，删除，更新，查询等。也可以定位其他的action，如对虚拟机的迁移等，即承担虚拟机生命周期管理的入口。</p><ul><li><strong>nova-conductor：</strong>Nova核心组件。nova-conductor主要起到nova-compute服务与数据库之间的交互承接作用，其在nova-compute的顶层实现了一个新的对象层以防止Nova-Compute直接访问数据库带来的安全风险。nova-conductor的内部逻辑如下图所示。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/LPrMMu3oIrzD.png?imageslim" alt="mark"></p><p>在实际运行中，nova-compute并不直接读写访问数据库，而是通过nova-conductor实现数据库访问。nova-conductor组件可以水平扩展到多个节点上同时运行，但是nova-conductor不能部署到运行nova-compute的计算节点上，否则将不能隔离nova-Compute对数据库的直接访问，从而不能真正起到降低数据安全风险的作用。除此之外，nova-conductor还负责Nova服务的复杂流程控制，比如创建、冷迁移、热迁移、虚拟机规格调整和虚拟机重建等，以及与其他组件心跳信息定时写入等功能。需要注意一点，nova-compute组件只有在nova-conductor组件正常启动之后才能启动，即nova-compute组件的启动依赖nova-conductor。</p><ul><li><strong>nova-scheduler：</strong>Nova核心组件。主要负责从队列中截取虚拟机实例创建请求，依据默认或者用户自定义设置的过滤算法（根据计算节点的CPU、内存和磁盘等参数过滤）从计算节点集群中选取某个节点，并将虚拟机实例创建请求转发到该计算节点上执行，即最终的虚拟机将运行在该计算节点上。采用的过滤算法也可以根据需求自定义并在nova.conf配置文件中指定，如在配置Host Aggregate功能时，通常就需要更改默认的Scheduler规则。nova-scheduler的内部逻辑如下图所示。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/XeAJwewzR8Ti.png?imageslim" alt="mark"></p><p>上图中，nova-scheduler选择计算节点主要采用过滤和权重两步进行。过滤就是通过过滤器选择符合条件的节点，权重就是通过权重对比选择最优的节点，默认的权重取计算节点的剩余内存大小，也可以自定义其他通用参数，比如CPU和磁盘空间等。如果在创建虚拟机时，指定主机或主机组创建，则权重这一步骤就会失效，根据过滤器中主机或主机组过滤规则直接选定某个主机进行创建。</p><ul><li><strong>nova-compute：</strong>Nova核心组件。主要功能是接收来自队列的请求，并执行一系列系统命令，如创建一个KVM虚拟机实例并在数据库中更新对应实例的状态等。nova-compute的内部逻辑如下图所示。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/9XB1B0IFlCtt.png?imageslim" alt="mark"></p><p>上图中，nova-compute组件实际上运行在计算节点上一个进程，其内部逻辑分为两个部分：Manager和Driver。Manager主要负责各类操作指令接收，并下发给Hypervisor去执行，同时通过resource_tracker进程从Hypervisor收取各节点的资源信息，并通过AMQP与OpenStack的其他服务进行交互，比如与Glance交互获取创建虚拟机时需要的镜像Image，与Cinder交互获取创建虚拟机时需要的虚拟磁盘Volume，与Neutron交互获取创建虚拟机时需要的IP和Port信息等等。Driver组件用于对接底层异构的Hypervisor，常见的Hypervisor API有支持KVM/QEMU虚拟化引擎的Libvirt API、支持XenServer/XCP虚拟化引擎的XenAPI和支持VMware虚拟化引擎的VMware API。OpenStack默认使用的是KVM虚拟化引擎，因此在OpenStack nova-compute中最常使用的还是Libvirt API。因此，nova-compute组件内部工作流程比较复杂，主要完成以下功能：</p><ol><li><strong>虚拟机生命周期操作的真正执行者worker，通过调用不同Hypervisor的Driver实现虚拟机的生命周期管理指令执行；</strong></li><li><strong>底层可以对接不同虚拟化平台，实现不同虚拟化解决方案的异构，不仅支持虚拟机，还支持裸机和容器；</strong></li><li><strong>内置周期性的任务调度，完成计算节点资源的刷新，并通过nova-conductor将计算节点资源状态写入nova-db，便于nova-scheduler在调度时查询nova-db获取最新的节点资源状态。</strong></li><li><strong>在各Hypervisor上以插件的方式植入resource_tracker资源管理模块，配合周期性的任务调度完成资源状态的统计；</strong></li></ol><p><strong>Nova服务的核心组件除了上述组件之外，还有一个nova-cert组件，该组件主要用于与Amazon EC2 API对接时启用，是一个服务器的守护进程，负责为基于X509认证的Nova Cert提供服务，通常用于对euca-bundle-image镜像生成X509证书。</strong>除此之外，还有其他一些便于虚拟机维护的支撑组件—<strong>虚拟机控制台服务</strong>，包括：nova-consoleauth、nova-novncproxy、nova-xvpvncproxy等，其主要功能如下：</p><ul><li><strong>nova-consoleauth：</strong>虚拟机控制台服务。nova-consoleauth主要为虚拟机控制台连接提供认证授权服务。在运行VNC代理服务的OpenStack集群中，必须运行nova-consoleauth服务。一个nova-consoleauth实例可为多种类型的代理服务提供认证授权服务。需要注意的是，不要混淆nova-consoleauth与nova-console，后者是XenAPI风格的控制台服务，而目前多数VNC代理软件都已经不再使用nova-console。</li><li><strong>nova-novncproxy：</strong>虚拟机控制台服务。nova-novncproxy主要提供对运行状态中的实例进行VNC连接访问的代理。其直接基于网页的novnc客户端连接，即支持基于Web网页的实例访问，是个非常方便的功能。</li><li><strong>nova-xvpvncproxy：</strong>虚拟机控制台服务。nova-xvpvncproxy主要提供对运行状态中的实例进行VNC连接访问的代理，其仅支持特定于OpenStack的Java客户端发起的VNC连接。</li></ul><h2 id="Nova中的几个重要概念"><a href="#Nova中的几个重要概念" class="headerlink" title="Nova中的几个重要概念"></a><strong>Nova中的几个重要概念</strong></h2><p>Nova服务是OpenStack所有服务中最复杂的一个组件，没有之一。不仅涉及虚拟机的一些重要概念，比如server、flavor，还包括主机服务器的一些概念，比如host、hypervisor，甚至还有一些数据中心DC规划方面的概念，比如Region、cell等等。因此，弄清楚这些概念是理解Nova服务的关键。</p><p><strong>1）虚拟机实例方面的概念包括：server/instance、server metadata、flavor、quota、server group、bdm</strong>，具体如下：</p><ul><li><strong>server/instance：</strong>Nova中最重要的数据对象，本质上就是虚拟机实例，是Nova管理提供的云服务资源。</li><li><strong>server metadata：</strong>虚拟机实例的元数据信息，key-value格式，用于对虚拟机附加必要的描述等信息，比如虚拟机实例名、当前状态、部署位置等等。</li><li><strong>flavor：</strong>虚拟机实例规格模版，用于定义一类虚拟机实例所占用的资源要求，比如2C8G40G，表示使用该flavor创建的虚拟机实例需要2vCPU，8G RAM，40G根磁盘。flavor只能由系统管理员admin创建，供普通用户/租户在创建虚拟机时调用。</li><li><strong>quota：</strong>资源配额，用于指定租户最多能够使用的资源上限。</li><li><strong>server group：</strong>虚拟机实例的亲和性/反亲和性组。同一个亲和性组的虚拟机在创建时会被调度到相同的物理主机上，而反亲和性组，顾名思义，在创建时，同一个反亲和性组的虚拟机实例会被调度不同的主机上。</li><li><strong>bdm：</strong>Block Device Mapping，块存储设备映射，用于描述虚拟机实例拥有的存储设备信息。</li></ul><p>2）主机服务器方面的概念包括：hypervisor/node、host**，具体如下：</p><ul><li><strong>hypervisor/node：</strong>即安装虚拟化组件的物理主机，对于KVM、Xen等虚拟化解决方案，一个node即对应一个物理主机；对于VMware虚拟化解决方案，一个node对应同一个vCenter下的一个CLuster。</li><li><strong>host：</strong>物理主机。对于KVM、Xen等虚拟化解决方案，一个host对应一个物理主机，同时对应一个node；对于VMWare的虚拟化解决方案，一个host对应一套vCenter，至少包含一个Cluster，一个Cluster至少包括一个物理主机。</li></ul><p><strong>3）数据中心DC规划方面的概念包括：Region、Cell、AZ和HA（Host Aggregate）</strong>，具体如下：</p><ul><li><strong>Region：</strong>是地理位置上隔离的数据中心区域，可以简单理解一个Region就代表一个数据中心DC。不同的Region是彼此独立的，即某个Region范围的人为或自然灾害并不会影响其他Region。Region的概念通常在公有云中出现，因为Region的多少是衡量一个公有云服务提供商运营能力的关键指标。而对于我们云化网络来说，要想实现自身业务的容灾和高可用设计，通常需要将同一个业务系统部署到不同的Region中，比如西安和汉中不同的DC中各部署一套，同时要借助负载均衡器才能实现容灾双活的功能。Region在实际部署中示意图如下所示：</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/400gol6sp6X7.png?imageslim" alt="mark"></p><p>在具体的实现过程中，不同Region通常共用相同的认证服务和控制面板服务，可以通过共享存储池进行数据复制同步来实现高可用。下图就是OpenStack官方推荐的私有云多区域部署架构，Region1和Region2共享Horizon、Keystone和Swift服务，其他OpenStack服务在各自Region中独立部署。所以，不同Region内部相同服务就会有不同的endpoint API，对不同Region内部服务的访问就需制定不同的API，在实际生产中，一般是通过负责均衡器实现。需要注意一点：<strong>在OpenStack的众多服务中，并不是所有服务都支持跨Region部署，除了Horizon、Keystone和Swift外，其余服务都被设计为在同一个Region运行。</strong>如对于计算资源的分区，OpenStack仅支持Cell部署、AZ和Host Aggregate划分，并不能跨Region；网络管理服务仅能管理相同广播域或者互联集中的网络资源；块存储管理服务也只能管理单个Region内相同存储网络内的存储资源。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/cw6dohWHHN8L.png?imageslim" alt="mark"></p><ul><li><strong>Cell：</strong>Nova服务中的Cell功能模块在OpenStack的G版本提出，主要为了解决大规模Nova计算节点部署过程中可能带来的集群瓶颈问题。原来社区中有过大范围讨论，一致认为在计算节点数目超过500个时，便会遇到共享消息队列系统的性能问题。在早期的OpenStack版本中，nova cell的架构为V1版本，以树型结构为基础，由一个API-Cell（父Cell）与多个Child-Cell构成。其中，API-Cell只运行Nova-API服务，而每个Child-Cell运行除nova-api以外的全部Nova服务，且每个Child-Cell运行自己的消息队列、数据库及Nova-Cells服务。这种架构，用户请求需要经历两层调度实现，即顶层的API-Cell在多个Compute Cell之间调度和各个Compute Cell内部的主机调度，不同的Cell之间通过nova-Cells服务进行消息传递。有很多bug，且社区维护人员基本为0，属于冷门。如下图所示，其基本流程就是：在启用nova-cell v1的OpenStack集群中，用户创建虚机的请求首先到达顶层API Cell，然后通过API Cell的调度算法决定虚机由哪个Cell负责创建，当某个Compute Cell接收到来自API Cell的请求后，Compute Cell将把这个请求通过Nova-Cells服务传递到Cell中的nova-scheduler服务进行主机调度，Compute Cell中的nova-scheduler服务接收到请求后，再根据主机资源统计信息将虚机创建某一台服务器上。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/U2UHbEJgD0EI.png?imageslim" alt="mark"></p><p>在2016年上半年的Austin峰会上，Nova核心领导成员发布了Nova Cell v2版本，并在OpenStack的N版本及后续版本中默认启动。与V1版本不同，V2版本采用单层调度的思想。即用户请求只需要通过一层调度即可抵达最终的物理服务器。也就是说API Cell不仅在Compute Cell之间进行调度，还同时对选定的Compute Cell内部的主机进行调度。在每一个Compute Cell中，消息队列和数据库独立实现，同时Cell v2版本将数据库进行分离实现，即将整个OpenStack的全局信息保存在API Cell的数据库中，而虚机的相关资源数据保存在各个独立的Cell的数据库中。同时，在设计上还新增了Cell0模块，一旦API Cell对全部Compute Cell调度失败，则用户请求暂时被放在Cell0中。其基本原理和流程如下，api和cell有了明显的边界 ，api层面只需要数据库，不需要Message Queue，nova-api只创建和依赖 nova_api和nova_cell0两个数据库，nova-scheduler服务只需要在api层面上安装 ，cell不需要参数调度 。这样实现了一次调度就可以确定到具体在哪个cell的哪台机器上启动。各个cell中里面只需要安装nova-compute和nova-conductor服务以及其依赖的DB和MQ，nova-api根据nova-scheduler的调度结果会直接连接相关cell的MQ和DB,，所以不需要类似nova-cell 这样的额外子服务存在，性能上也会有及大的提升。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/9catXexARw0T.png?imageslim" alt="mark"></p><p>上面的nova-scheduler调度各个cell本质上是依赖placement 服务获取各个cell的资源使用情况从而实现调度。placement一个比较独立的REST API栈，主要为了追踪记录resources provider目录和resource使用情况。例如，resource provider可以是一个计算节点、共享存储池或是IP地址池。placement 服务追踪每种resource provider的服务目录，使用情况。这样就可以从placement API获取resource provider目录，并获取resource provider的资源使用情况。这里需要重点提一下nova_cell0数据库，从nova_cell0数据库与nova数据库的数据表对比发现（如下图），nova_cell0数据库的schema和nova是一样的，其存在的主要用途为：<strong>当VM调度失败时，VM的信息不属于任何一个cell时， 可以放到cell0上面 。因此，nova_cell0这里面的数据并不是太重要 。</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/EPfItN8FDnVv.png?imageslim" alt="mark"></p><p>Cell相关的数据库表都在nova_api里面，包括cell_mappings, host_mappings, instance_mappings。其表结构如下 ，cell_mappings表包含了cell的Database和Mesage Queue连接信息（transport_url和database_connection），用于和子cell 通讯。host_mappings是用于存储nova-scheduler可以调度的主机物理节点的信息。instance_mapping表里有所有instance id，这样在查询instance时，就可以从这个表里查到他所在的cell_id， 然后通过cell_mappings表就可以查询到对应cell的具体信息 。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/77jiL46n7oLK.png?imageslim" alt="mark"></p><p>这里其实也有一个坑，之前 nova-compute 启动起来，就可以直接使用了，但是cell v2之后就需要手动运行nova-manage cell_v2 discover_host，把host mapping映射到cell_mappings表里面 ，那台计算节点才会加入到调度中 。因此，在小型一些的环境上，推荐打开自动发现功能 ，就不用手动跑命令了。比如，我们在配置控制节点的nova.conf文件中，设置Scheduler的会话的discover_hosts_in_cells_interval = 300，表示每5分钟就进行计算节点扫描，同步cell_mappings数据库表。如下图，我们当前的环境规划了一个cell1，该cell下包含两个计算主机：rocky-controller和rocky-compute，host_mappings表与cell_mappings表的关联就是通过cell_id主键进行关联。同理，我们也可以通过虚拟机实例instance_id，查询到关联的cell_id，通过cell_id查询到cell的具体信息及内部主机节点的信息。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/h3dSRYfHNTQ2.png?imageslim" alt="mark"></p><ul><li><strong>AZ：</strong>可用域（Availability Zone，AZ）是对计算资源的额另一种划分，在AWS的区域划分设计中，AZ是对Region的再次划分。按照AWS解释，划分AZ的主要目的是为了提高容灾和提供廉价的故障隔离服务。在OpenStack中，我们会启动nova cell功能，一般是将单个Region部署为compute cell，然后再到cell中去划分AZ。<strong>注意：AZ与cell不同，AZ中没有独立的数据库和消息队列，是一种物理资源的划分。多个AZ共享cell内部的消息队列和数据库服务。</strong>因此，OpenStack中的AZ主要起到故障隔离的作用，一般在数据中心规划时，按照动力、网络、存储等物理资源故障隔离的角度将多个物理主机划分为一个AZ，在部署OpenStack时，可以将一个AZ对应一个Cell，然后在数据库中创建虚拟资源与物理资源的映射关系。下面就是AZ的逻辑架构图。</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/u6pygFmDz1Ud.png?imageslim" alt="mark"></p><ul><li><strong>HA：主机聚合（Host Aggregate）</strong>，是OpenStack平台管理员用户进行主机划分的另一种方法，即管理员可以根据硬件资源的某一属性，将具有相同硬件属性的服务器归类划分的方式。其对普通用户是不可见的。这里举个例子说明下。比如运营商的DC中部署了不同类型的服务器，有些服务器具有强大的计算能力，有些服务器不仅有强大的计算能力其IO能力也很强大，而另一些服务器配置高速网卡具有强大的网络吞吐能力。这时，作为运营商的云平台管理员就可以对这些服务器进行主机聚合划分，比如将具有强大计算能力的服务器放在主机聚合A中，将既有强大计算能力同时具备强大IO能力的服务器放在主机聚合B中，将具备高速网卡的服务器放在主句聚合C中。此时，如果某企业用户想要建设一套高性能、高可用集群的私有云业务，而高性能、高可用集群通常有“胖节点”和“瘦节点”之分（“胖节点”既充当计算也充当IO转发的角色，“瘦节点”只充当计算的角色），这样运营商就可以从主机聚合A中选择创建虚机提供“胖节点”，从主机聚合B中选择创建虚机提供“瘦节点”，同时进行差异性的服务收费。除了上述计算能力、IO能力、网路吞吐能力可以用来做聚合参考外，服务器硬盘类型、内存大小等都可用来主机聚合的参考。<strong>同一个Host可以规划在多个HA中，但是必须属于同一个AZ。</strong></li></ul><p>在OpenStack的解决方案，上述数据中心规划方面的概念，对资源的使用范围从大到小的排序为：<strong>Region&gt;Cell&gt;AZ&gt;HA。</strong></p><h2 id="Nova中的主机调度策略"><a href="#Nova中的主机调度策略" class="headerlink" title="Nova中的主机调度策略"></a><strong>Nova中的主机调度策略</strong></h2><p>在Nova的各个服务组件中，Nova-scheduler是个非常关键的组件，其主要作用便是为Nova的主机选取提供智能决策功能。当Nova客户端发起创建实例请求时，Nova-API会将请求转发到Nova-scheduler，由Nova-scheduler在运行Nova-compute的计算节点中选取用于创建符合请求虚拟机资源条件的宿主机。Nova-scheduler对宿主机的选取分为两个步骤：第一步从计算节点集群中选取符合请求虚拟机资源条件的全部节点，这一过程称为过滤（Filter）；第二步从符合创建请求虚拟机的计算节点中选取唯一最佳的计算节点，作为本次请求虚拟机的宿主机，这一过程称为加权（Weigh）。在Nova-scheduler的过滤阶段，Nova的Filter Scheduler根据配置文件nova.conf中设置的策略集合对OpenStack集群中全部计算节点进行过滤迭代，通过三个参数来配置，即<strong>scheduler_driver</strong>、<strong>scheduler_available_filters</strong>和<strong>scheduler_default_filters，这三个参数都在/etc/nova/nova.conf文件的DEFAULT段配置，其配置项默认值如下：</strong></p><ul><li><strong>scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler ,这里表示使用默认FilterScheduler驱动，使用/usr/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py文件中的FilterScheduler类，如下图所示。</strong></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Xg19OYtaAABx.png?imageslim" alt="mark"></p><ul><li><strong>scheduler_available_filters = nova.scheduler.filters.all_filters，这里表示可以使用的Filter，默认使用全部filter，所支持的filter在/usr/lib/python2.7/site-packages/nova/scheduler/filters/目录下，选择all_filters表示全部。该目录下的文件列表如下：</strong></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/g19meGHDTnts.png?imageslim" alt="mark"></p><ul><li><p><strong>scheduler_default_filters = RetryFilter, AvailabilityZoneFilter，这里显式制定要使用的过滤器filter，多个过滤器之间用逗号“,”隔开。</strong></p><p>在上面的是三个配置项中，配置参数scheduler_driver用于设置使用的Scheduler，FilterScheduler是Nova-scheduler默认使用的Scheduler，此外，Nova-scheduler允许使用第三方的Sched-uler，只需设置scheduler_driver即可。配置参数scheduler_available_filters用于指定可以使用的过滤器，默认情况下Nova自带的全部过滤器均可被使用（nova.scheduler.filters.all_filters）。可用过滤器参数可以重复指定，如用户自己实现了一个过滤器myfilter.WarriorFilter，然后用户既想使用Nova自带的过滤器又想使用自建的过滤器，可以配置多行scheduler_available_filters实现。配置参数scheduler_default_filters用于指定使用的过滤器列表。在进行主机过滤时，主机会被此参数指定的列表过滤器顺序过滤一遍。在主机进入每个过滤器时，如果主机条件满足请求中的实例需求，则返回True，否则返回False。当返回False后，该主机的过滤流也就结束，不再进入后续过滤器继续过滤。Nova-Scheduler内建自带的过滤器主要包括以下几种：</p></li><li><p><strong>AggregateCoreFilter：</strong>以Aggregate为前缀的过滤器通常只在创建了Host Aggregate的情况下才会使用到。AggregateCoreFilter表示通过主机CPU核数来过滤每个Aggregate中的主机。主机可用CPU核心数通过每个主机集配置文件中的cpu_allocation_ratio值（默认为16）来计算。如果主机CPU核心数满足请求创建实例的vCPUS需求，则返回True。如果当前主机属于多个Host Aggregate，并且有多个cpu_allocation_ratio值，则使用cpu_allocation_ratio的最小值。cpu_allocation_ratio参数主要用于设置主机CPU的overcommit，如cpu_allocation_ratio=16，而主机vCPUs为8，则调度时Scheduler认为主机可用vCPU为128。</p></li><li><strong>AggregateDiskFilter：</strong>表示通过主机磁盘使用率来过滤每个Aggregate中的主机。主机磁盘使用率会根据每个主机集配置文件中的disk_allocation_ratio（默认值为1，即按实际磁盘容量来调度）参数来计算，如果OpenStack集群中没有创建主机集，则disk_allocation_ratio便会成为全局性的参数。如果主机集中主机上的可用磁盘容量（单位为GB）满足请求虚拟机中的磁盘需求（镜像系统盘大小+临时磁盘大小），则返回True。</li><li><strong>AggregateImagePropertiesIsolation：</strong>表示根据实例创建请求中的镜像属性来过滤主机集中的主机，通常用于将基于特定镜像的实例创建到与镜像匹配的特定主机上。镜像属性与主机匹配与否根据如下规则来判断：如果当前主机属于某个主机集，并且此主机集定义了相应的元数据来匹配某个镜像的属性，则认为该主机与镜像属性匹配，并且该主机将成为启动这个匹配镜像的候选主机；如果当前主机不属于任何主机集，则该主机可以启动任何镜像。</li><li><strong>AggregateInstanceExtraSpecsFilter：</strong>表示匹配的是Nova中Flavor与主机集的元数据，通常需要为Flavor的属性aggregate_instance_extra_specs设置一个属性值。过滤器将会根据主机集的元数据与实例创建请求中Flavor的aggregate_instance_extra_specs属性值来判断主机是否符合要求。</li><li><strong>AggregateIoOpsFilter：</strong>表示根据主机的IO负载来过滤每个主机集中的主机。主机IO负载根据参数max_io_ops_per_host值（默认为8）来决定，此过滤器会将IO负载过高的主机过滤掉。</li><li><strong>AggregateNumInstancesFilter：</strong>表示根据主机已有实例数目来过滤每个主机集中的主机，过滤标准为主机集中允许的最大主机实例数目参数max_instances_per_host（默认50）值，当前运行实例数目超过此参数值的主机将会被过滤掉。</li><li><strong>AggregateMultiTenancyIsolation：</strong>用于将特定某个租户或多个租户的实例创建到特定的主机集或可用域中。如果主机属于某个具有元数据且key为filter_tenant_id的主机集，则对应filter_tenant_id值的某个租户或多个租户发起的创建实例请求将会在该主机上创建实例。而如果主机不属于元数据key为filter_tenant_id的主机集，则所有组合都可以在该主机上创建实例。AggregateMultiTenancyIsolation过滤器并不会将具有元数据Key为filter_tenant_id的主机集与其他租户隔离，任何租户仍然可以在指定的主机集上创建实例。</li><li><strong>AggregateRamFilter：</strong>表示通过主机可用内存RAM来过滤每个主机集中的主机，主机的可用内存根据主机集配置文件中ram_allocation_ratio值（默认为1.5）来计算。</li><li><strong>AggregateTypeAffinityFilter：</strong>用于通过主机集的instance_type键来过滤主机，如果主机集没有设置Key为instance_type的元数据，或者将主机集中key为instance_type的元数据值设为实例创建请求中所包含的instance_type值，则请求中的instance_type的元数据值可以是单个字符串或者逗号分开的多个字符串，如m1.nano或m1.nano，m1.small。</li><li><strong>AllHostsFilter：</strong>不会发生任何过滤操作，所有正常运行的计算节点都会通过该过滤器。</li><li><strong>AvailabilityZoneFilter：</strong>是可用域AZ过滤器，当在请求中指定创建实例的AZ时，必须在Scheduler中使用该过滤器。</li><li><strong>ComputeCapabilitiesFilter：</strong>与AggregateInstanceExtraSpecsFilter类似，也是通过设置Nova中主机类型的元数据来对主机进行过滤，不过ComputeCapabilitiesFilter过滤器中与Flavor的元数据匹配的对象不是主机集中的metadata，而是主机的Capabilities。Capabilities可以是主机的Architecture、Cores、Features、Model和Vendor等，在使用ComputeCapabi-litiesFilter过滤器时，Flavors的extra specs格式为“Capabilities：key：value”形式，“Capabilities”表示命名空间，“key：value”表示extra specs键值对。如extra specs为“Capabilities：architecture：x86”的Flavor在创建实例时，ComputeCapabilitiesFilter只会允许X86架构的主机通过，如果命名空间不是“Capabilities”，则Flavors的extra specs中命名空间字段会被忽略而仅提取“key：value”部分。在使用ComputeCapabilitiesFilter过滤器时，强烈建议不要省略命名空间（extra specs中第一个冒号前的字符串），否则将会与AggregateInstanceExtraSpecsFilter过滤器冲突（两个过滤器同时启用的情况下），同时建议将命名空间字符串设为Capabilities。</li><li><strong>ComputeFilter：</strong>意味着正常运行nova-compute服务的计算节点才能被Nova-scheduler调度，通常而言，ComputeFilter是必须的过滤器。</li><li><strong>CoreFilter：</strong>CoreFilter与AggregateCoreFilter过滤器类似，按主机CPU核数来过滤主机。如果未启用此过滤器，则Scheduler调度给实例的CPU核数会超出物理主机的实际CPU核数，即运行在实例上的虚拟CPU核数之和可以超出物理主机CPU核数。通过启用CoreFilter过滤器，并且设置超额使用比率参数cpu_allocation_ratio，限定实例可以超额使用的虚拟CPU核数。例如在启用CoreFilter的情况下，cpu_allocation_ratio设置为8，主机物理CPU核数为8，则该主机可供实例使用的全部vCPU核数为64。如果希望实例vCPU与主机物理CPU为1：1的分配使用关系，则可以设置cpu_allocation_ratio为1。</li><li><strong>DifferentHostFilter：</strong>将请求实例创建在与请求所指定的实例主机不同的主机上，即指定新创建的实例不能位于某个或多个实例的宿主机上。如Nova中已经有两个实例，并且用户希望新创建的实例不能与这两个已有实例的宿主机相同，则可以启用DifferentHostFilter过滤器。</li><li><strong>DiskFilter：</strong>与AggregateDiskFilter过滤器类似，即仅调度磁盘空间满足请求实例的root和临时存储空间的主机。在Nova中，主机磁盘可以超额使用，可以通过配置磁盘超额使用比率参数disk_allocation_ratio来限定调度器可见的最大虚拟主机磁盘容量。在配置文件nova.conf中，disk_allocation_ratio的默认值为1，即不允许主机磁盘空间被超额分配，在调度时以主机的实际可用磁盘容量为准。此外，需要注意的是，Scheduler提取的主机磁盘空间并不是hypervisor统计结果中free_disk_gb的值，而是disk_available_least的值。</li><li><strong>RamFilter：</strong>RamFilter过滤器将根据主机是否有足够的可用RAM来进行过滤。如果未启用RamFilter过滤器，则Scheduler会从主机上提取超额的RAM，即分配给实例的RAM之和会大于主机物理RAM容量。在启用RamFilter后，可以通过配置RAM超额使用比率参数ram_allocation_ratio的值来限定该主机可用的最大虚拟内存值，ram_allocation_ratio的默认值为1.5，因此如果当前主机可用物理RAM为10GB，则Scheduler提取到的主机RAM为15GB。</li><li><strong>JsonFilter：</strong>JsonFilter过滤器允许用户通过Json格式的Scheduler调度提示（hint）来自定义主机调度。JsonFilter过滤器允许的操作符有=、&gt;、&lt;、in、&lt;=、&gt;=、not、or和and，允许使用的变量有$free_ram_mb、$free_disk_mb、$total_usable_ram_mb、$vcpus_total和$vcpus_used。如希望将实例创建在主机可用RAM大于等于2GB的主机上，则在启用JsonFilter过滤器的情况下，使用如下实例创建语句：nova boot –image 827d564a-e636-4fc4-a376-d36f7ebe1747 –flavor 1 –hint query=’[“&gt;=”,”$free_ram_mb”,2048]’ VM01</li><li><strong>RetryFilter：</strong>用于将之前已经调度过的主机过滤掉。此过滤器只有在主机过滤重复参数scheduler_max_attempts值大于1的情况下才有效。如A、B、C三台主机在某次过滤后均符合标准，并且A主机因为权重最优而被选为创建实例的主机，但是由于某些原因实例创建失败，同时参数scheduler_max_attempts值大于1，因此Scheduler将进行再次调度，为了避免再次失败，当启用RetryFilter过滤器时，A主机将不再参与调度。为了避免反复过滤失败主机，RetryFilter通常是默认过滤器列表中的第一个过滤器。</li><li><strong>SameHostFilter：</strong>与DifferentHostFilter相反，即将请求实例创建在指定实例的宿主机上，使得要创建的实例与一个或多个已经存在的实例位于同一主机上。要使用此过滤器，客户端在创建实例时需要通过hint传入same_host键，键值为特定实例的UUID。</li><li><strong>ServerGroupAffinityFilter：</strong>与GroupAffinityFiltre功能相同，而GroupAffinityFilter即将被淘汰，过滤器ServerGroupAffinityFilter的作用是将请求实例调度到指定的Server Group中（此处的Server表示虚拟机），即将实例尽量创建到同一台主机上。要使用此过滤器，用户必须使用“affinity”策略预先创建一个服务器组（实例组），并在创建实例时通过hint参数的group键值对指定该实例属于此服务器组，group键的值为服务器组的UUID。</li><li><strong>ServerGroupAntiAffinityFilter：</strong>与ServerGroupAffinityFilter正好相反，以前的过滤器GroupAntiAffinityFilter即将被丢弃，ServerGroupAntiAffinityFilter表示将实例尽量不要创建在同一台主机上。要启用此过滤器，用户需要使用“anti-affinity”策略预先创建服务器组，并在创建实例时通过hint参数的group键值对指定该实例属于此服务器组，group键的值为服务器组的UUID。</li><li><strong>SimpleCIDRAffinityFilter：</strong>是一个基于网络的过滤器，主要用于根据创建实例时指定的IP地址范围来过滤主机。要使用此过滤器，用户在创建实例时必须指定两个hint参数，hint参数的两个Key分别为build_near_host_ip和cidr，第一个键的值为有效的IP地址，第二个键的值为CIDR格式的掩码。</li></ul><p>在经过SchedulerFilter中一系列的过滤器过滤之后，Nova-scheduler会选出符合请求实例资源的主机，但是如果有多台主机同时符合要求，则经过SchedulerFilter之后，仍然会得到一个主机列表。为了得到最终的实例创建主机，Nova-scheduler会给过滤后的主机列表中的计算节点进行“评分”，分数最高的主机将成为最终的实例创建主机，这个“评分”的过程即是所谓的权重计算。<strong>权重计算就是给有效主机列表中的主机进行权重赋值并选出最优权重值主机的过程。每个计算节点都可以通过不同的维度参数来进行评估，如内存、磁盘、CPU和IO负载，由于各个权重的单位不同，如内存单位为MB，磁盘单位为GB，CPU单位为核数等，因此不能对这些权重进行简单的线性相加来获取计算节点最终的权重值。同时，不同的权重对用户而言可能重要程度并不一样（如A用户认为内存比CPU重要，而B用户认为CPU比内存更重要）。为了解决这一问题，需要预先为每个权重定义关联的权重因子（multiplier），如果认为此权重要优于其他权重，则可以为其设置较大的权重因子</strong>，nova.conf配置文件中与权重相关的默认参数配置在DEFAULT会话中，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line"></span><br><span class="line">scheduler_host_subset_size = 1 <span class="comment">#&lt;===按照排序后主机列表仅选取一台主机</span></span><br><span class="line"></span><br><span class="line">scheduler_weight_classes = nova.scheduler.weights.all_weighers</span><br><span class="line"></span><br><span class="line">ram_weight_multiplier = 1.0 <span class="comment">#&lt;===权重因子</span></span><br><span class="line"></span><br><span class="line">io_ops_weight_multiplier = 2.0 <span class="comment">#&lt;===权重因子</span></span><br><span class="line"></span><br><span class="line">soft_affinity_weight_multiplier = 1.0 <span class="comment">#&lt;===权重因子</span></span><br><span class="line"></span><br><span class="line">soft_anti_affinity_weight_multiplier = 1.0 <span class="comment">#&lt;===权重因子</span></span><br><span class="line"></span><br><span class="line">[metrics]</span><br><span class="line"></span><br><span class="line">weight_multiplier = 1.0</span><br><span class="line"></span><br><span class="line">weight_setting = var_name1=1.0, var_name2=-1.0</span><br><span class="line"></span><br><span class="line">required = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">weight_of_unavailable = -10000.0</span><br></pre></td></tr></table></figure><p>最后，各个权重与其权重因子的乘积之和便是最终的计算节点权重值。需要指出的是，各个权重在参与计算之前会事先进行标准化（normalize），因此，节点最终的权重计算公式为：<strong>weight = w1_multiplier * norm（w1） + w2_multiplier * norm（w2） + ……</strong>，这里的norm(w1)就是标准化的权重值，原始权重值的标准化公式为：<strong>ratio_weight =（raw_weight - min_weight）/（max_weight - min_weight）。</strong>现假设有host1、host2、host3、host4、host5和host6六台主机需要进行权重排序，并且有3个权重，分别为W1、W2和W3，而3个权重对应的权重因子分别为1、1、2。六台主机经过W1、W2和W3计算后得到的原始权重值分别如下：</p><table><thead><tr><th>主机名称</th><th>HOST-01</th><th>HOST-02</th><th>HOST-03</th><th>HOST-04</th><th>HOST-05</th><th>HOST-06</th></tr></thead><tbody><tr><td>W1原始权重</td><td>110</td><td>20</td><td>10</td><td>80</td><td>50</td><td>90</td></tr><tr><td>W2原始权重</td><td>4</td><td>2</td><td>1</td><td>11</td><td>6</td><td>8</td></tr><tr><td>W3原始权重</td><td>5</td><td>25</td><td>10</td><td>15</td><td>5</td><td>20</td></tr></tbody></table><p>根据标准化计算公式，六台主机针对W1、W2和W3原始权重值进行标准化计算后的结果分别如下：</p><table><thead><tr><th>主机名称</th><th>HOST-01</th><th>HOST-02</th><th>HOST-03</th><th>HOST-04</th><th>HOST-05</th><th>HOST-06</th></tr></thead><tbody><tr><td>W1标准化权重</td><td>1</td><td>0.1</td><td>0</td><td>0.7</td><td>0.4</td><td>0.8</td></tr><tr><td>W2标准化权重</td><td>0.3</td><td>0.1</td><td>0</td><td>1</td><td>0.5</td><td>0.7</td></tr><tr><td>W3标准化权重</td><td>0</td><td>1</td><td>0.25</td><td>0.5</td><td>0</td><td>0.75</td></tr></tbody></table><p>由于W1、W2和W3的权重因子分别为1、1、2，因此根据节点最终的权重计算公式，六台主机节点的最终权重值分别如下：</p><table><thead><tr><th>主机名称</th><th>HOST-01</th><th>HOST-02</th><th>HOST-03</th><th>HOST-04</th><th>HOST-05</th><th>HOST-06</th></tr></thead><tbody><tr><td>weight</td><td>1.3</td><td>2.2</td><td>0.5</td><td>2.7</td><td>0.9</td><td>3.0</td></tr></tbody></table><p>对主机列表中的主机依据各自权重值进行降序排列后，得到的排序结果为host6、host4、host2、host1、host5、host3，因此根据权重排序后的最优节点应该是host6节点。如果各个权重的权重因子为负数，则host3节点为最优节点。关于节点加权排序的过程描述，OpenStack的官方网站给出了形象图示，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/S1T1AjXa1VUz.png?imageslim" alt="mark"></p><p>在实际使用中，FilterScheduler根据主机配置文件nova.conf中的scheduler_weight_classes配置参数对主机进行不同的加权计算，此参数的默认配置为：<strong>scheduler_weight_classes=nova.scheduler.weights.all_weighers</strong>，即默认使用全部Nova自带的Weigher，也可以自定义scheduler_weight_classes参数值。Nova自带的常用Weigher包括：RAMWeigher、DiskWeigher、MetricsWeigher、IoOpsWeigher、ServerGroupSoftAffinity-Weigher和ServerGroupSoftAntiAffinityWeigher，其具体含义如下：</p><ul><li><strong>RAMWeigher：</strong>表示根据计算节点的可用RAM进行权重计算，剩余可用RAM最大的节点具有最优权重值。如果权重因子是负数，则剩余可用RAM最小的节点具有最优权重值。</li><li><strong>DiskWeigher：</strong>表示根据计算节点的可用磁盘空间进行权重计算，剩余可用磁盘空间最大的节点具有最优权重值。如果权重因子是负数，则剩余可用磁盘空间最小的节点具有最优权重值。</li><li><strong>MetricsWeigher：</strong>表示根据计算节点的各自定义指标进行权重计算，要成为计算权重的自定义指标参数需要在配置文件中指定，设置语法如下：</li></ul><p><strong>metrics_weight_setting = var_name1=1.0, var_name2=-1.0</strong></p><ul><li><strong>IoOpsWeigher：</strong>表示根据计算节点的工作负载来计算权重值，默认选择负载较轻的计算节点作为最优节点，如果权重因子是正数，则会认为负载较大的节点最优，即此时效果刚好也默认情况相反。</li><li><strong>ServerGroupSoftAffinityWeigher：</strong>表示根据主机上运行在服务器组（Server Group）中的实例数目来计算节点权重值，实例将会创建到权重值最大的主机上。在使用该Weigher时，只有对应的权重因子设为正数才有效，因为负权重因子意味着不要将实例创建在同一台主机上，正好与目标结果相反。</li><li><strong>ServerGroupSoftAntiAffinityWeigher：</strong>表示根据主机上运行在服务器组（Server Group）中的实例数目来计算节点权重值，其权重值为负数，实例将会创建到权重值最大的主机上（意味着服务器组中的实例数目最少）。在使用该Weigher时，只有对应的权重因子设为负数才有效，因为负权重因子意味着将实例尽量创建在同一台主机上，正好与目标结果相反。</li></ul><h2 id="Nova创建虚拟机实例流程分析"><a href="#Nova创建虚拟机实例流程分析" class="headerlink" title="Nova创建虚拟机实例流程分析"></a><strong>Nova创建虚拟机实例流程分析</strong></h2><p>在OpenStack中，创建实例主要有两种方式：Nova客户端命令行和Dashboard用户接口GUI界面。无论是哪种方式，其实例创建过程的后端调用流程本质上是相同的，都要经过Nova内部组件之间和Nova与其他服务项目之间的频繁交互才能完成实例的创建工作。正常情况下，要完成一个实例的创建，需要参与的OpenStack服务项目除了Nova之外，还有Dashboard、Keystone、Glance、Cinder和Neutron等项目，其中，Nova、Keystone、Glance和Neutron为必须项目，而Dashboard和Cinder为非必须。此外，在Nova内部，参与实例创建的组件包括nova-api、nova-conductor、nova-scheduler、nova-compute、nova-consoleauth、nova-novncproxy、nova-cert和nova-objectstore等，其中，nova-api、nova-conductor、nova-scheduler、nova-compute为必须使用到的组件。如果要使用虚拟网络控制台VNC与实例交互，则nova-consoleauth和nova-novncproxy是必须的；如果使用AWS风格API，则nova-cert和nova-objectstore是必须的。</p><p>在OpenStack中创建实例的大致流程为：<strong>用户通过Dashboard界面或命令行发起实例创建请求，Keystone从请求中获取用户相关信息并进行身份验证；验证通过后，用户获得认证Token，实例创建请求进入nova-api；在向Keystone验证用户Token有效后，nova-api将请求转入nova-scheduler；nova-scheduler进行实例创建目的主机的调度选择，目标主机选取完成后，请求转入nova-compute；nova-compute与nova-conductor交互以获取创建实例的信息，在成功获取实例信息后，nova-compute分别与Glance、Neutron和Cinder交互以获取镜像资源、网络资源和云存储资源；一切资源准备就绪后，nova-compute便通过Libvirt API与具体的Hypervisor交互并创建虚拟机。</strong>在创建虚拟机实例的过程中，各组件的交互流程如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/q5e1qob1Qy00.png?imageslim" alt="mark"></p><h3 id="Keystone的工作流程"><a href="#Keystone的工作流程" class="headerlink" title="Keystone的工作流程"></a>Keystone的工作流程</h3><p>当Nova客户端发起实例创建请求时，Nova客户端便与Keystone进行交互以获取授权Token，如果Nova客户端通过Keystone的身份验证，则Keystone为其生成授权Token，并将此Token存储到后端数据库中。Keystone的后端数据库可以是关系型的SQL数据库，也可以是如Memcache这样的缓存系统。Keystone的身份验证过程可以在本地进行，也可以通过LDAP服务器进行身份验证。Nova客户端与Keystone交互进行身份验证和Token颁发（使用LDAP进行身份验证）流程如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/BVFyxsmAcd9D.png?imageslim" alt="mark"></p><h3 id="nova-api的工作流程"><a href="#nova-api的工作流程" class="headerlink" title="nova-api的工作流程"></a><strong>nova-api的工作流程</strong></h3><p>Nova客户端在通过Keystone的验证并获取Token后，便向nova-api发出实例创建的请求，nova-api接收到请求后向Keystone验证Token的有效性，验证成功后，nova-api开始判断实例创建请求所携带的参数是否有效合法，如检查虚拟机名字是否符合命名规范，使用的虚拟机模板flavor_id在数据库中是否存在，使用的镜像image_uuid是否是有效的uuid格式等。检查instance、vCPU、RAM的数量是否超出了配置文件中的限制，通常每个Project拥有的资源都是有限的，如创建虚拟机的个数、vCPU个数、内存大小和volume个数等，这些限制是管理员通过与Project相关的Quota来设置的，如quota_instances、quota_cores、quota_ram、quota_volumes等。如果管理员未做设置更改，则默认情况下所有Project拥有相等的资源数量。此外，nova-api还会检查相关metadata的长度是否超过限制，inject_files的数目是否超过限制，一般情况下这些参数都为空，因此都能通过检查。实例请求中的网络、镜像和flavor也是nova-api主要的检查对象，其主要检查请求中的network是否存在且可用以及image和flavor是否存在且可用，同时还会检测请求中flavor的磁盘是否满足image需求等。当所有资源检查都通过后，<strong>nova-api便在nova数据库中初始化虚拟机相关的记录（intial entry），主要包括instance、block_device_mapping和quota等记录，并将instance记录中的vm_states字段设为building，task_state字段设为scheduling。之后，nova-api调用nova-conductor并将请求传递到消息队列（MQ）中。由于nova-api将请求以RPC cast的方式发送给nova-conductor，而cast（）方法发送的请求并不会返回消息，因此Nova客户端此时只会接收到请求已被接受的返回，但是具体的虚拟创建过程还在后台继续执行</strong>，而Nova客户端可以查到的虚拟机vmstate为building，task_state为scheduling。nova-api在实例创建过程的工作流程如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/WdylG8lamgFL.png?imageslim" alt="mark"></p><h3 id="nova-conductor工作流程"><a href="#nova-conductor工作流程" class="headerlink" title="nova-conductor工作流程"></a><strong>nova-conductor工作流程</strong></h3><p>由于nova-conductor提供了build_instances（）这个RPC方法，因此一直处于消息队列监听状态，一旦监听的队列有消息进入，nova-conductor便开始执行build_instances（）方法。nova-conductor还向nova-schduler发出RPC call调用，并要求其返回计算节点调度结果，在收到nova-scheduler的调度结果后，nova-conductor的build_instances（）方法将请求传递到nova-compute的消息队列中。nova-conductor在实例创建过程中的流程如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GKlHPOJazK2B.png?imageslim" alt="mark"></p><h3 id="nova-scheduler工作流程"><a href="#nova-scheduler工作流程" class="headerlink" title="nova-scheduler工作流程"></a><strong>nova-scheduler工作流程</strong></h3><p>nova-scheduler的功能就是负责监听消息队列，并在获取消息队列的请求消息之后进行计算节点的调度选取，同时将调度结果传递给消息队列。为了获取创建实例的目标主机，<strong>nova-conductor会向nova-scheduler发起RPC call调用</strong>，并调用nova-scheduler的select_destin-ations（）方法；nova-scheduler在消息队列中接收到调用消息后，根据nova.conf配置文件中关于Filters和Weight的配置参数对计算节点主机列表进行过滤和加权调度，在这个过程中，nova-scheduler需要访问数据库以获取节点相关的信息；在获取信息后，nova-scheduler便开始进行节点调度，默认使用的调度驱动为FilterScheduler，调度完成之后，将节点调度结果传递到消息队列，以完成nova-conductor的RPC call调用过程。实际上，nova-scheduler从topic：scheduler消息队列获取调用信息后，开始执行select_destinations（）方法，由于Nova通常采用默认的FilterScheduler调度器，因此FilterScheduler的select_destinations（）和_scheduler（）方法被执行，_scheduler（）函数最终会调用获取主机信息的函数和各种节点过滤器以及加权方法以对计算节点进行过滤和计算权重值，并将最终调度选取的计算节点信息传递到消息队列，返回给nova-conductor。nova-scheduler在实例创建过程中的流程如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/fOz6YailId45.png?imageslim" alt="mark"></p><h3 id="nova-compute工作流程"><a href="#nova-compute工作流程" class="headerlink" title="nova-compute工作流程"></a><strong>nova-compute工作流程</strong></h3><p>nova-compute是Nova服务项目中最复杂和最关键的组件，运行nova-compute的节点称为计算节点，通常nova-compute部署在独立的计算节点上，并与Nova项目的其他组件分开部署。在实例创建过程中，<strong>nova-compute随时监听topic：compute-computeN（computeN为计算节点名称）消息队列</strong>，在监听到nova-conductor传递的实例创建请求后，nova-compute开始启动内部工作流程以响应实例创建请求。在nova-api的工作流程中，请求中创建实例所需的镜像、网络和存储等资源已经做过有效性和可用性的检查，因此nova-compute将直接与Glance交互以获取镜像资源，与Cinder交互获取块存储资源。如果配置了Ceph块存储或对象存储服务，nova-compute还会与Ceph RADOS进行交互以访问Ceph存储集群。</p><p>在获取镜像和存储资源后，nova-compute还需与OpenStack的网络服务项目Neutron进行交互访问以获取网络资源。由于网络资源的有效性和可用性已经在nova-api工作流程中完成，这里主要是获取虚拟机的Fixed IP等网络资源。在准备好常见实例所需的一切资源后，nova-compute将通过Libvirt与对应的Hypervisor API进行交互，并通过Libvirt API接口进行虚拟机的创建工作。以上就是nova-compute的大致工作流程，实际上nova-compute接收到消息队列中由nova-conductor发起的RPC cast调用请求，该请求调用manager.py中的ComputeManager.build_and_run_instance（）方法，该方法继而调用ComputeManager类中的<strong>_build_and_run_instance（）方法以尝试创建实例；这里创建实例不一定成功，如果创建失败并且是因为Scheduler调度结果的原因，则会发起Re-scheduler操作以重新调度计算节点，并再次尝试创建实例。_</strong></p><p>_进入_build_and_run_instance（）方法开始创建实例后，nova-compute便开始准备网络和存储等资源（这两个资源分别由_build_networks_for_instance（）和_prep_block_device（）方法来完成）。其中，在进行网络资源准备时，实例的vm_state被设为building，而task_state被设置为networking；在准备块存储资源时，实例的vm_state被设为building，而task_state被设置为blockdevicemapping。在实例所需资源均准备完成后，nova/virt/libvirt/driver.py的LibvirtDriver.spawn（）方法被调用，此时实例vm_state被设为building，而task_state被设置为spawning。</p><p>在spawn过程中，首先进行镜像加载，如果镜像较大，则此过程可能会花费较长时间，然后创建虚拟机的资源定义文件并通过Libvirt发起创建虚拟机的操作。虚拟机创建完成之后，spawn（）将调用_wait_for_boot（）方法以等待虚拟机的power_state状态变为running才返回，此时虚拟机的各个状态应该是，vm_state为active，power_state为running，task_state为none。因此，nova-compute在实例创建过程中的流程主要包括三个子流程，如下所示：</p><ul><li><strong>与Glance、Cinder交互获取镜像和云磁盘等资源（后端采用ceph分布式存储）</strong></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kcq8SRsJnxdd.png?imageslim" alt="mark"></p><ul><li><strong>与Neutron交互获得网络资源</strong></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pKS8YjuJE5yO.png?imageslim" alt="mark"></p><ul><li><strong>与libvirt对应的Hypervisor交互创建虚拟机实例（虚拟机磁盘由分布式存储ceph提供）</strong></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/X0VQF184hxWu.png?imageslim" alt="mark"></p><h2 id="Nova实例的状态机"><a href="#Nova实例的状态机" class="headerlink" title="Nova实例的状态机"></a><strong>Nova实例的状态机</strong></h2><p>在虚拟机实例的创建和运行维护过程中，Nova使用三个变量来描述虚拟机当前的状态，分别为<strong>vm_state、power_state</strong>和<strong>task_state</strong>。其中，power_state代表虚拟机电源状态，其本质上反应的是Hypervisor的状态，其状态值遵循“由下至上（bottom-up）”的变更过程，即先是底层计算节点上Hypervisor状态变更，然后上层数据库对应值变更；vm_state反应的是基于API调用的一种稳定状态，其状态变更比较符合用户的逻辑思维，是一种由上至下（top-down）的API实现过程；task_state代表的是API调用过程中的过渡状态，反映了不同阶段所调用API对实例进行的操作。</p><p><strong>1）power_state</strong></p><p>power_state是Nova程序调用特定虚拟机上的虚拟驱动所获取的状态。数据库中关于特定实例的power_state值仅是虚拟机最近一段时间的快照值，并不能真正反映当前虚拟机的实际power_state，虚拟机当前power_state的实际值位于Hypervisor中。<strong>如果出现可能影响到power_state的任务，则在此任务结束时，位于数据库中的power_state值就会被更新。其更新过程遵循“bottom-top”原则，即计算节点首先报告虚拟机power_state已经更新，然后再刷新数据库中的power_state字段值。</strong>power_state状态值衍生自Libvirt，其中BLOCKED状态值本质上代表的是RUNNING状态，所以不会记录。SHUTOFF被映射为SHUTDOWN状态，FAILED被映射为NOSTATE状态，因此power_state目前有<strong>RUNNING、SHUTDOWN和NOSTATE</strong>三种状态。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/99MPrRa6NBQ1.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Yvh9vxplnkas.png?imageslim" alt="mark"></p><p><strong>2）vm_state</strong></p><p>vm_state状态值是对虚拟机当前稳定状态的描述，而不是过渡状态。也就是说，如果针对特定虚拟机已经没有API继续调用，则应该用vm_state来描述此时虚拟机的稳定状态。例如，当vm_state状态值为ACTIVE，则表示虚拟机正常运行。再如SUSPENDING状态，其表示虚拟机正处于挂起过程中，并且在几秒之后会过渡到SUSPENDED状态，因此这是一种过渡状态，即SUSPENDING应该属于task_state的状态值。vm_state状态值更新的前提是针对此虚拟机有任务发生，并且任务成功完成，同时task_state被置为NOSTATE。如果没有API调用发生，则vm_state状态值绝对不会改变；如果对虚拟机的操作任务失败，但是成功回滚（rollback），则vm_state状态值也不会改变；如果不能回滚成功，则vm_state被置为ERROR状态。</p><p><strong>vm_state状态与power_state状态之间没有必然的对应关系，即不能由vm_state的某一状态值推导出power_state的状态，反之亦然。</strong>当有针对虚拟机的操作任务正在执行时，power_state与vm_state的状态值很可能不协调：出现不协调的主要原因是vm_state仅代表虚拟机的稳定状态，而在任务执行中，虚拟机的状态一直在过渡，而且不能及时同步更新。<strong>如果没有任务正在执行，则vm_state与power_state不应该冲突，除非出现错误或任务失败，此时就需要具体问题具体分析，常见的不一致有以下几种：</strong></p><ul><li><strong>power_state为SHUTDOWN，而vm_state为ACTIVE：</strong>这种情况最可能的原因是虚拟机内部执行shutdonw命令时出现异常，解决这个问题最简单粗暴的方法就是手动调用stop（）API，之后，vm_state应该变为STOPPED。</li><li><strong>power_state为RUNNING，而vm_state为HARD_DELETE：</strong>如果出现这种情况，则表明用户已经发出删除虚拟机的命令，但是执行过程出错，可以尝试再次删除虚拟机。</li><li><strong>power_state为RUNNING，而vm_state为PAUSED：</strong>出现这个情况，表明虚拟机在执行pause（）之前出现了意外情况，这个问题的解决办法就比较多样，可以尝试将其强设为ERROR。</li></ul><p>vm_state有多个状态值，不同状态值表示虚拟机当前处于不同的稳定状态。有些状态值彼此之间可以互相转换，如PAUSED与ACTIVE状态；而有些状态值之间没有任何直接联系，如PAUSED与STOPPED；没有联系的状态值之间只有借助第三个状态值才能转换，如ACTIVE状态值。vm_state状态机示意图如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ee4uL8W29L7O.png?imageslim" alt="mark"></p><p><strong>3）task_state</strong></p><p><strong>task_state代表的是过渡状态，并且与调用的API函数密切相关，其状态值描述了虚拟机当前正在执行的任务。</strong>只有对虚拟机发起了操作任务时，才会有task_state状态值出现，而当vm_state处于稳定值时，task_state通常为NOSTATE。简单来说，当虚拟机在执行任务时，task_state便会有反映操作任务的状态值，而如果没有执行任何任务，则task_state一定是NOSTATE。如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/utft3njJwDvK.png?imageslim" alt="mark"></p><p><strong>task_state的一个特殊任务状态是FORCE_DELETE（或HARD_DELETE）</strong>。正常情况下，任何时候用户都可以成功删除虚拟机，删除虚拟机后便可使用更多受Quota限制的资源，同时删除的虚拟机不会被计费，但是虚拟机删除任务可能会因为某些原因而失败，例如之前的任务堵塞或卡死而导致虚拟机删除失败，删除虚机时虚拟驱动卡死或计算节点因为网络/硬件不可用导致操作失败等。失败的虚拟机删除任务意味着task_state状态值不能成功过渡到NOSTATE，而虚拟机状态值vm_state更新的前提是task_state从具体的任务状态过渡到NOSTATE，因此，对于force_delete（）任务，不应该等到任务执行到计算节点并完成全部相关工作才更新vm_state的状态值为HARD_DELETE，而是应该发起force_delete（）任务之后便立即更新vm_state状态值。也就是说，<strong>force_delete（）是个纯粹的数据库操作任务，其对数据库中vm_state字段值刷新之后便结束，而相应的虚拟机清除工作随后才进行。</strong></p><p>当任务被确认为虚拟机上唯一执行的任务时，task_state就会被设置为具体的状态值。虚拟机中每个正在执行的任务都会被分配一个与虚拟机相关的唯一task_id（UUID格式）。如果虚拟机已经有个task_id，则表明有任务正在执行，<strong>要在任务执行中途更新task_state，就必须确保虚机的task_id匹配当前的task_id，否则当前执行的任务会被抢占（目前只有force_delete任务能抢占）</strong>。通常在某个具体的任务执行期间，task_state的值绝对不能改变。当一个任务执行完成后，task_state被置为NOSTATE，而task_id被置为NONE。task_state的状态值名称通常是代表某个API方法动词的正在进行时（“ing”形式），如下表所示，因此，从task_state的状态值便可推出虚拟机当前正在执行什么任务。</p><table><thead><tr><th><strong>vm_state</strong></th><th><strong>task_state</strong></th><th><strong>status</strong></th></tr></thead><tbody><tr><td>active</td><td>rebooting</td><td>REBOOT</td></tr><tr><td></td><td>reboot_pending</td><td>REBOOT</td></tr><tr><td></td><td>reboot_started</td><td>REBOOT</td></tr><tr><td></td><td>rebooting_hard</td><td>HARD_REBOOT</td></tr><tr><td></td><td>reboot_pending_hard</td><td>HARD_REBOOT</td></tr><tr><td></td><td>reboot_started_hard</td><td>HARD_REBOOT</td></tr><tr><td></td><td>rebuild_block_device_mapping</td><td>REBUILD</td></tr><tr><td></td><td>rebuilding</td><td>REBUILD</td></tr><tr><td></td><td>rebuild_spawning</td><td>REBUILD</td></tr><tr><td></td><td>migrating</td><td>MIGRATING</td></tr><tr><td></td><td>resize_prep</td><td>RESIZE</td></tr><tr><td></td><td>resize_migrating</td><td>RESIZE</td></tr><tr><td></td><td>resize_migrated</td><td>RESIZE</td></tr><tr><td></td><td>resize_finish</td><td>RESIZE</td></tr><tr><td></td><td>default</td><td>ACTIVE</td></tr><tr><td><strong>vm_state</strong></td><td><strong>task_state</strong></td><td><strong>status</strong></td></tr><tr><td>stopped</td><td>resize_prep</td><td>RESIZE</td></tr><tr><td></td><td>resize_migrating</td><td>RESIZE</td></tr><tr><td></td><td>resize_migrated</td><td>RESIZE</td></tr><tr><td></td><td>resize_finish</td><td>RESIZE</td></tr><tr><td></td><td>default</td><td>SHUTOFF</td></tr></tbody></table><p>对任一实例，不管是在实例创建过程中，还是创建完成后的维护操作中，vm_state、power_state和task_state总是同时存在，并共同决定了虚拟机的当前运行状态。下图为虚拟机创建过程中vm_state、power_state和task_state的状态值变更过程，可以看到，实例创建过程中，vm_state状态由Building变为Active，power_state则由NoState变为Running，经历状态值变更最多的是task_state，其经历了Scheduling、None、Networking、Block_Device_Mapping和Spawning等状态值的变更。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/plErMynznnr9.png?imageslim" alt="mark"></p><p>虚拟机之间的状态可以相互转换，上图就描述了这种状态转换关系。让虚拟机由一个或多个状态转换到另外一种或几种状态，需要对虚拟机发出相关的任务操作，或者说要让虚拟机最终处于某种状态，则需要对当前状态的虚拟机执行相应的操作命令。<strong>虚拟机当前状态与目标状态之间的对应关系可以是一对一或多对一</strong>，如当前为Paused状态的虚拟机通过unpause命令可以变为Active状态，当前是Active、Shutoff和Rescued状态的虚拟机都可以通过pause命令变为Paused状态，而对于某些如backup或snapshot等备份命令则不会改变虚拟机状态。</p><h2 id="Nova虚拟机实例的迁移"><a href="#Nova虚拟机实例的迁移" class="headerlink" title="Nova虚拟机实例的迁移"></a><strong>Nova虚拟机实例的迁移</strong></h2><h3 id="冷迁移—resize-migrate"><a href="#冷迁移—resize-migrate" class="headerlink" title="冷迁移—resize/migrate"></a>冷迁移—resize/migrate</h3><p>Nova为虚拟机提供了资源升级（resize）和主机迁移（migrate）操作。从底层调用的API接口来看，resize与migrate本质上是相同的，不同之处在于resize需要提供新的资源配置flavor，并使用新的flavor参数在目标主机上重新启动实例，而migrate则无须flavor参数，或者可以认为当resize操作提供的flavor与原实例的flavor一致时，则resize操作就是migrate操作。由于resize/migrate操作在迁移过程中会关闭源主机上的实例，并在新的主机上重新启动实例，因此resize/migrate对实例的迁移并非实时在线，而是先关闭实例再以copy镜像的形式迁移，迁移完成之后再在新的主机上启动实例，resize/migrate迁移也称为“冷迁移”。在虚拟机实例冷迁移结束之前，无法对虚拟机进行其他操作。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7yMVJkj95nXg.png?imageslim" alt="mark"></p><p>resize/migrate迁移的优点在于其无须共享存储支持，而且允许用户重新自定义虚拟机资源和宿主机。例如，由于Nova调度或者物理设备的原因导致某台宿主机负载过重，而且其上的某些虚拟机比较关键，同时需要更多的物理资源，此时便可申请维护窗口，通过Nova的resize/migrate功能将此宿主机上的某些关键虚拟机迁移到硬件资源更强大的物理宿主机上。resize/migrate操作无须用户指定目标主机，而是由Nova-scheduler调度。resize操作允许源主机与目标主机相同，前提是当前主机的资源满足虚拟机resize后的资源需求，同时<strong>用户需要设置nova.conf配置文件中的allow_resize_to_same_host参数为true（默认为false）</strong>，并重新启动Nova相关服务。而migrate操作则不允许scheduler将模板节点选取为源主机，如果scheduler调度的目标节点与源节点相同，则会引起re-scheduler操作（再次调度时源主机会因RetryFilter而被排除）。</p><p>对于resize/migrate操作，在迁移过程接近尾声，准备在目标主机启动实例和虚拟机状态为resized时，还有两个相关操作需要用户执行，即confirm_resize和revert_resize。confirm_resize操作表示用户接受此次实例迁移操作，revert_resize表示用户不接受（反悔）此次迁移操作。如果用户需要Nova自动确认迁移操作，则可以将resize_confirm_window参数设置为某个大于0的时间值（一般设置为1即可），当迁移完成并且虚拟机处于reszied的时间大于此参数值时，迁移操作将会被自动确认。<strong>要对虚拟机进行resize操作，需要修改计算节点上/etc/passwd中nova条目并为用户nova设置密码，配置可登录系统，同时需要在计算节点之间设置SSH免密互信机制。如果不需要进行虚拟机资源调整，只需进行纯粹的“冷迁移”migrate操作即可。migrate与resize类似，但是migrate操作无须指定flavor参数，仅仅是将实例迁移至其他主机，而不进行资源调整，也无需进行上述配置。</strong></p><h3 id="热迁移—live-migration"><a href="#热迁移—live-migration" class="headerlink" title="热迁移—live-migration"></a><strong>热迁移—live-migration</strong></h3><p>为了进行服务非中断的实时维护，Nova提供了live-migration功能。<strong>相对于resize/migrate的“冷迁移”，live-migration称为“热迁移”，即实时在线迁移</strong>，其可以将实例由一个计算节点在线迁移到另一个计算节点，期间仅有非常短暂的访问延时。live-migration迁移按其实现方式可以划分为三种类型，即<strong>基于非共享存储的块迁移（block live migration）、基于共享存储的迁移（Shared storage-based live migration）和基于Volume后端的迁移（Volume-backed live migration）。其中，后两种是使用最多的live-migration方式，而块迁移在使用上一直存在很多问题，而且也不符合实时迁移的基本设计思想，故不推荐使用。</strong>块迁移在迁移过程中需要拷贝临时存储及镜像文件，对于节点之间的网络带宽要求极高，尤其是在临时存储很大的情况下，还有可能出现消耗很长时间却不能成功的情况，如当实例I/O负载很大时，如果应用程序对临时存储的写入速率快于块迁移对临时存储的拷贝速率，则块迁移将不能完成。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/xVKjy5lMLBbA.png?imageslim" alt="mark"></p><p>使用live-migration功能时，不管是基于共享存储还是Volume的迁移，都需要对nova.conf和libvirt.conf做相应修改，然后重启nova-compute和libvirtd服务，即需要完成一些迁移前的准备工作。准备工作完成之后，便可开始live-migration迁移—基于Volume后端的实例迁移其实就是对SAN BOOT形式的实例进行迁移，由于实例系统位于Volume而非临时磁盘上，因此无须共享存储，其迁移原理就是将Volume从源主机卸载，并重新挂载到目标主机的过程。与基于Volume后端的live-migration不同，其原理是需要计算节点之间共享实例镜像文件目录，通常是/var/lib/nova/instances，即每个计算节点都可以对共享的/var/lib/nova/instances目录进行读写，然后在共享目录创建实例镜像，这样每个计算节点都能读取该镜像。</p><p>上述的Nova实例的迁移配置和实战详情请参考本站的《OpenStack虚拟机实例迁移实战》一文。</p><h2 id="Nova计算管理服务实战"><a href="#Nova计算管理服务实战" class="headerlink" title="Nova计算管理服务实战"></a><strong>Nova计算管理服务实战</strong></h2><h3 id="Hypervisor、主机聚合和可用分区管理"><a href="#Hypervisor、主机聚合和可用分区管理" class="headerlink" title="Hypervisor、主机聚合和可用分区管理"></a>Hypervisor、主机聚合和可用分区管理</h3><p><strong>步骤1：</strong>执行以下命令，查看OpenStack Hypervisor的列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack hypervisor list --long</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yOLBU3znjpxn.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，查看OpenStack Hypervisor的列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack host list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/J97HRphYELqH.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，创建主机聚合“Aggregate_kk”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack aggregate create --zone nova Aggregate_kk</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/riIYsu6E8TpM.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，为主机聚合“Aggregate_kk”添加主机“rocky-compute”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack aggregate add host Aggregate_kk rocky-compute</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/KQis72N3N6Ry.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>执行以下命令，验证同一个主机是否可以加入不同的AZ</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dDn4EArgPnHW.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kX4E4l3fucCt.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>执行以下命令，验证同一个主机是否可以加入不同的AZ</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack aggregate <span class="built_in">set</span> --zone nova Aggregate_Test</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5xEETw84c3mQ.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5AXjpY1tviJR.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，从主机聚合“Aggregate_Test”移除主机“rocky-compute”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack aggregate remove host Aggregate_Test rocky-compute</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/oOEpfiQtBIiy.png?imageslim" alt="mark"></p><p><strong>步骤8：</strong>执行以下命令，删除主机聚合“Aggregate_Test”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack aggregate delete Aggregate_Test</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/fJOlrLeXcv0g.png?imageslim" alt="mark"></p><h3 id="虚拟机规格管理实战"><a href="#虚拟机规格管理实战" class="headerlink" title="虚拟机规格管理实战"></a><strong>虚拟机规格管理实战</strong></h3><p><strong>步骤1：</strong>执行以下命令，查看现有的虚拟机规格列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/YgndsCag3qxY.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，创建规则Flavor_Test ，vCPU=1，RAM=128M，ROOT DISK=1G，仅对项目kkproject可见</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor create --vcpu 1 --ram 128 --disk 1 --private --project kkproject Flavor_Test</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HSnoXAoUSYPe.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，移除规格Flavor_Test仅对项目kkproject可见</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor <span class="built_in">unset</span> --project kkproject Flavor_Test</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/L97I2wL7jyPy.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，删除规格Flavor_Test</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor delete Flavor_Test</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/se6ex0GCdW8G.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>执行以下命令，创建规格Flavor_ubuntu，vCPU=2，RAM=2048M，ROOT DISK=20G，其他默认</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor create --vcpu 2 --ram 2048 --disk 20 Flavor_ubuntu</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/LOwDdBaCpjPd.png?imageslim" alt="mark"></p><h3 id="秘钥对和虚拟机组的管理实战"><a href="#秘钥对和虚拟机组的管理实战" class="headerlink" title="秘钥对和虚拟机组的管理实战"></a><strong>秘钥对和虚拟机组的管理实战</strong></h3><p><strong>步骤1：</strong>执行以下命令，创建密钥对“KeyPair_kk01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack keypair create KeyPair_kk01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GRvOdaLnk8vN.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，创建虚拟机组“ServerGroup_kk01”，策略设置为“affinity”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server group create --policy affinity ServerGroup_kk01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/lF7lXJnP29Yl.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，创建虚拟机组“ServerGroup_kk02”，策略设置为“anti-affinity”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server group create --policy anti-affinity ServerGroup_kk02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yy06Y22k9StB.png?imageslim" alt="mark"></p><h3 id="虚拟机实例实战"><a href="#虚拟机实例实战" class="headerlink" title="虚拟机实例实战"></a><strong>虚拟机实例实战</strong></h3><p><strong>步骤1：</strong>执行以下命令，创建虚拟机实例“instance_01”，要求配置为：AZ=nova，image=kk_img，flavor=Flavor_Web，KeyPair=KeyPair_kk01，Server Group=ServerGroup_kk01</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server create --availability-zone nova --image kk_img --flavor Flavor_Web --key-name KeyPair_kk01 --hint group=e320072b-1a76-4b87-854b-53c40db3e265 instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/N25QqkOLh9z2.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，查看虚拟机实例“instance_01”的状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server show instance_01 | grep status</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/KyPvbhyYi9vm.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，软重启虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server reboot instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/jL4MLIt8nhJt.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，硬重启虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server reboot --hard instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/NUMSH53bmc9n.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>执行以下命令，关闭虚拟机实例“instance_01”</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/RPasYqaKJK8d.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>执行以下命令，启动虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server start instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/HUzlta1t86DJ.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，锁定虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server lock instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pF0nkwPK9CYd.png?imageslim" alt="mark"></p><p><strong>步骤8：</strong>执行以下命令，解锁虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server unlock instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Qdq86DkTgsYs.png?imageslim" alt="mark"></p><p><strong>步骤9：</strong>执行以下命令，暂停虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server pause instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7LvQr0Ranwrw.png?imageslim" alt="mark"></p><p><strong>步骤10：</strong>执行以下命令，恢复暂停的虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server unpause instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/PuADnYJi11nU.png?imageslim" alt="mark"></p><p><strong>步骤11：</strong>执行以下命令，挂起虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server <span class="built_in">suspend</span> instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Vf2D8Eo68NOK.png?imageslim" alt="mark"></p><p><strong>步骤12：</strong>执行以下命令，恢复挂起的虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server resume instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5zKXSc7jKdPP.png?imageslim" alt="mark"></p><p><strong>步骤13：</strong>执行以下命令，搁置虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server shelve instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/1ipDMkPNsoek.png?imageslim" alt="mark"></p><p><strong>步骤14</strong>：执行以下命令，恢复搁置的虚拟机实例“instance_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server unshelve instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Leh7FmOfzKQ2.png?imageslim" alt="mark"></p><p><strong>步骤15：</strong>执行以下命令，创建虚拟机实例“instance_01”的快照“instance01_snap_01”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server image create --name instance01_snap_01 instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/xdlaGADR8WJo.png?imageslim" alt="mark"></p><p><strong>步骤16：</strong>执行以下命令，查看镜像列表</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/a9d4dJ8q82tu.png?imageslim" alt="mark"></p><p><strong>步骤17：</strong>执行以下命令，创建一个新的规格“Flavor_Web_new”，将内存大小调整为256M，其余与规格“Flavor_Web”保持一致</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor create --vcpu 1 --ram 256 --disk 1 Flavor_Web_New</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/QdKohImGAXUd.png?imageslim" alt="mark"></p><p><strong>步骤18：</strong>执行以下命令，将虚拟机实例“instance_01”的规格调整为新的规格“Flavor_Web_New”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server resize --flavor Flavor_Web_New instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/9NLIf91aC7NV.png?imageslim" alt="mark"></p><p><strong>步骤19：</strong>执行以下命令，确认虚拟机实例“instance_01”的Resize调整</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server resize --confirm instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dEsn0lBBxYxw.png?imageslim" alt="mark"></p><p><strong>步骤20：</strong>执行以下命令，确认虚拟机实例“instance_01”的规格信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server show instance_01 | grep flavor</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/srKJMDh0cVll.png?imageslim" alt="mark"></p><p><strong>步骤21：</strong>执行以下命令，从虚拟机快照恢复实例instance_01，并查看虚拟机实例的镜像信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack server rebuild --image instance01_snap_01 instance_01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/J23NNI8L0sSS.png?imageslim" alt="mark"></p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a><strong>思考</strong></h2><p><strong>1、请从nova的重要概念角度阐述虚拟机故障迁移的最大范围是多少？</strong></p><p><strong>2、多个AZ可以通过一个nova-scheduler进行调度，那么在虚拟机故障时或部署虚拟机的计算节点主机故障时，虚拟机实例是否可以跨AZ迁移？请阐述理由？</strong></p><p><strong>3、请阐述为什么一个主机Host只能属于一个AZ？</strong></p><p><strong>4、请阐述nova cell的V1与V2版本的区别？并分析在V2版本中，nova_cell0S数据库的作用？在nova-api数据库中请分析与cell相关的数据表之间的挂链关系？</strong></p><p><strong>5、请分析虚拟机实例在软重启和硬重启时内部流程的区别？并解释为什么虚拟机软重启和硬重启中状态status一直为active？</strong></p><p><strong>6、请分析虚拟机实例暂停状态下pause和suspend的区别？关机状态下Poweroff和shelve的区别？</strong></p><p><strong>7、请根据虚拟机实战的2-14步画出虚拟机实例的状态变更图，并标注出引起状态变化的相关操作指令？</strong></p><p><strong>8、请分析虚拟化场景下和云场景下虚拟机快照的区别？</strong></p><p><strong>9、请分析虚拟机实例resize过程中虚拟机的在线状态？并分析为什么在VERIFY_RESIZE状态下需要通过cofirm操作进行确认？确认后虚拟机状态的的变化是什么？同时，除了confirm操作外，还有回滚操作revert，那么revert后虚拟机实例状态变为ACTIVE前是否需要再次confirm？</strong></p><p><strong>10、请分析为什么在你们的环境中虚拟机实例resize不成功？请从日志关联的角度分析产生错误请求400 Bad Request的原因？</strong></p><p><strong>11、请从Ubuntu官网下载一个cloud镜像，并通过命令行拉起一个ubuntu虚拟机实例，并要求能够通过SSH的用户名密码方式登录。</strong></p><p><strong>12、请手动制作一个win7/win10的cloud镜像，并通过命令行拉起一个win7/win10的虚拟机实例，并要求该实例能够正常运行。</strong></p><p><strong>13、请阐述冷迁移resize和migrate的区别？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Compute Service Nova是OpenStack最核心的服务，负责维护和管理云环境的计算资源。OpenStack作为IaaS的云操作系统，虚拟机生命周期管理就是通过Nova来实现的。因此，Nova是OpenStack云中的计算组织控制器，首次出现在OpenStack的Austin版本，也就是伴随着OpenStack的诞生就存在。提供大规模、可扩展、按需自助的计算资源服务，现在的版本同时支持管理裸机、虚拟机和容器。而在OpenStack早期的几个版本中计算、存储和网络均由Nova来提供，也就说不仅存在nova-compute组件，同时也有nova-volume和nova-network组件，后续随着项目拆分，nova-volume演变为现在Cinder服务，nova-network演变为现在Nuetron服务，而Nova自身则专注于计算服务，主要依赖Keystone提供认证服务，Glance提供镜像服务，Cinder提供块存储服务和Nuetron提供网络服务，其在OpenStack整个系统中位置如下，处于系统整体的核心地位。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-镜像管理服务Glance</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Glance/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-镜像管理服务Glance/</id>
    <published>2020-03-03T10:16:18.000Z</published>
    <updated>2020-03-03T10:31:46.977Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Glance在OpenStack中主要为实例提供公共镜像服务能力以及镜像/虚拟机快照管理功能，属于OpenStack的核心组件之一，与Swift、Cinder并列OpenStack存储的三驾马车。在实际生产中，Glance本身并不负责大量数据的存储，对镜像的存储主要依赖于后端Swift，一般都是采用对象存储方式。如果后端对象存储采用Swift，则镜像存在Swift中；如果后端存储采用分布式存储ceph，则主要存储在ceph的对象存储rgw模块中。而在我们的实验环境中（考虑大家PC的配置问题），没有部署swift对象存储，因此镜像采用部署Glance服务的节点本地磁盘进行存储。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pNHLTzUVWUlw.png?imageslim" alt="mark"></p><p>Glance首次出现在OpenStack的Bexar版本中，提供发现、注册和检索虚拟机镜像的功能，只依赖于OpenStack的Keystone服务，属于OpenStack八大核心组件之一，为OpenStack的核心项目之一。</p><h2 id="Image的概念"><a href="#Image的概念" class="headerlink" title="Image的概念"></a><strong>Image的概念</strong></h2><p>OpenStack 由 Glance 提供 Image 服务。要理解 Image Service 先得搞清楚什么是 Image 以及为什么要用 Image？在传统 IT 环境下，安装一个系统是要么从安装 CD 从头安装，要么用 Ghost 等克隆工具恢复。这两种方式有如下几个问题：</p><ul><li>如果要安装的系统多了效率就很低、时间长，工作量大。</li><li>安装完还要进行手工配置，比如安装其他的软件，设置 IP 等</li><li>备份和恢复系统不灵活。</li></ul><p>云环境下需要更高效的解决方案，这就是 Image。Image 是一个模板，里面包含了基本的操作系统和其他的软件。举例来说，有家公司需要为每位员工配置一套办公用的系统，一般需要一个 Win7 系统再加 MS office 软件。OpenStack 是这么玩的：先手工安装好这么一个虚机、然后对虚机执行 snapshot，这样就得到了一个 image。当有新员工入职需要办公环境时，立马启动一个或多个该 image 的 instance（虚机）就可以了。在这个过程中，第 1 步跟传统方式类似，需要手工操作和一定时间。但第 2、3 步非常快，全自动化，一般都是秒级别。而且 2、3 步可以循环做。比如公司新上了一套 OA 系统，每个员工的 PC 上都得有客户端软件。那么可以在某个员工的虚机中手工安装好 OA 客户端，然后执行 snapshot ，得到新的 image，以后就直接使用新 image 创建虚机就可以了。另外，snapshot 还有备份的作用，能够非常方便的恢复系统。各位在安装OpenStack时可以通过VMWare的快照（snapshot）来进行备份，每安装完一个服务就备份一次，这样即使后面服务安装出现失误等，且无法调整，就可以通过恢复快照的方式来解决，但是这样也存在一个占用本地磁盘较大的问题。就如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/A7pseeuR9zuz.jpeg?imageslim" alt="mark"></p><p><strong>镜像、实例和规格的关系：</strong></p><ul><li>用户可以从同一个镜像启动任意数量的实例。</li><li>每个启动的实例都是基于镜像的一个副本，实例上的任何修改都不会影响到镜像。</li><li>启动实例时，必须指定一个规格，实例按照规格使用资源。</li></ul><p>在创建Glance服务管理的Image时，需要指定Image的容器格式container format和磁盘格式disk format，其含义具体如下：</p><ul><li><strong>disk format：</strong>是虚拟机镜像文件的磁盘格式，不同的虚拟化厂商有不同的格式，用来标明虚拟机磁盘镜像包含的信息，可以设置的镜像磁盘格式有：raw，vhd ，vmdk，vdi， iso ，qcow2 ，aki ，ami ，ari等。</li></ul><table><thead><tr><th><strong>镜像磁盘格式</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>raw</td><td>一种非结构化的镜像磁盘格式</td></tr><tr><td>vhd</td><td>VMware，Xen，Microsoft，VirtualBox等使用的常见磁盘格式</td></tr><tr><td>vhdx</td><td>vhd格式的增强版本，支持更大的磁盘容量和其他功能</td></tr><tr><td>vmdk</td><td>常见的磁盘镜像格式</td></tr><tr><td>vdi</td><td>Virtual Box和QEMU支持的磁盘格式</td></tr><tr><td>iso</td><td>光盘（例如CDROM）的存档格式</td></tr><tr><td>qcow2</td><td>QEMU支持的磁盘格式，支持动态扩展和写时复制</td></tr><tr><td>aki</td><td>Amazon Kernel Image</td></tr><tr><td>ari</td><td>Amazon Ramdisk Image</td></tr><tr><td>ami</td><td>Amazon Machine Image</td></tr></tbody></table><ul><li><strong>container format：</strong>是指虚拟机镜像是以一种什么样的文件格式存放，同时包含实际虚拟机有关的metadata格式。container format有以下几种：bare，ovf ，aki， ari ，ami等。</li></ul><table><thead><tr><th><strong>镜像容器格式</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td>ovf</td><td>开放式虚拟机磁盘格式， 由Vmware发起， 目前已被多种虚拟化设备支持。</td></tr><tr><td>bare</td><td>这表示镜像没有container或者元数据。</td></tr><tr><td>aki</td><td>Amazon Kernel Image</td></tr><tr><td>ari</td><td>Amazon Ramdisk Image</td></tr><tr><td>ami</td><td>Amazon Machine Image</td></tr></tbody></table><p>镜像不仅有存储的磁盘格式和容器格式，同时还有访问权限，主要有四种：</p><ul><li><strong>public：</strong>公共的，可以被所有的tenant使用；</li><li><strong>private：</strong>私有的，只能被owner的tenant使用；</li><li><strong>shared：</strong>共享的，一种非公有的Image，可以被其owner共享给其他tenant使用；</li><li><strong>protected：</strong>受保护的，不能被非管理员以外的用户删除；</li></ul><h2 id="Glance的架构"><a href="#Glance的架构" class="headerlink" title="Glance的架构"></a><strong>Glance的架构</strong></h2><p>Glance 使用C/S架构，提供REST API，用户可以通过API执行对服务的请求，其架构在OpenStack版本的演进中发生了一定的变化，主要有V1和V2两个版本。在老版本（V1）的Glance架构中，主要通过glance-api、glance-registry、glance-DB、image-store四个组件构成，glance-api和glance-registry同时提供WSGI接口，只是glance-api对外，glance-registry只能被内部调用。如下图所示，各个组件的功能如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/l9F9eGQJqQXN.png?imageslim" alt="mark"></p><ul><li><strong>glance-api：</strong>接收REST ful的API请求，通过其他组件（glance-registry和image-store）来完成镜像的CRUD操作。</li><li><strong>glance-registry：</strong>负责镜像的元数据管理，与glance-DB交互，完成存储或获取镜像的元数据。</li><li><strong>glance-DB：</strong>通过关系型数据库组件提供镜像管理、控制数据及元数据的存储功能，在开源解决方案中采用MySQL数据库实现glance-DB，在华为的解决方案中采用GuassDB实现glance-DB。</li><li><strong>image-store：</strong>本质上是一个存储接口层，可以异构对接不同的后端存储，将镜像数据通过该接口层持久化存储在后端存储中。</li></ul><p>而在新版本（V2）中，Glance的架构发生了一定的变化，在源代码中仍然保留glance-registry函数的入口点（这也是为什么我们在配置glance的服务时，仍需配置glance-registry.conf的原因），但是实际实现时将glance-registry的功能整合到了glance-api中，通过在glance-api中提供一个registry共享层来实现镜像元数据的数据库存取。新版本OpenStack的Glance镜像管理服务的架构如下，<strong>镜像的元数据主要包括：镜像资源的访问策略Policy，镜像资源的规格、镜像资源的存储位置等信息。</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dLgyMzO330r2.png?imageslim" alt="mark"></p><ul><li><strong>glance-client：</strong>任何使用glance服务的应用程序，接收API请求，发起操作认证并调用glance-api。</li><li><strong>glance-api：</strong>通过REST ful对外开放API接口提供镜像管理功能，并接收镜像管理的相关API操作请求。</li><li><strong>glance-domain-controller：</strong>管理Glance的内部服务组件，分层实现特定的任务，如认证（Auth）、事件通知（Notifier）、策略控制（Policy）和数据库连接等。</li><li><strong>registry-layer：</strong>实现glance-domain-controller与DAL之间的安全访问控制。</li><li><strong>DAL（DataBase-Abstraction-Layer）：</strong>提供glance-domain-controller、registry-layer与glance-DB之间的统一API接口，实现glance-domain-controller、registry-layer对glance-DB的安全访问。</li><li><strong>glance-DB：</strong>在所有组件之间共享，存放镜像的管理、配置数据及元数据。</li><li><strong>Glance Store Drivers：</strong>就是image store，本质上是一个存储接口层，负责与外部异构存储后端或本地文件系统的交互，持久化存储镜像文件。</li></ul><p>了解了V1和V2版本的glance架构，这里需要重点了解下Glance服务的缓存机制。Glance服务提供缓存机制主要是为了解决节点扩容后带来多个节点同时提供镜像管理服务带来的镜像服务性能问题。其原理就是通过glance-api提供缓存机制，就是在glance-api服务节点存放一份原始镜像的拷贝，本质上是为了实现api服务器的数量扩展，提高为同一个镜像提供服务的效率。<strong>这里需要说明一下，网上针对glance服务提供缓存的位置有两种说法：一是将镜像缓存到计算节点；二是将镜像缓存到glance-api节点。从官网的如下描述我理解是第二种方式-</strong>—This local image cache is transparent to the end user – in other words, the end user doesn’t know that the Glance API is streaming an image file from its local cache or from the actual backend storage system。但是，在创建虚拟机时，nova-compute会将镜像缓存拷贝一份到计算节点，便于后续再次创建同样镜像的虚拟机时可以利用本地存储的镜像完成（通过镜像ID识别是否同一镜像的虚拟机）。上述镜像的缓存对用户是不可见的，用户并不知道使用的是本地缓存的镜像还是后端存储的镜像。镜像的缓存服务开关、缓存大小和缓存周期可通过命令行配置，命令行操作缓存的指令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">glance-cache-pruner --image_cache_max_size= * &lt;===控制cache总量的大小，周期性的运行</span><br><span class="line"></span><br><span class="line">glance-cache-cleaner &lt;===清理状态异常的cache文件</span><br><span class="line"></span><br><span class="line">glance-cache-manage --host=&lt;HOST&gt; queue-image &lt;IMAGE_ID&gt; &lt;===预取某些热门镜像到新增的api节点中</span><br><span class="line"></span><br><span class="line">glance-cache-manage --host=&lt;HOST&gt; delete-cached-image &lt;IMAGE_ID&gt; &lt;===手动删除image cache来释放空间</span><br></pre></td></tr></table></figure><h2 id="Glance的状态机"><a href="#Glance的状态机" class="headerlink" title="Glance的状态机"></a><strong>Glance的状态机</strong></h2><p>镜像Image在虚拟机实例的全生命周期管理中，也是有各种不同的状态的。类似Nova服务管理虚拟机的各种状态，Cinder负责管理Volume的各种状态，Glance负责管理Image的各种状态。下图就是创建镜像时，Image的不同状态之间的转换图。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/g8G6KYPTM27F.png?imageslim" alt="mark"></p><p>在Glance中有两种状态机，一种是镜像状态，另一种是任务状态。镜像状态表示镜像在数据库中的各种状态，任务状态表示操作镜像的各种任务的状态，两种状态结合的上图状态机流程图表示了镜像在不同任务的不同时期的一种状态形式，只有任务执行成功success时的active状态的镜像才能被正常使用。</p><p><strong>任务状态包括pending、processor、success、failure</strong>四种，其具体含义如下：</p><ul><li><strong>pending：</strong>表示任务被挂起</li><li><strong>processor：</strong>表示任务正在执行中</li><li><strong>success：</strong>表示任务执行成功</li><li><strong>failure：</strong>表示任务失败</li></ul><p><strong>镜像状态包括queued、saving、uploadding、importing、active、deactive、killed、deleted、pending_delete</strong>九种，其具体含义如下：</p><ul><li><strong>queued：</strong>已在glance-registry中保留镜像的标识符，但镜像数据未上传，镜像大小未初始化。</li><li><strong>saving：</strong>镜像的原始数据正在上传到glance中。</li><li><strong>uploading：</strong>对镜像调用了import data-put请求。</li><li><strong>importing：</strong>导入镜像过程中，但镜像尚未就绪。</li><li><strong>active：</strong>镜像创建完成，可以使用。</li><li><strong>deactived：</strong>禁止任务非管理员用户访问镜像，即镜像去激活。</li><li><strong>killed：</strong>镜像上传时出错，镜像不可用。</li><li><strong>deleted：</strong>glance保留了镜像的信息，但不能被使用，一定时间后自动清理镜像信息。</li><li><strong>pending_delete：</strong>类似deleted，此时glance尚未删除镜像的数据，处于该状态的镜像可以恢复。</li></ul><p>例如上图中，当发起创建一个镜像任务Create image时，首先在glance-registry中保存镜像的标识符，但此时镜像文件还未上传，镜像大小还未初始化（Queued），此时任务状态为processor。下一步将初始化镜像的元数据并保存到数据库中，并开始上传镜像（saving），此时任务状态仍为processor。镜像上传如果成功，此时镜像状态转换为active，任务状态为success，镜像可以正常使用；镜像上传如果失败，此时镜像状态为killed，任务状态为failure。同时根据用户的操作镜像的状态和任务状态有不同的表现形式：用户如果选择重新上传，则镜像状态转为queued，任务状态转为processor，即重新开始上传；用户如果选择删除镜像，则镜像状态变为deleted，任务状态为success，表示镜像删除成功。同时，在镜像上传过程中，即saving态时，也可以选择删除镜像，此时镜像状态直接变为deleted，任务状态变为success时，表示镜像删除成功。同理，镜像的导入过程与创建过程类似，只是镜像状态不包含saving态，只有importing态，这里不再赘述。</p><h3 id="OpenStack不同操作系统类型镜像的制作"><a href="#OpenStack不同操作系统类型镜像的制作" class="headerlink" title="OpenStack不同操作系统类型镜像的制作"></a><strong>OpenStack不同操作系统类型镜像的制作</strong></h3><p>OpenStack中虚拟机使用的镜像并不是我们平时装系统使用的iso光盘文件，而是经过裁剪后的适合各个云主机使用公共模板img，且支持加入个性化化定制任务和软件。获取与OpenStack兼容的虚拟机映像的最简单方法是下载别人已经创建的虚拟机映像，最简单的Glance镜像制作方法是下载系统供应商官方发布的OpenStack镜像文件，官方发布的OpenStack大多数镜像预安装了cloud-init包，支持SSH密钥对登录和用户数据注入功能，下图表示了OpenStack官方镜像支持的主流操作系统类型。镜像的具体下载链接，请参考OpenStack社区网站：<a href="https://docs.openstack.org/image-guide/obtain-images.html。" target="_blank" rel="noopener">https://docs.openstack.org/image-guide/obtain-images.html。</a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5vsUT0RAtvGi.png?imageslim" alt="mark"></p><p>但是，有时候官方下载的镜像的并不符合我们的实际需要，比如我们想要安装一些预置软件或脚本。一种简单的方法是将官方镜像进行改造，然后再生成新的模板文件，这种方法虽然简单，但是我们无法完全掌握镜像具体包含哪些工具和软件，存在一定的安全风险。另一种方法就是我们自己手动创建镜像，这样我们自己可以定义安装哪些软件和定制哪些脚本，并自行设定镜像生成的云主机的访问权限等等。下面就以创建一个ubuntu18.04镜像为例，简单说明一下手动创建镜像的流程，并介绍几种常用镜像制作工具。</p><ul><li><strong>Step1：</strong>使用virt-manager创建一个Ubuntu 18.04虚拟机并安装系统</li><li><strong>Step2：</strong>登录虚拟机并安装cloud-init，sudo apt install cloud-init</li><li><strong>Step3：</strong>在虚拟机内部，停止虚拟机，sudo shutdown -h now（或init 0）</li><li><strong>Step4：</strong>预清理虚拟机，在KVM宿主机上执行virt-sysprep -d VM_ID</li><li><strong>Step5：</strong>释放虚拟机定义，在KVM宿主机上执行virsh undefine VM_ID</li><li><strong>Step6：</strong>制作镜像，在KVM宿主机上执行qemu-img create or qemu-img convert</li><li><strong>Step7：</strong>上传镜像，在OpenStack控制节点上执行 openstack image create</li></ul><p>按照上面的步骤，就可以创建各种操作类型各类虚拟机，具体指令的差异由安装的镜像操作系统决定。需要注意一点，在制作windows操作系统类型虚拟机时，需要额外安装virtio驱动，但这不是OpenStack的原因引起的，而是KVM虚拟化解决方案要求的。virtio是一种IO的半虚拟化解决方案，在KVM虚拟机中一般要求磁盘IO使用virtio的半虚拟化驱动，有时网卡的IO也要求使用virtio的半虚拟化驱动，目的就是提高IO虚拟化后的性能。而windows操作系统自身并不支持virtio驱动，因此需要额外安装第三方驱动。</p><p>除了上面手工制作OpenStack的镜像外，开源社区还提供了一些镜像制作工具，用于快速批量制作镜像。常见的工具有以下几种：</p><ul><li><strong>Diskimage-builder：</strong>自动化磁盘映像创建工具，可以制作Fedora，Red Hat Enterprise Linux，Ubuntu，Debian，CentOS和openSUSE镜像，示例：disk-image-create ubuntu VM</li><li><strong>Packer：</strong>使用Packer制作的镜像，可以适配到不同云平台，适合使用多个云平台的用户，一种兼具图形化和指令集的制作工具。</li><li><strong>virt-builder：</strong>快速创建新虚拟机的工具，可以在几分钟或更短的时间内创建各种用于本地或云用途的虚拟机镜像，其本质上是利用一个已存在的虚拟机作为模板，快速创建一个新的虚拟机。</li></ul><h2 id="Glance镜像管理实战"><a href="#Glance镜像管理实战" class="headerlink" title="Glance镜像管理实战"></a><strong>Glance镜像管理实战</strong></h2><p><strong>步骤1：</strong>执行以下命令，查看已上传的镜像列表，OpenStack上传镜像的默认保存位置在/var/lib/glance/images目录下，上传后保存的镜像文件名为镜像的id值</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /var/lib/glance/images/</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/3yPeTN1l6PxB.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，查看上传镜像的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-img info /var/lib/glance/images/3d1a7c9b-d954-4a52-bb80-bd2682972de3</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/zXrdBICzEfdD.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，创建镜像“kk_img”，镜像格式为“QCOW2”，镜像设置为“Private”和“Protected”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image create --disk-format qcow2 --container-format bare --min-disk 1 --min-ram 128 --private --protected --file ~/cirros-0.4.0-x86_64-disk.img kk_img</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/4xi5WziBH2zo.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，查看刚创建镜像“kk_img”的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image show kk_img</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/9oUqnxH9YhIT.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>执行以下命令，将镜像“kk_img”设置为public和unprotected状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image <span class="built_in">set</span> --public --unprotected kk_img</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/oHlHVi3cW5eA.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>执行以下命令，将镜像“kk_img”设置为共享状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image <span class="built_in">set</span> --shared kk_img</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/PtCqi7R9A12U.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，将镜像“kk_img”加入项目kkproject中，并查看镜像的状态信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image add project cfa6d427-f6e2-48c0-91da-0365101da223 fa23a9822e8449239d8f9d7befbabc75</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/jto1zWoSr9kK.png?imageslim" alt="mark"></p><p><strong>步骤9：</strong>执行以下命令，在普通用户kkutysllb登录情况，接收共享镜像“kk_img”，再次观察权限上的变化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image <span class="built_in">set</span> --accept cfa6d427-f6e2-48c0-91da-0365101da223</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/0MpYLq6POj9i.png?imageslim" alt="mark"></p><p><strong>步骤10：</strong>执行以下命令，查看vmdk格式的Ubuntu18.04镜像详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-img info bionic-server-cloudimg-amd64.vmdk</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/RAA99yW2vwYV.png?imageslim" alt="mark"></p><p><strong>步骤11：</strong>执行以下命令，将vmdk格式的Ubuntu18.04镜像转换为qcow2格式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-img convert -f vmdk -O qcow2 -c -p bionic-server-cloudimg-amd64.vmdk bionic-server-cloudimg-amd64.qcow2</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/UfUmERlXQTGB.png?imageslim" alt="mark"></p><p><strong>步骤12：</strong>执行以下命令，上传bionic-server-cloudimg-amd64.qcow2镜像，命名为ubuntu-18.04-s，镜像状态设为public和unprotected</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image create --disk-format qcow2 --container-format bare --min-disk 1 --min-ram 128 --public --unprotected --file ~/bionic-server-cloudimg-amd64.qcow2 ubuntu-18.04<span class="_">-s</span></span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/4UDSA9XDJ0R1.png?imageslim" alt="mark"></p><p><strong>步骤13：</strong>执行以下命令，将镜像ubuntu-18.04-s导出保存到本地磁盘</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image save --file ubuntu-18.04-s.qcow2 ubuntu-18.04<span class="_">-s</span></span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5osdG6c67Qes.png?imageslim" alt="mark"></p><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、请分析镜像导入任务过程中，镜像状态变化过程及任务状态的变化过程？</strong></p><p><strong>2、将激活状态的镜像去激活再次激活时，请分析镜像状态的变化过程及任务状态变化过程。那么将激活态的镜像直接删除，此时镜像状态和任务状态的变化流程如何？</strong></p><p><strong>3、将镜像设置为private和protected时 ，有哪些限制？将镜像改为public和unprotected后，限制上有哪些变化？将镜像改为shared后，限制上有哪些变化？</strong></p><p><strong>4、普通用户是否可以创建public状态的镜像？为什么？shared状态的镜像呢？又为什么？</strong></p><p><strong>5、创建一个虚拟机实例时，实例是如何从镜像启动的（多磁盘场景下，即实例有vda、vdb、vdc三块虚拟磁盘，其中vda是系统盘，vdb是临时盘、vdc是数据盘）？删除虚拟机实例时，实例关联的惊险会怎么样？（答出glance、nova、cinder的大致交互流程即可）</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Glance在OpenStack中主要为实例提供公共镜像服务能力以及镜像/虚拟机快照管理功能，属于OpenStack的核心组件之一，与Swift、Cinder并列OpenStack存储的三驾马车。在实际生产中，Glance本身并不负责大量数据的存储，对镜像的存储主要依赖于后端Swift，一般都是采用对象存储方式。如果后端对象存储采用Swift，则镜像存在Swift中；如果后端存储采用分布式存储ceph，则主要存储在ceph的对象存储rgw模块中。而在我们的实验环境中（考虑大家PC的配置问题），没有部署swift对象存储，因此镜像采用部署Glance服务的节点本地磁盘进行存储。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-认证管理服务Keystone</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E8%AE%A4%E8%AF%81%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Keystone/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-认证管理服务Keystone/</id>
    <published>2020-03-03T09:24:21.000Z</published>
    <updated>2020-03-03T09:49:25.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Keystone作为OpenStack中的一个独立的提供安全认证的模块，作为OpenStack的身份管理服务（Identity Service）主要负责OpenStack用户的身份认证、令牌管理、提供访问资源的服务目录，以及基于用户角色的访问控制。用户访问系统的用户名及密码是否正确、令牌的颁发、服务端点的注册，以及该用户是否具有访问特定资源的权限等，这些都离不开Keystone服务的参与。无论是公有云还是私有云，都会开放接口给众多的用户。为保证系统安全和用户的安全，就需要有完善的安全认证服务，无非包含几个方面：用户认证、服务认证和口令认证。Keystone提供的认证服务是双向的，既对用户进行合法性认证，对用户的权限进行了限制，又对提供给用户的服务进行认证，保证提供给用户的服务是安全可靠的。<a id="more"></a></p><p>在OpenStack的整体框架结构中，Keystone与Horizon类似，相当于一个服务总线与Nova、Glance、Horizon、Swift、Cinder及Neutron等其他核心服务全互联，其他核心服务通过Keystone来注册其服务的入口Endpoint，针对这些服务的任何调用都需要经过Keystone的身份认证，并获得服务的Endpoint来进行访问。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nvJOFgxfrMLJ.png?imageslim" alt="mark"></p><p>Keystone服务作为独立的身份认证模块，首次出现来OpenStack的Essex版本，提供身份认证、服务发现和分布式多用户授权，OpenID Connect，SAML和SQL。它不依赖于其他任何服务，相反其他服务的访问都需要依赖它来提供访问安全机制。</p><h2 id="Keystone的基本概念"><a href="#Keystone的基本概念" class="headerlink" title="Keystone的基本概念"></a><strong>Keystone的基本概念</strong></h2><p>作为 OpenStack 的基础支持服务，Keystone主要完成<strong>身份认证、令牌管理、服务管理、端点注册</strong>和<strong>访问控制</strong>5大功能，如下</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pSosOiKw4diQ.png?imageslim" alt="mark"></p><p>总结起来，Keystone服务主要完成以下三件工作：</p><ul><li>管理用户及其权限（Identity、Policy）；</li><li>维护 OpenStack Services 的 服务目录和入口（catalog、Endpoint）；</li><li>Authentication（认证）和 Authorization（鉴权）（Identity、Token）；</li></ul><p>以上三件工作彼此之间不是相互隔离独立的，反而是有紧密的联系。用户访问云服务时，首先提供用户凭证（Credentials），Keystone完成认证（Identity）后返回给用户一个临时身份凭证（Token），同时根据的用户的权限（Policy）返回用户可以访问的服务和权限（Endpoint、Catalog）。随后，用户就使用Keystone分配的临时身份凭证（Token）按照服务的入口（Endpoint）去访问相关的核心服务并进行相关的资源操作。各核心服务接收到用户的访问请求后，会使用用户的临时身份凭证（Token）去Keystone进行认证（Identity），同时根据权限列表（Policy）对用户的操作进行授权。通过后，才允许用户进行服务访问并进行相关资源操作。</p><p>要想理解Keystone服务的机制，首先要掌握Keystone的服务架构，并根据架构去归类理解Keystone服务的对象模型的各个概念，最后贯穿起来去拉通整个认证服务的流程。Keystone服务的架构如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/13r3TYX1E1dg.png?imageslim" alt="mark"></p><p>从Keystone服务的整体架构来看，整个服务架构分为入口访问模块，核心服务模块、后端存储模块和扩展插件模块，其中：</p><ul><li><strong>入口访问模块：</strong>主要由两个子组件构成，即Keystone API和Middleware。Keystone API接收外部的REST访问请求，而Middleware模块是一个安全认证的中间件，主要用来缓存Token，减轻核心服务模块的认证压力。当用户首次访问时，会通过Keystone API去Middleware模块验证Token的签名，由于此时用户并没有被分配Token，所以Middleware模块会向核心服务模块Keystone Service的Identity子模块进行认证，认证通过后Token子模块会给用户分配一个Token并缓存到Middleware模块中（对Token进行签名），后续用户使用该Token认证时，Middleware模块会代替Identity模块完成认证（Token有效期内），从而减轻Keystone Service核心模块的访问压力。</li><li><strong>核心服务模块：</strong>Keystone Service，由不同的子模块构成，不同子模块提供不同身份认证服务。比如：Identity子模块对用户凭证进行合法性认证，Token子模块用于给用户分配一个临时身份凭证，Assignment子模块用于分配用户的资源配额，Policy子模块用户指定用户访问资源的权限，Catalog子模块用户指定用户可以访问的服务目录等等。</li><li><strong>后端存储模块：</strong>Keystone Backends，主要用来存储实现不同核心服务的数据，不同核心服务数据可以使用不同后端存储来存储。支持的后端存储有：LDAP、SQL、Template、Rules、KVS等。</li><li><strong>扩展插件模块：</strong>Kesytone Plugins，针对不同的身份认证服务提供不同的扩展插件。比如基于密码的认证，通过Password插件完成，基于Token的认证基于Token插件完成等等。</li></ul><p>了解Keystone服务的整体架构后，我们再来看看Keystone服务的对象模型。如下图所示，所谓的对象就是Keystone服务管理的对象，而对象模型就是指Keystone服务管理的几类对象的集合及其子集。其顶层的对象模型就是Service和Policy，而Service对象模型的子集则包括：Identity、Resource、Assignment、Token和Catalog等多个子对象模型。因此，要想彻底掌握Keystone的作用，就需要重点理清这些对象模型的概念及其子集的对应关系，以及对象模型在整个Keystone服务中的作用。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7JJjqyQPt78D.png?imageslim" alt="mark"></p><ul><li><strong>Policy：</strong>安全访问策略，每个OpenStack服务都在相关的策略文件中定义其资源的访问策略（Policy） 。类似于Linux中的权限管理，不同角色的用户或用户组将会拥有不同的操作权限。在OpenStack中也可以针对每种服务（内部或外部）设定不同用户或用户组的访问策略，从而实现不同用户或用户组对访问服务的分权分域管理。各服务的访问策略文件是一个JSON格式的文件，并不是必须的，可以由管理员自行手动创建和配置，配置后无需重启服务即刻生效，一般放在服务配置目录下，即/etc/SERVICE_NAME/policy.json。</li><li><strong>Service：</strong>服务，在一个或多个端点（Endpoint）上公开的一组内部服务。包括：Identity、Resource、Assignment、Token、Catalog等。除内部服务外，Keystone还负责与OpenStack其他服务（ Service ）进行交互，例如计算，存储或镜像，提供一个或多个端点，用户可以通过这些端点访问资源并执行操作。因此，Keystone提供Service对象模型，可以简单理解为Service=内部服务+外部服务。</li><li><strong>Identity：</strong>提供身份凭证数据以及用户和用户组的数据。它有两个子集，分别是User和Group。User就是指单个OpenStack服务的使用者，必须属于某个特定域（Domain），用户名不是全局唯一，但是在其归属域（Domain）内是唯一的。Group是借鉴了Linux操作系统中Group的概念，在M版本之后才引入，作用就是把多个用户作为一个整体进行管理。Group也必须属于某个特定域（Domain），与User一样，在全局范围内Group不是唯一的，但是在其归属域（Domain）内必须唯一。通常情况下，用户和用户组数据由Identity服务管理，允许它处理与这些数据关联的所有CRUD操作。如果使用LDAP进行认证，则Keystone的Identity服务就是LDAP认证的前端，User和Group的管理由权威后端LDAP负责，Keystone的Identity此时只是作为认证的中继。</li><li><strong>Resource：</strong>资源，提供有关项目（Project）和域（Domain）的数据。它也有两个子集，分别是Project和Domain。Project就是以前OpenStack中tenant，也就租户，在N版本以后改为项目Project，是OpenStack中资源拥有者的基本单元，OpenStack中所有的资源都属于特定的项目。Project必须属于某个特定域，如果创建Project时没有指定Domain，则它默认属于Default域。与User和Group一样，Project也并非全局唯一，但是在其归属域（Domain）必须唯一。Domain这个概念也是Keystone v3版本开始引入，可以将其理解为资源的集合，也就是说某一资源必须属于一个特定域。为了将Domain内部的资源进一步细分使用权限和使用者，因此在Domain中可以定义所有归属Project的资源配额，在Project中又可以定义所有归属User和Group的资源配额，在Group中可以集中定义所有归属User的权限、角色和资源配额。Domain只可以通过命令行CLI或API接口创建，无法通过Web UI创建。</li><li><strong>Assignment：</strong>角色分配，提供有关角色（Role）和角色分配（Assignment Role）的数据。Role规定最终用户可以获得的授权级别，角色Role可以在域Domain或项目Project级别授予，也可以在单个用户User或组Group级别分配角色，角色名称在该角色的归属域中必须是唯一的。角色分配Role Assignment是一个三元组，包括：Role、Resource和Identity等服务，也就是说OpenStack通过Identity、Resource和Role三类对象模型决定某个用户User拥有哪些资源，对拥有的资源能够执行哪些操作。</li><li><strong>Catalog：</strong>服务目录，用于提供查询端点（Endpoint）的端点注册表，以便外部访问OpenStack的服务，因此它只有Endpoint一个子集。Endpoint本质上就是一个URL，拥有三种类型，分别是public、admin和internal，分别提供给不同的User，用于其访问OpenStack的服务。比如，提供给普通用户使用的就是public类型的Endpoint，提供给管理员使用的就是admin类型的Endpoint，而提供给OpenStack内部其他服务的就是internal类型的Endpoint。</li><li><strong>Token：</strong>令牌服务，提供用户访问服务的凭证，代表着用户的账户信息。Token一般包含User信息，Scope信息（Project、Domain或者Tenant），Role信息，本质上就是代替用户的一个凭证。用户首次访问OpenStack时，均需要将用户名和密码发给Keystone，Keystone将其转换为有时效性的Token发给用户，用户将其缓存在本地客户端，后续访问其他服务，Token将代替用户进行。因此，Token必须和用户绑定。客户端在调用POST/Tokens后拿到返回结果，如果用户名和密码验证成功，则拿到诸如User、Project、Token、metadata、catalog等数据。从catalog中找到要访问Service的Endpoint，让后在Headers中放入｛X-Auth-Token:token_id｝信息，就向该Service发起访问请求。Service的WSGI APP在收到该Request请求后，首先验证该token_id是否有效，该验证一般都使用一个keystonemiddlerware的中间件来完成，如下图所示：</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/YkBLatIvmo2w.png?imageslim" alt="mark"></p><p>在OpenStack中Token支持的类型有：UUID、PKI、PKIZ和Fernet集中，OpenStack官方社区目前默认采用Fernet类型Token。几种Token类型对比如下：</p><table><thead><tr><th><strong>Token 类型</strong></th><th><strong>UUID</strong></th><th><strong>PKI</strong></th><th><strong>PKIZ</strong></th><th><strong>Fernet</strong></th></tr></thead><tbody><tr><td>大小</td><td>32 Byte</td><td>KB 级别</td><td>KB 级别</td><td>约 255 Byte</td></tr><tr><td>支持本地认证</td><td>不支持</td><td>支持</td><td>支持</td><td>不支持</td></tr><tr><td>Keystone 负载</td><td>大</td><td>小</td><td>小</td><td>大</td></tr><tr><td>存储于数据库</td><td>是</td><td>是</td><td>是</td><td>否</td></tr><tr><td>携带信息</td><td>无</td><td>user, catalog 等</td><td>user, catalog 等</td><td>user 等</td></tr><tr><td>涉及加密方式</td><td>无</td><td>非对称加密</td><td>非对称加密</td><td>对称加密(AES)</td></tr><tr><td>是否压缩</td><td>否</td><td>否</td><td>是</td><td>否</td></tr></tbody></table><p>UUID类型的令牌，token_id只是一个纯粹的32位十六进制字符串；PKI类型的令牌token_id使用完整性和加密算法对token_data进行加密；PKIZ类型的令牌实际上就是对PKI加密后的令牌再进行压缩，压缩率约50%左右；Fernet类型的令牌是一个JSON格式的字符串，约255字节大小，在我们的实验环境中，通过API方式得到的Fernet类型的Token令牌如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7mEF4DbSHYGc.png?imageslim" alt="mark"></p><p>上图中，令牌信息部分包含了Token当前的时间，有效截止时间，审计id，验证的方法等内容。在账户部分包含了生成该Token的账户id信息、账户名、域信息等内容。Token令牌类型的选择涉及多个因素，包括Keystone server的负载、region数量、安全因素、维护成本以及令牌本身的成熟度。Region的数量影响PKI/PKIZ令牌的大小。从安全的角度上看，UUID无需维护密钥，PKI需要妥善保管Keystone server上的私钥，Fernet需要周期性的更换密钥。因此，从安全、维护成本和成熟度上看，UUID &gt; PKI/PKIZ &gt; Fernet 。采用何种类型Token，目前华为在自有FusionSphere OpenStack解决方案的建议如下：</p><ul><li><strong>Keystone server 负载低，region少于3个，采用UUID令牌。</strong></li><li><strong>Keystone server 负载高，region少于3个，采用PKI/PKIZ令牌。</strong></li><li><strong>Keystone server 负载低，region大于或等于3个，采用UUID令牌。</strong></li><li><strong>Keystone server 负载高，region大于或等于3个，目前OpenStack新版本默认采用Fernet令牌。</strong></li></ul><p>除了上述对象模型外，OpenStack还有Region、AZ、HA（Host Aggregate）的资源模型概念（这些其实属于计算资源池的概念）。Region是物理位置上划分的资源使用范围，可以简单理解一个数据中心DC就是一个Region，而AZ是Region内部从故障隔离角度划分的一个资源使用范围，其本质上是一组主机构成的资源池，资源池与资源池之间故障隔离（电源、网络、布线等），主要的划分依据还是动力故障隔离。HA（Host Aggregate）主机组是从物理资源的规格角度划分的资源使用范围，比如同是Dell 骁龙处理器的主机可以划分为一个HA，同是万兆网卡的主机可以划分一个HA等等。从资源使用者角度来看，其使用资源的范围从大到小的排序为：Region&gt;AZ&gt;HA&gt;Domain&gt;Project&gt;Group&gt;User，整个Keystone的对象模型的分配关系如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/DxQuPR6hUSHT.png?imageslim" alt="mark"></p><h2 id="Keystone的认证方式"><a href="#Keystone的认证方式" class="headerlink" title="Keystone的认证方式"></a><strong>Keystone的认证方式</strong></h2><p>Keystone的认证方式主要包括三种：<strong>基于令牌的认证方式、基于外部的认证方式</strong>和<strong>基于本地的认证方式</strong>。<strong>生产环境中，最常用的是基于令牌的认证方式</strong>，需要重点学习。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Bvgfm9TbYhPJ.png?imageslim" alt="mark"></p><ul><li><strong>基于令牌的认证方式：</strong>最常用的认证方式，认证请求发送时在HTTP/HTTPS头部添加一个X-Auth-Token的头域，Keystone检查该头域中token_id值，并与数据库SQL中的令牌值比对验证。</li><li><strong>基于外部的认证方式：</strong>采用集成第三方的认证系统，客户端在认证请求头中天剑remote_user信息，Keystone的Identity服务作为接收该认证请求的前端，将认证请求转发给后端的外部认证系统进行认证，并将认证结果返回给客户端。在这种认证方式中，Keystone作为认证的中继服务单元。</li><li><strong>基于本地的认证方式：</strong>这是默认的认证方式，即我们采用的用户名和密码认证。</li></ul><h3 id="基于令牌的认证方式—UUID令牌类型"><a href="#基于令牌的认证方式—UUID令牌类型" class="headerlink" title="基于令牌的认证方式—UUID令牌类型"></a><strong>基于令牌的认证方式—UUID令牌类型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/TQWMdYM4Gjzl.png?imageslim" alt="mark"></p><p><strong>该方案由于UUID令牌长度为固定32位十六进制字符串，不携带其他信息，OpenStack其他服务API收到该令牌后会向Keystone验证，并获得用户的其他信息，因此Keystone必须实现令牌的存储和认证，所有的验证均由Keystone完成</strong>，故采用这种认证方式的实际生产环境中部署Keystone服务的节点负荷较大。总体上，采用UUID类型的令牌认证共需要5步完成：</p><ul><li><strong>Step1：</strong>客户端发起认证请求，在请求的HTTP/HTTPS的Body体中携带用户名和密码信息；</li><li><strong>Step2：</strong>Keystone通过认证后，给用户返回一个token信息，用于该用户后续访问其他服务的身份凭证，该凭证并非永久有效，而是有一个有效期限，默认为1小时。</li><li><strong>Step3：</strong>客户端后续访问其他OpenStack服务时，比如访问nova-api，会在请求的HTTP/HTTPS的Header部分增加一个key-value格式的头域X-Auth-Token，其值就是Keystone返回给用户token_id。</li><li><strong>Step4：</strong>其他服务接收到用户的访问请求后，会将请求消息头部的token_id发送给Keystone进行验证。</li><li><strong>Step5：</strong>验证通过后，Keystone会返回该用户的三元组信息（User、Project、Role）给其他服务API，其他服务根据本地Policy的设定决定该用户使用其资源的权限。</li></ul><h3 id="基于令牌的认证方式—PKI令牌类型"><a href="#基于令牌的认证方式—PKI令牌类型" class="headerlink" title="基于令牌的认证方式—PKI令牌类型"></a><strong>基于令牌的认证方式—PKI令牌类型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kuvMCI3PHG8r.png?imageslim" alt="mark"></p><p><strong>该方案需要部署一台证书服务器（可以由部署Keystone服务的节点兼任），证书服务器将用户的私钥保存在Keystone本地，将用户的公钥分发给OpenStack的其他服务。这种认证方式客户的认证由OpenStack各个服务利用公钥完成，部署Keystone服务节点的负荷较小。</strong>总体上，采用PKI类型令牌认证需要4步完成：</p><ul><li><strong>Step1：</strong>客户端发起认证请求，在请求的HTTP/HTTPS的Body体中携带用户名和密码信息。</li><li><strong>Step2：</strong>Keystone利用本地的私钥对用户认证请求中的用户名和密码进行认证，通过后返回Token给客户端，在Token中携带签名后安全key值。</li><li><strong>Step3：</strong>客户端后续访问其他OpenStack服务时，比如访问nova-api，会在请求的HTTP/HTTPS的Header部分增加一个key-value格式的头域X-Auth-Token，其值就是Keystone返回给用户前面后的安全key值。</li><li><strong>Step4：</strong>其他OpenStack服务利用本地存储的公钥对token进行验证，通过后根据本地Policy的设定决定该用户使用其资源的权限。</li></ul><h3 id="基于令牌的认证方式—PKIZ令牌类型"><a href="#基于令牌的认证方式—PKIZ令牌类型" class="headerlink" title="基于令牌的认证方式—PKIZ令牌类型"></a><strong>基于令牌的认证方式—PKIZ令牌类型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Pb1acFCnbrU6.png?imageslim" alt="mark"></p><p><strong>该方案与PKI认证方案的流程类似，主要考虑到PKI认证方式的HTTP/HTTPS header头域过大（达到8K字节），为了减小管理平面带宽压力，故对PKI认证的头部采用压缩机制，而解压缩则在OpenStack的其他服务中进行。即与PKI认证方式相比，在Keystone返回的Token是一个压缩过的携带签名证书的Token-Key，而在OpenStack的其他服务首先需要对该token进行解压缩，然后再通过公钥进行认证。</strong>流程请参考PKI认证方式，这里不再赘述。</p><h3 id="基于令牌的认证方式—Fernet"><a href="#基于令牌的认证方式—Fernet" class="headerlink" title="基于令牌的认证方式—Fernet"></a><strong>基于令牌的认证方式—Fernet</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/WsQhs3fmf5CX.png?imageslim" alt="mark"></p><p><strong>该方案基于Fernet（一种对称加密算法）的对用户信息进行加密而生成的令牌，与用户相关的所有的认证信息都保存在令牌中，故令牌本身就包含了用户的三元组信息（User、Project、Role），也因此Fernet令牌无需持久化，但是一旦Fernet令牌被攻破，那么就可以做所有该用户能做到的事情，因此采用该令牌类型的认证方式其令牌一般都有一个有效期，默认为1小时，当有效期到期后，需要重新申请令牌。</strong>与UUID、PKI、PKIZ令牌类型认证方式相比，Fernet类型的令牌值无需存储在后端数据库中，只需要放在内存中即可。相比UUID方式，Keystone数据库的眼里不大，无需查表；相比PKI/PKIZ方式，HTTP/HTTPS的header头部较小，占用管理平面带宽较小。采用该令牌类型的认证方式，如果Keystone服务采用高可用集群部署，需要在所有部署Keystone服务的节点共享秘钥用于Token的解密和认证。总体上，该流程也需要5步：</p><ul><li><strong>Step1：</strong>客户端发起认证请求到Keystone服务器，请求HTTP/HTTPS的Body体中携带用户名和密码信息。</li><li><strong>Step2：</strong>Keystone验证通过后，利用对称加密算法AES对返回给客户端的Token进行加密，该加密后的Token中携带用户的三元组信息（User、Project、Role）。</li><li><strong>Step3：</strong>客户端在后续访问其他OpenStack服务的API时，会在HTTP/HTTPS的Header头部携带X-Auth-Token头域，其值就是加密后的token_id，也就是上面Token令牌示意图中的adjust_ids值。</li><li><strong>Step4：</strong>其他OpenStack服务的API将该token_id发送给Keystone进行验证。</li><li><strong>Step5：</strong>Keystone首先完成token_id的解密，验证通过后，将用户的三元组信息（User、Project、Role）发给其他OpenStack服务，其他服务根据本地Policy的设定决定该用户使用其资源的权限。</li></ul><h2 id="基于角色的访问控制—RBAC"><a href="#基于角色的访问控制—RBAC" class="headerlink" title="基于角色的访问控制—RBAC"></a><strong>基于角色的访问控制—RBAC</strong></h2><p>在上面分析过程中，Keystone服务只用于验证Token是否有效，而验证通过后用户资源的操作权限是通过各服务的Policy来决定。至于Policy通过什么设定来决定？首先需要了解下Policy文件的格式，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/L3qGbNTxqchN.png?imageslim" alt="mark"></p><p>上图是Neutron服务的Policy文件的一部分，可以看出该文件采用JSON格式，每一对｛｝内部通过key-value格式的键值对来定义服务的访问规则，每一条规则（Rule）就是一个键值对key-value。比如，上图中创建子网create_subnet这个操作，只有具备admin角色或该子网归属network的属主才能执行，那么可以推理出执行创建的子网的操作只有管理员admin，用户neutron或该子网归属network的租户project才具有权限，不符合上述条件的其他用户执行该命令时都会返回鉴权失败错误码403 Forbidden。而创建带VLAN ID的子网create_subnet:segment_id这个操作，只有具备admin角色的用户（管理员admin或用户neutron）才能执行，其他不符合条件的用户执行该操作都会返回鉴权失败错误码403 Forbidden。因此，通过我们的推理，就可以理解Policy模块在权限检查中所起的作用，以及整个基于角色的访问控制流程RBAC，而policy.json文件就是RBAC流程实现的关键，如下图：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/FGV9IgAfTKI7.png?imageslim" alt="mark"></p><p>上图中，用户发起操作请求Request到OpenStack服务的api进程，OpenStack服务的api进程首先去Keystone的Token模块进行Token有效性验证。验证通过后，返回的Token上下文中携带用户的三元组信息（User、Project、Role），并且Keystone的Policy模块调用OpenStack各服务本地Policy.json文件，根据用户操作请求Request的action结合用户的三元组信息判断用户该操作是否合法？不通过，则直接向客户端返回403 Forbidden；通过，则将验证结果返回给OpenStack的其他相关服务，OpenStack的其他相关服务按照用户的操作请求执行下一步操作Next。</p><p>从上面的分析中，我们可以知道Keystone的Policy模块根据各服务的policy.json文件来检查用户的操作权限，其检测时主要需要三方面的数据：</p><ul><li><strong>各服务的policy.json策略配置文件；</strong></li><li><strong>Auth_token添加到HTTP/HTTPS头部的token数据；</strong></li><li><strong>用户的请求资源数据；</strong></li></ul><p><strong>最后，放上一张Keystone的流程示意图总结Keystone如何实现认证和权限控制，如下：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/AX4TU0f9fJuG.png?imageslim" alt="mark"></p><h2 id="Keystone服务的实战操作（CLI命令行方式）"><a href="#Keystone服务的实战操作（CLI命令行方式）" class="headerlink" title="Keystone服务的实战操作（CLI命令行方式）"></a><strong>Keystone服务的实战操作（CLI命令行方式）</strong></h2><p>这里只展示Keystone服务的命令CLI操作方式，至于Web UI的操作太过于简单，这里不再赘述。需要注意一点，<strong>Keystone的资源模型Domain只有命令行方式可以操作。</strong></p><h3 id="域Domain的实战操作"><a href="#域Domain的实战操作" class="headerlink" title="域Domain的实战操作"></a>域Domain的实战操作</h3><p><strong>步骤1：</strong>执行以下命令，导入admin-openrc.sh的环境变量，进入管理员视角。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/cU8q6GyP2svT.png?imageslim" alt="mark"></p><p><strong>步骤 2：</strong>执行以下命令，查看OpenStack域相关命令的用法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain -h</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/cmxe7xSDRaUg.png?imageslim" alt="mark"></p><p>各子命令具体用法可通过如下方式进行查看：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ziDE22a3WQfT.png?imageslim" alt="mark"></p><p><strong>步骤 3：</strong>执行以下命令，创建域“Demo_Domain”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain create --<span class="built_in">enable</span> --description <span class="string">"This is a temporary test domain"</span> Demo_Domain</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/mGV6W7MOOREH.png?imageslim" alt="mark"></p><p><strong>步骤 4：</strong>执行以下命令，查看OpenStack域列表信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/NjnELDXYU2nQ.png?imageslim" alt="mark"></p><p><strong>步骤 5：</strong>执行以下命令，查看刚刚创建的Demo_Domain域的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain show 3c660f1059b94bc3864074200cc12981 <span class="comment">#&lt;===这里我用了域的id信息进行查询，也可以使用域名，因为一个Region中域名唯一</span></span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/QJanlBOwyI9i.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>执行以下命令，设置刚刚创建的Demo_Domain为非激活态，再次查看刚刚创建的Demo_Domain域的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain <span class="built_in">set</span> --<span class="built_in">disable</span> Demo_Domain</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/z5fQJfKVSN2y.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，删除刚刚创建的Demo_Domain为非激活态，再次查看域列表信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain delete Demo_Domain</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6YUCRjcRuawq.png?imageslim" alt="mark"></p><h3 id="角色role、用户user及用户组group的相关操作实战"><a href="#角色role、用户user及用户组group的相关操作实战" class="headerlink" title="角色role、用户user及用户组group的相关操作实战"></a><strong>角色role、用户user及用户组group的相关操作实战</strong></h3><p><strong>步骤1：</strong>执行以下命令，查看OpenStack角色相关命令的用法（后续其他资源的操作指令使用帮助查询与此类似，不再赘述）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role -h</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/TczbUnrISbdh.png?imageslim" alt="mark"></p><p>上图列出了角色role的相关操作包括CRUD，角色的分配，角色的移除等等。。。</p><p><strong>步骤2：</strong>执行以下命令，查看OpenStack角色列表信息</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/RaL5t0HjPz1i.png?imageslim" alt="mark"></p><p>上图中展示了当前OpenStack中角色有member、user、heat_stack_owner、admin、heat_stack_user、creator和reader。</p><p><strong>步骤3：</strong>执行以下命令，创建角色”Kk_Role”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role create --domain kkutysllb Kk_Role &lt;===创建角色时，可以指定角色的归属域，也可以后续通过<span class="built_in">set</span>指令指定，建议在创建时指定</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Tmw3Hq6hFG4X.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，查看新创建角色”Kk_Role”的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role show Kk_Role</span><br></pre></td></tr></table></figure><p>我们会发现刚刚创建的角色Kk_Role不存在，但是刚刚明明是创建成功的。这是为什么？原因是我们在刚刚创建Kk_Role时制定了角色归属域为kkutysllb，而当前我们直接使用openstack role show Kk_Role显示角色Kk_Role详细信息时，由于没有显式制定归属域参数，所以默认显示的是Default域的角色信息，而Kk_Role并不归属Default域，自然就没有相关信息展示。因此，执行如下命令显示角色Kk_Role的详细信息和域kkutysllb的所有角色列表。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role show --domain kkutysllb Kk_Role</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pxKRM5cvagLK.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role list --domain kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/CPlJtGATsYQU.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>执行以下命令，查看域kkutysllb下用户user列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user list --domain kkutysllb &lt;===可以发现没有任何用户信息</span><br></pre></td></tr></table></figure><p><strong>步骤6：</strong>执行以下命令，创建一个域kkutysllb下的租户project</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project create --domain kkutysllb kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/83zcMzxnGm9p.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，创建一个域kkutysllb下的两个用户kk_user01和kk_user02，密码均为123456</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain kkutysllb --project kkutysllb --password-prompt kk_user01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/fa6F9P9Xyz46.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain kkutysllb --project kkutysllb --password-prompt kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/jiMlsnjz6ywI.png?imageslim" alt="mark"></p><p><strong>步骤8：</strong>执行以下命令，给两个用户kk_user01和kk_user02添加角色Kk_Role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack role add --domain kkutysllb --project-domain kkutysllb --role-domain kkutysllb --user kk_user01 Kk_Role</span><br><span class="line"></span><br><span class="line">openstack role add --domain kkutysllb --project-domain kkutysllb --role-domain kkutysllb --user kk_user02 Kk_Role</span><br></pre></td></tr></table></figure><p><strong>步骤9：执行以下命令，查看两个用户kk_user01和kk_user02的角色分配情况</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role assignment list --names | grep kk_user</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/MdmI6sh6dabs.png?imageslim" alt="mark"></p><p><strong>步骤10：</strong>执行以下命令，创建一个用户组Kk_Group，将上面创建的两个用户kk_user01和kk_user02加入该组</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack group create --domain kkutysllb Kk_Group</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GuIwikpUb6Oz.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack group add user --group-domain kkutysllb --user-domain kkutysllb Kk_Group kk_user01 kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Y1VVT6nATWtO.png?imageslim" alt="mark"></p><p><strong>步骤11：</strong>执行以下命令，将kk_user02用户从用户组Kk_Group中移除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack group remove user Kk_Group kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ebNe7qsf6fbm.png?imageslim" alt="mark"></p><p><strong>步骤 12：</strong>执行以下命令，禁用用户“kk_user02”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user <span class="built_in">set</span> --domain kkutysllb --<span class="built_in">disable</span> kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/PmuEbCtoIkhz.png?imageslim" alt="mark"></p><p><strong>步骤13：</strong>执行以下命令，删除用户“kk_user02”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user delete --domain kkutysllb kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dQka6P2tvGUP.png?imageslim" alt="mark"></p><h3 id="项目project相关操作实战"><a href="#项目project相关操作实战" class="headerlink" title="项目project相关操作实战"></a><strong>项目project相关操作实战</strong></h3><p><strong>步骤1：</strong>执行以下命令，查看域kkutysllb下的项目列表，可以查看到所有的项目</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project list --domain kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/y5XP0Qxg9YK5.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，将项目kkutysllb去激活必过查看该项目详情</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project <span class="built_in">set</span> --<span class="built_in">disable</span> --domain kkutysllb kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/uFnCa3Yeg12u.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，将项目kkutysllb删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project delete --domain kkutysllb kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ALwtLXVy4LGb.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，在Default域下的创建libing用户，并添加admin的角色</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/lUcNgLUipDas.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>执行以下命令，查看Default域下的admin项目的资源配额</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack quota show admin</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/DdJBxHpMRGlR.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，修改项目“admin”的默认配额，如将实例数量修改为“5”，卷数量修改为“5”，网络修改为“10”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack quota <span class="built_in">set</span> --instance 5 --volumes 5 --network 10 admin</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/VlQiHGsEow7d.png?imageslim" alt="mark"></p><h3 id="服务和服务端点的实战"><a href="#服务和服务端点的实战" class="headerlink" title="服务和服务端点的实战"></a><strong>服务和服务端点的实战</strong></h3><p><strong>步骤 1：</strong>执行以下命令，查看OpenStack服务相关命令的用法和子命令的用法</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/CB9H0tABA8zl.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/p89WdumlBs0z.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，创建swift服务，服务类型位object-store</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack service create --name swift --description <span class="string">"OpenStack Object Storage"</span> object-store</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yTIzjEwmxUNA.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，创建服务“swift”的服务端点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne object-store publick http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/IYQw1t6TOvwy.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne object-store admin http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/bb0TBXG9zq6D.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne object-store internal http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nhmgVAUzu1UY.png?imageslim" alt="mark"></p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a><strong>思考</strong></h2><p><strong>1、为什么不能修改admin帐号域归属信息为kkutysllb后，再来展示域kkutysllb内部的角色信息？</strong></p><p><strong>2、在给用户添加角色时，能否同时指定用户的归属域Domain和项目Project？为什么？通过什么操作实现？</strong></p><p><strong>3、用户通过CLI创建一个VM，Keystone如何验证该用户？如何验证该用户具有创建VM的权限？</strong></p><p><strong>4、用户通过Horizon创建一个VM，Keystone如何验证该用户？如何验证该用户具有创建VM的权限？</strong></p><p><strong>5、上述两种创建VM的方式，Keystone的验证流程有何区别？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Keystone作为OpenStack中的一个独立的提供安全认证的模块，作为OpenStack的身份管理服务（Identity Service）主要负责OpenStack用户的身份认证、令牌管理、提供访问资源的服务目录，以及基于用户角色的访问控制。用户访问系统的用户名及密码是否正确、令牌的颁发、服务端点的注册，以及该用户是否具有访问特定资源的权限等，这些都离不开Keystone服务的参与。无论是公有云还是私有云，都会开放接口给众多的用户。为保证系统安全和用户的安全，就需要有完善的安全认证服务，无非包含几个方面：用户认证、服务认证和口令认证。Keystone提供的认证服务是双向的，既对用户进行合法性认证，对用户的权限进行了限制，又对提供给用户的服务进行认证，保证提供给用户的服务是安全可靠的。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-Web UI管理服务Horizon</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-Web-UI%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Horizon/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-Web-UI管理服务Horizon/</id>
    <published>2020-03-03T09:07:48.000Z</published>
    <updated>2020-03-03T09:23:50.219Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>OpenStack的用户界面由两部分组成：一是Web界面，二是Shell CLI界面。Horizon负责展现Web仪表盘，用户可以通过浏览器直接操作、管理、运维OpenStack的一些功能。由于OpenStack项目队伍不断壮大，Danshboard并不能展现所有的OpenStack功能，因此，最新的功能一般会先开发Shell命令行，也就是将CLI（Command Line Interface）提供给Linux用户操作。<a id="more"></a></p><p>在OpenStack的7个核心服务中，Horizon服务本质上就是一个Web Server，基于Django框架开发的，与Keystone服务一样，需要与其他提供业务的服务（计算nova、存储cinder、网络neutron等）进行全互联。如下图所示，如果将OpenStack的7个核心服务分为三层，从上到下依次是<strong>入口层、业务层</strong>和<strong>认证层</strong>。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/65YVSq76MuHT.png?imageslim" alt="mark"></p><p>Horizon服务最早出现OpenStack的Essex版本中，提供基于Web的控制界面，使得云管理员和租户/用户能够可视化的管理OpenStack的服务和资源。Horizon唯一依赖的服务就是Keystone认证服务，同时还可以与其他服务结合使用，例如镜像服务、计算服务、网络服务等。并且，Horizon还可以在独立服务（比如对象存储中）的环境中单独使用。</p><h2 id="REST-ful与WSGI介绍"><a href="#REST-ful与WSGI介绍" class="headerlink" title="REST ful与WSGI介绍"></a><strong>REST ful与WSGI介绍</strong></h2><p>OpenStack各个项目都提供了API接口服务，通过endpoint提供访问URL。无论我们通过Horizon访问OpenStack的服务和资源，还是通过命令行CLI访问OpenStack的服务和资源，其本质都是通过各服务/资源的API的endpoint进行访问，即均是基于HTTP/HTTPS协议的Web服务访问。而这些API接口都是基于一种叫做“REST ful”的架构。RESTful是目前流行的一种互联网软件架构，<strong>REST（Representational State Transfer）就是表述状态转移的意思。</strong></p><p>RESTful架构的一个核心概念是<strong>“资源”</strong>（resource）。从RESTful的角度看，网络里的任何东西都是资源，它可以是一段文本、一张图片、一首歌曲、一种服务等，每个资源都对应一个特定的URL（统一资源定位符）并用它进行标示，访问这个URL就可以获得这个资源。资源可以有多种具体表现形式，也就是资源的“表述”（representation），例如，一张图片可以使用JPEG格式，也可以使用PNG格式。URL只是代表了资源的实体，并不能代表它的表现形式。客户端和服务端之间进行互动传递的就只是资源的表述，我们上网的过程就是调用资源的URL，获取它不同表现形式的过程。这个互动只能使用无状态协议HTTP，也就是说，服务端必须保存所有的状态，客户端可以使用HTTP的几个基本操作，包括GET（获取）、POST（创建）、PUT（更新）、DELETE（删除），使服务端上的资源发生状态转化（State Transfer），也就是所谓的“表述性状态转移”。</p><p>OpenStack各个项目都提供了RESTful架构的API作为对外提供的接口，而RESTful架构的核心是资源与资源上的操作，也就是说，OpenStack定义了很多的资源，并实现了针对这些资源的各种操作函数。OpenStack的API服务进程接收到客户端的HTTP请求时，一个所谓的“路由”模块会将请求的URL转换成相应的资源，并路由到合适的操作函数上。这个所谓的路由模块Routes源自于对Rails（Ruby on Rails）路由系统的重新实现，采用MVC（Model-View-Controller）模式，收到浏览器发出的HTTP请求后，路由系统会将这个请求指派到对应的Controller。RESTful只是设计风格而不是标准，Web服务中通常使用基于HTTP的符合RESTful风格的API。而WSGI（Web Server Gateway Interface，Web服务器网关接口）则是Python语言中所定义的Web服务器和Web应用程序或框架之间的通用接口标准。</p><p>WSGI，顾名思义就是Web服务器网关接口。所有客户端访问Web服务，都需要经过该网关接口进行转发。而Web应用的本质就是客户端发送一个HTTP的请求，服务器收到请求生成一个HTML文档，服务器将该HTML文档作为HTTP响应的Body发送给客户端，客户端从HTTP响应的Body体中提取内容并显示出来。像这类生成HTML，接受HTTP请求、解析HTTP请求、发送HTTP响应等代码如果由我们自己来写，则需要耗费大量的时间和精力。正确的做法是底层代码由专门的服务器软件实现，需要一个统一的接口，让程序员专心编写业务层面的代码，这个接口就是WSGI。</p><p><strong>WSGI有两方：服务器方和应用程序方</strong>，如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/OpC56bzccRzi.png?imageslim" alt="mark"></p><p><strong>1）服务器方：</strong>其调用应用程序，给应用程序提供环境信息和回调函数，这个回调函数用来将应用程序设置的HTTP Header和Status等信息传递给服务器方。</p><p><strong>2）应用程序：</strong>用来生成返回的Header、Body和Status，以便返回给服务器方。</p><p>从名称上看，WSGI是一个网关，作用就是在协议之间进行转换。换句话说，WSGI就是一座桥梁，桥梁的一端称为服务端或者网关端，另一端称为应用端或者框架端。当处理一个WSGI请求时，服务端为应用端提供上下文信息和一个回调函数，应用端处理完请求后，使用服务端所提供的回调函数返回相对应请求的响应。</p><p>WSGI的服务方和应用程序之间还可以加入Middleware中间件，从服务端方面看，中间件就是一个WSGI应用；从应用端方面看，中间件则是一个WSGI服务器。这个中间件可以将客户端的HTTP请求，路由给不同的应用对象，然后将应用处理后的结果返回给客户端。如下图示，我们也可以将WSGI中间件理解为服务端和应用端交互的一层包装，经过不同中间件的包装，便具有不同的功能，比如URL路由分发，再比如权限认证。这些不同中间件的组合便形成了WSGI的框架，比如Paste。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/BtdCU1gmj1Jv.png?imageslim" alt="mark"></p><p>OpenStack使用Paste的Deploy组件（<a href="http://pythonpaste.org/deploy/）来完成WSGI服务器和应用的构建，每个项目源码的etc目录下都有一个Paste配置文件，比如Nova中的etc/nova/api-paste.ini，部署时，这些配置文件会被复制到系统/etc/" target="_blank" rel="noopener">http://pythonpaste.org/deploy/）来完成WSGI服务器和应用的构建，每个项目源码的etc目录下都有一个Paste配置文件，比如Nova中的etc/nova/api-paste.ini，部署时，这些配置文件会被复制到系统/etc/</a><project>/目录下。Paste Deploy的工作便是基于这些配置文件。除了Routes与Paste Deploy外，OpenStack中另一个与WSGI密切相关的是WebOb（<a href="http://webob.org/）。WebOb通过对WSGI的请求与响应进行封装，来简化WSGI应用的编写。" target="_blank" rel="noopener">http://webob.org/）。WebOb通过对WSGI的请求与响应进行封装，来简化WSGI应用的编写。</a></project></p><p>WebOb中两个最重要的对象，<strong>一是webob.Request</strong>，对WSGI请求的environ参数进行封装；<strong>一是webob.Response</strong>，包含了标准WSGI响应的所有要素。此外，还有<strong>一个webob.exc对象</strong>，针对HTTP错误代码进行封装。除了这3个对象，<strong>WebOb提供了一个修饰符（decorator），即webob.dec.wsgify</strong>，以便可以不使用原始的WSGI参数和返回格式，而全部使用WebOb替代。</p><p>除了Paste提供的WSGI框架外，为了解决Paste组合框架的Restful API代码过于臃肿，导致项目的可维护性变差的弊端。一些OpenStack的新项目选了Pecan框架来实现Restful API。Pecan是一个轻量级的WSGI网络框架，其设计并不想解决Web世界的所有问题，而是主要集中在对象路由和Restful支持上，并不提供对话（session）和数据库支持，用户可以自由选择其他模块与之组合。</p><h2 id="Horizon服务的实操体验"><a href="#Horizon服务的实操体验" class="headerlink" title="Horizon服务的实操体验"></a><strong>Horizon服务的实操体验</strong></h2><p><strong>Step1：</strong>使用浏览器登录OpenStack Dashboard：<a href="http://rocky-controller/dashboard/，输入**域名，用户名**和**密码**，单击“Sign" target="_blank" rel="noopener">http://rocky-controller/dashboard/，输入**域名，用户名**和**密码**，单击“Sign</a> In”，进入管理员视图Overview主界面。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/M7Y72gWhRjpi.png?imageslim" alt="mark"></p><p><strong>Step2</strong>：登录成功后，默认进入项目概览页面，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Ur5FUimEscnn.png?imageslim" alt="mark"></p><p>后面，将以发放一个简单的cirros虚拟机实例为例，来帮助大家快速熟悉OpenStack Dashboard的基本操作界面。</p><p>首先，要创建一个虚拟机VM实例，需要完成虚拟机规格的定义，在左侧导航栏，选择“Admin &gt; Compute &gt; Flavors”，进入规格列表，单击页面右上角的“Create Flavor”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/0yDa9n0Sl4Uy.png?imageslim" alt="mark"></p><p>弹出创建规格对话框，输入如下信息：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rFSLxjMBYOzU.png?imageslim" alt="mark"></p><p>然后，需要创建虚拟机VM实例挂载的网络信息，其本质上就是创建虚拟机的虚拟网卡（后面Neutron部分会讲）。在左侧导航栏，选择“Admin &gt; Network &gt; Networks”，进入网络列表，单击页面右上角的“Create Network”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yDMvo07AVSew.png?imageslim" alt="mark"></p><p>在Network标签页面，填入如下信息，然后点击Next。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6FOfEuvFdIyk.png?imageslim" alt="mark"></p><p>在Subnet标签页，填写如下信息，然后点击Next。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kXb3VUDuH6nN.png?imageslim" alt="mark"></p><p>在Subnet Details页面，填写如下信息，然后点击Create按钮。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Y5hLGzsY01JK.png?imageslim" alt="mark"></p><p>由于虚拟机VM实例所需要的镜像cirros在安装Glance服务时已经完成上传，因此完成上述虚拟机VM实例所需的最小资源配置后，下面就可以通过页面拉起一个虚拟机VM实例。</p><p><strong>Step1：</strong>在左侧导航栏，选择“Project &gt; Compute &gt; Instances”，进入虚拟机实例列表，单击页面右上角的“Launch Instance”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/qpUYI9Rf9VHy.png?imageslim" alt="mark"></p><p><strong>Step2</strong>：弹出发放实例对话框，在“Details”页签，输入虚拟机实例的名称“Web_Srv01”，其他保持默认，单击“Next”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/sOvk3ntEsIuS.png?imageslim" alt="mark"></p><p><strong>Step3</strong>：在“Source”页签，“Create New Volume”下方选择“No”，单击“Available” 下方列表中cirros镜像（环境安装完成后，系统默认创建的测试镜像）的上传箭头，“Allocated”下方列表中将显示选择的镜像，其他保持默认，单击“Next”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/zMyiklwup9Be.png?imageslim" alt="mark"></p><p><strong>Step4：</strong>在“Flavor”页签，单击“Available”下方列表中刚刚创建的规格“Flavor_web”的上传箭头 ，“Allocated”下方列表中将显示选择的规格，单击“Launch Instance”，完成虚拟机实例的创建。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/FA4LMLf22bUh.png?imageslim" alt="mark"></p><p><strong>Step5：</strong>返回虚拟机实例列表，查看创建的虚拟机实例的状态，等待状态变为“Active”，表示虚拟机VM实例启动成功。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/uu2NNbXOLy5I.png?imageslim" alt="mark"></p><p><strong>Step6：</strong>单击虚拟机实例名称“Web_Srv01”，进入虚拟机实例概览页面，可查看虚拟机实例的详细信息，如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Y7T4WkIbKHUc.png?imageslim" alt="mark"></p><p><strong>Step7：</strong>单击“Console”页签，可进入虚拟机实例的终端页面，如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5NE2aPIKMUyW.png?imageslim" alt="mark"></p><p>以上，就是通过Horizon服务利用Web页面可视化创建一个虚拟机VM实例的全过程。通过上图中Interface页签，log页签还可以查看虚拟机VM实例的网卡信息及操作日志。同时，Horizon页面不仅用来创建虚机，还可以创建Domain、Project、User、UserGroup、Network、Glance、Volume等资源，在后续各服务的实操环节会有展示。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a><strong>思考</strong></h2><p><strong>1、注销admin用户，通过普通用户登录后，观察Project的Overview视图与admin用户下的Overview视图的区别，并分析之间产生差异的原因？</strong></p><p><strong>2、尝试通过命令行CLI的方式完成上述虚拟机VM实例创建的操作。**</strong>（提示：命令行的用法可以通过openstack &lt;资源类型名&gt; &lt;动作&gt; –help方式查看帮助获得，比如：虚机资源类型名为server，网络资源类型名为network，子网资源类型名为subnet，镜像资源类型名为image，规格资源类型名为flavor等等）**</p><p><strong>3、如果在创建虚拟机VM实例的过程中，选择镜像时要求创建卷，那么所创建的VM实例的系统盘由哪个服务创建？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;OpenStack的用户界面由两部分组成：一是Web界面，二是Shell CLI界面。Horizon负责展现Web仪表盘，用户可以通过浏览器直接操作、管理、运维OpenStack的一些功能。由于OpenStack项目队伍不断壮大，Danshboard并不能展现所有的OpenStack功能，因此，最新的功能一般会先开发Shell命令行，也就是将CLI（Command Line Interface）提供给Linux用户操作。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-中间件分布式缓存Memcache和Redis</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98Memcache%E5%92%8CRedis/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-中间件分布式缓存Memcache和Redis/</id>
    <published>2020-03-03T09:00:03.000Z</published>
    <updated>2020-03-04T00:53:06.366Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>缓存系统在OpenStack集群部署中有着非常重要的应用，在开源OpenStack的解决方案中一共使用了两种分布式缓存技术，一种是用于前端API服务访问的Memcache缓存，另一种就是用于后端计量和告警信息上报的Redis缓存。无论是Memcache缓存还是Redis缓存，均是一种分布式缓存。所谓分布式缓存，就是指缓存服务可以部署多个相互独立的服务器节点上，可以彼此分散独立存取数据，减少单节点的数据存取压力，但是各节点之间的数据并非要求一定保持一致性。大多数的OpenStack服务都会使用Memcache或Redis缓存系统存储如Token等临时数据，缓存技术的应用场景一般都是应用在有大并发访问需求的地方，比如使用Web服务的门户网站，淘宝、京东等购物网站等等。<strong>缓存系统可以认为是基于内存的数据库</strong>，相对于后端大型生产数据库MySQL等，基于内存的缓存系统能够提供快速的数据访问操作，从而提高客户端的数据请求访问，并降低后端数据库的访问压力，避免了访问重复数据时数据库查询所带来的频繁磁盘IO和大型关系表查询时的时间开销。<a id="more"></a></p><h2 id="Memcache分布式缓存"><a href="#Memcache分布式缓存" class="headerlink" title="Memcache分布式缓存"></a><strong>Memcache分布式缓存</strong></h2><p>Memcache利用系统内存对客户端经常进行反复读写和访问的数据进行缓存，来减轻后端数据库的访问负载和提高客户端的数据访问效率。Memcache中缓存的数据经过HASH之后被存放到位于内存上的HASH表内，而HASH表中的数据以Key-Value的形式存放。在Memcache集群中，Memcache客户端的API通过32Bit的循环冗余校验码（CRC-32）将存储数据的键值对进行HASH计算后，存储到不同的Memcached节点服务器上，而当Memcached服务器节点的物理内存剩余空间不足时，Memcached将使用最近<strong>最少使用算法（LRU，LeastRecentlyUsed）</strong>对最近不活跃的数据进行清理，从而腾出多余的内存空间供缓存服务使用。在OpenStack的集群部署中，大部分子项目都会用到内存缓存系统进行客户端访问数据的缓存，从而提高访问速率并减轻数据库的负载压力。</p><p>Memcache缓存系统在工作流程的设计上比较简单，其<strong>主要思想就是Memcache的客户端根据用户访问请求中的Key，到Memcached服务器的内存HASH表中获取对应此Key的Value，Memcache客户端取到值之后直接返回给用户，而不用再到数据库中进行数据查询。当然，内存HASH表中不可能预知和存储全部客户端需要的Key-Value值对，因此，如果在Memcache服务器中没有找到与请求中的Key对应的Value，则转向后端数据库进行查询，并将查询到的Value与Key进行HASH后存入Memcached服务器中，从而保证曾经存储过的数据一定能被查询到，并将数据库中查询到的数据缓存到Memcache服务器中，从而提高下一次缓存查询的命中率，</strong>其工作流程如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/I9ekUo8D42DH.png?imageslim" alt="mark"></p><p>上述流程中，Memcache的整个工作流程被分为七步实现，但是并非每次查询都需要经历七个步骤，这需要根据是否在缓存中命中请求Key对应的Value来决定。具体流程如下：</p><p><strong>1~2：</strong>用户终端向Web服务器发起请求（Web服务器中部署有Memcache客户端），Web服务器中的Memcache客户端向Memcached服务器发起查询请求，查询目标为用户请求中包含的Key所对应的Value。</p><p><strong>1~2~3~7：</strong>如果Memcache客户端在Memcached服务器中查询到与请求Key对应的Value，则直接返回结果，本次数据查询过程结束，即整个过程不会访问数据库。</p><p><strong>1~2~4~5~7~6：</strong>如果经过步骤1和2没有查询到数据，则表明Memcached服务器没有缓存请求中的Key对应的Value，即本次缓存查询未命中，但是查询到此并不结束，而是转向数据库中继续查询该值，并将查询到的数据返回给客户端，同时将该数据缓存到Memcached服务器的HASH表中，以便客户端再次发起相同请求时，该值在Memcached服务器中能够直接命中，而无需再次查询数据库。</p><p><strong>需要注意的是：</strong>当数据库中存放的永久数据发生更新时，Memcached服务器中缓存的值也需要同时更新。如果Memcached服务器中分配给Memcached使用的内存空间耗尽，则Memcache缓存系统将使用LRU算法和数据的到期失效策略清理非活跃的冷数据，在这个过程中，已达到策略设置中的保存期限的数据将会首先被清除，然后再清除最近最少使用的数据。</p><p><strong>由于Memcache缓存系统中的每个Memcached服务器节点独立存取数据，彼此之间不存在数据的镜像同步机制</strong>，因此，如果某一个Memcached服务节点故障或者重启，则该Memcached节点上缓存在内存中的全部数据均会丢失，丢失缓存数据后，访问将无法在缓存中命中任何Key，所有Key的访问都需要到数据库重新查询一遍，同时将查询的数据再缓存到Memcached服务器的内存缓存中，即Memcached服务器每重启一次，其储存的数据就要被重新缓存一次。同时，Memcache缓存系统以Key-Value为单元进行数据存储，能够存储的数据Key尺寸大小为250字节，能够存储的Value尺寸大小为1MB，超过这个值不允许存储。并且Memcache缓存系统的内存存储单元是按照Chunk来分配的，这意味着不可能所有存储的Value数据大小都正好等于一个Chunk的大小，因此必然会造成内存数据碎片而浪费存储空间。</p><p>Memcache集群的配置极为简单，服务器端只需运行Memcached服务，客户端只需在应用程序的配置文件中指定要访问的Memcached服务器节点IP地址和端口组成的列表。因为Memcache集群无需在Memcached服务器端进行配置，数据的分布存储完全取决于Memcache客户端的节点选取算法。如下图所示，当应用程序向Memcache集群存储数据时，Memcache客户端会根据请求存储数据的Key-Value值对，利用一定算法从Memcached集群存储节点中选取某一个Memcached服务器节点来存储数据，当算法选定该节点后，对应Key的Value就会被存储到该节点中。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Idtaxk8yWcu6.png?imageslim" alt="mark"></p><p>Memcache客户端在选取缓存节点时所采用的算法对数据的最终存储位置有着决定性的作用，而节点选取算法也是Memcache缓存系统中最重要的部分。在通常使用时，Memcache缓存系统中最常使用的两个算法分别是<strong>余数HASH算法</strong>和<strong>一致性HASH算法</strong>。</p><p><strong>余数HASH算法就是将需要存储的Key-Value值对的Key字符通过HASH算法后得到HASH Code，这里的HASH Code是一个数字，而HASH算法的作用就是将一个字符串通过HASH后得到一个数值。通常，相同的字符串经过相同的HASH算法后一定会得到相同的HASH Code，因此，存储和提取相同Key的Memcached目标节点一定是同一个节点。</strong>比如Key经过HASH之后得到的HASH Code为100，Memcache集群缓存系统中有3个Memcached节点，那么余数HASH就是将HASH Code 100除以节点数目3，然后取余得到的结果，这里结果为1，这样选取到的Memcached服务器节点为编号为1，即Memcached1节点。因为不同的Key值得到的HASH Code是不同的，这样取余后得到的结果是比较随机均匀的，数据就会随机存储到不同的Memcached服务器节点上，因此，如果在一个系统架构中，Memcached服务器节点数目比较固定，使用过程中无需扩容节点，则余数HASH算法可以很好地满足Memcache集群中的节点选取过程。然后，当需要扩容节点时，比如当增加3个Memcached节点后，现在Memcached服务器节点数目为6，HASH Code仍然为100，可余数HASH的结果却为2，而不是之前的1，即Memcache客户端在提取这个Key的值Value时，算法将会路由到Memcached2节点去，但是扩容之前该Key的Value是存储在Memcached1节点中的，因此客户端将不能在缓存系统命中此Key。</p><p>为了解决这种扩容引起的缓存命中失效问题，引入一致性HASH算法。其实，引入一致性HASH并不能完全解决缓存命中失效的问题，但是可以大大降低缓存命中失效发生的概率，而且随着缓存节点数量逐渐增多，这种缓存命中失效带来的IO瓶颈问题几乎可以忽略不计。一致性HASH通过将Memcached节点和Key字符串HASH后放到一个HASH环上，然后Key字符串的HASH值再以顺时针的方式找到距其最近的节点HASH值，该节点HASH值对应的节点便是Key的Value存储节点（一致性HASH在分布式存储部分会详细讲解，这里简单提一下）。当集群中增加缓存节点，只有HASH Code靠近增加节点HASH值的数据会受影响，其余节点仍保持原来的缓存映射关系，所以一致性HASH相比余数HASH在节点变化的场景下，缓存命中失效的概率要低很多。对于Memcache缓存系统而言，究竟选择余数HASH还是一致性HASH，需要根据Memcache缓存系统的实际使用情况而定。</p><p>如前所述，Memcache集群只能解决数据访问的瓶颈问题，并不能解决数据的高可用问题。如果要<strong>实现Memcache集群的数据HA，就必须借助第三方工具来实现。Memcache缓存系统最常使用的HA方式是Memcached节点代理，其中最常使用的代理软件为Magent</strong>。通过Magent缓存代理，可以防止缓存系统的单点故障，同时将缓存代理服务器配置为主备模式，即可实现代理服务器的高可用。在配置有Magent代理的Memcache缓存系统中，当客户端发起缓存数据请求时，客户端将首先连接到缓存代理服务器，然后缓存代理服务器再连接到Memcached服务器，Magent缓存代理服务器可以连接多台后端Memcached服务器，进而可以将相同的缓存数据同时存储到多个Memcached服务器，最终实现缓存数据的同步存储。如下图所示，实现Memcache服务的数据HA非常简单，只需在两台同样配置的服务器上分别安装memcache和magent软件，稍做简单配置即可实现Magent代理的高可用缓存系统。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/V9JQA8Gipqj2.png?imageslim" alt="mark"></p><h2 id="Redis分布式缓存"><a href="#Redis分布式缓存" class="headerlink" title="Redis分布式缓存"></a><strong>Redis分布式缓存</strong></h2><p>Redis，俗称远程字典服务器（Remote Dictionary Server，Redis），是一款由ANSI C语言编写、遵守BSD协议、支持网络访问、基于系统内存的开源软件，同时，Redis也是支持日志型和Key-Value键值对类型数据持久化的数据库。由于Redis中存储的Value值类型可以是字符串（String）、哈希（HASH）、列表（List）、集合（Sets）和有序集合（Sorted Sets）等类型，因此<strong>Redis通常也被称为数据结构服务器</strong>。在Redis中，这些数据类型都支持Push/Pop、Add/Remove以及针对集合的交集、并集和差集等丰富的数据操作，并且这些操作都具有保持数据一致性的原子性，而且Redis还支持各种不同的数据排序方式。</p><p>与Memcache缓存系统类似，为了保证高效的数据存取效率，Redis的数据都被缓存在内存中，但是Redis与Memcache在处理数据存储方式上有很大的差异，即Redis默认会周期性地把缓存中更新的数据写入磁盘进行永久保存，并将对缓存数据的修改操作追加到日志文件中，由于Redis本质上就是一个基于内存的数据库，因此Redis具备很多如日志记录等通用数据库的共性。<strong>Redis为了实现数据高可用，其利用已存入磁盘中的数据在不同节点之间进行同步操作，从而实现了Master-Slave模式的主从数据高可用，Redis的数据可以从Master服务器向任意数量的Slave服务器上进行同步。</strong></p><p><strong>在Redis缓存系统中，并非所有通过Redis客户端写入的数据都永久性地存储在物理内存中，这是Redis和Memcache缓存系统最为明显的区别，Redis会定期性地将物理内存中的数据写入磁盘，其原理类似内存虚拟化技术中内存置换技术。Redis实现了用虚拟内存（VM，Virtual Memory）机制来扩充逻辑可用的内存空间，当Redis认为物理内存空间使用量达到某个预设值时，Redis将自动进行内存页的交换（Page Swap）操作，从而将缓存中的数据写入磁盘中。</strong></p><p><strong>Redis的VM机制使得Redis可以存储超过系统本身物理内存大小的数据，但是，这并不意味着可以无限减小物理内存并不断增加虚拟内存以满足Redis的整体内存需求</strong>，主要有以下3点约束：</p><p>1）Redis将全部的Key保存在物理内存中，从而可以快速命中Key，并且由于Redis在读取数据时只在物理内存中进行Key的索引，找到Key之后再到物理内存或者虚拟内存中读取对应的Value，因此为Redis预留的物理内存空间至少要满足Key的存储。</p><p>2）过度地减小为Redis预留的物理内存空间，会使得Redis频繁发生Swap操作，极大降低Redis的数据稳定性，并对Redis的数据访问造成响应缓慢等影响。</p><p>3）在Redis将内存中的数据交换到磁盘的时候，提供访问服务的Redis主线程和进行交换操作的子线程会共享这部分内存空间，所以如果此时有请求要更新这部分内存空间的数据，则Redis将会阻塞这个请求操作，直到子线程完成Swap操作后才可以进行数据的更新修改。</p><p>上述的第三点约束是一种默认情况下的机制，即Redis会使用阻塞方式来使I/O请求等待Swap操作将全部所需的Value数据块读回内存，然后才进行下一步操作。这种机制在小量访问对数据访问的影响可以忽略，但是在大并发场景下，其影响直接造成客户体验变差。因此，在大并发场景下，需要配置Redis的IO线程池来解决，使得Redis并发进行磁盘到物理内存的数据读回操作，减少IO阻塞时间。</p><p>在Redis节点重新启动之后，丢失的缓存数据会从磁盘重新加载或者从日志文件中重新恢复，与Memcache相比，天生具备数据持久化功能。<strong>针对两种不同的缓存数据恢复方式，Redis提供了两种数据持久化的方式，分别是Redis数据库方式（RDB，Redis DataBase）和日志文件方式（AOF，Append Only File）</strong>。RDB持久化方式就是将Redis缓存数据进行周期性的快照，并将其写入磁盘等永久性存储介质中，而AOF采用的是与RDB完全不一样的数据持久化方式，在AOF方式下，Redis会将所有对缓存数据库进行数据更新的指令全部记录到日志文件中，待Redis重新启动时，Redis将会把日志文件中的数据更新指令从头到尾重复执行一遍，这是一个重现数据变更的过程，当执行到日志文件末尾时即可实现缓存数据的恢复，AOF恢复方式与DB2数据库的redo日志回滚恢复方式原理类似。在实际的Redis使用过程中，RDB和AOF这两种数据持久化方式可以同时使用，并且这也是官方推荐的数据持久化方式，在两种方式同时开启的情况下，如果Redis数据库重新启动，则会优先采用AOF方式来进行数据恢复，这是因为AOF方式对数据恢复的完整性要高于RDB方式（RDB因为是周期性存储快照数据，所以数据完整性至少丢失一个快照存取周期）。</p><p>在Redis3.0版本发行之前，Redis自身并不具备集群高可用功能，尽管Redis3.0中增加了Redis的Cluster功能，但是，使用Redis Cluster功能的用户相对还是很少，其可靠性和稳定性相对来说还需要进一步验证。目前，针对Redis的集群高可用功能，更多的是借助第三方负载均衡和集群管理软件（Pacemaker+HAProxy/Keepalived）来实现或者是通过对Redis进行二次开发来实现。在借助第三方负载均衡和集群管理软件的方案中，通过负载均衡软件的VRRP协议创建虚拟IP资源以对外提供高可用的Redis虚拟IP，并通过集群管理软件创建Redis的Master-Slave多状态资源，当Redis的Master节点故障后，通过集群管理软件的资源管理功能，利用Redis的Agent脚本将Slave节点提升为Master，同时虚拟IP迁移到新的Master节点继续对外提供服务。</p><p><strong>Redis主从模式的集群能够实现数据高可用的关键，是Redis提供的数据Replication功能</strong>，即数据能够在Redis的主从节点之间进行复制，从而保证了数据的高可用性。其复制是异步复制，在复制的过程中，Master节点是非阻塞的，不会受到Slave节点数据同步的影响，在Slave进行同步的时候，Master继续提供高效的内存数据查询服务。同时，在复制的过程中，Slave节点也是非阻塞的。在数据同步时，通过配置可以允许Slave节点利用之前的数据集提供查询功能活着此时返回一个错误，并提示数据正在同步中。但是不管怎么设置，在Slave同步完成之后，Slave节点的陈旧数据将会被删除，新同步来的数据将会被加载到内存，而在这个删除与加载的过程中，Slave的访问是暂时被禁止的。Redis的Replication功能除了数据冗余之外，还可以将数据的只读查询分散到多个Slave节点，以让Slave节点来负责这类数据的排序操作。</p><p>在Redis的Master-Slave集群模式下，如果配置了一个或多个Slave节点并将其连接到Master节点，则不管Slave节点是第一次连接Master还是重新连接到Master，当连接完成之后，Slave都会发送一个同步命令给Master，Master接到命令后开始在后台进行内存数据快照，并将在快照期间对当前缓存数据进行更新的命令全部缓存起来。当Master上的数据以RDB方式保存完成后，Master将该RDB数据文件传送给Slave，Slave将数据文件写入本地磁盘，然后再将这些来自Master的RDB数据文件加载到内存中，之后Master将数据快照期间缓存的命令发送给Slave，Slave依次执行这些数据变更命令，使得每次同步操作后都能确保自身数据与Master节点缓存数据的一致性。当Slave与Master的连接因某些原因断开后，Slave会自动与Master重新建立连接。如果Master同时收到多个Slave发送的数据同步请求，则Master仅会对缓存数据做一次快照保存，然后将保存的RDB数据文件发送给多个Slave节点。当同步中的Slave与Master断开又重新连接后，Slave将会重新发送同步全部数据的请求，但是从Redis2.8开始，重新连接的Slave可以通过断点续传功能仅同步上次未完成同步的数据请求，其机制是Master和所有的Slave将同步操作的日志记录到内存文件中，同时还记录了数据同步的偏移量和Master此时的运行标志（Run ID），如果数据同步过程被断开，Slave将会重新连接并请求Master继续同步。此时，如果Master的Run ID还没有改变，并且设置的同步偏移量在同步日志中可用，那么同步操作将会从中断点继续同步，如果不能满足这两个条件，则数据同步操作只能重新开始。此外，如果Master的Run ID没有保存到磁盘上，则无法进行续传同步。如果Master节点的磁盘I/O性能较差，而不同的Slave节点不在同一时间点发生同步请求，则数据同步过程必然增加Master的工作负载。为了解决这个问题，<strong>Redis2.8.18中提出了无盘复制（Diskless Replication）的设计</strong>，在这种模式下，Master直接将RDB文件传送到Slave节点，而无须再在本机上进行I/O操作，无盘复制技术的提出极大提升了Master在数据同步复制过程中的响应效率。</p><h2 id="Redis-Cluster集群技术"><a href="#Redis-Cluster集群技术" class="headerlink" title="Redis Cluster集群技术"></a><strong>Redis Cluster集群技术</strong></h2><p>上面提到Redis Cluster是Redis官方在3.0版本以后推出一种集群技术，作为官方的重点推荐，虽然其可靠性和稳定性还有待验证，但是不影响我们理解其工作原理和机制。Redis Cluster是Redis的分布式实现，RedisCluster在设计之初就考虑到了去中心化的问题，因此集群中不存在任何中心控制节点，每一个节点都是功能对等的集群成员，每个节点除了保存有自己的配置元数据外，还保存着整个集群的状态信息。由于每个集群节点之间彼此互联，并且随时保持活动连接，客户端只需连接到集群中的任一个节点，便可获取到存储在其他节点上的数据。机制示意图如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/m6sxxegXy5o3.png?imageslim" alt="mark"></p><p>Redis Cluster中的各个节点之间保持相互连接，并且彼此之间可以通信，客户端随意连接到任何一个集群节点就能将整个Redis集群作为一个整体来访问，同时客户端也无需知道Redis Cluster将其提交的数据存入哪个Redis节点中，数据存储完全由Redis Cluster根据自己的算法来决定。在Redis Cluster中，数据被分散存储到不同的Redis节点上，Redis Cluster并没有采用一致性HASH来分配数据存放到不同的节点，而是采用了一种称为<strong>哈希槽（HASH Slot）</strong>的技术来分配数据。Redis Cluster默认将存储空间分配为16384 Slots，每个节点承担其中的一部分Slots。如果Redis需要扩充Redis节点数，只需重新分配每个节点的槽位。当客户端通过SET命令来保存一个Key-Value键值的时候，Redis Cluster<strong>会采用CRC16（Key）对16384取模来决定该Key应该被存储到哪个Slots中</strong>，具体的计算公式就为：<strong>slot_num=CRC（’Key_name’）％16384</strong>。假设通过公式计算得到的Slot_num为14201，则数据会被存储在14201槽位对应的节点上。为了保证数据的高可用，Redis Cluster用到了Master-Slave的主从数据复制模式，即为每个Master都设置一个或多个Slave，Master负责数据存取，而Slave负责数据的同步复制，当Master故障时，其中的某个Slave会被提升为Master。假设集群中有A、B、C三个主节点，同时分别为其设置了A1、B1、C1三个Slave节点，此时如果B节点故障，则集群会将B1节点选取为Master节点继续提供服务，当B节点恢复之后，它就变成了B1的Slave节点。当然，如果B和B1同时故障，则集群B就不可用了。</p><h2 id="Redis在OpenStack中的应用"><a href="#Redis在OpenStack中的应用" class="headerlink" title="Redis在OpenStack中的应用"></a><strong>Redis在OpenStack中的应用</strong></h2><p>在OpenStack开源云的计费项目Ceilometer中，当进行规模化集群部署时，通常使用Redis插件来为运行在控制节点集群上的多个Ceilometer Agents提供协调机制，Redis插件使用具有Redis后端的Tooz库来为Agents提供一组轮询使用的资源集。在部署了Redis插件之后，Ceilometer Agents的部署可以分布到每个控制节点上，并且这些分布的Agents将会自动加入协调组（Coordination Group）中，在这种分布式集群部署中，通常使用Pacemaker创建Redis-server资源以监控Redis插件进程的运行状态，同时插件会自动配置Redis-sentinel进程来监控Redis集群状态，而Redis-sentinel的主要作用是当Redis集群的Master节点故障时，通过一定的机制重新选取新的Master节点，同时将Ceilometer的代理重新定向到新的Master节点，并进行Redis集群节点之间的数据同步。</p><p>在OpenStack集群高可用部署中，Redis被用作Ceilometer组件的分布式协调组后端缓存，同时集合集群资源管理软件Pacemaker，将Redis配置为Pacemaker的资源以实现Redis-server的高可用Master-Slave模式。此外，还需要为Redis集群配置一个高可用IP地址，高可用IP可以通过Pacemaker的虚拟IP资源实现。在三节点的OpenStack控制节点集群架构中，将其中一个控制节点作为Redis的Master节点，另外两个控制节点作为Redis的Slave节点，Redis和Ceilometer的Central agent在三个控制节点中的部署拓扑如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/eiMCG194h29S.png?imageslim" alt="mark"></p><p>在集成Pacemaker的集群中，Redis服务被配置成为Pacemaker的多状态资源，即Master/Salve资源组。通过Pacemaker配置的Redis高可用Master/Slave集群中，controller01为Redis的Master节点，controller02和controller03为Redis的Slave节点，同时Redis对外提供服务的虚拟IP地址运行在Master节点，这是通过Pacemaker的Colocation约束设置的，即<strong>Redis的VIP只会运行在Master节点上</strong>。如果Redis的Master节点故障，则Pacemaker将会从两个Slave节点中重新选举一个Mastre节点，而VIP也会相应地自动移动。</p><p>总的来说，memcache缓存技术主要用来存储见到key-value格式的数据，如OpenStack各个服务的API交互数据。如果需要缓存数据类型多样、数据可靠性高的场景，如OpenStack的ceilometer服务提供的计量和告警等数据，则采用Redis这种支持数据持久化且支持多种数据结构的缓存技术更加合适。</p><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、Memcache技术的服务端口号是多少？在OpenStack的最小系统配置下（keystone、nova、neutron、cinder、glance、horizon），控制节点最少需要开启多少个memcache服务进程？</strong></p><p><strong>2、nova服务至少需要开启多少个memcache服务进程？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;缓存系统在OpenStack集群部署中有着非常重要的应用，在开源OpenStack的解决方案中一共使用了两种分布式缓存技术，一种是用于前端API服务访问的Memcache缓存，另一种就是用于后端计量和告警信息上报的Redis缓存。无论是Memcache缓存还是Redis缓存，均是一种分布式缓存。所谓分布式缓存，就是指缓存服务可以部署多个相互独立的服务器节点上，可以彼此分散独立存取数据，减少单节点的数据存取压力，但是各节点之间的数据并非要求一定保持一致性。大多数的OpenStack服务都会使用Memcache或Redis缓存系统存储如Token等临时数据，缓存技术的应用场景一般都是应用在有大并发访问需求的地方，比如使用Web服务的门户网站，淘宝、京东等购物网站等等。&lt;strong&gt;缓存系统可以认为是基于内存的数据库&lt;/strong&gt;，相对于后端大型生产数据库MySQL等，基于内存的缓存系统能够提供快速的数据访问操作，从而提高客户端的数据请求访问，并降低后端数据库的访问压力，避免了访问重复数据时数据库查询所带来的频繁磁盘IO和大型关系表查询时的时间开销。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-02-中间件消息队列服务RabbitMQ</title>
    <link href="https://kkutysllb.cn/2020/03/02/2020-03-02-%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%9C%8D%E5%8A%A1RabbitMQ/"/>
    <id>https://kkutysllb.cn/2020/03/02/2020-03-02-中间件消息队列服务RabbitMQ/</id>
    <published>2020-03-01T16:02:05.000Z</published>
    <updated>2020-03-01T16:17:30.646Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>OpenStack是由Nova、Neutron、Cinder等多个组件构成的开源云计算项目，各组件之间通过REST接口进相互通信和彼此调用，而<strong>组件之间的REST接口调用，是建立在基于高级消息队列协议（AdvancedMessageQueueProtocol，AMQP）上的RPC通信</strong>。在OpenStack中，AMQP Broker（可以是Qpid Broker，也可以是RabbitMQ Broker）位于OpenStack的任意两个内部组件之间，并使得OpenStack内部组件<strong>以松耦合的方式进行通信</strong>。纵观OpenStack的组件架构设计，其中的消息队列在OpenStack全局架构中扮演着至关重要的组件通信作用，也正是因为基于消息队列的分布式通信技术，才使得OpenStack的部署具有灵活、模块松耦合、架构扁平化和功能节点弹性扩展等特性，所以消息队列在OpenStack的架构设计和实现中扮演着极为核心的消息传递作用。同时，消息队列系统的消息收发性能和消息队列的高可用性也将直接影响OpenStack的集群性能。<a id="more"></a></p><p>在OpenStack的部署过程中，用户可以选择不同的消息队列系统来为OpenStack提供消息服务，但是在众多的消息队列系统中，RabbitMQ是使用最多也是综合性能最接近生产系统要求的消息队列系统。华为FusionSPhere OpenStack的解决方案中，也是采用的RabbitMQ作为消息队列系统。</p><h2 id="AMQP-高级消息队列协议"><a href="#AMQP-高级消息队列协议" class="headerlink" title="AMQP-高级消息队列协议"></a><strong>AMQP-高级消息队列协议</strong></h2><p>AMQP是应用层协议的一个开放标准，专为面向消息的中间件而设计，同时，AMQP也是面向消息、队列、路由、可靠性和安全性而设计的高级消息队列系统。AMQP提供统一消息服务的应用层标准协议，客户端与消息中间件基于此协议传递消息，并且消息传递不受不同客户端/中间件产品和不同开发语言等条件的限制。在消息的传递过程中，消息中间件主要用于组件之间的<strong>解耦</strong>，即<strong>消息生产者不用关系消息由谁来消费，消息消费者也不用关系消息由谁产生</strong>。</p><p>AMQP协议本质上是一种二进制协议，可以让客户端应用与消息中间件之间异步、安全、高效地交互。该协议框架也是分层结构，总体上分为三层，分别是：<strong>Model层、Seesion层</strong>和<strong>Transport层</strong>，消息在Session层与Transport层之间传递，用于执行Model层的Command，本质上就是二进制的封装和解封装过程。</p><ul><li><strong>Model层：</strong>决定了基本域模型所产生的行为，这种行为在AMQP中用“Command”表示。</li><li><strong>Session层：</strong>定义客户端与Broker之间的通信，通信双方都是一个Peer，可互称作Partner，会话层为模型层的Command提供可靠的传输保障。</li><li><strong>Transport层：</strong>专注于数据传送，并与Session保持交互，接受上层的数据并组装成二进制流，数据传送到Receiver后再进行解析并交付给Session层。Session层需要Transport层完成网络异常情况的汇报，顺序传送Command等工作。</li></ul><h2 id="RabbitMQ中的基本概念"><a href="#RabbitMQ中的基本概念" class="headerlink" title="RabbitMQ中的基本概念"></a><strong>RabbitMQ中的基本概念</strong></h2><p>RabbitMQ用Erlang语言开发，是AMQP的开源实现。在RabbitMQ的部署使用和故障排除过程中，将会接触到很多RabbitMQ的基础概念，了解和掌握这些概念，是使用RabbitMQ为集群系统提供消息服务的基础。</p><h3 id="Connection和Channel"><a href="#Connection和Channel" class="headerlink" title="Connection和Channel"></a><strong>Connection和Channel</strong></h3><p><strong>ConnectionFactory、Connection和Channel都是RabbitMQ对外提供的API中最基本的对象。</strong>Connection是RabbitMQ的Socket连接，本质上是一个TCP协议连接，消息的生产者Producer和消费者Consumer都通过Connection建立的TCP链接连接到RabbitMQ Server，因此RabbitMQ服务启动时的初始化过程就是创建一个Connection的连接，而ConnnectionFactory是Connection对象的工厂函数，用来初始化Connection对象。Channel是客户端与RabbitMQ交互消息最重要的一个接口，客户端大部分的业务操作是在Channel这个API接口中完成的，包括定义Queue和Exchange、绑定Queue与Exchange、发布消息等操作。Channel本质上是一个虚拟连接，它建立在TCP连接中，数据流动都是在Channel中进行，通常，程序启动后首先建立TCP连接，接着就是建立Channel对象。RabbitMQ使用Channel而不直接使用TCP，就是因为TCP的建立和拆除系统开销过大，容易引起消息队列系统瓶颈。</p><h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a><strong>Queue</strong></h3><p>Queue是RabbitMQ的内部对象，用于存储消息，Queue可以看成是存储消息的容器。消息的生产者与消费者之间可以是一对多或者多对多的关系，即多个消费者可以订阅同一个Queue，这时，<strong>Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理</strong>。如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/TuHomnveYICk.png?imageslim" alt="mark"></p><h3 id="Message-Acknowledgment"><a href="#Message-Acknowledgment" class="headerlink" title="Message Acknowledgment"></a><strong>Message Acknowledgment</strong></h3><p>RabbitMQ会要求消费者在处理完消息后发送一个回执应答给RabbitMQ Broker，RabbitMQ收到消息回执（Message-Acknowledgment）后才将该消息从Queue中移除。如果RabbitMQ没有收到回执并检测到消费者与RabbitMQ Broker的连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）继续处理。<strong>RabbitMQ的消息处理过程中并不存在Timeout的概念</strong>，即一个消费者处理消息所花的时间再长，只要其连接还存在，RabbitMQ就不会将该消息发送给其他消费者。这种情况会产生另外一个问题，如果开发人员在处理完业务逻辑后，忘记发送回执给RabbitMQ，这将会导致Queue中堆积的消息越来越多，而消费者重新连接到RabbitMQ后会重复消费这些消息并重复执行业务逻辑。因此，<strong>在处理完业务逻辑后一定要向RabbitMQ发送应答回执，否则会造成应用系统存在重大BUG</strong>。</p><h3 id="Message-Durability"><a href="#Message-Durability" class="headerlink" title="Message Durability"></a><strong>Message Durability</strong></h3><p>如果希望即使在RabbitMQ服务重启的情况下，消息也不会丢失，则可以<strong>将Queue与Message均做持久化设置，即MessageDurability</strong>，这样便可保证绝大部分情况下RabbitMQ的消息不会丢失。但是，消息持久化方法依然解决不了小概率丢失消息事件的发生，如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了，如果需要解决这种小概率事件下的消息丢失情况，那么就要用到事务RabbitMQ的高级功能或者将RabbitMQ部署为高可用集群。</p><h3 id="Prefetch-Count"><a href="#Prefetch-Count" class="headerlink" title="Prefetch Count"></a><strong>Prefetch Count</strong></h3><p>如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平分给多个消费者。这时如果每个消费者处理消息的时间不同，就有可能会导致某些消费者一直处于繁忙状态，而另外一些消费者因为处理能力较强而很快就处理完并一直处于空闲状态。要解决这一情况以提高整个消息系统的消息处理效率，可以设置预获取数目（PrefetchCount）来限制Queue每次发送给某个消费者的消息数，默认PrefetchCount=1，则Queue每次给每个消费者发送一条消息，消费者处理完这条消息后Queue会再给该消费者发送下一条消息。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/q4PhV0IJedYz.png?imageslim" alt="mark"></p><h3 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a><strong>Exchange</strong></h3><p>消息投递过程实际上是生产者将消息发送到Exchange，由Exchange将符合转发规则的消息路由到一个或多个Queue中，而将不符合规则的消息直接丢弃。从功能实现上来看，Exchange的功能就像一个路由器，符合路由规则的数据则转发到对应的目标地址，其他数据则被拒绝或者丢弃。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/1uuidxvSy3jq.png?imageslim" alt="mark"></p><h3 id="Routing-Key"><a href="#Routing-Key" class="headerlink" title="Routing Key"></a><strong>Routing Key</strong></h3><p>RoutingKey定义了消息的路由规则，但是，RoutingKey需要与ExchangeType和BindingKey共同使用才能最终决定消息投递到哪个队列中。在实际使用中，ExchangeType与BindingKey通常为预先设定值，而消息生产者在发送消息给Exchange时，需为消息设定相应的RoutingKey，便可决定消息应该投递到哪个Queue。通常，<strong>RabbitMQ为RoutingKey设定的长度限制为255字节。</strong></p><h3 id="Binding和BindingKey"><a href="#Binding和BindingKey" class="headerlink" title="Binding和BindingKey"></a><strong>Binding和BindingKey</strong></h3><p>Binding是RabbitMQ中将Exchange与Queue关联起来的操作，绑定的过程需要用到消息的RoutingKey和绑定自身的BindingKey。BindingKey代表了Queue与Exchange之间的对应关系。消息生产者将消息发送给Exchange时，通常会给消息指定一个RoutingKey。在Exchange中，如果BindingKey与RoutingKey相匹配，则带有该RoutingKey的消息将会被路由到BindingKey所对应的Queue中，即实现了消息到特定队列的投递过程。但是，消息到队列的投递另一个决定因素就是Exchange Type，投递过程会因Exchange Type的不同而有所不同。根据Exchange Type类型不同，在绑定多个Queue到同一个Exchange的时候，Binding操作允许使用相同的BindingKey，也可以不使用BindingKey。比如：在Fanout类型的Exchange中，绑定操作就不会用到BindingKey，而是将消息路由到所有绑定到该Exchange的Queue中。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/4z0qQ8QJccq4.png?imageslim" alt="mark"></p><h3 id="Exchange-Type"><a href="#Exchange-Type" class="headerlink" title="Exchange Type"></a><strong>Exchange Type</strong></h3><p>RabbitMQ使用不同的交换器类型来将不同的消息投递到特定的队列中，不同类型的交换器使用不同的方式进行消息投递，常用的ExchangeType有<strong>Fanout、Direct、Topic</strong>和<strong>Headers</strong>四种。</p><ul><li><strong>Fanout类型：</strong>会把所有发送到该Exchange的消息直接投递到所有与它绑定的Queue中，其功能就像路由交换中的广播，主要作用就是将多个Queue绑定到同一个Exchange中，从而实现消息生产者与队列之间一对多的对应关系。在Fanout类型的Exchange中，消息到队列的投递过程并不依赖消息的RoutingKey和Binding的BindingKey，Exchange仅起到消息传递的作用。</li><li><strong>Direct类型：</strong>Direct仅把RoutingKey与Binding的BindingKey匹配的消息路由到对应的Queue中，其功能就像路由交换中的点到点路由，主要作用就是实现消息的精确匹配投递。</li><li><p><strong>Topic类型：</strong>其与Direct类型的Exchage相似，也是将消息路由到BindingKey与RoutingKey相匹配的Queue中，但Topic采用的匹配规并非完全匹配，而是更加灵活的通配符模式匹配，Topic匹配规则具有如下限定：</p></li><li><ol><li>RoutingKey由点号“.”分隔的字符串组成（通常将句点号“.”分隔开的每一段独立字符串称为一个单词），如值为“stock.usd.nyse”的RoutingKey，其有三个单词组成，而值为“nyse.vmw”的RoutingKey，则由两个单词组成。</li><li>BindingKey与RoutingKey一样也是由句点号“.”分隔的字符串。</li><li>BindingKey中可以包含两种特殊字符“<em>”与“#”，用于做模糊匹配，其中“</em>”用于匹配一个单词，“#”用于匹配零个或多个单词，但是，BindingKey中的匹配最小单位为第一条约束中定义的单词，而不是常见的字母匹配。</li></ol></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/Pqx1RBsCsYSp.png?imageslim" alt="mark"></p><p>如上图所示，RoutingKey为“quick.orange.rabbit”的消息会被Exchange同时路由到Q1与Q2队列，因为根据BindingKey的匹配规则，Q1与Q2的BindingKey均与此RoutingKey匹配。RoutingKey为“lazy.brown.fox”的消息只会被路由到Q2队列，因为BindingKey中的“#”字符匹配零个或多个单词，只有Q2中的BindingKey匹配此消息的RoutingKey。同样，根据匹配规则，RoutingKey为“lazy.orange.fox”的消息只会被路由到Q1队列。RoutingKey为“lazy.pink.rabbit”的消息尽管与Q2的两个BindingKey都匹配，但是此消息只会投递给Q2一次。当消息的RoutingKey为“quick.brown.fox”、“orange”或“quick.orange.male.rabbit”时，由于没有任何匹配的BindingKey规则，这些消息将会被Exchange丢弃。</p><p><strong>Topic是一种推送-订阅（Pub-Sub，Publish-Subscribe）模式的模糊路由匹配，由于Topic类型的Exchange具有灵活自动的匹配模式，其在OpenStack的应用场景中是使用最多的Exchange类型。</strong></p><ul><li><strong>Headers类型：</strong>Headers类型的Exchange不依赖于RoutingKey和BindingKey的匹配规则来路由消息，而是根据消息内容中的Headers属性来进行Queue选择。在Headers类型的Exchange中，在绑定Queue与Exchange时通常会设定一组Key-Value键值对，而当消息发送到Exchange时，RabbitMQ会提取此消息的Headers值（其值也是一个Key-Value键值对），并对比其中的键值对是否完全匹配Queue与Exchange绑定时设定的键值对，如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。该类型交换机并不常见。</li></ul><h3 id="Remote-Procedure-Call"><a href="#Remote-Procedure-Call" class="headerlink" title="Remote Procedure Call"></a><strong>Remote Procedure Call</strong></h3><p>在实际的应用场景中，用户很可能需要进行某些同步处理，因此需要同步等待客户端将用户发送的消息处理完成后再进行下一步处理，而这相当于远程过程调用（RPC，Remote Procedure Call），RabbitMQ也支持远程过程调用RPC。<strong>RabbitMQ中实现RPC的机制为：</strong>消息生产者在发送消息时，在消息的属性（AMQP协议中定义了14种消息属性，这些属性会随着消息一起发送）中设置两个值，分别为<strong>ReplyTo</strong>和<strong>CorrelationId</strong>。其中，ReplyTo的值是一个Queue名称，用于告诉消息的消费客户端消息处理完成后将应答消息发送到指定的这个Queue中，CorrelationId表示此次请求的标识号，客户端处理完成后需要将此属性一并返还，消息发送端将根据返回值中的这个id值来判断执行成功或失败的是已经发送出去的那条消息。消息接收客户端收到消息并处理，处理完消息后，将会生成一条应答消息到ReplyTo指定的Queue，同时带上CorrelationId属性，消息发送端之前已订阅了ReplyTo指定的Queue，因此可以从中接收客户端的应答消息，并根据其中的CorrelationId属性分析哪条请求已经被执行，然后根据执行结果进行后续的业务处理。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/qMugcYjPeKea.png?imageslim" alt="mark"></p><h2 id="RabbitMQ的工作原理和集群配置"><a href="#RabbitMQ的工作原理和集群配置" class="headerlink" title="RabbitMQ的工作原理和集群配置"></a><strong>RabbitMQ的工作原理和集群配置</strong></h2><p>RabbitMQ是AMQP的一个开源实现，其主要作用是在分布式集群中进行功能组件之间的异步消息传递，简单的描述就是消息生产者将带有RoutingKey的消息发送至交换器Exchange，Exchange使用BindingKey与Queue进行绑定，然后Exchange将RoutingKey与BindingKey进行匹配对比，并将匹配的消息投递至对应的Queue中。如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/0AXlMvj7dBGY.png?imageslim" alt="mark"></p><p>1）客户端连接至消息队列服务器Broker，在TCP连接中建立一个虚拟Channel。</p><p>2）客户端声明一个Exchange，并设置相关属性。</p><p>3）客户端声明一个Queue，并设置相关属性。</p><p>4）客户端在Exchange和Queue之间建立好绑定关系，并设置BindingKey。</p><p>5）客户端投递带有RoutingKey的消息到Exchange中。</p><p>6）Exchange接收到消息后，根据消息的RoutingKey和已经设置的BindingKey，进行消息路由，将消息投递到一个或多个队列里。</p><p>上图主要由三个环节构成，分别是<strong>发送消息的客户端、解耦消息发送端与接收端的RabbitMQServer</strong>以及<strong>消息的接收客户端</strong>。RabbitMQ Sever也称为Broker Server，其作用主要是负责消息从Producer到Consumer的传递路径，Broker接收从客户端发送过来的消息，然后转发给接收消息的客户端，Broke将发送与接收客户端进行了解耦，从而实现消息发送端与接收端的异步工作。客户端A和B是消息的发送方，即消息的生产者Producer，Producer发出的Messages由Playload和Label两部分数据组成。其中Playload是消息的主体部分，Label是消息的属性部分，属性部分包含了详细的RoutingKey，当消息到达Broker Server后，消息会被RabbitMQ根据消息属性（Label中的RoutingKey）转发到相应的队列中。客户端1、2和3是消息的接收者，即消息消费者Consumers。Broker Server根据消息的Label属性和Consumers对队列（队列已经绑定到Exchange中）的订阅（Subscribe）情况，将消息的Playload主体转发给相应的Consumers。Consumers接收到的消息是没有Label属性的，而仅有消息的主体Playload部分，此外，Consumers也不知道该消息是来自发送消息的客户端A还是B。</p><p>正常情况下，系统中成功安装RabbitMQServer程序后，用户只需启动RabbitMQ服务便可使其正常运行，即RabbitMQ使用自带的默认配置便可提供强大的异步消息传递服务。在某些情况下，用户可能希望自定义RabbitMQ服务，因此RabbitMQ提供了三种自定义配置Broker Server的方式，分别是<strong>环境变量配置方式、配置文件修改方式</strong>和<strong>运行时参数修改方式</strong>。具体的配置方式，可以根据各厂家产品解决方案的描述和RabbitMQ社区相关文档进行了解，这里不再赘述。但是，需要强调一点的是，无论采用哪种配置方式，以下几项参数的配置是必须有的：</p><ul><li><strong>RABBITMQ_NODE_IP_ADDRESS：</strong>该变量主要用于设定RabbitMQServer服务运行时监听的接口IP地址，默认为/etc/hosts配置文件中主机名对应的接口IP地址。</li><li><strong>RABBITMQ_NODE_PORT：</strong>该变量主要用于设置RabbitMQServer服务运行时监听的IP端口号，默认为系统的5672端口。</li><li><strong>RABBITMQ_NODENAME：</strong>该变量表示RabbitMQ集群的节点名称，默认为rabbit@hostname格式，其中“hostname”为当前节点的主机名，对于FQN格式的主机名，如node1.exmple.com，则RabbitMQ节点的名称为默认为rabbit@node1。</li><li><strong>RABBITMQ_USE_LONGNAME：</strong>该变量的定义与RABBITMQ_NODENAME类似，不过此变量代表的是RabbitMQ的长节点名，而RABBITMQ_NODENAME为短节点名称。在RabbitMQ集群的配置中，如果此变量设置为True，则RabbitMQ的节点名称将使用FQN全名，即<a href="mailto:rabbit@node1.exmple.com" target="_blank" rel="noopener">rabbit@node1.exmple.com</a>。</li></ul><p>RabbitMQ Broker是一个或几个运行RabbitMQ应用的Erlang节点的组合，这些节点之间共享Users、VirtualVosts、Queues、Bindings、Exchanges和运行时参数，通常把这些运行RabbitMQ服务的节点组合称为RabbitMQ的集群。在RabbitMQ集群中，RabbitMQ Broker运行所需的元数据和状态信息会自动在集群节点之间进行复制。但是，<strong>在普通的RabbitMQ集群配置中，消息队列Queues不会在多个节点之间复制</strong>，即集群Queues通常只位于集群中的某一个节点上，而其他节点虽然可以看到和访问这个节点上的消息队列，但是不会将该节点上的消息队列复制到其本地。对于普通的RabbitMQ集群模式，假设集群由A和B两个RabbitMQ节点构成，则A、B两个节点都有相同的集群元数据，但是只有A（或者B）节点持有集群消息队列，当消息进入A节点的Queues后，如果Consumer从B节点提取消息，则RabbitMQ会临时在A、B间进行消息传输，把A中的消息取出并经过B发送给Consumer。由于消息集中存放在A节点的队列中，无论Consumer从A或B提取消息，消息总要从A发出，这势必会导致A节点出现性能瓶颈。此外，这种普通集群模式当A节点故障后，B节点无法提取到A节点中还未消费的消息实体，最终因A节点的单点故障而导致整个集群消息系统不可用。解决这个问题一种办法就是将A节点的Queues持久化，尽管可以对A中的Queues做消息持久化，但在A故障后，也必须等待A节点恢复才可继续提供消息传递服务。但是，如果没有消息持久化，则即使A节点恢复，也无法恢复A中的队列消息。<strong>如果要将集群队列Queues进行镜像复制，则需要用到RabbitMQ的HeightAvailable功能。</strong></p><p>在RabbitMQ的集群中，节点通常被划分为两类，即<strong>磁盘（Disk/Disc）节点</strong>和<strong>内存（RAM）节点</strong>。而在多数情况下，集群中的节点均默认是Disk节点，内存节点主要用于具有深度队列和大量Exchanges的集群中以提高消息传递的性能。由于内存节点将消息数据保存到内存中，因此重启内存节点会导致消息丢失。所以，如果存在内存节点，则消息队列必须做持久化，在做了持久化的消息队列中，即使在内存节点上，消息也会被保存到磁盘上，因此重启内存节点也可保证不丢失消息队列。</p><p>如上所述，在默认的RabbitMQ集群中，消息队列Queue不会在集群节点之间进行复制备份，而仅位于集群中的某个节点上，通常该节点是最初声明Queues的节点。与Queues不同，Exchanges和Bindings信息会复制到集群中的每个节点上，因此在默认的RabbitMQ集群配置中，尽管集群的Exchanges和Bindings信息能够避免单点故障，但是由于Queues及其保存的Messages是中心化单点存放的，所以集群中的消息队列仍然具有单点故障而无法实现彻底的高可用，如果拥有Queues的节点发生故障，则虽然整个RabbitMQ集群可以继续提供消息服务，但是即使在消息做了持久化存储，之前位于Queues中还未被取走的消息将会丢失或者暂时不可用。</p><p>为了避免消息队列Queue的单点故障，通常的做法是将队列在节点之间进行镜像复制。在队列镜像模式下，每个镜像的队列由一个Master和一个或多个Slave提供。如果当前的Master故障，则最先成为Slave角色的节点会被选举为新的Master节点。同时，消息生产者投递到Queues中的消息会被复制到所有Slave节点上，并且不论消费者Consumers连接到哪个集群节点，其最终都是到Master节点的Queues中提取消息。又因为Master需要将消息复制到多个Slaves节点，所以<strong>队列镜像模式虽然增加了RabbitMQ集群的高可用性，但是并没有将集群的消息服务负载分散到每个集群节点中。</strong></p><p>实现RabbitMQ集群队列镜像的最主要方式就是RabbitMQ提供的<strong>Policy功能</strong>，Policy功能可以在集群运行的任何时候使用，即可以动态地将一个未镜像的集群消息队列改变为镜像队列。因此，创建镜像队列最简单有效的方式就是先创建一个非镜像的队列，然后通过Policy设置成为镜像队列。但是，刚开始设置的镜像队列与非镜像队列是有区别的：<strong>非镜像队列因为没有额外的镜像操作，所以其运行效率相对要高很多。因此，在RabbitMQ集群的镜像队列设置中，我们可以选择性地对某些队列进行镜像，而其他队列可以不用镜像。</strong>RabbitMQ设置Policy时，最关键的两个参数分别是<strong>ha-mode</strong>和<strong>ha-params</strong>，ha-params参数的值根据ha-mode的取值不同而不同，具体的组合如下：</p><ul><li>ha-mode=all，ha-params=NULL时，将队列镜像到全部集群节点上。</li><li>ha-mode=exactly，ha-params=count时，将队列镜像到count个节点上。如果实际节点数小于count，则镜像到全部节点；如果实际节点数据大于count，则节点孤战过后，重新镜像到count个节点上。</li><li>ha-mode=nodes，ha-params=node name时，将队列镜像到node name指定的节点上。</li></ul><p>示例如下：</p><ul><li>RabbitMQctl set_policy ha-all “^ha.“ ‘{“ha-mode”:”all”}’ #&lt;===将名称全部以“ha.”开头的队列镜像到所有集群节点中</li><li>RabbitMQctl set_policy ha-two “^ha.“ ‘{“ha-mode”:”exactly”,”ha-params”:2,”ha-sync-mode”:”automatic”}’ #&lt;===将名称全部以“ha.”开头的队列镜像到集群中任意两个节点</li><li>RabbitMQctl set_policy ha-nodes “^nodes.“‘{“ha-mode”:”nodes”,”ha-params”:[“rabbit@nodeA”, “rabbit@nodeB”]}’ #&lt;===将名称全部以“nodes.”开头的队列镜像到集群中特定节点上</li></ul><p><strong>队列镜像中的一个特殊情况是独占队列，独占队列在声明队列的连接终止后会被RabbitMQ自动清除，就如程序段中的一个临时变量。因此，针对独占队列没有必要持久化存储。而且，即使队列名称匹配镜像策略的模式，独占队列也不会被RabbitMQ的镜像策略进行镜像。同时，即使对独占队列进行持久化声明，其也不会被持久化。</strong></p><p>在RabbitMQ集群中，设置了镜像策略节点上的队列并非任何时候都是彼此同步的。如果一个节点新加入集群，并且集群Policy将其设为队列镜像的节点，则队列会将此节点当做一个新的Slave节点。但是，此时的新Slave节点上并没有任何的队列，或者说此时新Slave节点上的队列是空的。正常情况下，新加入的Slave节点只会接收晚于其加入镜像队列时间点新增的消息，并最终随着时间推移，新加入的Slave节点中的队列消息会逐步<strong>与原集群队列尾部的消息同步</strong>，并且随着原有队列头部消息被不断消耗，新Slave节点队列中不同步的消息数目会不断减少并最终达到完全同步。因此，基于RabbitMQ的这种队列同步模式，默认设置下，新加入的Slave节点对其加入前已经存在的集群消息并不能形成任何的冗余和高可用，除非对原有集群队列执行显式的同步操作。</p><p>从RabbitMQ3.6开始，RabbitMQ新增了ha-sync-batch-size参数，使得显式的队列同步在速度上有了极大提升，显示队列同步可以有两种实现方式，即<strong>手工同步</strong>和<strong>自动同步</strong>。如果队列被配置为自动同步，则不论新的Slave节点何时加入集群，集群中的队列都会被自动同步到新增的Slave节点。但是，同步过程中的消息队列相应地会变迟钝，这种响应变慢的过程会一直持续到队列同步完成为止。队列自动同步的设置很简单，只需在Policy的镜像设置中指定ha-sync-mode参数，使其值为automatic即可。<strong>ha-sync-mode</strong>参数允许的值是manual和automatic，如果不显式指定automatic，则其值默认为manual。同时，默认情况下，队列在镜像时对消息进行逐条同步，而在RabbitMQ3.6之后，新增了批量同步参数<strong>ha-sync-batch-size</strong>，用户通过设置该参数的值，即可实现批量同步消息，默认值为1。</p><p>RabbitMQ可以工作在<strong>Active/Passive</strong>或者<strong>Active/1active</strong>模式的集群中。当RabbitMQ集群运行在Active/Passive模式时，如果Active节点故障，则正常运行时由Active节点持久化写入磁盘中的消息队列可以被Passive节点恢复。当然，在Active/Passive模式下，如果Active节点的消息队列没有进行持久化操作，则Active节点故障后位于其上的消息就会丢失，尽管Passive节点可以重新提供消息服务，但是之前未被取走的消息却已无法恢复，因此Active/Passive模式下的消息队列必须进行持久化操作。Active/Passive模式的另一可能的问题是，当Active节点故障，并需要提升Passive节点为Active节点以恢复和接管消息时，RabbitMQ集群的消息服务可能会出现短暂中断。<strong>Active/1active高可用模式的本质是将RabbitMQ集群中的队列在集群节点上实现彼此镜像</strong>，在Active/1active模式下，集群中的某一RabbitMQ节点故障后，RabbitMQ服务会自动切换到其他节点，并使用该节点上的镜像队列继续提供消息服务，从而实现消息系统的服务高可用性。在部署OpenStack的高可用集群中，推荐部署的RabbitMQ高可用模式便是Active/1active高可用集群模式。</p><h2 id="RabbitMQ在OpenStack中的应用分析"><a href="#RabbitMQ在OpenStack中的应用分析" class="headerlink" title="RabbitMQ在OpenStack中的应用分析"></a><strong>RabbitMQ在OpenStack中的应用分析</strong></h2><p>在实际应用中，RabbitMQ Broker位于OpenStack的任意两个组件之间，OpenStack的任意两个组件通过RabbitMQ Broker进行松耦合通信。更准确地说，OpenStack的各个组件之间通过远程过程调用RPC来实现彼此的通信，并且组件间的RPC消息传递是基于RabbitMQ的推送-订阅（Publish/Subscribe）模式实现的。OpenStack在消息传递过程中使用到的交换器Exchange的类型主要是Direct、Fanout和Topic类型。OpenStack组件之间关系示意如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/yytpQMWeUkwI.png?imageslim" alt="mark"></p><p>在OpenStack中，默认使用kombu（一种实现了AMQP协议的Python函数库）连接到RabbitMQ服务器，因为消息的收发者都需要一个连接到RabbitMQ服务器的Connetion对象。以nova为例，在Nova的各个连接到RabbitMQ Broker的组件中，其中某个组件可能是消息的发送者Publisher，如Nova-api和Nova-scheduler，也可能是消息的接收者Consumer，如Nova-compute和Nova-network（现已被Neutron代替，但是不影响我们理解AMQP交互）。此外，某个组件在不同的时刻既可以是Publisher，也可以是Consumer。组件发送消息有两种方式，即<strong>同步调用（RPC.CALL）</strong>和<strong>异步调用（RPC.CAST）</strong>，当Nova发起RPC.CALL调用的时候，Nova-api充当的就是消息的Consumer，否则是消息的Publisher。</p><p>在启动Nova的服务时，初始化函数会创建两个队列，其中的一个队列用于接收<strong>RoutingKey格式为“NODE-TYPE.NODE-ID”</strong>的消息，如compute.node1，其中的compute表示该节点为计算节点；另一个队列用于接收R<strong>outingKey格式为”NODE-TYPE”格式的消息</strong>，如compute。比如当Nova的客户端发送“nova stop instance_name”命令到Nova-api时，Nova-api就会将此命令以消息形式投递到第一种队列中（具体消息的路由投递由Exchange操作），从而通过RoutingKey中的NODE-ID（节点主机名）找到目标计算节点，并将命令转发到此Hypervisor主机上以进行实例的停止操作。</p><p>在实际应用中，Nova的各个功能模块根据其逻辑功能可以划分为两类，即<strong>Invoker组件</strong>和<strong>Worker组件</strong>。其中Invoker组件的主要功能是向消息队列中发送系统请求消息，如Nova-api和Nova-Scheduler通常属于Invoker；而Worker模块则负责从消息队列中提取Invoker模块发送的消息并向其返回应答消息，如Nova-Compute和Nova-Network通常属于Worker。<strong>Invoker通过RPC.CALL和RPC.CAST两个进程发送系统请求消息，然后Worker从消息队列中接收消息，并对RPC.CALL做出应答响应，</strong>如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/7ow5j9p9PiR4.png?imageslim" alt="mark"></p><ul><li><strong>Topic Publisher：</strong>该对象在Nova中的组件发起RPC.CALL调用时创建，消息发送出去之后便结束，生命周期短暂，主要用于将消息发送到队列系统中，每个Topic Publisher都会连接到Topic类型的Exchange中。</li><li><strong>Direct Consumer：</strong>该对象创建于RPC.CALL调用之后，专门用来接收RPC.CALL调用返回的应答消息，接收到应答消息后便结束。每个Direct Consumer连接到一个以msg_id为BindingKey的专有队列中，同时该队列绑定到一个只接收消息的RoutingKey为msg_id的交换器Exchange中，而这个消息的msg_id被封装在Topic Publisher发出的消息中，并且是由一个专门的UUID生成器所生成。</li><li><strong>Topic Consumer：</strong>该对象在Nova组件服务启动时创建，在服务结束时候销毁，主要用于接收消息队列中的消息。每个Worker都有两个Topic Consumer，一个连接BindingKey为Topic的队列，一个连接BindingKey为Topic.host的队列。每个Topic Consumer通过一个独占或者共享的队列与Topic Publisher连接到同一个Topic类型的Exchange。</li><li><strong>Direct Publisher：</strong>该对象创建于Nova中的RPC.CALL调用返回应答消息时，当Response消息发送完成后便结束，与BindingKey为特定msg_id的Direct类型Exchange连接。</li></ul><p>以Nova客户端发出创建实例请求为例，实例的创建过程伴随着Nova组件之间的消息传递，Topic Publisher（Nova-api）会将消息发送至NODE-TYPE（这里节点类型为compute）的共享队列中，从而全部Topic Consumer（全部运行Nova-compute的节点）都可以提取到共享队列的消息。如果客户端发出的是针对已有实例的Update、Reboot、Stop和Start等操作，由于消息必须转发到拥有该实例的某个特定计算节点上，因此Topic Publisher（Nova-api）会将消息转发至NOTE-TYPE.HOST（compute.hostname）的专有队列中。</p><p>在Openstack的Nova项目中，主要通过两种RPC调用来实现消息传递，即<strong>RPC.CALL和RPC.CAST</strong>。</p><p><strong>1）RPC.CALL属于请求/响应类型的调用</strong>，当请求发送出去以后，需要等待执行结果的响应，这类调用需要指定执行请求的目标对象节点，并等待目标返回的执行结果。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/EVkykmoPYWIo.png?imageslim" alt="mark"></p><p>Step1：初始化一个TopicPublisher，用以将发送消息到RabbitMQ的消息队列。此外，在发送消息之前，需要初始化一个DirectConsumer并以msg_id作为Direct类型Exchange的名称，用于等待消息执行后的应答响应。</p><p>Step2：请求消息被Exchange路由到NODE-TYPE.HOST消息队列，然后，订阅了此队列的相应服务节点（Nova中为compute节点）上的Topic-Consumer会从该队列中获取消息。</p><p>Step3：TopicConsumer从队列中提取消息后，服务结点根据消息内容（调用函数及参数）调用相应函数完成消息请求处理，处理完成之后，DirectPublisher被初始化，并根据请求消息msg_id，将应答消息发送到相应的Exchange和消息队列中。</p><p>Step4：应答消息被位于发起RPC.CALL调用方的DirectConsumer获取到，RPC.CALL调用过程完成。</p><p><strong>2）RPC.CAST属于单向RPC请求</strong>，即只将请求发送出去，无须等待返回执行结果。因此，RPC.CAST调用不关心请求由哪个服务节点完成，只需将请求发送到消息队列即可，而接收消息的队列通常为共享队列，该队列中的消息可以被某一类型的多个节点（如Nova中的全部计算节点）接收。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/LYkO22AfsQ44.png?imageslim" alt="mark"></p><p>Step1：Invoker初始化TopicPublisher，并将消息发送到RabbitMQ消息服务器中。</p><p>Step2：RabbitMQ的交换器将消息转发到NODE-TYPE类型的共享消息队列中，并被相应服务节点（Nova中为Compute节点）的TopicConsumer获取。</p><p>Step3：TopicConsumer提取到订阅队列的消息后，Woker便开始处理消息请求，至此，RPC.CAST的过程已经完成，Invoker不会再等待Woker返回请求消息执行后的结果。</p><h2 id="RabbitMQ集群管理实战"><a href="#RabbitMQ集群管理实战" class="headerlink" title="RabbitMQ集群管理实战"></a><strong>RabbitMQ集群管理实战</strong></h2><h3 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h3><p>现假设三个节点系统中都安装了RabbitMQ-server软件包，RabbitMQ服务已经正常启动，并且RabbitMQ的命令行工具RabbitMQctl已经可以正常使用。由于RabbitMQ节点之间使用Erlang Cookie来建立连接，因此要想让各个RabbitMQ节点之间彼此可以通信，则各个节点需要共享同一个Erlang Cookie。Erlang Cookie是在RabbitMQ-server启动过程中创建的一个随机字符串，在Linux系统中，Cookie保存在/var/lib/RabbitMQ/.erlang.cookie文件中。要让RabbitMQ的各个节点共享同一个Erlang Cookie，最简单的方式就是在某个节点（kvm-server01）中启动RabbitMQ-server，然后将kvm-server01中的/var/lib/RabbitMQ/.erlang.cookie拷贝到kvm-server02和kvm-server03中。另外一种方式就是在启动RabbitMQ-sever过程中或者在RabbitMQctl的命令行中通过参数“-setcookie cookie_str”的方式将特定的Erlang Cookie传入给当前节点的RabbitMQ服务。如果节点之间的Erlang Cookie未能正确匹配，则RabbitMQ的log中会有“Connection attempt from disallowed node”和“Could not auto-Cluster”的记录。我们这里采用第一种方式，如下：</p><p><strong>Step1：</strong> kvm-server01、kvm-server02、kvm-server03三个节点首先停止服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop rabbitmq-server.service</span><br></pre></td></tr></table></figure><p><strong>Step2：</strong>将kvm-server01的erlang.cookie拷贝到kvm-server02和kvm-server03节点上：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/jnWdoCzbWgmb.png?imageslim" alt="mark"></p><p><strong>Step3：</strong>重启三个节点的rabbitmq服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart rabbitmq-server.service</span><br></pre></td></tr></table></figure><p><strong>Step4：</strong>待各个节点的RabbitMQ-server服务启动完成后，便创建了三个独立的RabbitMQ Broker，即每个节点是一个独立的RabbitMQ Broker，通过RabbitMQctl命令行工具可以验证各个节点的Broker的运行状态：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/mVnY7x0BhLyH.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/3HwQgizFzOvp.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/a7xWk1oPueHW.png?imageslim" alt="mark"></p><p><strong><em># 为了实现一个三节点的RabbitMQ集群，通常的做法是首先创建一个两节点的集群，然后再通过新增节点的形式扩展至三节点集群</em></strong></p><p><strong>Step5</strong>：停止kvm-server02上的服务，完成如下操作，并观察节点的服务状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl join_cluster rabbit@kvm-server01</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/mpLKS1WWfFGx.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/wzW0sa1k7WLY.png?imageslim" alt="mark"></p><p><strong>Step6</strong>：停止kvm-server03上的服务，完成如下操作，并观察节点的服务状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl join_cluster rabbit@kvm-server02</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/M2Vi3cVazt3P.png?imageslim" alt="mark"></p><p>至此，三节点的RabbitMQ集群创建完毕，如上图通过RabbitMQctl命令行工具在任何一个节点上均可验证RabbitMQ集群的运行状态，并且在正常情况下，在任一节点上所看到的集群状态应该是一致的。从节点的集群状态输出中可以看到，每个RabbitMQBroker节点都加入到了集群中，并且集群由三个节点组成，每个节点上都正在运行RabbitMQ服务，并且节点默认都是磁盘类型的节点。</p><h3 id="集群节点的启停"><a href="#集群节点的启停" class="headerlink" title="集群节点的启停"></a>集群节点的启停</h3><p>在一个正在运行的RabbitMQ集群中，可以将任何一个或多个节点停止，并且集群中剩下的节点将会正常运行而不会受到某个节点停止的影响。而当停止的节点重新启动后，该节点又会自动加入集群，集群状态自动恢复正常。</p><p><strong>Step1：</strong>在kvm-server01节点上，执行如下操作，停止服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop</span><br></pre></td></tr></table></figure><p><strong>Step2：</strong>在kvm-server02和kvm-server03上观察集群的状态</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/3LjtqFXRA15d.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/msBLuPrWthUp.png?imageslim" alt="mark"></p><p><strong>Step3：</strong>重新启动kvm-server01上的服务，并再次观察kvm-server02和kvm-server03节点上集群运行状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmq-server -detached</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/kO64BRmCx1Jl.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/93uRXP3iJU9g.png?imageslim" alt="mark"></p><p>可以看到，当重新启动kvm-server01节点后，kvm-server01节点会自动加入集群运行，三节点集群状态自动恢复如初，即集群中三个集群成员都在运行，并可同时对外提供消息服务。在RabbitMQ全部集群节点均被停止并需重新重启时，有两点需要特别指出：</p><ul><li><strong>如果整个RabbitMQ集群中的节点都停止，则重启时应该根据节点停止顺序的逆序重新启动节点</strong>。如果首先启动的不是最后停止的节点，则启动过程中会给出30s的等待时间，以等待最后停止的节点变为运行状态，如果在30s内最后停止节点仍然未能激活，则节点启动过程就以失败告终。而<strong>如果最后停止的节点无法启动，则可以使用forget_Cluster_node命令将该节点从集群中移除</strong>，forget_Cluster_node命令的具体使用方式可以参考RabbitMQctl命令行工具的帮助页面。</li><li><strong>如果整个集群节点被同时停止或者发生了掉电等意外情况</strong>导致全部RabbitMQ节点同时关闭，则集群中每个节点都会认为自己不是集群中最后停止的节点，而会认为其他节点将在自己后面停止。如果出现这种情况，<strong>则可使用RabbitMQctl的force_boot命令来重新启动节点</strong>。</li></ul><h3 id="集群节点的移除"><a href="#集群节点的移除" class="headerlink" title="集群节点的移除"></a><strong>集群节点的移除</strong></h3><p>在正常运行的RabbitMQ集群中，当一个节点再也无须加入RabbitMQ集群并作为其成员节点运行的时候，就需要将此节点从集群中移除。要移除集群节点，首先需要停止该节点的RabbitMQ应用，然后重置节点，最后重启该节点的RabbitMQ应用。</p><p><strong>Step1：</strong>执行如下操作，将kvm-server03节点从集群中移除，移除后的节点将作为一个独立节点提供服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl reset</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><p><strong>Step2：</strong>观察三个节点的运行状态</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/3m8oOBIBqHYn.png?imageslim" alt="mark"></p><p>可见，在移除kvm-server03后，集群中只有kvm-server01和kvm-server02两个节点，同时kvm-server03节点中只有一个RabbitMQ Broker在运行。如果集群中某个节点已经失去了响应，且不能通过正常方式将其移除，则可以在本地节点通过forget_Cluster_node命令以远程移除的方式来将此节点从集群中移除。</p><p><strong>Step3：</strong>在kvm-server01节点上停止服务，并在kvm-server02上将kvm-server01节点强制移除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server01</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line"><span class="comment"># server02</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl forget_cluster_node rabbit@kvm-server01</span><br></pre></td></tr></table></figure><p><strong>Step4：</strong>kvm-server01在本地节点以远程方式从集群中移除后，其仍然会认为自己还属于集群节点，因此在启动本地kvm-server01节点的RabbitMQ应用时会报错，在重启之前将其重置即可解决</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/Ctn9zT4qKQOw.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server01</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl reset</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/jfG7pegVyang.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/l9FoAwNFS8Pg.png?imageslim" alt="mark"></p><p>如果此时将kvm-server03以同样办法从集群移除，则三个节点将恢复独立运行状态。但是kvm-server02仍然保存有集群的状态信息，即kvm-server02仍然还是集群成员，只不过集群有且只有kvm-server02一个成员。而kvm-server01和kvm-server03已经重新初始化为独立的RabbitMQ Broker，因此这两个节点不会保存任何集群信息，而如果要将kvm-server02也重新初始化为独立的RabbitMQ Broker，需要完成如下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server02</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl reset</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、在OpenStack中哪些服务之间交互使用了消息队列机制？哪些场景在哪些服务之间或子服务之间的RPC.CALL的调用关系？哪些场景在哪些服务之间或子服务之间是RPC.CAST的调用关系？（从虚拟机全生命周期管理的场景举例即可）</strong></p><p><strong>2、在请详细描述一下虚拟机实例销毁流程中，Nova各个组件之间的RPC调用流程？</strong></p><p><strong>3、在OpenStack中哪些服务或子服务之间没有使用消息队列的交互机制？（从keystone、glance、nova、neutron、cinder、ceilometer、heat等服务中举例即可）</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;OpenStack是由Nova、Neutron、Cinder等多个组件构成的开源云计算项目，各组件之间通过REST接口进相互通信和彼此调用，而&lt;strong&gt;组件之间的REST接口调用，是建立在基于高级消息队列协议（AdvancedMessageQueueProtocol，AMQP）上的RPC通信&lt;/strong&gt;。在OpenStack中，AMQP Broker（可以是Qpid Broker，也可以是RabbitMQ Broker）位于OpenStack的任意两个内部组件之间，并使得OpenStack内部组件&lt;strong&gt;以松耦合的方式进行通信&lt;/strong&gt;。纵观OpenStack的组件架构设计，其中的消息队列在OpenStack全局架构中扮演着至关重要的组件通信作用，也正是因为基于消息队列的分布式通信技术，才使得OpenStack的部署具有灵活、模块松耦合、架构扁平化和功能节点弹性扩展等特性，所以消息队列在OpenStack的架构设计和实现中扮演着极为核心的消息传递作用。同时，消息队列系统的消息收发性能和消息队列的高可用性也将直接影响OpenStack的集群性能。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack概述</title>
    <link href="https://kkutysllb.cn/2020/03/01/OpenStack%E6%A6%82%E8%BF%B0/"/>
    <id>https://kkutysllb.cn/2020/03/01/OpenStack概述/</id>
    <published>2020-03-01T15:44:51.000Z</published>
    <updated>2020-06-25T13:14:30.736Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OpenStack是什么？"><a href="#OpenStack是什么？" class="headerlink" title="OpenStack是什么？"></a><strong>OpenStack是什么？</strong></h2><p>2010年7月，RackSpace和美国国家航空航天局合作，分别贡献出RackSpace云文件平台代码和NASA Nebula平台代码，并以Apache许可证开源发布了OpenStack。OpenStack由此诞生，至此已经走过10个年头，最新发布的版本编号已到达T版本。OpenStack是目前最成熟并且符合生产部署的一套开源云操作系统，全球各大IT，CT厂商，甚至运营商都为此投入了大量的人力、物力和精力。由于其开源的特性，全世界的云计算爱好者也为此贡献了很多。OpenStack是开源云操作系统，可控制整个数据中心的大型计算，存储和网络资源池。用户能够通过Web界面、命令行或API接口配置资源。<a id="more"></a></p><p> <img src="https://pic.imgdb.cn/item/5ef4a28f14195aa594479f80.jpg" alt></p><h2 id="OpenStack不是虚拟化"><a href="#OpenStack不是虚拟化" class="headerlink" title="OpenStack不是虚拟化"></a><strong>OpenStack不是虚拟化</strong></h2><p>在1959年国际信息处理大会上，Christopher Strachey亲手为虚拟化埋下了种子，他在名为《大型高速计算机中的时间共享》的报告中，提出了“虚拟化”的概念，从此拉开了虚拟化发展的帷幕。到21世纪，VMware的亮相，开启了虚拟化的X86时代，虚拟化的发展进入了一个爆发期。至此，目前业界主流的虚拟化解决方案有开源的KVM、Xen，商业的VMware、Hyper-V、Xen-Server和华为的FusionSphere。虚拟化技术的出现，主要出于提升资源利用率，降低资源投入成本的目的。从这一点上说，显然OpenStack不是虚拟化，因为OpenStack只是系统的控制面，并不包括系统的数据面组件。比如：Hypervisor、存储和网络设等。OpenStack与虚拟化有着本质上的区别，它本身并不提供虚拟化技术，而是对多种虚拟化技术架构进行管理，并能实现异构统一纳管。就像我们的win10操作系统，能够利用驱动程序对底层多种硬件进行统一管理，协调各类硬件的I/O占用。所以，这就是OpenStack被成为Cloud OS的原因，它本身只是个管理层面的系统。反而，虚拟化只是OpenStack的底层具体实现技术之一，但不是核心关注点。</p><p><img src="https://pic.imgdb.cn/item/5ef4a29f14195aa59447aa98.jpg" alt></p><h2 id="OpenStack不是云计算"><a href="#OpenStack不是云计算" class="headerlink" title="OpenStack不是云计算"></a><strong>OpenStack不是云计算</strong></h2><p>1983年，Sun提出“网络即是电脑”（“The Network is the Computer”），这被认为是云计算的雏形，而随后计算机技术的迅猛发展以及互联网行业的兴起，似乎都在向这个概念不断靠拢。在这个不断靠拢的过程中，首先写上浓重一笔的是亚马逊。2006年3月，亚马逊推出弹性计算云（Elastic Computing Cloud，EC2），按用户使用的资源进行收费，开启了云计算商业化的元年。云计算概念诞生之初，市场上对其概念有很多种理解，经过一段时间的争论，现在大家一般来说都认可的就是美国标准与技术研究院的它给出的一个最标准的定义。它把云计算定义为一种模式，而不是一种技术。这种模式既可以是商业模式，也可以是服务模式。显然，OpenStack并不是一种模式，它只是实现云计算这种模式的技术之一，但却是构建云计算模式的关键技术，整个云计算模式的内核、骨干、框架、总线均由OpenStack来构建。随着容器云概念的提出，另一种构建云计算的关键技术也随之出现，也就是我们常说的K8S（Kubernetes）。需要明确一点，OpenStack和K8S这两种构建云计算的关键技术既可以独立部署，也可以混合部署，并且在混合部署下丝毫没有违和感。</p><p><img src="https://pic.imgdb.cn/item/5ef4a2ac14195aa59447b2f9.jpg" alt></p><h2 id="OpenStack的体系架构"><a href="#OpenStack的体系架构" class="headerlink" title="OpenStack的体系架构"></a><strong>OpenStack的体系架构</strong></h2><p>在介绍OpenStack的体系架构之前，不得不提一下AWS Cloud。因为，OpenStack最早就是作为AWS Cloud的追随者出现的。AWS由一系列服务组成，去实现IaaS系统所需要的功能。AWS架构由5层组成，自下而上分别是AWS全球基础架构、基础服务、应用平台服务、管理和用户应用程序（华为的FusionSphere解决方案也很类似，不过华为是4层架构）而就服务类型本身而言，AWS主要提供6类服务：计算和网络、存储和内容分发、数据库、分析、部署管理和应用服务。AWS的功能十分强大，而且还在不断发展之中，OpenStack从诞生之初就一直向AWS模仿和学习，同时，OpenStack也提供开放接口去兼容各种AWS服务。做为一个IaaS范畴的云平台，完整的OpenStack系统通过网络将用户和网络背后丰富的硬件资源分离开。OpenStack一方面负责与运行在物理节点上的Hypervisor进行交互，实现对各种硬件资源的管理与控制；另一方面为用户提供一个满足要求的虚拟机。至于OpenStack内部，作为AWS的一个跟随者，它的体系结构里不可避免地体现着前面所介绍的AWS各个组件的痕迹。比如：AWS中最为核心的EC2模块，负责计算资源的管理，以及虚拟机的调度和管理，在OpenStack中对应的就是Nova项目；AWS中的简单存储服务S3，在OpenStack中有Swift项目与其功能相近；AWS中的块存储模块EBS，对应OpenStack的Cinder项目；AWS的验证访问管理服务IAM，对应OpenStack的KeyStone项目；AWS的监控服务CloudWatch，对应OpenStack的Ceilometer项目；AWS有CloudInformation，OpenStack则有Heat项目；AWS支持关系数据库RDS和NoSQL数据库DynamoDB，OpenStack也支持mySQL、postgre和NoSQL数据库MongoDB等等。</p><p>OpenStack共有7个核心组件，分别是计算（Nova）、对象存储（Swift）、认证（Keystone）、用户界面（Horizon）、块存储（Cinder）、网络（Neutron）、镜像服务（Glance）。每个组件都是多个服务的集合，一个服务意味着运行着的一个进程，根据部署Openstack的规模，决定了你是选择将所有服务运行在同一个机器上还是多个机器上。随着OpenStack发展至今，除了上述7个组件，还出现了其他社区高度关注并重点发展的项目，所有组件按照运营层、编排层、服务层和共享层的全景视图如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a2bf14195aa59447c0d1.jpg" alt></p><p>OpenStack的生产环境部署架构如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a2d314195aa59447cf14.jpg" alt></p><h2 id="OpenStack的关键组件介绍"><a href="#OpenStack的关键组件介绍" class="headerlink" title="OpenStack的关键组件介绍"></a><strong>OpenStack的关键组件介绍</strong></h2><ul><li><p><strong>认证服务Keystone：</strong>首次出现在OpenStack的“Essex”版本中。Keystone提供身份验证，服务发现和分布式多租户授权，支持LDAP，OAuth，OpenID Connect，SAML和SQL。Keystone服务不依赖其他OpenStack服务，为其他OpenStack服务提供认证支持。</p></li><li><p><strong>操作界面Horizon</strong>：首次出现在OpenStack的“Essex”版本中。Horizon提供基于Web的控制界面，使云管理员和用户能够管理各种OpenStack资源和服务，它依赖于Keystone服务。</p></li><li><strong>镜像服务Glance：</strong>首次出现在OpenStack的“Bexar”版本中。Glance提供发现、注册和检索虚拟机镜像功能，提供的虚拟机实例镜像可以存放在不同地方，例如本地文件系统、对象存储、块存储等。它同样依赖于Keystone服务。</li><li><strong>计算服务Nova：</strong>首次出现在OpenStack的“Austin”版本中。Nova提供大规模、可扩展、按需自助服务的计算资源，支持管理裸机，虚拟机和容器。它依赖于Keystone、Neutron和Glance三个服务。</li><li><p><strong>块存储服务Cinder：</strong>首次出现在OpenStack的“Folsom”版本中。Cinder提供块存储服务，为虚拟机实例提供持久化存储，调用不同存储接口驱动，将存储设备转化成块存储池，用户无需了解存储实际部署的位置或设备类型。它依赖于服务Keystone。</p></li><li><p><strong>对象存储服务Swift：</strong>首次出现在OpenStack的“Austin”版本中。Swift提供高度可用、分布式、最终一致的对象存储服务，可以高效、安全且廉价地存储大量数据，非常适合存储需要弹性扩展的非结构化数据。它同样不依赖于其他服务，可以为其他服务提供对象存储服务。</p></li><li><p><strong>网络服务Neutron：</strong>首次出现在OpenStack的“Folsom”版本中。Neutron负责管理虚拟网络组件，专注于为OpenStack提供网络即服务。它依赖于服务Keystone。=</p></li><li><p><strong>编排服务Heat</strong>：首次出现在OpenStack的“Havana”版本中。Heat为云应用程序编排OpenStack基础架构资源，可以提供OpenStack原生Rest API和CloudFormation兼容的查询API。它依赖于服务Keystone。</p></li></ul><h2 id="OpenStack服务间的交互示例"><a href="#OpenStack服务间的交互示例" class="headerlink" title="OpenStack服务间的交互示例"></a><strong>OpenStack服务间的交互示例</strong></h2><p><img src="https://pic.imgdb.cn/item/5ef4a32b14195aa594480ba8.jpg" alt></p><p>以创建虚拟机为例，各组件的配合工作基本流程为：</p><p>1）用户首先接触到的是界面，即Horizon。通过Horizon上的简单界面操作，一个创建虚拟机的请求被发送到OpenStack系统后端。</p><p>2）既然要启动一个虚拟机，就必须指定虚拟机操作系统是什么类型，同时下载启动镜像以供虚拟机启动使用。这件事情就是由Glance来完成的，而此时Glance所管理的镜像有可能存储在Swift上，所以需要与Swift交互得到需要的镜像文件。</p><p>3）在创建虚拟机的时候，自然而然地需要Cinder提供块服务和Neutron提供网络服务，以便该虚拟机有volume可以使用，能被分配到IP地址与外界网络连接，而且之后该虚拟机资源的访问要经过Keystone的认证之后才可以继续。</p><p>4）至此，OpenStack的所有核心组件都参与了这个创建虚拟机的操作。</p><p><strong>建议多思考总结，从生活中熟悉的例子去理解OpenStack各服务之间的交互关系。</strong></p><h2 id="如何学习OpenStack？"><a href="#如何学习OpenStack？" class="headerlink" title="如何学习OpenStack？"></a><strong>如何学习OpenStack？</strong></h2><p>OpenStack需要很强的动手能力，最好能够准备好带虚拟化设备的物理机或者服务器供学习研究使用，动手是最重要的！此外，还须查阅各种关于OpenStack的资料。首先，OpenStack官网是不容错过的。在OpenStack官方网址上，发布了关于OpenStack各种最新的动态。此外，还提供了极为详细的文档，可以说OpenStack的官方文档是所有开源社区写得最好的，没有之一。在官方网址的博客中，提供了各种关于OpenStack的有趣的活动及技术沙龙。</p><p>学习好OpenStack，首先需要顺利地安装OpenStack的各个组件。在安装成功的基础上，学会使用OpenStack系统创建和管理虚拟机、虚拟网络及存储资源。如果需要再深入地研究，那么就需要阅读OpenStack的源代码了。代码的获得主要有两个来源，较为稳定的发行版位于 https:// launchpad. net/。而OpenStack最新的代码，则位于GitHub（https:// github. com/ openstack）。在学习的时候，建议使用launchpad网站上的稳定版本的代码。对OpenStack有了相当了解之后，在学习的过程中，发现一些Bug需要提交Patch的时候，就需要用到GitHub上面最新的代码了。</p><p>学习好OpenStack之后，也可以基于OpenStack做一些有意思的二次开发，无论是开发公有云或者私有云，都变得比较有意思。了解OpenStack的内部机制之后，添加一些自定义的模块或者驱动都是相当容易。也就真正地能够将OpenStack握在手中，为我所用了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;OpenStack是什么？&quot;&gt;&lt;a href=&quot;#OpenStack是什么？&quot; class=&quot;headerlink&quot; title=&quot;OpenStack是什么？&quot;&gt;&lt;/a&gt;&lt;strong&gt;OpenStack是什么？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;2010年7月，RackSpace和美国国家航空航天局合作，分别贡献出RackSpace云文件平台代码和NASA Nebula平台代码，并以Apache许可证开源发布了OpenStack。OpenStack由此诞生，至此已经走过10个年头，最新发布的版本编号已到达T版本。OpenStack是目前最成熟并且符合生产部署的一套开源云操作系统，全球各大IT，CT厂商，甚至运营商都为此投入了大量的人力、物力和精力。由于其开源的特性，全世界的云计算爱好者也为此贡献了很多。OpenStack是开源云操作系统，可控制整个数据中心的大型计算，存储和网络资源池。用户能够通过Web界面、命令行或API接口配置资源。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-CentOS-7%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2Openstack-Rocky%E7%89%88%E6%9C%AC%E7%AC%94%E8%AE%B0%EF%BC%88%E5%90%AB%E5%BC%80%E6%BA%90NFVO-VNFM%EF%BC%89/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-CentOS-7手动部署Openstack-Rocky版本笔记（含开源NFVO-VNFM）/</id>
    <published>2020-03-01T12:03:21.830Z</published>
    <updated>2020-06-25T05:08:24.372Z</updated>
    
    <content type="html"><![CDATA[<p>title: 2020-03-01-CentOS 7手动部署Openstack Rocky版本笔记（含开源NFVO/VNFM）<br>date: 2020-03-01 20:03:21<br>tags:</p><ul><li>云计算<br>category:</li><li>OpenStack</li></ul><p>本篇博文记录了在VMware Workstations上手动部署双节点OpenStack Rocky发行版本的部署笔记，包括OpenStack的9大基础核心组件：Keystone、Glance、Nova、Neutron、Cinder、Swift、Heat和Telemetry等服务，以及开源NFVO/VNFM项目tacker，同时将本地OpenStack的环境注册为VIM，构造一个开源的MANO环境。其中，9大核心组件的Swift在对象存储管理服务—Swift文中介绍部署方法，通过单节点双硬盘的方式模拟vNode，同时模拟实际网络云环境中对象存储主要用于数据备份和镜像存储的功能。<a id="more"></a></p><p><strong>需要说明一点：服务器操作系统安装和初始化配置请参考本站Linux相关文章或网上其他文章，本篇博文不会再赘述。</strong></p><h2 id="部署前准备"><a href="#部署前准备" class="headerlink" title="部署前准备"></a>部署前准备</h2><h3 id="节点规划："><a href="#节点规划：" class="headerlink" title="节点规划："></a>节点规划：</h3><p>共两个节点，一个控制节点，一个计算节点。其中，控制节点兼做计算节点和块存储节点，计算节点主要做业务计算。各节点主机名、地址规划如下：</p><p><img src="https://i.loli.net/2020/06/25/X7wdC6MTBlkGWIn.png" alt="image.png"></p><h3 id="网络规划："><a href="#网络规划：" class="headerlink" title="网络规划："></a>网络规划：</h3><p><img src="https://i.loli.net/2020/06/25/6HphIzWqr7Mk9ix.png" alt="image.png"></p><h3 id="初始化配置要求：以下为要求为基础配置，请自行完成。"><a href="#初始化配置要求：以下为要求为基础配置，请自行完成。" class="headerlink" title="初始化配置要求：以下为要求为基础配置，请自行完成。"></a>初始化配置要求：以下为要求为基础配置，请自行完成。</h3><ol><li>所有节点关闭防火墙和SELinux；</li><li>所有节点开启三层转发功能；</li><li>所有节点修改YUM源和PIP源为阿里云的源； </li><li>所有节点如果存在双网关，需要配置明细路由策略；</li></ol><h2 id="安装时钟同步服务"><a href="#安装时钟同步服务" class="headerlink" title="安装时钟同步服务"></a>安装时钟同步服务</h2><h3 id="控制节点："><a href="#控制节点：" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install chrony -y</span><br></pre></td></tr></table></figure><p>2）配置时钟同步服务文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/chrony.conf文件中添加修改如下选项</span></span><br><span class="line">第26行修改：allow 10.28.101.0/24  <span class="comment">#&lt;===修改为管理平面网段</span></span><br></pre></td></tr></table></figure><p>3）配置时钟同步服务开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> chronyd.service &amp;&amp; systemctl start chronyd.service</span><br></pre></td></tr></table></figure><h3 id="计算节点："><a href="#计算节点：" class="headerlink" title="计算节点："></a>计算节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install chrony -y</span><br></pre></td></tr></table></figure><p>2）配置时钟同步服务文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/chrony.conf&#123;,.bak&#125;</span><br><span class="line"><span class="comment"># 在/etc/chrony.conf文件中添加修改如下选项</span></span><br><span class="line"><span class="comment"># 注释掉3，4，5，6行</span></span><br><span class="line"><span class="comment"># 第7行添加：</span></span><br><span class="line">server rocky-cotroller iburst  <span class="comment">#&lt;===添加控制节点主机名为NTP Server</span></span><br></pre></td></tr></table></figure><p>3）配置时钟同步服务开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> chronyd.service &amp;&amp; systemctl start chronyd.service</span><br></pre></td></tr></table></figure><p>在所有节点执行验证操作：chronyc sources，验证结果如下：</p><p><strong>控制节点：</strong></p><p><img src="https://i.loli.net/2020/06/25/GeCEOAp2n6QRvFH.png" alt="image.png"></p><p><strong>计算节点：</strong></p><p><img src="https://i.loli.net/2020/06/25/3IwVAjp7sofk9gd.png" alt="image.png"></p><h2 id="安装OpenStack软件包"><a href="#安装OpenStack软件包" class="headerlink" title="安装OpenStack软件包"></a>安装OpenStack软件包</h2><p><strong>控制节点&amp;&amp;计算节点：</strong></p><p>1）添加rocky版本软件仓库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install centos-release-openstack-rocky -y</span><br></pre></td></tr></table></figure><p>2）更新系统：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum upgrade -y</span><br></pre></td></tr></table></figure><p>3）安装OpenStack客户端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install python-openstackclient -y</span><br></pre></td></tr></table></figure><p>4）安装OpenStack安全策略组件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-selinux -y</span><br></pre></td></tr></table></figure><h2 id="安装SQL数据库"><a href="#安装SQL数据库" class="headerlink" title="安装SQL数据库"></a>安装SQL数据库</h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb mariadb-server python2-PyMySQL -y</span><br></pre></td></tr></table></figure><p>2）创建和编辑/etc/my.cnf.d/openstack.cnf文件，创建一个[mysqld]的Session，并将bind-address 密钥设置为控制器节点的管理IP地址，以允许其他节点通过管理网络进行访问。设置其他键以启用有用的选项和UTF-8字符集，如下：</p><p><img src="https://i.loli.net/2020/06/25/6pRs8JOCgyV4vBm.png" alt="image.png"></p><p>3）配置数据库服务开机启动：vim</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> mariadb.service &amp;&amp; systemctl start mariadb.service</span><br></pre></td></tr></table></figure><p>4）通过执行如下脚本，设置mysql数据root帐号和密码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql_secure_installation</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/OlnxUQsf2MDKo3S.png" alt="image.png"></p><h2 id="安装消息队列服务"><a href="#安装消息队列服务" class="headerlink" title="安装消息队列服务"></a><strong>安装消息队列服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install rabbitmq-server -y</span><br></pre></td></tr></table></figure><p>2）配置开机启动消息队列服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> rabbitmq-server.service &amp;&amp; systemctl start rabbitmq-server.service</span><br></pre></td></tr></table></figure><p>3）添加OpenStack用户并添加密码:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl add_user openstack openstack</span><br></pre></td></tr></table></figure><p>4）设置openstack用户的访问权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl set_permissions openstack <span class="string">".*"</span> <span class="string">".*"</span> <span class="string">".*"</span></span><br></pre></td></tr></table></figure><h2 id="安装分布式缓存memcache"><a href="#安装分布式缓存memcache" class="headerlink" title="安装分布式缓存memcache"></a><strong>安装分布式缓存memcache</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install memcached python-memcached -y</span><br></pre></td></tr></table></figure><p>2）配置/etc/sysconfig/memcached文件，在OPTIONS中添加控制节点主机名：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/sysconfig/memcached&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/NKWuZv7zQg5lGfh.png" alt="image.png"></p><p>3）配置开机启动memcache服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> memcached.service &amp;&amp; systemctl start memcached.service</span><br></pre></td></tr></table></figure><h2 id="安装分布式键值数据库etcd服务"><a href="#安装分布式键值数据库etcd服务" class="headerlink" title="安装分布式键值数据库etcd服务"></a><strong>安装分布式键值数据库etcd服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install etcd -y</span><br></pre></td></tr></table></figure><p>2）编辑/etc/etcd/etcd.conf文件，并设置ETCD_INITIAL_CLUSTER， ETCD_INITIAL_ADVERTISE_PEER_URLS，ETCD_ADVERTISE_CLIENT_URLS， ETCD_LISTEN_CLIENT_URLS控制器节点，以使经由管理网络通过其他节点的访问的管理IP地址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/etcd/etcd.conf&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/Pg3imyoGDns9dA2.png" alt="image.png"></p><p>3）配置开机启动ETCD服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> etcd &amp;&amp; systemctl start etcd</span><br></pre></td></tr></table></figure><h2 id="安装Horizon仪表盘服务"><a href="#安装Horizon仪表盘服务" class="headerlink" title="安装Horizon仪表盘服务"></a><strong>安装Horizon仪表盘服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-dashboard -y</span><br></pre></td></tr></table></figure><p>2）编辑 /etc/openstack-dashboard/local_settings 文件并完成以下修改操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/openstack-dashboard/local_settings&#123;,.bak&#125;</span><br><span class="line"><span class="comment"># 184行修改为：OPENSTACK_HOST = "rocky-controller"</span></span><br><span class="line"><span class="comment"># 38行修改为：ALLOWED_HOSTS = ['*'] #&lt;===生产环境建议根据访问需求修改，不建议修改为*</span></span><br><span class="line"><span class="comment"># 160行添加：SESSION_ENGINE = 'django.contrib.sessions.backends.cache'</span></span><br><span class="line"><span class="comment"># 166行添加：'LOCATION': 'rocky-controller:11211', #&lt;===注意后面的逗号不能少</span></span><br><span class="line"><span class="comment"># 75行修改为：OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True</span></span><br><span class="line"><span class="comment"># 64、66、67、68、70、97行去掉注释</span></span><br><span class="line"><span class="comment"># 189行修改为：OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"</span></span><br><span class="line"><span class="comment"># 467行修改为：TIME_ZONE = "Asia/Shanghai"</span></span><br></pre></td></tr></table></figure><p>3）在/etc/httpd/conf.d/openstack-dashboard.conf中添加以下行 ：</p><p><img src="https://i.loli.net/2020/06/25/7RoDFscpWXaKAwq.png" alt="image.png"></p><p>4）重启web服务和分布式缓存服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart httpd.service memcached.service</span><br></pre></td></tr></table></figure><h2 id="安装keystone服务"><a href="#安装keystone服务" class="headerlink" title="安装keystone服务"></a><strong>安装keystone服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装keystone数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE keystone;</span><br><span class="line">GRANT ALL PRIVILEGES ON keystone.* TO <span class="string">'keystone'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'keystone'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON keystone.* TO <span class="string">'keystone'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'keystone'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-keystone httpd mod_wsgi -y</span><br></pre></td></tr></table></figure><p>3）配置/etc/keystone/keystone.conf文件，完成[database]和[token]部分配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/keystone/keystone.conf&#123;,.bak&#125;</span><br><span class="line">[database] <span class="comment">#部分：</span></span><br><span class="line">connection = mysql+pymysql://keystone:keystone@rocky-controller/keystone</span><br><span class="line"></span><br><span class="line">[token] <span class="comment">#部分：</span></span><br><span class="line">provider = fernet</span><br></pre></td></tr></table></figure><p>4）初始化keystone数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"keystone-manage db_sync"</span> keystone</span><br></pre></td></tr></table></figure><p>5）初始化fernet秘钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone</span><br><span class="line">keystone-manage credential_setup --keystone-user keystone --keystone-group keystone</span><br></pre></td></tr></table></figure><p>6）建立引导服务：（注意修改密码，更改主机名）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keystone-manage bootstrap --bootstrap-password &lt;你自己的admin用户密码&gt; --bootstrap-admin-url http://rocky-controller:5000/v3/ --bootstrap-internal-url http://rocky-controller:5000/v3/ --bootstrap-public-url http://rocky-controller:5000/v3/ --bootstrap-region-id RegionOne</span><br></pre></td></tr></table></figure><p>7）编辑/etc/httpd/conf/httpd.conf文件并配置 ServerName选项以引用控制器节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/httpd/conf/httpd.conf&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/GwlHhbZfP3XeAcK.png" alt="image.png"></p><p>8）创建到/usr/share/keystone/wsgi-keystone.conf文件的链接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/</span><br></pre></td></tr></table></figure><p>9）启动Apache HTTP服务，并将其配置为在系统启动时启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> httpd.service &amp;&amp; systemctl start httpd.service</span><br></pre></td></tr></table></figure><p>10）创建和编辑admin-openrc文件</p><p><img src="https://i.loli.net/2020/06/25/8Y1QGInLbE5DZKx.png" alt="image.png"></p><p>11）创建domain、project、user和role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">openstack domain create --description <span class="string">"KkutysLLB-Domain"</span> kkutysllb</span><br><span class="line">openstack project create --domain default --description <span class="string">"Service Project"</span> service</span><br><span class="line">openstack project create --domain default --description <span class="string">"Demo Project"</span> kkproject</span><br><span class="line">openstack user create --domain default --password-prompt kkutysllb</span><br><span class="line">openstack role create kkrole</span><br><span class="line">openstack role add --project kkproject --user kkutysllb kkrole</span><br></pre></td></tr></table></figure><p>12）创建和编辑kkutysllb-openrc文件</p><p><img src="https://i.loli.net/2020/06/25/XQlTemwrdY8K2hf.png" alt="image.png"></p><p>13）验证admin用户和kkutysllb用户的token</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br><span class="line">openstack token issue</span><br><span class="line"><span class="built_in">source</span> kkutysllb-openrc.sh</span><br><span class="line">openstack token issue</span><br></pre></td></tr></table></figure><h2 id="安装Glance服务"><a href="#安装Glance服务" class="headerlink" title="安装Glance服务"></a><strong>安装Glance服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装glance数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE glance;</span><br><span class="line">GRANT ALL PRIVILEGES ON glance.* TO <span class="string">'glance'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'glance'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON glance.* TO <span class="string">'glance'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'glance'</span>;</span><br><span class="line"><span class="built_in">exit</span>;</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建glance用户、角色和服务endpoint</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt glance</span><br><span class="line">openstack role add --project service --user glance admin</span><br><span class="line">openstack service create --name glance --description <span class="string">"OpenStack Image"</span> image</span><br></pre></td></tr></table></figure><p>4）创建glance服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne image public http://rocky-controller:9292</span><br><span class="line">openstack endpoint create --region RegionOne image internal http://rocky-controller:9292</span><br><span class="line">openstack endpoint create --region RegionOne image admin http://rocky-controller:9292</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-glance -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/glance/glance-api.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/glance/glance-api.conf&#123;,.bak&#125;</span><br><span class="line">[database] <span class="comment">#部分添加：</span></span><br><span class="line">connection = mysql+pymysql://glance:glance@rocky-controller/glance</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加：</span></span><br><span class="line">www_authenticate_uri = http://rokcy-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = Default</span><br><span class="line">user_domain_name = Default</span><br><span class="line">project_name = service</span><br><span class="line">username = glance</span><br><span class="line">password = glance</span><br><span class="line"></span><br><span class="line">[paste_deploy]<span class="comment">#部分添加：</span></span><br><span class="line">flavor = keystone</span><br><span class="line"></span><br><span class="line">[glance_store] <span class="comment">#部分添加：</span></span><br><span class="line">stores = file,http</span><br><span class="line">default_store = file</span><br><span class="line">filesystem_store_datadir = /var/lib/glance/images/</span><br></pre></td></tr></table></figure><p>7）编辑/etc/glance/glance-registry.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/glance/glance-registry.conf&#123;,.bak&#125;</span><br><span class="line">[database] <span class="comment">#部分添加：</span></span><br><span class="line">connection = mysql+pymysql://glance:glance@rocky-controller/glance</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加：</span></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = Default</span><br><span class="line">user_domain_name = Default</span><br><span class="line">project_name = service</span><br><span class="line">username = glance</span><br><span class="line">password = glance</span><br><span class="line"></span><br><span class="line">[paste_deploy] <span class="comment">#部分添加：</span></span><br><span class="line">flavor = keystone</span><br></pre></td></tr></table></figure><p>8）初始化glance数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"glance-manage db_sync"</span> glance</span><br></pre></td></tr></table></figure><p>9）设置开机启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-glance-api.service openstack-glance-registry.service</span><br><span class="line">systemctl start openstack-glance-api.service openstack-glance-registry.service</span><br></pre></td></tr></table></figure><p>10）验证服务，上传镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img</span><br><span class="line">openstack image create <span class="string">"cirros"</span> --file cirros-0.4.0-x86_64-disk.img --disk-format qcow2 --container-format bare --public</span><br><span class="line"></span><br><span class="line">openstack image list</span><br></pre></td></tr></table></figure><h2 id="安装Nova服务"><a href="#安装Nova服务" class="headerlink" title="安装Nova服务"></a><strong>安装Nova服务</strong></h2><h3 id="控制节点：-1"><a href="#控制节点：-1" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）创建nova数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE nova_api;</span><br><span class="line">CREATE DATABASE nova;</span><br><span class="line">CREATE DATABASE nova_cell0;</span><br><span class="line">CREATE DATABASE placement;</span><br><span class="line">GRANT ALL PRIVILEGES ON nova_api.* TO <span class="string">'nova'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON nova_api.* TO <span class="string">'nova'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON nova.* TO <span class="string">'nova'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON nova.* TO <span class="string">'nova'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON nova_cell0.* TO <span class="string">'nova'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON nova_cell0.* TO <span class="string">'nova'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON placement.* TO <span class="string">'placement'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'placement'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON placement.* TO <span class="string">'placement'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'placement'</span>;</span><br><span class="line"><span class="built_in">exit</span>;</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建nova用户、添加角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt nova</span><br><span class="line">openstack role add --project service --user nova admin</span><br><span class="line">openstack service create --name nova --description <span class="string">"OpenStack Compute"</span> compute</span><br></pre></td></tr></table></figure><p>4）创建API服务端点endpoint</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne compute public http://rocky-controller:8774/v2.1</span><br><span class="line">openstack endpoint create --region RegionOne compute internal http://rocky-controller:8774/v2.1</span><br><span class="line">openstack endpoint create --region RegionOne compute admin http://rocky-controller:8774/v2.1</span><br></pre></td></tr></table></figure><p>5）创建placement用户、添加角色和服务实体，placement用于资源的追踪展示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt placement</span><br><span class="line">openstack role add --project service --user placement admin</span><br><span class="line">openstack service create --name placement --description <span class="string">"Placement API"</span> placement</span><br></pre></td></tr></table></figure><p>6）创建placement API服务端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne placement public http://rocky-controller:8778</span><br><span class="line">openstack endpoint create --region RegionOne placement internal http://rocky-controller:8778</span><br><span class="line">openstack endpoint create --region RegionOne placement admin http://rocky-controller:8778</span><br></pre></td></tr></table></figure><p>7）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api -y</span><br></pre></td></tr></table></figure><p>8）编辑/etc/nova/nova.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/nova/nova.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加：</span></span><br><span class="line">enabled_apis = osapi_compute,metadata</span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">my_ip = 10.28.101.81</span><br><span class="line">use_neutron = <span class="literal">true</span></span><br><span class="line">firewall_driver = nova.virt.firewall.NoopFirewallDriver</span><br><span class="line"></span><br><span class="line">[api_database] <span class="comment">#部分添加：</span></span><br><span class="line">connection = mysql+pymysql://nova:nova@rocky-controller/nova_api</span><br><span class="line"></span><br><span class="line">[database] <span class="comment">#部分添加：</span></span><br><span class="line">connection = mysql+pymysql://nova:nova@rocky-controller/nova</span><br><span class="line"></span><br><span class="line">[placement_database] <span class="comment">#部分添加：</span></span><br><span class="line">connection = mysql+pymysql://placement:placement@rocky-controller/placement</span><br><span class="line"></span><br><span class="line">[api] <span class="comment">#部分添加：</span></span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加：</span></span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = Default</span><br><span class="line">user_domain_name = Default</span><br><span class="line">project_name = service</span><br><span class="line">username = nova</span><br><span class="line">password = nova</span><br><span class="line"></span><br><span class="line">[vnc] <span class="comment">#部分添加：</span></span><br><span class="line">enabled = <span class="literal">true</span></span><br><span class="line">server_listen = <span class="variable">$my_ip</span></span><br><span class="line">server_proxyclient_address = <span class="variable">$my_ip</span></span><br><span class="line"></span><br><span class="line">[glance] <span class="comment">#部分添加：</span></span><br><span class="line">api_servers = http://rocky-controller:9292</span><br><span class="line"></span><br><span class="line">[oslo_concurrency] <span class="comment">#部分添加：</span></span><br><span class="line">lock_path = /var/lib/nova/tmp</span><br><span class="line"></span><br><span class="line">[placement] <span class="comment">#部分添加：</span></span><br><span class="line">region_name = RegionOne</span><br><span class="line">project_domain_name = Default</span><br><span class="line">project_name = service</span><br><span class="line">auth_type = password</span><br><span class="line">user_domain_name = Default</span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line">username = placement</span><br><span class="line">password = placement</span><br></pre></td></tr></table></figure><p>9）修复一个bug，修改 /etc/httpd/conf.d/00-nova-placement-api.conf配置如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Directory</span> /<span class="attr">usr</span>/<span class="attr">bin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">IfVersion</span> &gt;</span>= 2.4&gt;</span><br><span class="line">   Require all granted</span><br><span class="line">  <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  &lt;IfVersion &lt; 2.4&gt;</span><br><span class="line">   Order allow,deny</span><br><span class="line">   Allow from all</span><br><span class="line">  <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Directory</span>&gt;</span></span><br></pre></td></tr></table></figure><p>10）重启web服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>11）初始化nova-api和placement数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage api_db sync"</span> nova</span><br></pre></td></tr></table></figure><p>12）注册cell0数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 map_cell0"</span> nova</span><br></pre></td></tr></table></figure><p>13）创建cell1网格：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 create_cell --name=cell1 --verbose"</span> nova</span><br></pre></td></tr></table></figure><p>14）初始化nova数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage db sync"</span> nova</span><br></pre></td></tr></table></figure><p>15）验证nova cell0和cell1是否正确注册：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 list_cells"</span> nova</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/LXUCT4ZbvIOe7Aa.png" alt="image.png"></p><p>16）配置开机启动服务：（备注：nova-consoleauth服务自18.0版本开始不再使用，建议以后每个单元都部署控制台代理）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-nova-api.service openstack-nova-consoleauth openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service</span><br><span class="line">systemctl start openstack-nova-api.service openstack-nova-consoleauth openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service</span><br></pre></td></tr></table></figure><h3 id="计算节点：-1"><a href="#计算节点：-1" class="headerlink" title="计算节点："></a>计算节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-nova-compute -y</span><br></pre></td></tr></table></figure><p>2）拷贝控制节点的nova.conf文件至计算节点，并修改以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/nova/nova.conf&#123;,.bak&#125;</span><br><span class="line">scp /etc/nova/nova.conf rocky-compute:/etc/nova/nova.conf</span><br><span class="line">[DEFAULT] <span class="comment">#部分修改计算节点管理网口IP：</span></span><br><span class="line">my_ip = 10.28.101.82</span><br><span class="line"></span><br><span class="line">[vnc] <span class="comment">#部分添加以下配置：</span></span><br><span class="line">novncproxy_base_url = http://rocky-controller:6080/vnc_auto.html</span><br><span class="line"></span><br><span class="line">[libvirt] <span class="comment">#部分添加以下配置：</span></span><br><span class="line">virt_type = qemu <span class="comment">#&lt;===因为我们环境是虚拟机安装，所以选择qemu，生产环境根据实际情况可以选择KVM、Xen、VMware等</span></span><br></pre></td></tr></table></figure><p>3）配置compute服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> libvirtd.service openstack-nova-compute.service</span><br><span class="line">systemctl start libvirtd.service openstack-nova-compute.service</span><br></pre></td></tr></table></figure><h3 id="控制节点：-2"><a href="#控制节点：-2" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>2）列出计算主机：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack compute service list --service nova-compute</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/cNLm3eCfKObXuM7.png" alt="image.png"></p><p>3）在数据库中同步发现的计算主机：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 discover_hosts --verbose"</span> nova</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/3yln8PmHvquGCEh.png" alt="image.png"></p><p>4）（可选）在控制节点的nova.conf添加scheduler选项，用于自动发现主机，扫描周期可配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[scheduler]</span><br><span class="line">discover_hosts_in_cells_interval = 300</span><br></pre></td></tr></table></figure><p>5）重启nova-api和nova-scheduler服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-api.service openstack-nova-scheduler.service</span><br></pre></td></tr></table></figure><h2 id="安装Neutron服务"><a href="#安装Neutron服务" class="headerlink" title="安装Neutron服务"></a><strong>安装Neutron服务</strong></h2><h3 id="控制节点-amp-amp-网络节点："><a href="#控制节点-amp-amp-网络节点：" class="headerlink" title="控制节点&amp;&amp;网络节点："></a>控制节点&amp;&amp;网络节点：</h3><p>1）安装neutron数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE neutron;</span><br><span class="line">GRANT ALL PRIVILEGES ON neutron.* TO <span class="string">'neutron'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'neutron'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON neutron.* TO <span class="string">'neutron'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'neutron'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建neutron用户、服务实体、添加角色：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt neutron</span><br><span class="line">openstack role add --project service --user neutron admin</span><br><span class="line">openstack service create --name neutron --description <span class="string">"OpenStack Networking"</span> network</span><br></pre></td></tr></table></figure><p>4）创建API的服务端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne network public http://rocky-controller:9696</span><br><span class="line">openstack endpoint create --region RegionOne network internal http://rocky-controller:9696</span><br><span class="line">openstack endpoint create --region RegionOne network admin http://rocky-controller:9696</span><br></pre></td></tr></table></figure><p><strong><em>选择网络选项二，私有网络</em></strong></p><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables -y</span><br></pre></td></tr></table></figure><p>6）配置neutron.conf文件，添加以下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/neutron.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加：</span></span><br><span class="line">core_plugin = ml2</span><br><span class="line">service_plugins = router</span><br><span class="line">allow_overlapping_ips = <span class="literal">true</span></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">auth_strategy = keystone</span><br><span class="line">notify_nova_on_port_status_changes = <span class="literal">true</span></span><br><span class="line">notify_nova_on_port_data_changes = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[database] <span class="comment">#部分添加：</span></span><br><span class="line">connection = mysql+pymysql://neutron:neutron@rocky-controller/neutron</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加：</span></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = neutron</span><br><span class="line">password = neutron</span><br><span class="line"></span><br><span class="line">[nova] <span class="comment">#部分添加：</span></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">region_name = RegionOne</span><br><span class="line">project_name = service</span><br><span class="line">username = nova</span><br><span class="line">password = nova</span><br><span class="line"></span><br><span class="line">[oslo_concurrency] <span class="comment">#部分添加：</span></span><br><span class="line">lock_path = /var/lib/neutron/tmp</span><br></pre></td></tr></table></figure><p>7）配置ml2_conf.ini文件，添加以下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/plugins/ml2/ml2_conf.ini&#123;,.bak&#125;</span><br><span class="line">[ml2] <span class="comment">#部分添加以下选项：</span></span><br><span class="line">type_drivers = flat,vlan,vxlan</span><br><span class="line">tenant_network_types = vxlan</span><br><span class="line">extension_drivers = port_security</span><br><span class="line">mechanism_drivers = openvswitch,l2population</span><br><span class="line"></span><br><span class="line">[ml2_type_vxlan]<span class="comment">#部分添加如下选项：</span></span><br><span class="line">vni_ranges = 1:1000</span><br><span class="line"></span><br><span class="line">[securitygroup]<span class="comment">#部分添加如下选项：</span></span><br><span class="line">enable_ipset = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>8）配置openvswitch_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/plugins/ml2/openvswitch_agent.ini&#123;,.bak&#125;</span><br><span class="line">[agent] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">tunnel_types = vxlan</span><br><span class="line">l2_population = True</span><br><span class="line"></span><br><span class="line">[ovs] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">bridge_mappings = provider:br-extnet</span><br><span class="line">local_ip = 99.64.101.81</span><br><span class="line"></span><br><span class="line">[securitygroup] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">firewall_driver = iptables_hybrid</span><br></pre></td></tr></table></figure><p>9）配置l3_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/l3_agent.ini&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">interface_driver = openvswitch</span><br><span class="line">external_network_bridge =</span><br></pre></td></tr></table></figure><p>10）配置dhcp_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/dhcp_agent.ini&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">interface_driver = openvswitch</span><br><span class="line">dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq</span><br><span class="line">enable_ioslated_metadata = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>11）配置metadata_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/metadata_agent.ini&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">nova_metadata_host = rocky-controller</span><br><span class="line">metadata_proxy_shared_secret = neutron</span><br></pre></td></tr></table></figure><p>12）修改nova.conf配置文件，添加neutron模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[neutron]</span><br><span class="line">url = http://rocky-controller:9696</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">region_name = RegionOne</span><br><span class="line">project_name = service</span><br><span class="line">username = neutron</span><br><span class="line">password = neutron</span><br><span class="line">service_metadata_proxy = <span class="literal">true</span></span><br><span class="line">metadata_proxy_shared_secret = neutron</span><br></pre></td></tr></table></figure><p>13）创建ml2_conf.ini文件的软连接，用户网络服务的初始化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini</span><br></pre></td></tr></table></figure><p>14）初始化neutron数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head"</span> neutron</span><br></pre></td></tr></table></figure><p>15）重启nova-api服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-api.service</span><br></pre></td></tr></table></figure><p>16）配置OVS服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openvswitch &amp;&amp; systemctl start openvswitch &amp;&amp; systemctl status openvswitch</span><br></pre></td></tr></table></figure><p>17）添加外部网桥br-extnet和端口port配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/sysconfig/network-scripts/</span><br><span class="line">touch ifcfg-br-extnet</span><br><span class="line">cp ifcfg-eth2&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/bTRdlujriX9hSeo.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/06/25/I4dtF6fB7CnegNu.png" alt="image.png"></p><p>18）添加OVS外部网桥，并绑定端口eth2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ovs-vsctl add-br br-extnet</span><br><span class="line">ovs-vsctl add-port br-extnet eth2</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/zYCgnX3e8o5y9mb.png" alt="image.png"></p><p>19）配置neutron服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service</span><br><span class="line">systemctl start neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service</span><br></pre></td></tr></table></figure><p>20）检查OVS网桥状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ovs-vsctl show</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/VuTPDdvL3ws2tfp.png" alt="image.png"></p><h3 id="计算节点：-2"><a href="#计算节点：-2" class="headerlink" title="计算节点："></a>计算节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y openstack-neutron-openvswitch ebtables</span><br></pre></td></tr></table></figure><p>2）修改neutron.conf配置文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/neutron.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = neutron</span><br><span class="line">password = neutron</span><br></pre></td></tr></table></figure><p>3）修改openvswitch_agent.ini 配置文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/plugins/ml2/openvswitch_agent.ini&#123;,.bak&#125;</span><br><span class="line">[ovs] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">local_ip = 99.64.101.82</span><br><span class="line"></span><br><span class="line">[agent] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">tunnel_types = vxlan</span><br><span class="line">l2_population = True</span><br></pre></td></tr></table></figure><p>4）修改nova.conf配置文件，添加neutron模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[neutron]</span><br><span class="line">url = http://rocky-controller:9696</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">region_name = RegionOne</span><br><span class="line">project_name = service</span><br><span class="line">username = neutron</span><br><span class="line">password = neutron</span><br><span class="line">service_metadata_proxy = <span class="literal">true</span></span><br><span class="line">metadata_proxy_shared_secret = neutron</span><br></pre></td></tr></table></figure><p>5）配置OVS服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openvswitch &amp;&amp; systemctl start openvswitch &amp;&amp; systemctl status openvswitch</span><br></pre></td></tr></table></figure><p>6）重启nova-compute服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-compute.service</span><br></pre></td></tr></table></figure><p>7）配合开机启动OVS AGENT服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> neutron-openvswitch-agent.service &amp;&amp; systemctl start neutron-openvswitch-agent.service &amp;&amp; systemctl status neutron-openvswitch-agent.service</span><br></pre></td></tr></table></figure><h3 id="控制节点：-3"><a href="#控制节点：-3" class="headerlink" title="控制节点："></a>控制节点：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#验证服务：</span></span><br><span class="line">openstack network agent list</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/06/25/dLaJzYD7pMTP314.png" alt="image.png"></p><h2 id="安装heat服务"><a href="#安装heat服务" class="headerlink" title="安装heat服务"></a><strong>安装heat服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装heat数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE heat;</span><br><span class="line">GRANT ALL PRIVILEGES ON heat.* TO <span class="string">'heat'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'heat'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON heat.* TO <span class="string">'heat'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'heat'</span>;</span><br><span class="line"><span class="built_in">exit</span>;</span><br></pre></td></tr></table></figure><p>2）获取管理员权限:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建heat用户、添加角色，创建heat和heat-cfn服务实体</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt heat</span><br><span class="line">openstack role add --project service --user heat admin</span><br><span class="line">openstack service create --name heat --description <span class="string">"Orchestration"</span> orchestration</span><br><span class="line">openstack service create --name heat-cfn --description <span class="string">"Orchestration"</span> cloudformation</span><br></pre></td></tr></table></figure><p>4）创建编排服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne orchestration public [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)</span><br><span class="line">openstack endpoint create --region RegionOne orchestration internal [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)</span><br><span class="line">openstack endpoint create --region RegionOne orchestration admin [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)</span><br><span class="line">openstack endpoint create --region RegionOne cloudformation public http://rocky-controller:8000/v1</span><br><span class="line">openstack endpoint create --region RegionOne cloudformation internal http://rocky-controller:8000/v1</span><br><span class="line">openstack endpoint create --region RegionOne cloudformation admin http://rocky-controller:8000/v1</span><br></pre></td></tr></table></figure><p>5）创建编排需要的身份服务中的其他信息来管理堆栈</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">openstack domain create --description <span class="string">"Stack projects and users"</span> heat</span><br><span class="line">openstack user create --domain heat --password-prompt heat_domain_admin</span><br><span class="line">openstack role add --domain heat --user-domain heat --user heat_domain_admin admin</span><br><span class="line">openstack role create heat_stack_owner</span><br><span class="line">openstack role add --project kkproject --user kkutysllb heat_stack_owner</span><br><span class="line">openstack role create heat_stack_user</span><br></pre></td></tr></table></figure><p>6）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine -y</span><br></pre></td></tr></table></figure><p>7）编辑/etc/heat/heat.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/heat/heat.conf&#123;,.bak&#125;</span><br><span class="line">[database] <span class="comment">#部分添加：</span></span><br><span class="line">connection = mysql+pymysql://heat:heat@rocky-controller/heat</span><br><span class="line"></span><br><span class="line">[DEFAULT] <span class="comment">#部分添加：</span></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">heat_metadata_server_url = http://rocky-controller:8000</span><br><span class="line">heat_waitcondition_server_url = http://rocky-controller:8000/v1/waitcondition</span><br><span class="line">stack_domain_admin = heat_domain_admin</span><br><span class="line">stack_domain_admin_password = heat</span><br><span class="line">stack_user_domain_name = heat</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加：</span></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = heat</span><br><span class="line">password = heat</span><br><span class="line"></span><br><span class="line">[trustee] <span class="comment">#部分添加：</span></span><br><span class="line">auth_type = password</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">username = heat</span><br><span class="line">password = heat</span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">[clients_keystone] <span class="comment">#部分添加：</span></span><br><span class="line">auth_uri = http://rocky-controller:5000</span><br></pre></td></tr></table></figure><p>8）初始化heat数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"heat-manage db_sync"</span> heat</span><br></pre></td></tr></table></figure><p>9）配置开机启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-heat-api.service openstack-heat-api-cfn.service openstack-heat-engine.service</span><br><span class="line">systemctl start openstack-heat-api.service openstack-heat-api-cfn.service openstack-heat-engine.service</span><br></pre></td></tr></table></figure><p>10）安装heat-dashboard</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install heat-dashboard</span><br><span class="line">cp /usr/lib/python2.7/site-packages/heat_dashboard/enabled/_[1-9]*.py /usr/share/openstack-dashboard/openstack_dashboard/enabled/</span><br><span class="line"></span><br><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>11）验证安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br><span class="line">openstack orchestration service list</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42ab414195aa5940ba380.jpg" alt></p><h2 id="安装cinder服务"><a href="#安装cinder服务" class="headerlink" title="安装cinder服务"></a><strong>安装cinder服务</strong></h2><h3 id="控制节点：-4"><a href="#控制节点：-4" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）安装cinder数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE cinder;</span><br><span class="line">GRANT ALL PRIVILEGES ON cinder.* TO <span class="string">'cinder'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'cinder'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON cinder.* TO <span class="string">'cinder'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'cinder'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建cinder用户、v2/v3服务实体，添加角色：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt cinder</span><br><span class="line">openstack role add --project service --user cinder admin</span><br><span class="line">openstack service create --name cinderv2 --description <span class="string">"OpenStack Block Storage"</span> volumev2</span><br><span class="line">openstack service create --name cinderv3 --description <span class="string">"OpenStack Block Storage"</span> volumev3</span><br></pre></td></tr></table></figure><p>4）创建块存储服务API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne volumev2 public http://rocky-controller:8776/v2/%\(project_id\)s</span><br><span class="line">openstack endpoint create --region RegionOne volumev2 internal http://rocky-controller:8776/v2/%\(project_id\)s</span><br><span class="line">openstack endpoint create --region RegionOne volumev2 admin http://rocky-controller:8776/v2/%\(project_id\)s</span><br><span class="line">openstack endpoint create --region RegionOne volumev3 public http://rocky-controller:8776/v3/%\(project_id\)s</span><br><span class="line">openstack endpoint create --region RegionOne volumev3 internal http://rocky-controller:8776/v3/%\(project_id\)s</span><br><span class="line">openstack endpoint create --region RegionOne volumev3 admin http://rocky-controller:8776/v3/%\(project_id\)s</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-cinder -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/cinder/cinder.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/cinder/cinder.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">auth_strategy = keystone</span><br><span class="line">my_ip = 10.28.101.81</span><br><span class="line"></span><br><span class="line">[database]<span class="comment">#部分添加如下选项：</span></span><br><span class="line">connection = mysql+pymysql://cinder:cinder@rocky-controller/cinder</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_id = default</span><br><span class="line">user_domain_id = default</span><br><span class="line">project_name = service</span><br><span class="line">username = cinder</span><br><span class="line">password = cinder</span><br><span class="line"></span><br><span class="line">[oslo_concurrency] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">lock_path = /var/lib/cinder/tmp</span><br></pre></td></tr></table></figure><p>7）初始化cinder数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"cinder-manage db sync"</span> cinder</span><br></pre></td></tr></table></figure><p>8）编辑/etc/nova/nova.conf文件并向其中添加cinder模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[cinder]</span><br><span class="line">os_region_name = RegionOne</span><br></pre></td></tr></table></figure><p>9）重启nova-api服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-api.service</span><br></pre></td></tr></table></figure><p>10）配置块存储服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-cinder-api.service openstack-cinder-scheduler.service</span><br><span class="line">systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service</span><br></pre></td></tr></table></figure><h3 id="存储节点（实验环境控制节点兼任）："><a href="#存储节点（实验环境控制节点兼任）：" class="headerlink" title="存储节点（实验环境控制节点兼任）："></a>存储节点（实验环境控制节点兼任）：</h3><p>1）挂载第二块硬盘，大小200G：</p><p><img src="https://pic.imgdb.cn/item/5ef42ada14195aa5940bb4a3.jpg" alt></p><p>2）针对第二块硬盘新建2个100G的分区：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">'n\np\n1\n\n+100G\nw'</span> | fdisk /dev/sdb</span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">'n\np\n2\n\n\nw'</span> | fdisk /dev/sdb</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42af614195aa5940bbfd2.jpg" alt></p><p>3）格式化两个分区为ext4文件系统格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkfs.ext4 /dev/sdb1</span><br><span class="line">mkfs.ext4 /dev/sdb</span><br></pre></td></tr></table></figure><p>4）创建nfs_volume目录，并将第二块/dev/sdb2挂载在其下，同时设置开机自动挂载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /nfs_volume</span><br><span class="line">mount -t ext4 /dev/sdb2 /nfs_volume</span><br><span class="line">df -h | grep /dev/sdb2</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42b4714195aa5940be529.jpg" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"mount -t ext4 /dev/sdb2 /nfs_volume"</span> &gt;&gt;/etc/rc.d/rc.local</span><br><span class="line">tail -1 /etc/rc.d/rc.local</span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><p>5）安装lvm2软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install lvm2 device-mapper-persistent-data -y</span><br></pre></td></tr></table></figure><p>6）设置LVM2逻辑卷服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> lvm2-lvmetad.service &amp;&amp; systemctl start lvm2-lvmetad.service</span><br></pre></td></tr></table></figure><p>7）创建物理卷PV、卷组cinder_kklvm01</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pvcreate /dev/sdb1</span><br><span class="line">vgcreate cinder_kklvm01 /dev/sdb1</span><br><span class="line">vgdisplay</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42bd214195aa5940c23e5.jpg" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在devices &#123; &#125;部分添加 filter = [ "a/sdb1/", "r/.*/"]</span></span><br><span class="line">sed -i <span class="string">'141a filter = [ "a/sdb1/", "r/.*/"]'</span> /etc/lvm/lvm.conf <span class="comment">#在141行后添加</span></span><br><span class="line">systemctl restart lvm2-lvmetad.service</span><br></pre></td></tr></table></figure><p>8）安装NFS服务，作为第二个后端存储：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum install nfs-utils rpcbind -y</span><br><span class="line">mkdir -p /nfs_volume/&#123;cinder_nfs1,cinder_nfs2&#125;</span><br><span class="line">chown root:cinder /nfs_volume/cinder_nfs1</span><br><span class="line">chmod a+w /nfs_volume/cinder_nfs1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'10.28.101.81:/nfs_volume/cinder_nfs1'</span>&gt;/etc/cinder/nfs_shares</span><br><span class="line">chmod a+w /etc/cinder/nfs_shares</span><br><span class="line">chown root:cinder /etc/cinder/nfs_shares</span><br></pre></td></tr></table></figure><p>9）配置NFS服务，并启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"/nfs_volume/cinder_nfs1 *(rw,root_squash,sync,anonuid=165,anongid=165)"</span>&gt;/etc/exports</span><br><span class="line">exportfs -r</span><br><span class="line">systemctl <span class="built_in">enable</span> rpcbind nfs-server &amp;&amp; systemctl restart rpcbind nfs-server</span><br><span class="line">showmount -e localhost</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42c6114195aa5940c664a.jpg" alt></p><p>10）安装cinder-volume服务软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-cinder targetcli python-keystone -y</span><br></pre></td></tr></table></figure><p>11）编辑/etc/cinder/cinder.conf文件，添加如下配置：（由于实验环境存储节点与控制节点合一，所以只需要添加个别选项即可。如果生产环境需要按照要求严格配置）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT] <span class="comment">#部分增加如下选项：</span></span><br><span class="line">log_dir = /var/<span class="built_in">log</span>/cinder</span><br><span class="line">state_path = /var/lib/cinder</span><br><span class="line">enabled_backends = lvm,nfs</span><br><span class="line">glance_api_servers = http://rocky-controller:9292</span><br><span class="line"></span><br><span class="line">[lvm] <span class="comment">#部分增加如下选项：</span></span><br><span class="line">volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver</span><br><span class="line">iscsi_helper = lioadm</span><br><span class="line">iscsi_protocol = iscsi</span><br><span class="line">volume_group = cinder_kklvm01</span><br><span class="line">iscsi_ip_address = 10.28.101.81</span><br><span class="line">volumes_dir = <span class="variable">$state_path</span>/volumes</span><br><span class="line">volume_backend_name = kklvm01</span><br><span class="line"></span><br><span class="line">[nfs] <span class="comment">#部分增加如下选项：</span></span><br><span class="line">volume_driver = cinder.volume.drivers.nfs.NfsDriver</span><br><span class="line">nfs_shares_config = /etc/cinder/nfs_shares</span><br><span class="line">nfs_mount_point_base = <span class="variable">$state_path</span>/mnt</span><br><span class="line">volume_backend_name = kknfs01</span><br></pre></td></tr></table></figure><p>12）配置开机启动cinder-volume服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-cinder-volume.service target.service</span><br><span class="line">systemctl start openstack-cinder-volume.service target.service</span><br></pre></td></tr></table></figure><h2 id="安装barbican服务"><a href="#安装barbican服务" class="headerlink" title="安装barbican服务"></a><strong>安装barbican服务</strong></h2><p><strong>控制节点：</strong></p><p>1）创建barbican数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE barbican;</span><br><span class="line">GRANT ALL PRIVILEGES ON barbican.* TO <span class="string">'barbican'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'barbican'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON barbican.* TO <span class="string">'barbican'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'barbican'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建barbican用户、添加角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt barbican</span><br><span class="line">openstack role add --project service --user barbican admin</span><br><span class="line">openstack role create creator</span><br><span class="line">openstack role add --project service --user barbican creator</span><br><span class="line">openstack service create --name barbican --description <span class="string">"Key Manager"</span> key-manager</span><br></pre></td></tr></table></figure><p>4）创建barbican服务API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne key-manager public http://rocky-controller:9311</span><br><span class="line">openstack endpoint create --region RegionOne key-manager internal http://rocky-controller:9311</span><br><span class="line">openstack endpoint create --region RegionOne key-manager admin http://rocky-controller:9311</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-barbican-api -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/barbican/barbican.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/barbican/barbican.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">sql_connection = mysql+pymysql://barbican:barbican@rocky-controller/barbican</span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">db_auto_create = False</span><br><span class="line"></span><br><span class="line">[keystone_authtoken] <span class="comment">#部分添加如下选项：</span></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = barbican</span><br><span class="line">password = barbican</span><br></pre></td></tr></table></figure><p>7）初始化数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"barbican-manage db upgrade"</span> barbican</span><br></pre></td></tr></table></figure><p>8）创建/etc/httpd/conf.d/wsgi-barbican.conf文件，添加如下内容：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Listen 9311</span><br><span class="line"><span class="tag">&lt;<span class="name">VirtualHost</span> *<span class="attr">:9311</span>&gt;</span></span><br><span class="line">  #ServerName rocky-controller</span><br><span class="line">  ## Logging</span><br><span class="line">  ErrorLog "/var/log/httpd/barbican_wsgi_main_error_ssl.log"</span><br><span class="line">  LogLevel debug</span><br><span class="line">  ServerSignature Off</span><br><span class="line">  CustomLog "/var/log/httpd/barbican_wsgi_main_access_ssl.log" combined</span><br><span class="line"></span><br><span class="line">  WSGIApplicationGroup %&#123;GLOBAL&#125;</span><br><span class="line">  WSGIDaemonProcess barbican-api display-name=barbican-api group=barbican processes=2 threads=8 user=barbican</span><br><span class="line">  WSGIProcessGroup barbican-api</span><br><span class="line">  WSGIScriptAlias / "/usr/lib/python2.7/site-packages/barbican/api/app.wsgi"</span><br><span class="line">  WSGIPassAuthorization On</span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">Directory</span> /<span class="attr">usr</span>/<span class="attr">lib</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">IfVersion</span> &gt;</span>= 2.4&gt;</span><br><span class="line">      Require all granted</span><br><span class="line">    <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    &lt;IfVersion &lt; 2.4&gt;</span><br><span class="line">      Order allow,deny</span><br><span class="line">      Allow from all</span><br><span class="line">    <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">Directory</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">VirtualHost</span>&gt;</span></span><br></pre></td></tr></table></figure><p>9）重启web服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>10）配置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-barbican-api.service &amp;&amp; systemctl restart openstack-barbican-api.service &amp;&amp; systemctl status openstack-barbican-api.service</span><br></pre></td></tr></table></figure><p>11）安装barbican客户端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install python-barbicanclient -y</span><br></pre></td></tr></table></figure><p>12）使用OpenStack CLI存储密钥：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack secret store --name kksecret --payload j4=]d21</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42c8f14195aa5940c7b69.jpg" alt></p><p>13）通过检索来确认机密已存储：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack secret get http://localhost:9311/v1/secrets/ba3380a4-bbb6-4666-bba2-6c1390478eb3</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42ca714195aa5940c8769.jpg" alt></p><p>14）检索秘钥的有效载荷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack secret get http://localhost:9311/v1/secrets/ba3380a4-bbb6-4666-bba2-6c1390478eb3 --payload</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42cbb14195aa5940c904c.jpg" alt></p><h2 id="安装Mistral服务"><a href="#安装Mistral服务" class="headerlink" title="安装Mistral服务"></a><strong>安装Mistral服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装依赖软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python-devel python-setuptools python-pip libffi-devel libxslt-devel libxml2-devel libyaml-devel openssl-devel</span><br></pre></td></tr></table></figure><p>2）安装Mistra软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openstack-mistral-api.noarch openstack-mistral-engine.noarch openstack-mistral-executor.noarch openstack-mistral-ui.noarch</span><br></pre></td></tr></table></figure><p>3）创建数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE mistral;</span><br><span class="line">GRANT ALL PRIVILEGES ON mistral.* TO <span class="string">'mistral'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'mistral'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON mistral.* TO <span class="string">'mistral'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'mistral'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>4）创建用户、添加角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password=mistral mistral</span><br><span class="line">openstack role add --project service --user mistral admin</span><br><span class="line">openstack service create --name mistral --description <span class="string">'OpenStack Workflow service'</span> workflowv2</span><br></pre></td></tr></table></figure><p>5）创建Mistral服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne workflowv2 public http://rocky-controller:8989/v2</span><br><span class="line">openstack endpoint create --region RegionOne workflowv2 internal http://rocky-controlle:8989/v2</span><br><span class="line">openstack endpoint create --region RegionOne workflowv2 admin http://rocky-controlle:8989/v2</span><br></pre></td></tr></table></figure><p>6）编辑配置/etc/mistral/mistral.conf文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/mistral/mistral.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT]</span><br><span class="line">debug = <span class="literal">true</span></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">auth_type = keystone</span><br><span class="line">rpc_backend = rabbit</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://mistral:mistral@rocky-controller/mistral</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = default</span><br><span class="line">user_domain_name = default</span><br><span class="line">project_name = service</span><br><span class="line">username = mistral</span><br><span class="line">password = mistral</span><br><span class="line"></span><br><span class="line">[oslo_messaging_rabbit]</span><br><span class="line">rabbit_host = rocky-controller</span><br><span class="line">rabbit_port = 5672</span><br><span class="line">rabbit_hosts = <span class="variable">$rabbit_host</span>:<span class="variable">$rabbit_port</span></span><br><span class="line">rabbit_userid = openstack</span><br><span class="line">rabbit_password = openstack</span><br></pre></td></tr></table></figure><p>7）初始化数据库，添加缺省项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mistral-db-manage --config-file /etc/mistral/mistral.conf upgrade head</span><br><span class="line">mistral-db-manage --config-file /etc/mistral/mistral.conf populate（报错请忽略）</span><br></pre></td></tr></table></figure><p>8）配置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.service</span><br><span class="line">systemctl restart openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.service</span><br><span class="line">systemctl status openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.service</span><br><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>9）启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mistral-server --server api,engine,executor,notifier --config-file /etc/mistral/mistral.conf</span><br></pre></td></tr></table></figure><h2 id="安装tacker服务"><a href="#安装tacker服务" class="headerlink" title="安装tacker服务"></a><strong>安装tacker服务</strong></h2><p><strong>控制节点：</strong></p><p>1）修改admin-openrc.sh文件如下，增加图中画红框的部分</p><p><img src="https://pic.imgdb.cn/item/5ef42cfc14195aa5940caaad.jpg" alt></p><p>2）创建tacker数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line">CREATE DATABASE tacker;</span><br><span class="line">GRANT ALL PRIVILEGES ON tacker.* TO <span class="string">'tacker'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'tacker'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON tacker.* TO <span class="string">'tacker'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'tacker'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>3）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>4）创建tacker用户、角色，服务实体和端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password tacker tacker</span><br><span class="line">openstack role add --project service --user tacker admin</span><br><span class="line">openstack service create --name tacker --description <span class="string">"Tacker Project"</span> nfv-orchestration</span><br></pre></td></tr></table></figure><p>5）创建tacker服务API的端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne nfv-orchestration public http://rocky-controller:9890/</span><br><span class="line">openstack endpoint create --region RegionOne nfv-orchestration internal http://rocky-controller:9890/</span><br><span class="line">openstack endpoint create --region RegionOne nfv-orchestration admin http://rocky-controller:9890/</span><br></pre></td></tr></table></figure><p>6）从github上下载源码，编译安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /app</span><br><span class="line"><span class="built_in">cd</span> /app</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/tacker -b stable/rocky</span><br><span class="line"><span class="built_in">cd</span> tacker/</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><p>7）创建日志目录和配置目录，生成服务配置文件并进行修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/<span class="built_in">log</span>/tacker</span><br><span class="line">mkdir -p /etc/tacker</span><br><span class="line">. tools/generate_config_file_sample.sh</span><br><span class="line">cp -rf /app/tacker/etc/tacker/* /etc/tacker/</span><br><span class="line">cp /app/tacker/etc/tacker/tacker.conf.sample /etc/tacker/tacker.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改/etc/tacker/tacker.conf文件如下：</span></span><br><span class="line">[DEFAULT]</span><br><span class="line">auth_strategy = keystone</span><br><span class="line">policy_file = /etc/tacker/policy.json</span><br><span class="line">debug = True</span><br><span class="line">use_syslog = False</span><br><span class="line">bind_host = 10.28.101.81</span><br><span class="line">bind_port = 9890</span><br><span class="line">service_plugins = nfvo,vnfm</span><br><span class="line">state_path = /var/lib/tacker</span><br><span class="line"></span><br><span class="line">[nfvo_vim]</span><br><span class="line">vim_drivers = openstack</span><br><span class="line">[keystone_authtoken]</span><br><span class="line">memcached_servers = 11211</span><br><span class="line">region_name = RegionOne</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_name = Default</span><br><span class="line">user_domain_name = Default</span><br><span class="line">username = tacker</span><br><span class="line">project_name = service</span><br><span class="line">password = tacker</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">www_authenticate_uri = http:/rocky-controller:5000</span><br><span class="line"></span><br><span class="line">[agent]</span><br><span class="line">root_helper = sudo /usr/bin/tacker-rootwrap /etc/tacker/rootwrap.conf</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://tacker:tacker@rocky-controller:3306/tacker?charset=utf8</span><br><span class="line"></span><br><span class="line">[tacker]</span><br><span class="line">monitor_driver = ping,http_ping</span><br></pre></td></tr></table></figure><p>8）初始化tacker数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/tacker-db-manage --config-file /etc/tacker/tacker.conf upgrade head</span><br></pre></td></tr></table></figure><p>9）将tacker.service and tacker-conductor.service复制到/etc/systemd/system/下，并重启systemctl守护进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp /app/tacker/etc/systemd/system/tacker.service /etc/systemd/system/</span><br><span class="line">cp /app/tacker/etc/systemd/system/tacker-conductor.service /etc/systemd/system/</span><br><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure><p>10）源码方式安装tacker客户端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /app</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/python-tackerclient -b stable/rocky</span><br><span class="line"><span class="built_in">cd</span> python-tackerclient</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><p>11）安装tacker的dashboard</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /app</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/tacker-horizon -b stable/rocky</span><br><span class="line"><span class="built_in">cd</span> tacker-horizon</span><br><span class="line">python setup.py install</span><br><span class="line">cp tacker_horizon/enabled/* /usr/share/openstack-dashboard/openstack_dashboard/enabled/</span><br><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>12）新开两个终端（或者让进程在后台运行也可以，我的选择），分别运行tacker的服务和执行体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tacker-server --config-file /etc/tacker/tacker.conf --<span class="built_in">log</span>-file /var/<span class="built_in">log</span>/tacker/takcer.log &amp;</span><br><span class="line">tacker-conductor --config-file /etc/tacker/tacker.conf --<span class="built_in">log</span>-file /var/<span class="built_in">log</span>/tacker/tacker-conductor.log &amp;</span><br></pre></td></tr></table></figure><p>13）创建vim_config.yaml文件，注册OPS为默认的VIM</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim vim_config.yaml</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42d5414195aa5940cd783.jpg" alt></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack vim register --config-file vim_config.yaml --description <span class="string">'kk first vim'</span> --is-default kkvim</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42d6514195aa5940ce086.jpg" alt></p><p>14）验证VIM是否注册成功</p><p><img src="https://pic.imgdb.cn/item/5ef42d7f14195aa5940ceedd.jpg" alt></p><p>15）创建简单实例VNFD模版文件和VNFD文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim sample_vnfd.yaml</span><br><span class="line">openstack vnf descriptor create --vnfd-file sample_vnfd.yaml kk-first-vnfd</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef42d9614195aa5940cfe16.jpg" alt></p><p>图太长，未截取全</p><p>16）创建VNF</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack vnf create --vnfd-name kk-first-vnfd sample_vnf01</span><br></pre></td></tr></table></figure><h2 id="安装aodh服务"><a href="#安装aodh服务" class="headerlink" title="安装aodh服务"></a><strong>安装aodh服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装aodh数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE aodh;</span><br><span class="line">GRANT ALL PRIVILEGES ON aodh.* TO <span class="string">'aodh'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'aodh'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON aodh.* TO <span class="string">'aodh'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'aodh'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建aodh用户、角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password aodh aodh</span><br><span class="line">openstack role add --project service --user aodh admin</span><br><span class="line">openstack service create --name aodh --description <span class="string">"Telemetry"</span> alarming</span><br></pre></td></tr></table></figure><p>4）创建aodh服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne alarming public http://rocky-controller:8042</span><br><span class="line">openstack endpoint create --region RegionOne alarming internal http://rocky-controller:8042</span><br><span class="line">openstack endpoint create --region RegionOne alarming admin http://rocky-controller:8042</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-aodh-api openstack-aodh-evaluator openstack-aodh-notifier openstack-aodh-listener openstack-aodh-expirer python-aodhclient -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/aodh/aodh.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/aodh/aodh.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT]</span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line">connection = mysql+pymysql://aodh:aodh@rocky-controller/aodh</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">auth_type = password</span><br><span class="line">project_domain_id = default</span><br><span class="line">user_domain_id = default</span><br><span class="line">project_name = service</span><br><span class="line">username = aodh</span><br><span class="line">password = aodh</span><br><span class="line"></span><br><span class="line">[service_credentials]</span><br><span class="line">auth_type = password</span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line">project_domain_id = default</span><br><span class="line">user_domain_id = default</span><br><span class="line">project_name = service</span><br><span class="line">username = aodh</span><br><span class="line">password = aodh</span><br><span class="line">interface = internalURL</span><br><span class="line">region_name = RegionOne</span><br></pre></td></tr></table></figure><p>7）初始化aodh数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aodh-dbsync</span><br></pre></td></tr></table></figure><p>8）配置开机启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-aodh-api.service openstack-aodh-evaluator.service openstack-aodh-notifier.service openstack-aodh-listener.service</span><br><span class="line">systemctl start openstack-aodh-api.service openstack-aodh-evaluator.service openstack-aodh-notifier.service openstack-aodh-listener.service</span><br></pre></td></tr></table></figure><h2 id="安装ceilometer服务"><a href="#安装ceilometer服务" class="headerlink" title="安装ceilometer服务"></a><strong>安装ceilometer服务</strong></h2><h3 id="控制节点"><a href="#控制节点" class="headerlink" title="控制节点"></a><strong>控制节点</strong></h3><p>1）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>2）创建ceilometer用户、角色：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password ceilometer ceilometer</span><br><span class="line">openstack role add --project service --user ceilometer admin</span><br></pre></td></tr></table></figure><p>3）创建gnocchi用户、角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password gnocchi gnocchi</span><br><span class="line">openstack service create --name gnocchi --description <span class="string">"Metric Service"</span> metric</span><br><span class="line">openstack role add --project service --user gnocchi admin</span><br></pre></td></tr></table></figure><p>4）创建Metric服务API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne metric public http://rocky-controller:8041</span><br><span class="line">openstack endpoint create --region RegionOne metric internal http://rocky-controller:8041</span><br><span class="line">openstack endpoint create --region RegionOne metric admin http://rocky-controller:8041</span><br></pre></td></tr></table></figure><p>5）安装gnocchi软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-gnocchi-api openstack-gnocchi-metricd python-gnocchiclient -y</span><br></pre></td></tr></table></figure><p>6）创建gnocchi数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line">CREATE DATABASE gnocchi;</span><br><span class="line">GRANT ALL PRIVILEGES ON gnocchi.* TO <span class="string">'gnocchi'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'gnocchi'</span>;</span><br><span class="line">GRANT ALL PRIVILEGES ON gnocchi.* TO <span class="string">'gnocchi'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'gnocchi'</span>;</span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>7）编辑/etc/gnocchi/gnocchi.conf文件变添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置gnocchi功能参数，log地址以及对接redis url端口</span></span><br><span class="line">[DEFAULT]</span><br><span class="line">debug = <span class="literal">true</span></span><br><span class="line">verbose = <span class="literal">true</span></span><br><span class="line">log_dir = /var/<span class="built_in">log</span>/gnocchi</span><br><span class="line">parallel_operations = 4</span><br><span class="line">coordination_url = redis://rocky-controller:6379</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置gnocchi工作端口信息，host为控制节点管理IP</span></span><br><span class="line">[api]</span><br><span class="line">auth_mode = keystone</span><br><span class="line">host = 10.28.101.81</span><br><span class="line">port = 8041</span><br><span class="line">uwsgi_mode = http-socket</span><br><span class="line">max_limit = 1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置ceilometer默认收集测试指标策略</span></span><br><span class="line">[archive_policy]</span><br><span class="line">default_aggregation_methods = mean,min,max,sum,std,count</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置允许的访问来源，这里是grafana的地址，允许前端直接访问gnocchi获取计量数据，需要配置允许跨域访问（Keystone中完成）</span></span><br><span class="line">[cors]</span><br><span class="line">allowed_origin = http://rocky-controller:3000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置keystone认证信息，该模块需要另外添加</span></span><br><span class="line">[keystone_authtoken]</span><br><span class="line">auth_type = password</span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line">project_domain_name = Default</span><br><span class="line">user_domain_name = Default</span><br><span class="line">project_name = service</span><br><span class="line">username = gnocchi</span><br><span class="line">password = gnocchi</span><br><span class="line">interface = internalURL</span><br><span class="line">region_name = RegionOne</span><br><span class="line">service_token_roles_required = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置元数据默认存储方式。</span></span><br><span class="line">[indexer]</span><br><span class="line">url = mysql+pymysql://gnocchi:gnocchi@rocky-controller/gnocchi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置gnocchi存储方式以及位置，在这种配置下将其存储到本地文件系统。</span></span><br><span class="line">[storage]</span><br><span class="line">coordination_url = redis://rocky-controller:6379</span><br><span class="line">file_basepath = /var/lib/gnocchi</span><br><span class="line">driver = file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置数据库检索的策略</span></span><br><span class="line">[metricd]</span><br><span class="line">workers = 4</span><br><span class="line">metric_processing_delay = 60</span><br><span class="line">greedy = <span class="literal">true</span></span><br><span class="line">metric_reporting_delay = 120</span><br><span class="line">metric_cleanup_delay = 300</span><br></pre></td></tr></table></figure><p>8）安装redis软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install redis -y</span><br></pre></td></tr></table></figure><p>9）编辑/etc/redis.conf ，修改以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置redis可以在后台启动：daemonize yes</span></span><br><span class="line"><span class="comment"># 配置redis关闭安全模式：protected-mode no</span></span><br><span class="line"><span class="comment"># 配置redis绑定控制节点主机：bind 10.28.101.81</span></span><br></pre></td></tr></table></figure><p>10）启动redis-server，设置开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">redis-server /etc/redis.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"redis-server /etc/redis.conf"</span> &gt;&gt; /etc/rc.local</span><br><span class="line">chmod a+x /etc/rc.local</span><br></pre></td></tr></table></figure><p>11）安装uwsgi插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install uwsgi-plugin-common uwsgi-plugin-python uwsgi -y</span><br></pre></td></tr></table></figure><p>12）赋予/var/lib/gnocchi文件可读写权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R 777 /var/lib/gnocchi</span><br></pre></td></tr></table></figure><p>13）初始化gnocchi数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gnocchi-upgrade</span><br></pre></td></tr></table></figure><p>14）配置gnocchi服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-gnocchi-api.service openstack-gnocchi-metricd.service</span><br><span class="line">systemctl start openstack-gnocchi-api.service openstack-gnocchi-metricd.service</span><br><span class="line">systemctl status openstack-gnocchi-api.service openstack-gnocchi-metricd.service</span><br></pre></td></tr></table></figure><p>15）安装ceilometer软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-ceilometer-notification openstack-ceilometer-central -y</span><br></pre></td></tr></table></figure><p>16）编辑/etc/ceilometer/pipeline.yaml文件并完成以下gnocchi配置：</p><p><img src="https://pic.imgdb.cn/item/5ef42e2b14195aa5940d426c.jpg" alt></p><p>17）编辑/etc/ceilometer/ceilometer.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/ceilometer/ceilometer.conf&#123;,.bak&#125;</span><br><span class="line">[DEFAULT]</span><br><span class="line">debug = <span class="literal">false</span></span><br><span class="line">auth_strategy = keystone</span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line">pipeline_cfg_file = pipeline.yaml</span><br><span class="line"></span><br><span class="line">[service_credentials]</span><br><span class="line">auth_type = password</span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line">project_domain_id = default</span><br><span class="line">user_domain_id = default</span><br><span class="line">project_name = service</span><br><span class="line">username = ceilometer</span><br><span class="line">password = ceilometer</span><br><span class="line">interface = internalURL</span><br><span class="line">region_name = RegionOne</span><br><span class="line"> </span><br><span class="line">[notification]</span><br><span class="line">store_events = <span class="literal">false</span></span><br><span class="line">messaging_urls = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"> </span><br><span class="line">[polling]</span><br><span class="line">cfg_file = polling.yaml</span><br></pre></td></tr></table></figure><p>13）初始化数据库，在Gnocchi上创建资源，要求Gnocchi已运行并在Keystone配置了endpoint。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceilometer-upgrade</span><br></pre></td></tr></table></figure><p>14）配置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-ceilometer-notification.service openstack-ceilometer-central.service</span><br><span class="line">systemctl start openstack-ceilometer-notification.service openstack-ceilometer-central.service</span><br></pre></td></tr></table></figure><h3 id="计算节点"><a href="#计算节点" class="headerlink" title="计算节点"></a>计算节点</h3><p>1）安装软件包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-ceilometer-compute openstack-ceilometer-ipmi -y</span><br></pre></td></tr></table></figure><p>2）编辑/etc/ceilometer/ceilometer.conf文件并完成以下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用scp指令将控制节点的文件拷贝过来即可</span></span><br></pre></td></tr></table></figure><p>3）编辑/etc/nova/nova.conf文件并在以下[DEFAULT]部分配置通知：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">instance_usage_audit = True</span><br><span class="line">instance_usage_audit_period = hour</span><br><span class="line"></span><br><span class="line">[notifications]</span><br><span class="line">notify_on_state_change = vm_and_task_state</span><br><span class="line">notification_format = unversioned</span><br><span class="line"></span><br><span class="line">[oslo_messaging_notifications]</span><br><span class="line">driver = messagingv2</span><br></pre></td></tr></table></figure><p>4）（可选）配置轮询ipmi（虚拟机方式下用不到）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑/etc/sudoers文件并包含：</span></span><br><span class="line"><span class="string">ceilometer</span> <span class="string">ALL</span> <span class="string">=</span> <span class="string">(root)</span> <span class="attr">NOPASSWD:</span> <span class="string">/usr/bin/ceilometer-rootwrap</span> <span class="string">/etc/ceilometer/rootwrap.conf</span> <span class="string">*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编辑/etc/ceilometer/polling.yaml，添加以下度量项目（注意格式对齐）：</span></span><br><span class="line"><span class="attr">- name:</span> <span class="string">ipmi</span></span><br><span class="line"><span class="attr">  interval:</span> <span class="number">300</span></span><br><span class="line"><span class="attr">  meters:</span></span><br><span class="line"><span class="bullet">   -</span> <span class="string">hardware.ipmi.temperature</span></span><br></pre></td></tr></table></figure><p>5）完成计算节点安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-ceilometer-compute.service openstack-ceilometer-ipmi.service</span><br><span class="line">systemctl start openstack-ceilometer-compute.service openstack-ceilometer-ipmi.service</span><br><span class="line">systemctl restart openstack-nova-compute.service</span><br></pre></td></tr></table></figure><p><em>—————————————————————————————结束————————————————————————————————</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;title: 2020-03-01-CentOS 7手动部署Openstack Rocky版本笔记（含开源NFVO/VNFM）&lt;br&gt;date: 2020-03-01 20:03:21&lt;br&gt;tags:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;云计算&lt;br&gt;category:&lt;/li&gt;
&lt;li&gt;OpenStack&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇博文记录了在VMware Workstations上手动部署双节点OpenStack Rocky发行版本的部署笔记，包括OpenStack的9大基础核心组件：Keystone、Glance、Nova、Neutron、Cinder、Swift、Heat和Telemetry等服务，以及开源NFVO/VNFM项目tacker，同时将本地OpenStack的环境注册为VIM，构造一个开源的MANO环境。其中，9大核心组件的Swift在对象存储管理服务—Swift文中介绍部署方法，通过单节点双硬盘的方式模拟vNode，同时模拟实际网络云环境中对象存储主要用于数据备份和镜像存储的功能。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>KVM虚拟机网络管理实战</title>
    <link href="https://kkutysllb.cn/2020/03/01/KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98/"/>
    <id>https://kkutysllb.cn/2020/03/01/KVM虚拟机网络管理实战/</id>
    <published>2020-03-01T11:45:16.000Z</published>
    <updated>2020-06-25T13:11:07.302Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章我们介绍完KVM的存储管理实战后，接下来我们本篇文章就开始主要介绍KVM的网络管理实战。KVM的网络管理模型与OpenStack Neutron中的网络管理非常类似，而在学些OpenStack时，网络服务Neutron往往是很多人初学OpenStack的一个难点。因此，掌握了本篇内容的知识点，对于后续学习OpenStack Neutron部分的内容有事半功倍的效果。<a id="more"></a></p><h2 id="virsh中的网络管理基础"><a href="#virsh中的网络管理基础" class="headerlink" title="virsh中的网络管理基础"></a><strong>virsh中的网络管理基础</strong></h2><p>在介绍KVM中常见的网络模型之前，我们先来看看virsh有哪些网络管理的命令。与存储管理实战和全生命周期管理实战类似，virsh主要提供对节点上的物理网络接口和分配给虚拟机的虚拟网络进行管理的命令。包括：创建节点上的物理接口、编辑节点上物理接口的XML配置文件，查询节点上物理接口、创建虚拟机的虚拟网络、编辑虚拟机的虚拟网络和删除虚拟机的虚拟网络等。常用的命令和作用如下：</p><table><thead><tr><th><strong>virsh网络管理常用命令</strong></th><th></th></tr></thead><tbody><tr><td><strong>命令</strong></td><td><strong>功能描述</strong></td></tr><tr><td>iface-list</td><td>显示出物理主机的网络接口列表</td></tr><tr><td>iface-list<if-name></if-name></td><td>根据网络接口名称查询其对应的MAC地址</td></tr><tr><td>iface-name<mac></mac></td><td>根据MAC地址查询其对应的网络接口名称</td></tr><tr><td>iface-edit<if-name or uuid></if-name></td><td>编辑一个物理主机的网络接口XML配置文件</td></tr><tr><td>iface-dumpxml<ifa-name or uuid></ifa-name></td><td>以XML配置文件转存出一个网络接口的状态信息</td></tr><tr><td>iface-destroy<if-name or uuid></if-name></td><td>关闭宿主机上的一个物理网络接口</td></tr><tr><td>net-list</td><td>列出libvirt管理的虚拟网络</td></tr><tr><td>net-info<net-name or uuid></net-name></td><td>根据名称查询一个虚拟网络的基本信息</td></tr><tr><td>net-uuid<net-name></net-name></td><td>根据名称查询一个虚拟网络的uuid</td></tr><tr><td>net-name<uuid></uuid></td><td>根据uuid查询一个虚拟网络的名称</td></tr><tr><td>net-create&lt;net.xml&gt;</td><td>根据一个网络的xml配置文件创建一个虚拟网络</td></tr><tr><td>net-edit<net-name or uuid></net-name></td><td>编译一个虚拟网络的XML配置文件</td></tr><tr><td>net-dumpxml<net-name or uuid></net-name></td><td>转存出一个虚拟网络的XML配置文件</td></tr><tr><td>net-destroy<net-name or uuid></net-name></td><td>销毁一个虚拟网络</td></tr></tbody></table><p>有了上面的命令基础，我们就可以几个简单验证。比如，我们现在想看下当前节点有哪几个物理接口，可以使用如下命令：</p><p><img src="https://pic.imgdb.cn/item/5ef4a05114195aa59445dfee.jpg" alt></p><p>现在，我们想把br0这个网桥的XML文件导出转存，可以使用下面的命令的实现。</p><p><img src="https://pic.imgdb.cn/item/5ef4a05d14195aa59445eb27.jpg" alt></p><p>以后就可以按照上面的xml配置文件，写一个网桥，然后通过iface-define定义，然后再通过iface-start命令启动即可（需要注意MAC不能重复，可以使用random工具随机生成MAC）。</p><p>我们完成物理接口的简单验证后，接下来看看虚拟网络的信息。比如，我们现在需要看下当前节点有哪些虚拟网络，可以通过net-list命令实现。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a06914195aa59445f3af.jpg" alt></p><p>上图表示当前节点只有一个虚拟网络default，我们通过net-info命令可以看下default网络的详细信息。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a07714195aa59445ff4e.jpg" alt></p><p>同理，我们也可以将default网络的xml配置转存，便于后续自己编写网络xml文件。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a08614195aa594460a5d.jpg" alt></p><h2 id="KVM中常见的网络模型"><a href="#KVM中常见的网络模型" class="headerlink" title="KVM中常见的网络模型"></a><strong>KVM中常见的网络模型</strong></h2><p>有了上面基础认知，下面我们就来看下KVM中常见的4种简单网络模型，分别如下：</p><ul><li><strong>隔离模型：</strong>虚拟机之间组建网络，该模式无法与宿主机通信，无法与其他网络通信，相当于虚拟机只是连接到一台交换机上。其对应OpenStack Neutron中local网络模型。</li><li><strong>路由模型：</strong>相当于虚拟机连接到一台路由器上，由路由器(物理网卡)，统一转发，但是不会改变源地址。</li><li><strong>NAT模型：</strong>在路由模式中，会出现虚拟机可以访问其他主机，但是其他主机的报文无法到达虚拟机，而NAT模式则将源地址转换为路由器(物理网卡)地址，这样其他主机也知道报文来自那个主机，在docker环境中经常被使用。</li><li><strong>桥接模型：</strong>在宿主机中创建一张虚拟网卡作为宿主机的网卡，而物理网卡则作为交换机。</li></ul><p>为了加深大家的理解，我们后面就对上述4种网络模型逐一进行验证。</p><h3 id="隔离网络模型"><a href="#隔离网络模型" class="headerlink" title="隔离网络模型"></a><strong>隔离网络模型</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4a09914195aa594461d25.jpg" alt></p><p>如上图所示，VM0和VM1都是在宿主机上创建的虚拟机，虚拟机的网卡分为前半段和后半段，前半段位于虚拟机上，后半段在宿主机上，按照图中所示，前半段就是eth0，它是在虚拟机内部看到的网卡名字，而后半段就是vnet0和vnet1，它们是在宿主机上看到的网卡名字。实际上，在VM1上所有发往eth0的数据就是直接发往vnet0，是由vnet0进行数据的传送处理。</p><p>在隔离模式下，宿主机创建一个虚拟交换机vSwitch，然后把vnet0和vnet1接入到该虚拟交换机，交换机也可以叫做bridge，因为vnet0和vnet1在一个网桥内，所以可以互相通信，而虚拟机的eth0是通过后半段进行数据传输，所以只要虚拟机的前半段ip在一个网段内，就可以互相通信，这就是隔离模式。下面，我们就通过实例进行验证。</p><p><strong>Step1：</strong>创建一个网桥br1，且不连接宿主机的任何物理网卡。</p><p><img src="https://pic.imgdb.cn/item/5ef4a0ad14195aa594462c78.jpg" alt></p><p><strong>Step2：</strong>我们用centos7.5模板虚拟机镜像直接迅速拉起两个测试虚拟机VM0和VM1。</p><p><img src="https://pic.imgdb.cn/item/5ef4a0bb14195aa5944636f5.jpg" alt></p><p>由于我们没有给网桥配置IP和DHCP分配范围，所以虚拟机没有IP地址。我们可以手动给虚拟机添加IP，VM0的虚拟机地址设置为10.10.101.251。如下</p><p><img src="https://pic.imgdb.cn/item/5ef4a0ca14195aa5944643b7.jpg" alt></p><p>同理，给虚拟机VM1设置IP为192.168.101.252。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a0d714195aa594464eac.jpg" alt></p><p>此时，我们对VM0与VM1进行ping测试，是可以ping通的。但是，即使两个虚拟机与宿主机（10.10.101.11）在同一个网段，仍然无法ping通。所以，这是一个隔离网络模型。</p><p><img src="https://pic.imgdb.cn/item/5ef4a0e514195aa594465b21.jpg" alt></p><p><strong>Step3：</strong>此时，我们在宿主机上查询网桥br1的挂接信息和虚拟机的虚拟网卡vnet信息。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a0f214195aa59446653e.jpg" alt></p><h3 id="路由网络模型"><a href="#路由网络模型" class="headerlink" title="路由网络模型"></a><strong>路由网络模型</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4a10014195aa594467219.jpg" alt></p><p>在隔离模型的基础上，将宿主机的一块虚拟网卡virnet0加入到虚拟网桥中，这样virnet0就可以和虚拟机通信，通过将虚拟机的默认网关设置为virnet0的IP地址，然后在宿主机中打开IP地址转发，使得虚拟机可以访问宿主机。不过此时虚拟机仅仅可以将报文发送到外部网络，因为外部网络没有路由到虚拟机中，所以外部网络无法将报文回传给虚拟机。</p><p><strong>step1：</strong>在宿主机上用tunctl创建一个虚拟网卡，也就是创建一个tap设备virnet0。忘了tunctl是什么的，请回顾本站Linux原生网络虚拟化内容。</p><p><img src="https://pic.imgdb.cn/item/5ef4a10e14195aa594467e48.jpg" alt></p><p><strong>step2：</strong>将virnet0加入到网桥br1中，并设置网桥br1为网关，地址为10.10.101.100</p><p><img src="https://pic.imgdb.cn/item/5ef4a11c14195aa594468950.jpg" alt></p><p><strong>step3：</strong>进入虚拟机设置网关为10.10.101.100，也就是配置一条默认路由即可。</p><p><img src="https://pic.imgdb.cn/item/5ef4a12914195aa5944691f7.jpg" alt></p><p><strong>step4：</strong>在宿主机中打开ip包转发功能，也就是将宿主机变成一个路由器（详见本站Linux原生网络虚拟化文章内容）。</p><p><img src="https://pic.imgdb.cn/item/5ef4a13614195aa594469a9a.jpg" alt></p><p><strong>step5：</strong>此时，我们在虚拟机中尝试ping宿主机，发现可以ping通。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a14414195aa59446a2bc.jpg" alt></p><p>但是，我们无法ping通宿主机的网关，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a15214195aa59446ab90.jpg" alt></p><p>这是因为报文发到网关后，网关找不到回包的路由，所以报文无法回复，这时候，我们通过在宿主机上添加一条iptables规则，使得网关可以回包。至于，iptables如何配置，请参见本站Linux常用运维工具分类中的iptable文章。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -s 10.10.101.0/24 -j MASQUERADE</span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef4a16014195aa59446b383.jpg" alt></p><p>添加包转发规则后，就可以在虚拟机中ping通宿主机网关了。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a16e14195aa59446bc1f.jpg" alt></p><p><strong>step6：</strong>但是，我们在宿主机外的机器上仍无法ping通虚拟机。比如，我们在真实的物理机（win10）上还是无法ping通虚拟机10.10.101.251。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a17c14195aa59446c591.jpg" alt></p><p>为了解决外部主机ping通虚拟机的问题，我们需要win10上也添加一条路由，将访问虚拟机的数据包发往VMware的虚拟网卡地址192.168.101.11上。完成后，再次进行尝试，发现网络已通，且可以通过win10直接ssh虚拟机vm0。如下：（<strong>这里注意必须指定VMware NAT网卡的地址段，因为在VMware内部只有NAT虚拟网卡有三层转发功能）</strong></p><p><img src="https://pic.imgdb.cn/item/5ef4a18b14195aa59446d206.jpg" alt></p><p>通过上述实践，也可以发现路由模型的缺陷，虽然虚拟机能同宿主机通信，也能将数据包发到外部网络，但是外部网络无法回传数据包，要想外部网络能与虚拟机通信，就要添加对应的路由规则。这对于大规模的虚拟环境，这显然是不科学的。</p><h3 id="NAT网络模型"><a href="#NAT网络模型" class="headerlink" title="NAT网络模型"></a><strong>NAT网络模型</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4a19914195aa59446dc35.jpg" alt></p><p>NAT模型其实就是SNAT的实现，路由中虚拟机能将报文发送给外部主机，但是外部主机因找不到通往虚拟机的路由因而无法回应请求。外部主机能同宿主机通信，所以在宿主机上添加一个NAT转发，从而在外部主机响应能够到达虚拟机，但是不能直接访问虚拟机。这种方式是将虚拟机的IP地址转换为宿主机上的某个地址，从而实现虚拟机与外部网络通信，其实际上只是通过iptables的nat表的POSTROUTING链实现地址转换罢了。如果要实现外部网络直接访问虚拟机，还需要在宿主机上配置DNAT转发，一般不采用这种方式。</p><p>在我们创建KVM虚拟机时，如果使用default网络类型，它就是一个NAT网络。我们可以看下它的XML配置文件内容：</p><p><img src="https://pic.imgdb.cn/item/5ef4a1a814195aa59446e68a.jpg" alt></p><p>有了上面的了解，那我们创建一个NAT网络就很轻松了。怎么办？重新写一份？no…no…这不符合互联网时代的要求！拷贝一份，直接修改。。。。同时，我们可以在自定义的nat网络配置文件中，指定两个虚拟机的IP地址。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a1b714195aa59446f356.jpg" alt></p><p>完成上面的配置后，现在定义一个网络natnet，并设置开启自动启动。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a1c614195aa59446ff08.jpg" alt></p><p>完成上面配置后，libvirt会自动帮我们创建一个XML文件描述的网桥natbr0，并且将网关IP和MAC按照XML文件的配置自动生成。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a1d414195aa594470a92.jpg" alt></p><p>同时，libvirt会在宿主机的iptables的nat转发表中，自动帮我增加nat网络的数据包转发策略。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a1e114195aa594471559.jpg" alt></p><p>完成上面的配置，此时我们需要修改两个虚拟机XML文件的网络配置信息，然后重新定义启动。虚拟机XML配置文件网络信息修改，只需要修改接口类型为network类型，network的上行接口修改为我们自定义的网络名称即可，其它不变。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a1ed14195aa594472001.jpg" alt></p><p>然后，我们重新定义虚拟机并启动。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a20014195aa594472f9c.jpg" alt></p><p>现在，我们进入虚拟机查询的虚拟机的IP确实为我们制定的IP地址188.64.100.2，且自动增加了默认路由指向natbr0网桥上联接口地址188.64.100.1，而且虚拟机和宿主机可以互相ping通。如下</p><p><img src="https://pic.imgdb.cn/item/5ef4a21314195aa594473efa.jpg" alt></p><p>而且，我们在虚拟机下可以ping通外部真实物理机win10，但是从真实物理机win10下ping测虚拟机vm0却发现还是ping不通，这就是典型的SNAT原理，也就是虚拟机可以访问外部网络，但是外部网络看不见内网的虚拟机，这样对内网的虚拟机中应用起到保护作用。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a22114195aa594474c45.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef4a22d14195aa594475625.jpg" alt></p><p>如果，需要虚拟机访问互联网internet，需要给虚拟机中增加DNS解析，nameserver就配置成虚拟网关188.64.100.1即可。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a24314195aa594476859.jpg" alt></p><p>至此，nat网络模型验证完毕。四个基本模型中还有一个桥接网络模型，这也是网上常见的配置模型，这种模型就是将虚拟机与宿主机放在一个局域网内，可以互访，且同一局域网的其他外部主机也能访问虚拟机。而且，如果宿主机能上互联网，那么互联网的服务器也能访问虚拟机。除非有特殊应用场景，否则一般在实际运维不采用这种方式，因为不安全。这种方式我就懒得举例了，网上的教程很多。最后，祝大家在KVM的世界里愉快的玩耍。。。。。。嘿嘿嘿嘿。。。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章我们介绍完KVM的存储管理实战后，接下来我们本篇文章就开始主要介绍KVM的网络管理实战。KVM的网络管理模型与OpenStack Neutron中的网络管理非常类似，而在学些OpenStack时，网络服务Neutron往往是很多人初学OpenStack的一个难点。因此，掌握了本篇内容的知识点，对于后续学习OpenStack Neutron部分的内容有事半功倍的效果。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>KVM虚拟机存储管理实战（下篇）</title>
    <link href="https://kkutysllb.cn/2020/03/01/KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://kkutysllb.cn/2020/03/01/KVM虚拟机存储管理实战（下篇）/</id>
    <published>2020-03-01T11:31:58.000Z</published>
    <updated>2020-06-25T12:38:37.758Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇中我们介绍qemu-img这个对磁盘镜像操作的命令，包括检查镜像磁盘，创建镜像磁盘、查看镜像磁盘信息、转换镜像磁盘格式、调整镜像磁盘大小以及镜像磁盘的快照链操作。但是，在镜像磁盘快照链操作中，我们也提到了通过qemu-img命令创建快照链只能在虚拟机关机状态下运行。如果虚拟机为运行态，只能通过virsh save vm来保存当前状态。那么，本文就专门讲述在KVM虚拟机中如何通过virsh命令来创建快照链，以及链中快照的相互关系，如何缩短链，如何利用这条链回滚我们的虚拟机到某个状态等等。<a id="more"></a></p><h2 id="什么是虚拟机快照链"><a href="#什么是虚拟机快照链" class="headerlink" title="什么是虚拟机快照链"></a><strong>什么是虚拟机快照链</strong></h2><p>虚拟机快照保存了虚拟机在某个指定时间点的状态（包括操作系统和所有的程序），利用快照，我们可以恢复虚拟机到某个以前的状态。比如：测试软件的时候经常需要回滚系统，以及新手安装OpenStack时为了防止重头再来，每安装成功一个服务就做一次快照等等。</p><p><strong>快照链就是多个快照组成的关系链</strong>，这些快照按照创建时间排列成链，像下面这样。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base-image&lt;--guest1&lt;--snap1&lt;--snap2&lt;--snap3&lt;--snap4&lt;--当前(active)</span><br></pre></td></tr></table></figure><p>如上，base-image是制作好的一个qcow2格式的磁盘镜像文件，它包含有完整的OS以及引导程序。现在，以这个base-image为模板创建多个虚拟机，简单点方法就是每创建一个虚拟机我们就把这个镜像完整复制一份，但这种做法效率底下，满足不了生产需要。这时，就用到了qcow2镜像的<strong>copy-on-write（写时复制）特性。</strong></p><p>qcow2(qemu copy-on-write)格式镜像支持快照，具有创建一个base-image，以及在base-image(backing file)基础上创建多个<strong>copy-on-write overlays镜像</strong>的能力。这里需要解释下backing file和overlay的概念。在上面那条链中，我们为base-image创建一个guest1，那么此时base-image就是guest1的backing file，guest1就是base-image的overlay。同理，为guest1虚拟机创建了一个快照snap1，此时guest1就是snap1的backing file，snap1是guest1的overlay。</p><p><strong>backing files和overlays十分有用，可以快速的创建瘦装备实例，特别是在开发测试过程中可以快速回滚到之前某个状态。</strong>以CentOS系统来说，我们制作了一个qcow2格式的虚拟机镜像，想要以它作为模板来创建多个虚拟机实例，有两种方法实现：</p><p><strong>1）每新建一个实例，把centosbase模板复制一份，创建速度慢。说白了就是复制原始虚拟机。</strong></p><p><strong>2）使用copy-on-write技术(qcow2格式的特性)，创建基于模板的实例，创建速度很快，可以通过查看磁盘文件信息，进行大小比较。也就是我们将存储虚拟化特性中提到的链接克隆。</strong></p><p>如下，我们有一个centosbase的原始镜像(包含完整OS和引导程序)，现在用它作为模板创建多个虚拟机，每个虚拟机都可以创建多个快照组成快照链，但是不能直接为centosbase创建快照。</p><p><img src="https://pic.imgdb.cn/item/5ef498c914195aa5944187fc.jpg" alt></p><p>上图中centos1，centos2，centos3等都是基于centosbase模板创建的虚拟机(guest)，接下来做的测试需要用到centos1_sn1、centos1_sn2、centos1_sn3等centos1的快照链实现。也就是说，我们可以只用一个backing files创建多个虚拟机实例(overlays)，然后可以对每个虚拟机实例做多个快照。<strong>这里需要注意：backing files总是只读的文件。换言之，一旦新快照被创建，他的后端文件就不能更改(快照依赖于后端这种状态)。</strong></p><h2 id="virsh命令实现KVM虚拟机的内置快照"><a href="#virsh命令实现KVM虚拟机的内置快照" class="headerlink" title="virsh命令实现KVM虚拟机的内置快照"></a><strong>virsh命令实现KVM虚拟机的内置快照</strong></h2><p>qemu/kvm有三种快照，分别是<strong>内部（保存在硬盘镜像中）/外部（保存为另外的镜像名）/虚拟机状态</strong> ，很多网站上提供的资料和教程也大多是内部快照功能。<strong>内部快照不支持raw格式的镜像文件，所以如果想要使作内部快照，需要先将镜像文件转换成qcow2格式。</strong></p><p>内置快照在虚拟机运行状态和关闭状态都可以创建。在关机状态下，它通过单个qcow2镜像磁盘存储快照时刻的磁盘状态，并没有新磁盘文件产生。在虚机开机状态下，可以同时保存内存状态，设备状态和磁盘状态到一个指定文件中。当需要还原虚拟机状态时，将虚机关机后通过virsh restore命令还原回去。以上这些就是虚拟机的内置快照链操作，一般用于测试场景中的不断的将vm还原到某个起点，然后重新开始部署和测试。下面，我们就来玩玩KVM虚拟机的内置快照链。</p><p><strong>Stpe1：</strong>首先，我们找一台运行的虚拟机，然后通过控制台console登录，建一个空目录flags并写一个测试文件test01作为标记。在后面快照回滚时，可以通过对比该文件查看具体的效果 。</p><p><img src="https://pic.imgdb.cn/item/5ef498d914195aa594419569.jpg" alt></p><p><strong>Step2：</strong>由于内部快照不支持raw格式的磁盘镜像文件，所以我们首先需要查看下当前虚拟机的磁盘镜像格式，如果不符合要求，需要利用qemu-img命令进行磁盘格式转换。</p><p><img src="https://pic.imgdb.cn/item/5ef498e814195aa59441a02c.jpg" alt></p><p><strong>Step3：</strong>使用snapshot-create-as命令创建第一个快照snap01。其实，还有个类似命令的snapshot-create，它创建的快照名称是系统随机生成的，一般我们使用snap-create-as命令指定快照名创建。</p><p><img src="https://pic.imgdb.cn/item/5ef498f814195aa59441a971.jpg" alt></p><p><strong>step4：</strong>使用snapshot-current命令查看当前快照的详细信息。这里，其实查看的是/var/libvirt/qemu/snapshot/centos7.5/下的虚拟机的快照xml配置文件信息。</p><p><img src="https://pic.imgdb.cn/item/5ef4990514195aa59441b1a1.jpg" alt></p><p><strong>step5：</strong>此时，我们再次在虚拟机创建测试内容，在test01文件追加内容456，然后再次创建快照snap02。</p><p><img src="https://pic.imgdb.cn/item/5ef4991414195aa59441ba6f.jpg" alt></p><p>同时，我们利用qemu-img命令查看虚拟机的磁盘信息，发现创建的快照后磁盘的容量有所增加，这也符合常理。</p><p><img src="https://pic.imgdb.cn/item/5ef4992214195aa59441c2d9.jpg" alt></p><p><strong>step6：</strong>此时，我们利用快照回滚虚拟机。在回滚之前最好先关闭虚拟机 virsh shutdown 或virsh destroy ，在不关闭的情况下也可以做回滚。但是，此时如有新数据写入时不过会出现问题。所以，在实际运维中，还是建议先停机再做回滚。如果真的要求不停机回滚，最好加上–force选项表示强制回滚，此时即使有数据写入也会被丢弃。不加–force选项时，老版本的libvirt会报错，但是新版本不会报错，不过不建议这样使用。</p><p><img src="https://pic.imgdb.cn/item/5ef4993014195aa59441cac1.jpg" alt></p><p>此时，我们在虚拟机运行态下，再次利用快照snap02进行回滚。而且，我们没有加–force选项。</p><p><img src="https://pic.imgdb.cn/item/5ef4994114195aa59441d74b.jpg" alt></p><p>上面完成快照的创建，回滚验证后，还有个操作时快照的删除的，也就是利用snapshot-delete命令删除，这个没有什么好说的。但是需要提示一点，快照删除后，虚拟机的磁盘大小并不会变小。</p><p><img src="https://pic.imgdb.cn/item/5ef4996314195aa59441ed68.jpg" alt></p><p>以上，就是KVM虚拟机的内置快照操作内容，我觉得我是讲明白了。。。关于kvm虚拟机的状态备份，也就是save命令，时间比较长，一般要5－10分钟左右，造成该问题的原因是：save vm保存的是当前客户机系统的运行状态（包括：内存、寄存器、CPU执行等的状态），保存为一个文件，而且要在load vm时可以完全恢复，这个过程比较复杂，如果客户机里面的内存很大、运行的程序很多，save vm比较耗时，也是可以理解的。暂时很难有什么改进方法。</p><p>而往往我们并不需要去备份一个虚拟机当前状态完整的快照，实际运维中中可能只需要对disk做一个快照就OK了。所以，这就要提到外部快照（External snapshot）。</p><h2 id="virsh命令实现KVM虚拟机的外置快照"><a href="#virsh命令实现KVM虚拟机的外置快照" class="headerlink" title="virsh命令实现KVM虚拟机的外置快照"></a><strong>virsh命令实现KVM虚拟机的外置快照</strong></h2><p>KVM的外部快照功能比较实用，可以支持仅对disk进行快照，也支持live snapshot，很多虚拟化云方案中一般也会使用外部快照功能创建快照链。不过，KVM虚拟机要支持外部快照功能，需要qemu版本在1.5.3及以上，否则只能通过下载最新的qemu源码包进行编译安装。可以通过rpm -aq qemu查看当前宿主机的qemu软件包的版本，也可以通过qemu-kvm –version命令来查看。</p><p>这里需要注意一点，centos系统默认将qemu-kvm命令放在/usr/libexec/目录下，所以在当前命令行执行qemu-kvm会提示找不到命令，需要带上命令的全路径执行，也就是/usr/libexec/qemu-kvm –version，或者通过创建软链接的方法，在/usr/bin目录下创建一个qemu-kmv命令的软链接，就可以执行qemu-kvm命令了。（我采用的就是这种办法）</p><p><img src="https://pic.imgdb.cn/item/5ef4997714195aa59441f97a.jpg" alt></p><p><strong>step1：</strong>在虚拟机关机状态下，我们创建一个外部快照ext_snap01。</p><p><img src="https://pic.imgdb.cn/item/5ef4998914195aa59442030f.jpg" alt></p><p>从上面的查询中不难看出centos75.ext_snap01的backing file来自于/kvm_host/vmdisk/centos75.img 。同样，也可以利用下面的命令进行查看：</p><p><img src="https://pic.imgdb.cn/item/5ef4999914195aa594420c5f.jpg" alt></p><p><strong>创建完外部快照后，原磁盘镜像文件会变成只读，新的变更都会写入到新的快照文件中。也就是我们常说链接克隆中创建的差分磁盘。</strong>忘了的，请回顾本站存储虚拟化相关文章。。。。。。</p><p><strong>step2：</strong>我们做完快照后，写一个200M的测试文件到虚拟机centos7.5的系统/opt/目录下，并做如下验证。</p><p><img src="https://pic.imgdb.cn/item/5ef499a814195aa5944214b4.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef499b814195aa594421df2.jpg" alt></p><p>如上图，在未写入测试文件data01前，虚拟机外部快照盘centos75.ext_snap01的大小为5.5M（上图红框部分），在写入data01测试文件后变为207M（上图黄框部分），且虚拟机原始磁盘大小仍为4.3G（上图蓝框部分）。至于，显示大小不准确的问题，是因为我们通过-h选项通过ls命令查看，在转换成可读模式大小时的转换误差导致，并非真实文件大小不准确。其实，用原始字节数表示就没有误差，但是那样反人类啊。。。</p><p>至于外部快照的删除、回滚等操作与内部快照一直，我就懒得演示验证了。。。。<strong>不过，外部快照多了一个快照合并的功能，也就是将差分磁盘中的数据变化写入到原始磁盘镜像中。主要有两种方式：一种是通过blockcommit命令完成，从top文件合并数据到base，也就是从快照文件向backing file合并；另一种是blockpull命令完成，从base文件合并数据到top，也就是从backing file向快照文件合并。</strong>截止目前只能将backing file合并至当前的active的快照镜像中，也就是说还不支持指定top的合并。</p><p>但是，在centos系统执行virsh blockcommit或者virsh blockpull等命令时，都会报QEMU不支持的二进制操作错误或者是版本不支持等错误，这是CentOS内部集成的qemu-kvm版本问题导致。因此，此时有两种解决办法，一种是升级QEMU-KVM，通过源码编译内核完成升级。另一种就是使用qemu-img命令，commit对应上面blockcommit，rebase对应blockpull操作。</p><p><img src="https://pic.imgdb.cn/item/5ef499c714195aa59442264e.jpg" alt></p><p>通过qemu-img commit命令将快照文件的数据的合并到原始镜像文件中（图中红色框部分），此时不需要指定backing file文件。而且，我们此时查看原始镜像文件大小，发现变大为4.5G（图中黄色框部分）。此时，我们可以删除磁盘快照文件，进入虚拟机查看我们当初的测试文件是否还存在。</p><p><img src="https://pic.imgdb.cn/item/5ef499d614195aa594422f36.jpg" alt></p><p><strong>以上，commit的操作，如果采用rebase命令，反向合并时，就必须要指定backing file文件。其他操作与commit操作一致，大家自己尝试。。。。。</strong></p><p>在openstack等平台中，使用的快照功能大都是基于外置快照的形式。这里需要特别注意一点，KVM的快照不要和Vmware的快照混为一谈，Vmware的所有快照默认情况下都是基于baseimage，创建的overlays，也就是所有快照的backing file都是baseimage，删除任一个快照对其他快照的使用和还原都不会有影响，而KVM不同，其快照之间存在链式关系，snap01是基于baseimage，snap02是基于snpa01，以此类推。。。基中任一环节出现问题，都会出现无法还原到之间状态。<strong>所以，在做快照的合并，删除等操作前，一定要提前通过qemu-img info –backing-chain查看虚拟机的快照链关系，避免发生不可挽回的数据丢失错误。</strong></p><h2 id="libguestfs-tools工具使用总结"><a href="#libguestfs-tools工具使用总结" class="headerlink" title="libguestfs-tools工具使用总结"></a><strong>libguestfs-tools工具使用总结</strong></h2><p>libguestfs是一组Linux下的C语言的API ，用来访问虚拟机的磁盘映像文件。其项目官网是<a href="http://libguestfs.org/" target="_blank" rel="noopener">http://libguestfs.org/</a> 。该工具包含的软件有virt-cat、virt-df、virt-ls、virt-copy-in、virt-copy-out、virt-edit、guestfs、guestmount、virt-filesystems、virt-partitions等工具，具体用法也可以参看官网。该工具可以在不启动KVM guest主机的情况下，直接查看guest主机内的文内容，也可以直接向img镜像中写入文件和复制文件到外面的物理机，甚至也可以像mount一样，支持挂载操作。</p><p>现在，我们将虚拟机centos7.5关机，然后查看其镜像磁盘使用情况，可以通过virt-df命令实现。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef499e714195aa5944238e0.jpg" alt></p><p>同样，我们想查看虚拟机关机态下根分区下详细文件/目录信息，可以使用virt-ls命令实现。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef499f714195aa594424270.jpg" alt></p><p>此时，我们需要从关机态下的虚拟中拷贝一个文件到宿主机中，可以使用copy-out命令实现。但是，需要带上-d选项指定哪个虚拟机。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a0514195aa594424a53.jpg" alt></p><p>既然能从虚拟机中拷贝文件到本地宿主机，自然也能从本地宿主机拷贝文件到虚拟机，通过copy-in命令就能实现。其实，这些命令工具的用法和Linux的原始命令非常类似，所以熟悉Linux常用命令的使用是必须要掌握的技能。</p><p>接下来，我们查看下虚拟机的文件系统以及分区信息，通过filesystems命令实现。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a1314195aa5944251ab.jpg" alt></p><p>完成虚拟机文件系统和分区信息的查询后，我们可以通过guestmount命令，将虚拟机centos7.4的系统盘离线挂载到宿主机的/mnt分区，与挂载光驱操作类似，但是我们可以设置挂载镜像的操作方式。比如：只读、只写、读写等。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a2014195aa5944259e1.jpg" alt></p><p>以上是挂载linux系统的镜像磁盘，如果需要挂载windows虚拟机的磁盘，需要额外安装ntfs -3g的工具来识别windows系统的NTFS文件系统格式。</p><p>实际应用中的KVM主机也会遇到像物理机一样的情况，如系统崩溃、无法引导等情况。物理机出现该情况时，我们可以通过光盘引导、单用户模式、PE引导、修复或升级安装等方式获取系统内的文件和数据，KVM中同样也可以使用上述方法，既可以上面的利用libguestfs-tools的工具进行挂载修改，也可以通过linux系统原生的mount -o loop方式进行挂载修改。下面，我们就通过mount命令对raw格式和qcow2格式磁盘进行挂载做个演示。</p><h2 id="raw磁盘镜像的挂载"><a href="#raw磁盘镜像的挂载" class="headerlink" title="raw磁盘镜像的挂载"></a><strong>raw磁盘镜像的挂载</strong></h2><p>由于raw格式简单原始，其通常做为多种格式互相转换的中转格式，所以对raw格式的磁盘挂载操作时需重点掌握。raw格式的分区挂载也有两种方法：<strong>一种是通过计算偏移量offset方式挂载。另一种是通过kpartx分区映射方式实现挂载。</strong></p><p><strong>计算偏移量offset方式的思路为找出分区的开始位置，使用mount命令的offset参数偏移掉前面不需要的，即可得到真正的分区。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a3014195aa594426226.jpg" alt></p><p>上图中，我们可以通过fdisk -lu命令查看磁盘镜像的分区信息，可以发现centos74.img的磁盘一共有3个分区，每个分区都有对应的起始扇区和结束扇区编号，且每个扇区的大小为512字节。通过这些信息，我们就可以计算分区的offset值，从而实现找到真正的分区内容存放位置。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a3f14195aa5944269fd.jpg" alt></p><p>然后，通过mount -o loop，offset=xxxx命令用其实扇区的偏移字节数实现分区挂载，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a4d14195aa5944271de.jpg" alt></p><p>除了上述计算偏移量的方法外，还可以通过<strong>kpartx分区映射方式实现挂载。首先通过kpartx工具对磁盘镜像做个分区映射。如下：</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49a5b14195aa594427a48.jpg" alt></p><p>上图中，通过-av选项添加磁盘镜像并将映射结果显示出来（图中红色框部分），然后就可以知道磁盘镜像有3个分区，分别映射为loop0p1、loop0p2和loop0p3（图中黄色框部分）。完成映射后，我们就可以像挂载普通磁盘或光驱那样对映射分区进行挂载。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a6814195aa5944281fb.jpg" alt></p><p>注意映射的设备的文件位置在/dev/mapper目录下。我们可以看下该目录下的具体信息，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a7814195aa594428aa6.jpg" alt></p><p>上图中可以很清楚的发现上面的映射分区是通过软链接的方式建立的与磁盘内部分区的映射关系。通过磁盘映射方式下实现挂载后，记得使用完成不仅要卸载挂载点，还需要删除映关系。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49a8614195aa59442927e.jpg" alt></p><p>以上就是raw格式镜像挂载，需要注意的如果虚拟机使用了LVM逻辑卷，那么针对逻辑卷的挂载操作需要使用losetup工具完成，具体可以查询使用方法，非常简单这里就不在赘述了。而qcow2格式的镜像的挂载不能通过kaprtx直接映射，可以先转换成raw的格式进行处理，也可以通过libguestfs-tools工具处理，还可以使用qemu-nbd直接挂载。就速度上而言qemu-nbd的速度肯定是最快的。不过由于centos/redhat原生内核和rpm源里并不含有对nbd模块的支持及qemu-nbd（在fedora中包含在qemu-common包里）工具，所以想要支持需要编译重新编译内核并安装qemu-nbd包 。</p><p>至此，KVM虚拟机的存储管理实战全部介绍完毕，后面我们进入KVM虚拟机的网络管理实战。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇中我们介绍qemu-img这个对磁盘镜像操作的命令，包括检查镜像磁盘，创建镜像磁盘、查看镜像磁盘信息、转换镜像磁盘格式、调整镜像磁盘大小以及镜像磁盘的快照链操作。但是，在镜像磁盘快照链操作中，我们也提到了通过qemu-img命令创建快照链只能在虚拟机关机状态下运行。如果虚拟机为运行态，只能通过virsh save vm来保存当前状态。那么，本文就专门讲述在KVM虚拟机中如何通过virsh命令来创建快照链，以及链中快照的相互关系，如何缩短链，如何利用这条链回滚我们的虚拟机到某个状态等等。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>KVM虚拟机存储管理实战（上篇）</title>
    <link href="https://kkutysllb.cn/2020/03/01/KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://kkutysllb.cn/2020/03/01/KVM虚拟机存储管理实战（上篇）/</id>
    <published>2020-03-01T11:17:55.000Z</published>
    <updated>2020-06-25T12:29:22.067Z</updated>
    
    <content type="html"><![CDATA[<p>我们上一篇介绍了KVM虚拟机的全生命周期管理实战，如果与OpenStack的实操对应，正好与nova服务的实操相符。在云计算体系中，运维管理主要涉及计算nova、存储cinder和网络neutron三部分。因此，本篇文章我们主要介绍KVM的存储管理实战。KVM的存储选项有多种，包括虚拟磁盘文件、基于文件系统的存储和基于设备的存储。同样，也有对应的virsh管理命令。<a id="more"></a></p><h2 id="KVM的存储选项和常用管理命令"><a href="#KVM的存储选项和常用管理命令" class="headerlink" title="KVM的存储选项和常用管理命令"></a><strong>KVM的存储选项和常用管理命令</strong></h2><p>为实现KVM存储管理，可以使用LVM逻辑卷和创建存储池。储存池Storage Pool 是宿主机上可以看到的一片存储空间，可以是多种型。而逻辑卷Volume 是在 存储池Storage Pool 中划分出的一块空间，宿主机将 Volume 分配给虚拟机，Volume 在虚拟机中看到的就是一块硬盘。</p><h3 id="虚拟磁盘文件"><a href="#虚拟磁盘文件" class="headerlink" title="虚拟磁盘文件"></a><strong>虚拟磁盘文件</strong></h3><p>当系统创建KVM虚拟机的时候，默认使用虚拟磁盘文件作为后端存储。安装后，虚拟机认为在使用真实的磁盘，但实际上看到的是用于模拟硬盘的虚拟磁盘文件。就是因为加了一层额外的文件系统层，所以系统的I/O读写性能会降低，但是基于文件系统的虚拟磁盘可以用于其他虚拟机，且具备快照、链接克隆、弹性扩缩容等特性，方便虚拟机的迁移。因此，可以说是各有利弊，根据实际使用场景来选择。</p><h3 id="基于文件系统的KVM存储"><a href="#基于文件系统的KVM存储" class="headerlink" title="基于文件系统的KVM存储"></a><strong>基于文件系统的KVM存储</strong></h3><p>在安装KVM宿主机时，<strong>可选文件系统为dir（directory）或fs（formatted block storage）</strong>作为初始KVM存储格式。默认为dir，通过指定本地文件系统中的一个目录用于创建磁盘镜像文件。而fs选项可以允许指定某个格式化文件系统的分区，把它作为专用的磁盘镜像文件存储。<strong>两种KVM存储选项之间最主要的区别在于：fs文件系统不需要挂载到某个特定的目录。</strong>两种选项所指定的文件系统，都可以是本地文件系统或位于SAN上某个物理宿主机上的网络文件系统。后者具备数据共享的优势，可以很轻易地实现多个主机同时访问。</p><p><strong>还有一种基于文件的磁盘存储方式是netfs</strong>，可以指定一个网络文件系统的名称，如NFS，用这种方式作为KVM存储与SAN类似，可以访问到位于其它服务器上的虚拟磁盘，同时也可以多台宿主机共享访问磁盘文件。</p><p>但是，所有的这些基于文件系统的KVM存储方式都有一个缺点：<strong>文件系统固有缺陷。</strong>因为虚拟机的磁盘文件不能直接读取或写入KVM存储设备，而是写入宿主机OS之上的文件系统。这也就意味着在访问和写入文件时中间增加了额外一层，因此会降低性能。所以，如果只是单纯要求性能好，就需要考虑考虑基于设备的存储。</p><h3 id="基于设备的KVM存储"><a href="#基于设备的KVM存储" class="headerlink" title="基于设备的KVM存储"></a><strong>基于设备的KVM存储</strong></h3><p>另外一种KVM存储的方式就是使用基于设备的方式。<strong>共支持四种不同的物理存储：磁盘、iSCSI、SCSI和lvm逻辑盘。</strong>磁盘方式指直接读写硬盘设备。iSCSI和SCSI方式可选，取决于通过SCSI或iSCSI地址与磁盘设备连接。这种KVM存储方式的<strong>优势在于磁盘的名称固定</strong>，不依赖宿主机OS搜索到磁盘设备的顺序。但是，这种连接磁盘的方式也有<strong>缺点：灵活性不足</strong>。虚拟磁盘的大小很难改变，而且这种连接方式的KVM存储不支持快照。</p><p><strong>如果要提升基于设备的KVM存储灵活性，可以使用LVM。</strong>LVM的优势在于可以使用快照，但是快照并不是KVM虚拟化的特性，而是LVM自带的特性。</p><p>LVM可以把所有存储放到一个卷组里，该卷组是物理磁盘设备的一个抽象，所以如果超出可用磁盘空间最大值，还可以向卷组中添加新的设备，增加的空间在逻辑卷中直接可以使用。使用LVM使得磁盘空间分配更加灵活，而且增加和删除存储也更为容易。LVM除了在单机场景下使用外，还可以在多机场景下使用。在多宿主机环境中，可以在SAN上创建逻辑卷，且如果使用Cluster LVM（集群LVM），可以很容易的配置成多个主机同时访问某个逻辑卷。详见本站Linux常用运维工具分类中LVM逻辑卷一文。</p><p>virsh也可以对节点上的存储池和存储卷进行管理。virsh常用于存储池和存储卷的管理命令如下：</p><table><thead><tr><th><strong>命令</strong></th><th><strong>功能描述</strong></th></tr></thead><tbody><tr><td>pool-list</td><td>显示libvirt管理的存储池</td></tr><tr><td>pool-define &lt;pool.xml&gt;</td><td>根据xml文件定义一个存储池</td></tr><tr><td>pool-define-as <pool-name>&lt;–type\</pool-name></td><td>target&gt;</td><td>定义一个存储池，指定pool名，并指定pool类型和存储路径</td></tr><tr><td>pool-build <pool-name></pool-name></td><td>构建一个存储池</td></tr><tr><td>pool-start <pool-name></pool-name></td><td>激活一个存储池</td></tr><tr><td>pool-autostart <pool-name></pool-name></td><td>设置存储池开机自动运行</td></tr><tr><td>pool-info<pool-name></pool-name></td><td>根据一个存储池名称查询其基本信息</td></tr><tr><td>pool-uuid<pool-name></pool-name></td><td>根据存储池名称查询其uuid信息</td></tr><tr><td>pool-create&lt;pool.xml&gt;</td><td>根据xml配置文件信息创建一个存储池</td></tr><tr><td>pool-edit<pool-name or uuid></pool-name></td><td>编辑一个存储池的xml配置文件</td></tr><tr><td>pool-destroy<pool-name or uuid></pool-name></td><td>关闭一个存储池</td></tr><tr><td>pool-delete<pool-name or uuid></pool-name></td><td>删除一个存储池，不可恢复</td></tr><tr><td>vol-list<pool-name or uuid></pool-name></td><td>查询一个存储池中的存储卷的列表</td></tr><tr><td>vol-name<vol-key-or-path></vol-key-or-path></td><td>查询一个存储卷的名称</td></tr><tr><td>vol-path –pool<pool><vol-name or key></vol-name></pool></td><td>查询一个存储卷的路径</td></tr><tr><td>vol-create&lt;vol.xml&gt;</td><td>根据xml配置创建一个存储卷</td></tr><tr><td>vol-create-as &lt;–pool\</td><td>name\</td><td>capacity\</td><td>allocation\</td><td>format&gt;</td><td>创建一个卷，指定归属pool，卷名、预分配大小、占用大小、磁盘格式</td></tr><tr><td>vol-clone<vol-name-path><name></name></vol-name-path></td><td>克隆一个存储卷</td></tr><tr><td>vol-delete<vol-name-or-key-or-path></vol-name-or-key-or-path></td><td>删除一个存储卷</td></tr><tr><td>vol-pool <vol-name or vol-path></vol-name></td><td>根据存储卷名或路径查询归属存储池信息</td></tr></tbody></table><p>与全生命周期管理一样，可以通过<strong>virsh help|egrep ‘(pool*|vol*)’</strong>查看全量存储管理命令。</p><p><img src="https://pic.imgdb.cn/item/5ef4966c14195aa594402fd3.jpg" alt></p><h2 id="KVM虚拟机的存储管理实战"><a href="#KVM虚拟机的存储管理实战" class="headerlink" title="KVM虚拟机的存储管理实战"></a><strong>KVM虚拟机的存储管理实战</strong></h2><h3 id="存储池和存储卷的信息查询"><a href="#存储池和存储卷的信息查询" class="headerlink" title="存储池和存储卷的信息查询"></a><strong>存储池和存储卷的信息查询</strong></h3><p>有了上面的命令知识储备，我们<strong>通过pool-list命令首先看下当前宿主机（192.168.101.251）上存储池信息。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4968a14195aa594404098.jpg" alt></p><p>上图中表示当前宿主机上有两个KVM虚拟机存储池，一个是网络文件系统NFS的存储池，名称为nfsdir，另一个是本地文件系统存储池，名称为root。其实，这两个存储池是在我们前面创建虚拟机通过disk选项制定虚拟机系统盘时，默认创建的。我们可以<strong>通过pool-info命令查看对应存储池的详细信息。</strong>如下：</p><p><strong>1）nfsdir存储池信息</strong></p><p><img src="https://pic.imgdb.cn/item/5ef4969f14195aa594404bcf.jpg" alt></p><p><strong>2）root存储池信息</strong></p><p><img src="https://pic.imgdb.cn/item/5ef496b414195aa59440579e.jpg" alt></p><p><strong>这里需要明确一个概念，上图中的存储池的总空间大小并不是创建的存储池空间大小，而是存储池所在的磁盘分区的大小，由于我们前面在创建虚拟机时指定的虚拟机系统盘文件都在系统根分区下创建（/root and /mydata/nfsdir/），所以这里的存储池总大小为根分区的大小。这一点可以通过查看根分区大小来确认。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef496c614195aa5944061e5.jpg" alt></p><p>掌握了存储池信息查看后，我们可以通<strong>过vol-list命令查看该存储池下有哪些存储卷</strong>。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef496d614195aa594406a65.jpg" alt></p><p>上图中表示nfsdir存储池下有一个存储卷cto67s.img，它位于/mydata/nfsdir目录下。接下来，我们还可以<strong>通过vol-info命令查看该存储卷的详细信息。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef496e514195aa59440722f.jpg" alt></p><p>如上图，在查看存储卷的详细信息时，需要通过–pool选项制定该存储卷对应的存储池名称。上图中表示cto67s.img存储卷的类型为file，总大小为2.28GB。</p><h3 id="存储池和存储卷的增、删、改实战"><a href="#存储池和存储卷的增、删、改实战" class="headerlink" title="存储池和存储卷的增、删、改实战"></a><strong>存储池和存储卷的增、删、改实战</strong></h3><p>上面查看存储池和存储卷的命令只能浏览大致的一些信息，其实，我们还可以通过<strong>命令pool-deumpxml查看具体的存储卷xml配置详细信息。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef496f514195aa594407a24.jpg" alt></p><p>上图中显示的存储池nfsdir的xml配置信息，从配置信息中我们可以知道存储池nfsdir的类型type、name、uuid、空间的大小、路径path、权限、归属的用户和用户组等信息。同理，我们如果<strong>想查看某个存储卷的xml配置信息，可以使用vol-dumpxml命令实现</strong>。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4970514195aa594408250.jpg" alt></p><p>在上图中，我们不仅知道存储卷的类型type、name、key、空间大小、路径、磁盘格式、文件权限，归属用户&amp;用户组，还知道该存储卷的访问时间，修改时间和状态改变时间。</p><p>有了上面的知识储备后，我们就可以通过xml文件创建一个存储池和一个存储卷。我们首先创建一个存储池POOLB，xml配置内容如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4971314195aa5944089cb.jpg" alt></p><p>如上图，我们只需要定义要创建的存储池类型、名称、路径、权限和归属用户及用户组等信息，其他如uuid、空间大小等信息会在存储池创建后，系统自动生成。由于我们定义的存储池类型为dir目录类型，因此实际上空间大小等信息就是存储池目录所在的磁盘分区大小信息。</p><p>有了上面存储池的xml配置文件后，就可以<strong>通过pool-create命令来创建一个存储池poolB了</strong>。需要注意，<strong>由于我们定义的存储池类型为dir目录类型，所以需要提前在xml文件指定的路径下创建目标目录，否则会提示创建存储池失败。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4972314195aa5944091c1.jpg" alt></p><p>在/mydata目录下创建poolB子目录后，结果如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4973214195aa594409abe.jpg" alt></p><p>上图中，我们完成存储池创建后，在该存储池中创建一个卷data01.img。同理，我们还是<strong>通过xml文件来创建，首先创建卷的xml配置文件data01.xml</strong>。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4974114195aa59440a2a8.jpg" alt></p><p>然后，我们通过<strong>命令vol-cretae命令来创建卷data01.img</strong>。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4975614195aa59440add4.jpg" alt></p><p>完成存储池和卷的创建验证后，我们现在来验证删除存储池和卷。既然我们创建的时候，先创建存储池，再创建卷。那么，删除的时候自然是反着来，是不是就是先删除卷再删除存储池。如果不删除存储池中的卷就直接删除存储池行不行？答案是可以的。。。所以，<strong>我们既可以先通过vol-delete命令删除卷，然后通过pool-destroy命令删除池。也可以直接通过pool-destroy命令删除池。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4976614195aa59440b6ad.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef4977514195aa59440bee0.jpg" alt></p><p>在删除池时还有一个pool-delete命令，这个命令请慎用。因为一旦通过它删除池就无法恢复。所以，我们一般使用pool-destroy命令来销毁一个池，一旦需要时还可以通过pool-start命令恢复。即使真的不需要这个池，也可以通过find+rm命令来删除。</p><p>大家可能发现了，通过上面的方式创建的存储池是不能自动开始的。其实，KVM的virsh命令还有另一种方式来创建存储池和存储卷。就是<strong>通过pool-define-as命令先定义一个存储池，然后通过pool-build命令来构建一个存储池，再通过pool-start命令激活存储池，最后通过pool-autostart命令设置该存储池开机自动运行。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4978414195aa59440c7d5.jpg" alt></p><p>其实，通过xml配置文件也能通过这种方式创建。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4979214195aa59440cef2.jpg" alt></p><p>完成上面pool的创建后，我们也可以<strong>通过vol-create-as命令来创建一个卷。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef497a214195aa59440dae5.jpg" alt></p><p>如上图，通过制定归属pool，卷名、预分配大小、占用大小和磁盘格式，通过vol-create-as命令完成一个卷的创建。</p><p>删除卷和池的方式与上面类似，这里就不再举例了。存储池和卷的其他命令大家可以参考virsh help的信息自行研究。接下来，我们来看一个经常使用qemu-img命令，也就是KVM存储磁盘管理命令。</p><h2 id="qemu-img命令实战"><a href="#qemu-img命令实战" class="headerlink" title="qemu-img命令实战"></a><strong>qemu-img命令实战</strong></h2><p>镜像算不算存储？在我初学OpenStack时，因为镜像管理服务是glance，存储管理服务是cinder和swift，所以当初单纯以为的镜像不算存储。后来通过深入了解和研究，发现其实镜像也是一种存储，可以把它简单理解为“<strong>虚拟机的元数据</strong>”，既然是元数据，当然是存储。</p><p>我们都知道，在创建KVM虚拟机时，首先需要通过qemu-img命令去创建一个虚拟系统盘。这个qemu-img命令就是是QEMU的磁盘镜像管理工具，在完成qemu-kvm源码编译或rpm包安装后就会默认编译好qemu-img这个二进制文件。qemu-img也是QEMU/KVM使用过程中一个比较重要的工具，我们下来就对其常用用法总结验证下。</p><p>qemu-img支持非常多种的文件格式，可以通过“qemu-img-h”查看其命令帮助得到，它支持20多种格式：file，quorum，blkverify，luks，dmg，sheepdog，parallels，nbd，vpc，bochs，blkdebug，qcow2，vvfat，qed，host_cdrom，cloop，vmdk，host_device，qcow，vdi，null-aio，blkreplay，null-co，raw等。</p><p><strong>qemu-img工具的命令行基本用法为：qemu-img command[command options]</strong>，也就是说qemu-img工具通过后面命令和命令选项实现各种磁盘镜像管理功能。它支持的命令分为如下几种：</p><h3 id="检查镜像磁盘的数据一致性check-f-fmt-filename"><a href="#检查镜像磁盘的数据一致性check-f-fmt-filename" class="headerlink" title="检查镜像磁盘的数据一致性check[-f fmt]filename"></a><strong>检查镜像磁盘的数据一致性check[-f fmt]filename</strong></h3><p><strong>该命令用于对磁盘镜像文件进行一致性检查，查找镜像文件中的错误</strong>，目前，仅支持对<strong>“qcow2”、”qed”、”vdi”格式文件的检查</strong>。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef497b714195aa59440e5e0.jpg" alt></p><p>如上图，提示该磁盘镜像没有错误，并且其中显示了该镜像磁盘在物理盘的I/O偏移地址等存储信息。在上面的命令中，我们使用<strong>-f参数指定镜像磁盘的格式</strong>为qcow2，它和qed都是QEMU支持的磁盘镜像格式，qed是qcow2的增强磁盘文件格式，避免了qcow2格式的一些缺点，也提高了性能，不过目前还不够成熟。而另一种磁盘镜像格式vdi（Virtual Disk Image）是Oracle的VirtualBox虚拟机中的存储格式。最后的<strong>filename参数是磁盘镜像文件的名称（包括路径）。</strong></p><h3 id="创建磁盘镜像文件create-f-fmt-o-options-filename-size"><a href="#创建磁盘镜像文件create-f-fmt-o-options-filename-size" class="headerlink" title="创建磁盘镜像文件create[-f fmt][-o options]filename[size]"></a><strong>创建磁盘镜像文件create[-f fmt][-o options]filename[size]</strong></h3><p><strong>该命令用于创建一个格式为fmt，大小为size，文件名为filename（包含路径）的镜像文件。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef497cf14195aa59440f1e9.jpg" alt></p><p>上图中，我们创建了一个raw格式的，大小为20G的镜像磁盘metdata001.raw，位于/mydata/poolB目录下。其实，可以根据文件格式fmt的不同，添加一个或多个选项（options）来附加对该文件的各种功能设置，可以使用”-o?”来查询某种格式文件支持哪些选项，在”-o”选项中各个选项用逗号来分隔。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef497de14195aa59440fa6c.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef497f214195aa594410510.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef4980114195aa594410d5f.jpg" alt></p><p>如上图，如果“-o”选项中使用了backing_file这个选项来指定其后端镜像文件，那么这个创建的镜像文件仅记录与后端镜像文件的差异部分。后端镜像文件不会被修改，除非在QEMU monitor中使用“commit”命令或者使用“qemu-img commit”命令去手动提交这些改动。这种情况下，size参数不是必须需的，其值默认为后端镜像文件的大小。另外，镜像文件的大小（size）也并非必须写在命令的最后，它也可以被写在“-o”选项中作为其中一个选项。而且，直接使用“-b backfile”参数也与“-o backing_file=backfile”效果等价。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4981114195aa5944116da.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef4982514195aa594412275.jpg" alt></p><h3 id="修改磁盘镜像文件格式convert-c-f-fmt-O-output-fmt-o-options-filename-filename2-…-output-filename"><a href="#修改磁盘镜像文件格式convert-c-f-fmt-O-output-fmt-o-options-filename-filename2-…-output-filename" class="headerlink" title="修改磁盘镜像文件格式convert[-c][-f fmt][-O output_fmt][-o options]filename[filename2[…]]output_filename"></a><strong>修改磁盘镜像文件格式convert[-c][-f fmt][-O output_fmt][-o options]filename[filename2[…]]output_filename</strong></h3><p><strong>该命令用于将fmt格式的filename镜像文件根据options选项转换为格式为output_fmt的名为output_filename的镜像文件。</strong>比如我们先创建test01的qcwo2格式磁盘文件，然后将其转换为raw格式文件。<strong>如下：</strong></p><p><img src="https://pic.imgdb.cn/item/5ef4983a14195aa594412e8b.jpg" alt></p><p>如上图，将创建的qcow2格式的test01文件转换为raw格式的文件（蓝色框部分）。<strong>这个命令一般在做虚拟机模板镜像文件时经常用到，将非raw格式的磁盘文件转换为raw格式的后缀为.img的镜像模板文件，用于后续的批量虚拟机发放。同时，它支持不同格式的镜像文件之间的转换，比如可以用VMware用的vmdk格式文件转换为qcow2文件，这对从其他虚拟化方案转移到KVM上的用户非常有用。</strong>一般来说，输入文件格式fmt由qemu-img工具自动检测到，而输出文件格式output_fmt根据自己需要来指定，默认会被转换为与raw文件格式（且默认使用稀疏文件的方式存储以节省存储空间）。</p><p>命令中，“-c”参数是对输出的镜像文件进行压缩，<strong>不过只有qcow2和qcow格式的镜像文件才支持压缩，而且这种压缩是只读的，如果压缩的扇区被重写，则会被重写为未压缩的数据。</strong>同样可以使用“-o options”来指定各种选项，如：后端镜像、文件大小、是否加密等等。<strong>使用backing_file选项来指定后端镜像，让生成的文件是copy-on-write的增量文件，这时必须让转换命令中指定的后端镜像与输入文件的后端镜像的内容是相同的，尽管它们各自后端镜像的目录、格式可能不同。</strong></p><p><strong>如果使用qcow2、qcow、cow等作为输出文件格式来转换raw格式的镜像文件（非稀疏文件格式），镜像转换还可以起到将镜像文件转化为更小的镜像，</strong>因为它可以将空的扇区删除使之在生成的输出文件中并不存在。</p><h3 id="查看镜像磁盘信息info-f-fmt-filename"><a href="#查看镜像磁盘信息info-f-fmt-filename" class="headerlink" title="查看镜像磁盘信息info [-f fmt] filename"></a><strong>查看镜像磁盘信息info [-f fmt] filename</strong></h3><p><strong>该命令用于显示镜像磁盘的详细信息。</strong>如果文件是使用稀疏文件的存储方式，也会显示出它的本来分配的大小以及实际已占用的磁盘空间大小。如果文件中存放有客户机快照，快照的信息也会被显示出来。比如，我们想查看cto67s.img这个镜像磁盘的信息。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4984c14195aa594413aa3.jpg" alt></p><p>这个命令比较简单，聪明如你，一学就会。。。。。。</p><h3 id="镜像磁盘的快照命令集snapshot-l-a-snapshot-c-snapshot-d-snapshot-filename"><a href="#镜像磁盘的快照命令集snapshot-l-a-snapshot-c-snapshot-d-snapshot-filename" class="headerlink" title="镜像磁盘的快照命令集snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename"></a><strong>镜像磁盘的快照命令集snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename</strong></h3><p><strong>这是一组命令，“-l” 选项是查询并列出镜像文件中的所有快照，“-a snapshot”是让镜像文件使用某个快照，“-c snapshot”是创建一个快照，“-d”是删除一个快照。</strong>比如，我们现在看当下宿主机中有没有虚拟机磁盘cto74d.img的快照，可以使用-l选项查看，结果发现没有。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4985d14195aa594414374.jpg" alt></p><p>此时，我们使用-c选项对该磁盘创建一个快照，此时需要注意<strong>raw格式的磁盘文件不支持快照，因此对raw格式的磁盘创建快照时，需要将其转换为qcow2格式。同时，无论是使用快照还是创建快照都需要在关闭虚拟机的情况下进行，如果虚拟机时运行状态需要使用另一个命令virsh save vm。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4986b14195aa594414c12.jpg" alt></p><p>上述快照创建成功后，我们在当前目录下查看是否新的镜像文件产生。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4987a14195aa59441556a.jpg" alt></p><p>发现并没有新的镜像文件产生，说明通过qemu-img该步并不会创建一个新的镜像，但是磁盘镜像的快照确实存在，因为通过-l选项可以查看。这样，我们在需要的时候，就可以使用<strong>-a选项利用快照恢复磁盘</strong>。同样，如果我们不需要快照，可以通过<strong>-d选项将其删除</strong>。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4988814195aa594415d96.jpg" alt></p><h3 id="修改磁盘镜像文件的大小resize-filename-size"><a href="#修改磁盘镜像文件的大小resize-filename-size" class="headerlink" title="修改磁盘镜像文件的大小resize filename[+|-]size"></a><strong>修改磁盘镜像文件的大小resize filename[+|-]size</strong></h3><p><strong>该命令用于改变镜像文件的大小。</strong>“+”和“-”分别表示增加和减少镜像文件的大小，size也支持K、M、G、T等单位的使用。比如，我们现在将cto67s.img磁盘镜像大小增大5G。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4989814195aa59441661b.jpg" alt></p><p>在上图中，源磁盘大小为40G（红色框部分），通过resize命令增大5G后（黄色框部分），变成了45G（蓝色框部分）。<strong>需要注意的是：使用resize命令时需要小心（做好备份），如果失败，可能会导致镜像文件无法正常使用，而造成数据丢失。同时，缩小镜像的大小之前，需要在虚拟机中保证其中的文件系统有空余空间，否则数据会丢失。另外，qcow2格式文件不支持缩小镜像的操作。</strong></p><p>至此，KVM虚拟机的存储管理实战上篇介绍完了。我们主要讲述了如何利用virsh命令行工具来管理KVM虚拟机的存储池以及存储卷等操作，以及介绍了qemu-img这个常用的命令几种用法等内容。这些内容是下一篇通过virsh命令完成快照链操作的基础，因此需要重点掌握。光说不练假把式，IT的东西就需要大量实践才能掌握，这也是IT领域没有科技创建只有最佳实践的原因。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们上一篇介绍了KVM虚拟机的全生命周期管理实战，如果与OpenStack的实操对应，正好与nova服务的实操相符。在云计算体系中，运维管理主要涉及计算nova、存储cinder和网络neutron三部分。因此，本篇文章我们主要介绍KVM的存储管理实战。KVM的存储选项有多种，包括虚拟磁盘文件、基于文件系统的存储和基于设备的存储。同样，也有对应的virsh管理命令。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>KVM虚拟机全生命周期管理实战</title>
    <link href="https://kkutysllb.cn/2020/03/01/KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%85%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98/"/>
    <id>https://kkutysllb.cn/2020/03/01/KVM虚拟机全生命周期管理实战/</id>
    <published>2020-03-01T10:44:01.000Z</published>
    <updated>2020-06-25T13:01:16.561Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇我们介绍了KVM最重要的管理工具libvirt，它是KVM其他管理工具的基础，处于KVM管理架构的中间适配层。本篇我们主要要介绍libvirt的命令行管理工具的virsh，它也是将libvirt作为基础，通过封包调用实现的。所以，在看本篇内容之前，最好将上一篇的内容做个预习。<a id="more"></a></p><h2 id="libvirt命令行工具virsh"><a href="#libvirt命令行工具virsh" class="headerlink" title="libvirt命令行工具virsh"></a><strong>libvirt命令行工具virsh</strong></h2><p>virsh通过调用libvirt API实现虚拟化管理，与virt-manager工具类似，都是管理虚拟化环境中的虚拟机和Hypervisor的工具，只不过virsh是命令行文本方式的，也是更常用的方式。</p><p>在使用virsh命令行进行虚拟化管理操作时，可以使用两种工作模式：交互模式和非交互模式。交互模式是连接到Hypervisor上，然后输入一个命令得到一个返回结果，直到输入quit命令退出。非交互模式是直接在命令上通过建立URI连接，然后执行一个或多个命令，执行完后将命令的输出结果返回到终端上，然后自动断开连接。两种操作模式的截图如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49d2314195aa59444056b.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49d3514195aa594440f04.jpg" alt></p><p>我们经常在本地使用virsh命令，这是一种特殊的交互模式，也是最常用的模式，它本质上就是默认连接到本节点的Hypervisor上。</p><p>libvirt中实现的功能和最新的QEMU/KVM中的功能相比有一定的滞后性，因此virsh只是实现了对QEMU/KVM中的大多数而不是全部功能的调用。同时，由于virsh还实现了对Xen、VMware等其他Hypervisor的支持，因此有部分功能对QEMU/KVM无效。下面，我们还是按照“边验证边理解”的套路，对virsh常用的命令进行分类整理说明。</p><h2 id="域管理（虚拟机管理）命令"><a href="#域管理（虚拟机管理）命令" class="headerlink" title="域管理（虚拟机管理）命令"></a><strong>域管理（虚拟机管理）命令</strong></h2><p>virsh的最重要的功能之一就是实现对域（虚拟机）的管理，但是与虚拟机相关的命令是很多的，包括后面的网络管理、存储管理也都有很多是对域（虚拟机）的管理。为了简单起见，本文使用“<id>”来表示一个域的唯一标识（而不专门指定为“<id or name uuid>”这样冗长的形式）。常用的virsh域管理命令如下：</id></id></p><table><thead><tr><th><strong>virsh中的虚拟机管理命令</strong></th><th></th></tr></thead><tbody><tr><td>命令</td><td>功能描述</td></tr><tr><td>list</td><td>获取当前节点上多有虚拟机的列表</td></tr><tr><td>domstate<id></id></td><td>获取一个虚拟机的运行状态</td></tr><tr><td>dominfo<id></id></td><td>获取一个虚拟机的基本信息</td></tr><tr><td>domid<name or uuid></name></td><td>根据虚拟机的名称或UUID返回虚拟机的ID</td></tr><tr><td>domname<id or uuid></id></td><td>根据虚拟机的ID或UUID返回虚拟机的名称</td></tr><tr><td>dommemstat<id></id></td><td>获取一个虚拟机的内存使用情况的统计信息</td></tr><tr><td>setmem<id> <mem-size></mem-size></id></td><td>设置一个虚拟机的大小（默认单位的KB）</td></tr><tr><td>vcpuinfo<id></id></td><td>获取一个虚拟机的vCPU的基本信息</td></tr><tr><td>vcpupin<id><vcpu><pcpu></pcpu></vcpu></id></td><td>将一个虚拟机的vCPU绑定到某个物理CPU上运行</td></tr><tr><td>setvcpus<id><vcpu-num></vcpu-num></id></td><td>设置一个虚拟机的vCPU的个数</td></tr><tr><td>vncdisplay<id></id></td><td>获取一个虚拟机的VNC连接IP地址和端口</td></tr><tr><td>create&lt;dom.xml&gt;</td><td>根据虚拟机的XML配置文件创建一个虚拟机</td></tr><tr><td>define&lt;dom.xml&gt;</td><td>定义一个虚拟机但不启动，此时虚拟机处于预定义状态</td></tr><tr><td>start<id></id></td><td>启动一个预定义的虚拟机</td></tr><tr><td>suspend<id></id></td><td>暂停一个虚拟机</td></tr><tr><td>resume<id></id></td><td>恢复一个虚拟机</td></tr><tr><td>shutdown<id></id></td><td>对一个虚拟机下电关机</td></tr><tr><td>reboot<id></id></td><td>让一个虚拟机重启</td></tr><tr><td>reset<id></id></td><td>与reboot的区别是，它是强制一个虚拟机重启，相当于在物理机上长按reset按钮，可能会循环虚拟机系统盘的文件系统</td></tr><tr><td>destroy<id></id></td><td>立即销毁一个虚拟机，相当于物理机上直接拔电源</td></tr><tr><td>save<id>&lt;file.img&gt;</id></td><td>保存一个运行中的虚拟机状态到一个文件中</td></tr><tr><td>restore&lt;file.img&gt;</td><td>从一个被保存的文件中恢复一个虚拟机的运行</td></tr><tr><td>migrate<id>&lt;dest_uri&gt;</id></td><td>将一个虚拟机迁移到另一个目标地址</td></tr><tr><td>dump<id>&lt;core.file&gt;</id></td><td>coredump一个虚拟机保存到一个文件</td></tr><tr><td>dumpxml<id></id></td><td>以XML格式转存出一个虚拟机的信息到标准输出中</td></tr><tr><td>attach-device<id>&lt;device.xml&gt;</id></td><td>向一个虚拟机添加xml文件中的设备，也就是热插拔</td></tr><tr><td>detach-device<id>&lt;device.xml&gt;</id></td><td>将一个XML文件中的设备从虚拟机中移除</td></tr><tr><td>console<id></id></td><td>连接到一个虚拟机的控制台</td></tr></tbody></table><p>上表中，只是列出常用的几个KVM虚拟机全生命周期管理命令，如果想查找全量KVM虚拟机全量生命周期管理命令，可以使用命令帮助。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49d4714195aa5944418e2.jpg" alt></p><p>如上图，由于输出太长，我们只截取了一部分。上图中每个命令后面都有详细的文字说明来描述命令的用途。</p><h2 id="虚拟机全生命周期管理实战"><a href="#虚拟机全生命周期管理实战" class="headerlink" title="虚拟机全生命周期管理实战"></a><strong>虚拟机全生命周期管理实战</strong></h2><h3 id="虚拟机状态信息查询"><a href="#虚拟机状态信息查询" class="headerlink" title="虚拟机状态信息查询"></a><strong>虚拟机状态信息查询</strong></h3><p>有了上面的命令储备后，我们下来在环境中进行实操验证。首先，我们通过<strong>list命令，查看下当前节点的虚拟机个数</strong>，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49d5714195aa594442220.jpg" alt></p><p>如上图，本节点有2个虚拟机，当前状态均为shut off，也就是没有启动。那么，我们现在<strong>通过start命令，将上述两个虚拟机同时启动</strong>，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49d7014195aa594442f14.jpg" alt></p><p>启动后，我们可以<strong>通过domstate命令查看虚拟机当前的状态</strong>，并且<strong>通过dominfo命令查看虚拟机的详细信息</strong>，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49d8314195aa59444398e.jpg" alt></p><p>同时，我们可以<strong>通过dommemstate和vcpuinfo命令，查看虚拟机的虚拟内存信息及vCPU信息。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49d9314195aa5944441a8.jpg" alt></p><h3 id="虚拟机vCPU与vMEM管理操作"><a href="#虚拟机vCPU与vMEM管理操作" class="headerlink" title="虚拟机vCPU与vMEM管理操作"></a><strong>虚拟机vCPU与vMEM管理操作</strong></h3><p>如上图，通过上面的vCPU的详细信息，我们发现cto74d的两个vCPU都与pCPU0默认绑定。那么，我们现在<strong>可以通过vcpupin命令将vCPU1与pCPU1进行绑定。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49da514195aa594444d07.jpg" alt></p><p>同时，我们发现cto74d虚拟机只有2个vCPU。现在，我们需要给它增加到4个vCPU怎么办？在KVM中，可以<strong>通过setvcpus命令完成cto74d虚拟机的vCPU在线热添加。</strong>但是，在热添加之前，我们需要明确一个概念，那就是<strong>虚拟机的vCPU最大可分配数与当前vCPU数的区别</strong>。为了讲明白这个概念，我们还是先来看虚拟机cto74d的XML配置文件，可以<strong>通过dumpxml命令将虚拟机XML配置文件输出到屏幕上。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49dbb14195aa594445831.jpg" alt></p><p>上图中表示cto74d虚拟机的vCPU最大可分配数为2。而且，我们还可以<strong>通过指令emulatorpin查看虚拟机当前的vCPU使用情况。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49ddc14195aa5944468dd.jpg" alt></p><p>上图中表示虚拟机cto74d当前使用的vCPU个数也为2。所以，在这种配置要求下，自然无法通过指令将cto74d虚拟机的vCPU数量调整为4个。为了实现我们的需求，首先需要<strong>通过shutdown命令将虚拟机下电</strong>，然后修改配置文件如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49df614195aa594447872.jpg" alt></p><p>上图中，红色框部分表示我们将虚拟机下电，黄色框部分我们将虚拟机的vCPU配置修改为：最大支持4个vCPU，当前使用2个vCPU。我们可以查看XML文件验证下：</p><p><img src="https://pic.imgdb.cn/item/5ef49e0914195aa594448345.jpg" alt></p><p>完成上述修改后，我们需要<strong>通过define命令重新定义下cto74d这个虚拟机</strong>，然后通过start命令启动虚拟机。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49e1c14195aa594448df9.jpg" alt></p><p>现在，我们具备条件后，就可以使用命令setvcpus命令在线对cto74d虚拟机的vCPU进行热添加，添加到4个。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49e2b14195aa594449815.jpg" alt></p><p>在完成vCPU的热插后，那么vCPU是否可以热拔呢？<strong>答案是不可以</strong>，因为当vCPU被分配给虚拟机使用时，虚拟机中的程序进程就会占用刚分配的vCPU，此时如果我们进行热拔操作，前提是需要将上面的进程迁移到别的vCPU，但是我们是无法确切知道有哪些进程的。所以，vCPU自然不支持热拔操作。那么，既然KVM支持CPU和内存的虚拟化，内存是否也支持在线调整呢？答案是必须支持。我们首先通过dominfo命令看下当前虚拟机的内存情况。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49e3a14195aa594449f89.jpg" alt></p><p>上图表示，虚拟机当前最大内存和可用内存都是4194304KB，也就是4GB大小。我们可以通过<strong>通过setmem指令设置虚拟机的可用内存。</strong>与vCPU概念类似，<strong>这种方式支持的在线调整范围只能小于当前虚拟机的最大内存配额，也就是4GB以内。否则，需要关机，先调整虚拟机支持的最大内存，然后再调整虚拟机支持的可用内存。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49e4914195aa59444a79c.jpg" alt></p><p>从上图，我们发现，内存在线调整内存是可以减小的，即在线将虚拟机原内存从4G调整为3G。另外，<strong>setmaxmem指令用于调整虚拟机的最大可支持内存，只能在虚拟机下电后执行。</strong>否则会报如下错误：</p><p><img src="https://pic.imgdb.cn/item/5ef49e5814195aa59444b07f.jpg" alt></p><h3 id="虚拟机磁盘和网卡的热插拔"><a href="#虚拟机磁盘和网卡的热插拔" class="headerlink" title="虚拟机磁盘和网卡的热插拔"></a><strong>虚拟机磁盘和网卡的热插拔</strong></h3><p>完成了vCPU和内存的在线调整实战后，我们接下来玩玩热插拔。先来玩热插拔磁盘，<strong>在KVM中通过virsh管理工具向虚拟机热插拔磁盘，可以使用attach-disk命令。</strong>为了实现上述需求，首先我们需要创建一个虚拟磁盘文件，然后通过attach-disk命令将其挂载到虚拟机中，然后通过控制台进入虚拟机对新挂载的磁盘进行格式化，并创建对应目录对其挂载。最后，我们向挂载目录中写入测试文件测试。同时，需注意向虚拟机挂载raw格式qcow2格式的磁盘是不一样的。</p><p>我们先来热插raw格式的磁盘，操作过程如下：</p><p><strong>Step1：创建一个虚拟磁盘cto74d_data.raw，磁盘格式为raw，大小为5G</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49e6f14195aa59444bbf9.jpg" alt></p><p><strong>Step2：为虚拟机热插磁盘，需要注意指定磁盘的位置要使用绝对路径，并设置虚拟盘符为vdb</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49e7d14195aa59444c3c4.jpg" alt></p><p><strong>Step3：通过控制台进入虚拟机cto74d，查看磁盘信息</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49e8b14195aa59444cafe.jpg" alt></p><p><strong>Step4：对新挂载的磁盘格式化，并创建一个/data目录进行挂载</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49e9d14195aa59444d49a.jpg" alt></p><p><strong>Step5：写入一个大文件进行测试</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49eac14195aa59444e145.jpg" alt></p><p><strong>Step6：我们在宿主机上通过domblklist命令查看虚拟机的磁盘信息</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49eba14195aa59444e805.jpg" alt></p><p>如上图，可以看见当前虚拟机有两块磁盘，一块系统盘，一块数据盘。我们查看虚拟机XML文件信息进行验证。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49ec714195aa59444eeb4.jpg" alt></p><p>以上就是raw格式磁盘的热插操作过程，如果想热插qcow2格式的磁盘（后缀为<em>.qcow2）或想热插qcow2格式的镜像文件（后缀为</em>.img），在Step1创建磁盘时，需要通过<strong>-o size选项指定预分配大小</strong>，否则系统会默认挂载一个几百K大小虚拟盘，这样会导致空间不够。同事，还需要通过preallocation选项指定磁盘格式为metadata，也就是元数据格式。除此之外，这两种类型磁盘的热插操作与raw格式磁盘一致。</p><p>上面完成了磁盘的热插，那么<strong>要实现磁盘的热拔呢？可以通过detach-disk这个命令完成。</strong>但是，这时需要注意一点：<strong>使用virsh指令删除磁盘会直接强制将虚拟机中磁盘删除，如果磁盘已经挂载使用，要停止该磁盘的写操作，否则会造成数据丢失，拔掉的磁盘并没有删除，仍然存储在当时创建的位置，需要使用可以再挂载使用。</strong>因此，在热拔磁盘之前，我们需要先进入虚拟机将对该磁盘的写入进程全部暂停，然后将磁盘从挂载点卸载，最后回到宿主机通过detach-disk命令完成磁盘的热拔。这里，大家自己玩吧，我就不陪着演示了。。。。。</p><p>磁盘的热插拔我们验证了，接下来我们玩网卡的热插拔。<strong>在KVM虚拟机中，要实现网卡的热插，可以使用attach-interface命令完成。同理，热拔的命令就是detach-interface。</strong>首先，我们通过domiflist命令查看当前虚拟机的网络和网卡信息，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49ed714195aa59444f6ee.jpg" alt></p><p>如上图，黄色框部分就表示当前虚拟机cto74d的网络信息，类型（type）为network，源设备（source）为default。如果在创建KVM虚拟机时，网络配置是自定义网桥，那么这里的类型（type）为bridge，源设备（source）为自定义网桥名称，如br0，</p><p>有了上面的知识储备后，下面我们<strong>通过attach-interface命令来热插一个网卡</strong>，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49ee614195aa59444fee9.jpg" alt></p><p>同时，我们可以<strong>通过domifaddr命令查看新增加的网卡动态获得的IP地址</strong>，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49ef314195aa5944505fe.jpg" alt></p><p>接下来，我们尝试下在宿主机通过SSH方式远程连接vnet2的地址是否成功。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f0014195aa594450cd1.jpg" alt></p><p>上图表示，可以通过vnet2的地址192.168.122.230远程连接虚拟机cto74d，表明我们新增加的网卡有效且地址分配正常（这个地址是通过DHCP方式动态分配的），通过输入密码就可以远程登录。</p><p>搞定网卡的热插后，我们接下来自然要实现网卡的热拔。与磁盘类似，在对网卡热拔时，首先需要停止网卡的工作，这样后续数据包就不会转发到该网卡上，不到导致网络丢包。而且，在对网卡热拔时，我们要根据网卡的MAC地址来实现，而不能通过网卡名称，否则libvirt内部会将该网卡名称归属的同一个二层设备上所有端口销毁。<strong>对网卡热拔的命令就是detach-ifinterface。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f0f14195aa594451554.jpg" alt></p><p>上图中，通过–mac参数指定mac地址删除，就不会发生误删除事件。</p><h2 id="虚拟机的热迁移"><a href="#虚拟机的热迁移" class="headerlink" title="虚拟机的热迁移"></a><strong>虚拟机的热迁移</strong></h2><p>接下来，我们再来玩一个更高级的操作，就是<strong>KVM虚拟机的热迁移</strong>。在KVM中虚拟机的迁移分为热迁移和冷迁移。每一种迁移方式又细分为基于本地存储和基于共享存储两种。</p><p>两种方式下的冷迁移均比较简单，就是将原虚拟机的磁盘文件和配置文件拷贝至目标服务器，然后通过配置文件定义一个新的虚拟机启动即可。而基于本地存储的热迁移与此类似，也需要在目标主机创建同一个存储文件，通过暴露TCP端口，基于socket完成迁移，同样比较简单，但是过程很慢。有兴趣的可参考<a href="https://www.cnblogs.com/lyhabc/p/6104555.html这篇文章。" target="_blank" rel="noopener">https://www.cnblogs.com/lyhabc/p/6104555.html这篇文章。</a></p><p>我们这里演示的是<strong>基于共享存储的热迁移（电信云采用分布式存储，每一个计算组规划区都是一个共享存储池）</strong>，它实现的前提就是需要源目服务器之间有共享存储。因此，为了演示这个操作，我们需要在源主机（192.168.101.251）和目标主机（192.168.101.252）之间通过NFS方式建立共享文件系统，也可以使用GFS2集群文件系统来实现。至于，NFS网络共享文件系统如何配置，请参见本站的Linux常用运维工具分类中的NFS网络文件共享系统一文，这里不再赘述。</p><p>整个实验的拓扑如下，源主机和目标主机都作为NFS的客户端与NFS服务器（192.168.101.11）之间互通，实现数据的共享存储。</p><p><img src="https://pic.imgdb.cn/item/5ef49f1f14195aa594451e45.jpg" alt></p><p><strong>Step1：首先确保两个NFS客户端服务器与NFS服务器端共享目录正常，且两个客户端服务器上存放虚拟机磁盘的目录一致。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f2d14195aa5944525dc.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49f3c14195aa594453127.jpg" alt></p><p><strong>Step2：在251服务器首先将虚拟机磁盘创建在/mydata/nfsdir下，格式为qcow2格式，然后去252服务器的对应目录下检查确保文件共享正常。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f4a14195aa594453908.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49f5714195aa594454001.jpg" alt></p><p><strong>Step3：在251服务器创建虚拟机cto67s，并启动。</strong>如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f6514195aa59445473f.jpg" alt></p><p>在安装过程中，我们手动通过图形化方式安装，因此需要查询vnc图形化界面的连接地址和服务端口。<strong>可以通过vncdisplay命令查看虚拟机的VNC客户端连接地址及服务端口</strong>，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f7414195aa594454f4a.jpg" alt></p><p>然后，还是通过vnc界面安装虚拟机，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f8214195aa594455686.jpg" alt></p><p>完成，虚拟机OS安装后，需要重启生效。然后，源主机（192.168.101.251）中通过start命令，启动虚拟机cto67s。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49f9714195aa59445623c.jpg" alt></p><p><strong>Step4：开始虚拟机cto67s的热迁移。</strong>如下：</p><p>如上图，当前虚拟机cto67s在源主机（192.168.101.251）处于开机运行状态，为了清晰说明热迁移的过程，我们需要知道虚拟机cto67s的ip地址，这样在虚拟机热迁移时，由于内存热数据的迭代拷贝，会有一个暂停-恢复-暂停-恢复的过程，反映在网络测就会出抖动或丢包。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49fa514195aa5944569d6.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49fb214195aa594457142.jpg" alt></p><p>完成上述的准备工作后，我们<strong>通过migrate命令开始进行虚拟机热迁移</strong>。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49fc014195aa594457f47.jpg" alt></p><p><strong>如上图，我们期望的热迁移失败啦。。。。这是为啥？</strong>我们看上图红色框部分的报错内容，<strong>提示：域名解析失败。</strong>其实，在迁移期间，目标主机上运行的libvirtd会从其希望接收迁移数据的地址和端口创建URI，并将其发送回在源主机上运行的libvirtd。在我们的例子中，目标主机（192.168.122.252）的名称设置为“c7-test02”。出于某种原因，在该主机上运行的libvirtd无法将该名称解析为可以发回的IP地址，因此它发送了主机名，希望源libvirtd在解析名称时会更成功。但是，源主机的libvirt因为也没有配置这个主机名的解析，所以提示：Name or service not know。</p><p>有了上面的分析，那就简单了，无非就是在源主机和目的主机上配置主机名和IP的解析就可解决上述问题。这里，有两种配置方法：一种是在/etc/resolve中配置DNS解析，但是不建议这样配置，因为这里一般是配置公网域名解析的地方。另一种就是在/etc/hosts中配置主机名与IP的映射关系。我们采用第二种方法解决。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49fce14195aa5944586c8.jpg" alt></p><p>完成上面的配置后，我们再次尝试热迁移。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49fdd14195aa594459562.jpg" alt></p><p>同时，我们在源主机和目标主机分别查看虚拟机cto67s的状态。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49fea14195aa594459d1d.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49ff814195aa59445a522.jpg" alt></p><p>此时，虽然虚拟机cto67s已经在目标主机上启动，但是目标主机上还没有虚拟机cto67s的配置文件。所以需要根据当前虚拟机的状态，创建配置文件并定义虚拟机。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a00714195aa59445adb5.jpg" alt></p><p>最后，我们从目标主机上进入虚拟机cto67s进行验证，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4a01314195aa59445b42e.jpg" alt></p><p>至此，KVM虚拟机热迁移演示完成。这里需要提示一下，由于我们创建的虚拟机都是nat网络模式的，这样在迁移后，在源主机ping虚拟机测试会提示目标不可达，但是在虚拟机内部ping源主机可以ping通，只是迁移过程中会有1-2个ICMP包丢失或抖动。如果，非要在宿主机上ping测试虚拟机来观察迁移过程中的丢包或抖动过程，可以将虚拟机的网络模式改为桥接bridge。而且，源主机和目标主机必须归属同一个网桥bridge。</p><p>以上，就是我们KVM虚拟机全生命周期管理实战的全部内容，至于虚拟机的挂起、恢复等操作都比较简单，我也就懒得举例演示，各位可以自行玩玩，挺有意思的，真的。。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇我们介绍了KVM最重要的管理工具libvirt，它是KVM其他管理工具的基础，处于KVM管理架构的中间适配层。本篇我们主要要介绍libvirt的命令行管理工具的virsh，它也是将libvirt作为基础，通过封包调用实现的。所以，在看本篇内容之前，最好将上一篇的内容做个预习。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>KVM虚拟机的各种安装方法</title>
    <link href="https://kkutysllb.cn/2020/03/01/KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%90%84%E7%A7%8D%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/"/>
    <id>https://kkutysllb.cn/2020/03/01/KVM虚拟机的各种安装方法/</id>
    <published>2020-03-01T10:19:09.000Z</published>
    <updated>2020-06-25T12:47:46.762Z</updated>
    
    <content type="html"><![CDATA[<h2 id="virt-instal工具简介"><a href="#virt-instal工具简介" class="headerlink" title="virt-instal工具简介"></a><strong>virt-instal工具简介</strong></h2><p>virt-install是一个命令行工具，它能够为KVM、Xen或其它支持libvrit API的hypervisor创建虚拟机并完成GuestOS安装；此外，它能够基于串行控制台、VNC或SDL支持文本或图形安装界面。安装过程可以使用本地的安装介质如CDROM，也可以通过网络方式如NFS、HTTP或FTP服务实现。对于通过网络安装的方式，virt-install可以自动加载必要的文件以启动安装过程而无须额外提供引导工具。当然，virt-install也支持PXE方式的安装过程，也能够直接使用现有的磁盘映像直接启动安装过程。<a id="more"></a></p><p><img src="https://pic.imgdb.cn/item/5ef49af914195aa59442ce22.jpg" alt></p><p>virt-install命令有许多选项，这些选项大体可分为下面几大类，同时对每类中的常用选项也做出简单说明。</p><table><thead><tr><th>普通选项</th><th></th><th style="text-align:left"></th></tr></thead><tbody><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>-n或–name</td><td></td><td style="text-align:left">虚拟机名称，全局唯一</td></tr><tr><td>-r或–ram</td><td></td><td style="text-align:left">虚拟机内存大小，单位为MB</td></tr><tr><td>–vcpus</td><td>maxvcpu，sockets，cores，threads</td><td style="text-align:left">vCPU的个数及相关配置</td></tr><tr><td>–cpu</td><td></td><td style="text-align:left">CPU模式及特性，可以使用qemu-kvm -cpu ? 来获取支持的类型</td></tr><tr><td>安装方法选项</td><td></td><td style="text-align:left"></td></tr><tr><td>选项</td><td>子选项</td><td style="text-align:left">说明</td></tr><tr><td>-c或–cdrom</td><td></td><td style="text-align:left">光盘安装介质</td></tr><tr><td>-l或–location</td><td></td><td style="text-align:left">安装源URL，支持FPT，HTTP及NFS，如:<a href="ftp://172.0.6.1/pub" target="_blank" rel="noopener">ftp://172.0.6.1/pub</a></td></tr><tr><td>–pxe</td><td></td><td style="text-align:left">基于PXE完成安装</td></tr><tr><td>–livecd</td><td></td><td style="text-align:left">把光盘当做启动引导CD</td></tr><tr><td>–os-type</td><td></td><td style="text-align:left">操作系统类型，如linux、windows或unix等</td></tr><tr><td>–os-variant</td><td></td><td style="text-align:left">某类型操作系统的发行版，如rhel7、Ubuntud等</td></tr><tr><td>-x或–extra-args</td><td></td><td style="text-align:left">根据–location指定的方式安装GuestOS时，用于传递给内核的参数选项，例如指定kickstart文件的位置。</td></tr><tr><td>–boot</td><td></td><td style="text-align:left">指定安装过程完成后的配置选项，如指定引导设备的次序、使用指定的而非安装kernel/initrd来引导系统。比如：–boot cdrom、hd、network：指定引导次序分别为光驱、硬盘和网络；–boot kernel=KERNEL,initrd=INITRD,kernel_args=”console=/dev/ttyS0”：指定启动系统的内核及initrd文件，并创建一个模拟终端</td></tr><tr><td><strong>存储配置</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–disk</td><td></td><td style="text-align:left">指定存储设备的路径及其属性</td></tr><tr><td></td><td>device</td><td style="text-align:left">设备类型，如cdrom、disk或floppy等，默认为disk</td></tr><tr><td></td><td>bus</td><td style="text-align:left">磁盘总线类型，可以为ide、scsi、usb、virtio或xen</td></tr><tr><td></td><td>perms</td><td style="text-align:left">访问权限，如rw、ro或sh（共享可读写），默认为rw</td></tr><tr><td></td><td>size</td><td style="text-align:left">新建磁盘镜像大小，单位为GB</td></tr><tr><td></td><td>cache</td><td style="text-align:left">缓存类型，可以为none、writethrouth及writeback</td></tr><tr><td></td><td>format</td><td style="text-align:left">磁盘镜像格式，如raw、qcow2、vmdk等</td></tr><tr><td></td><td>sparse</td><td style="text-align:left">磁盘镜像的存储数据格式为稀疏格式，即不立即分配指定大小的空间</td></tr><tr><td>–nodisks</td><td></td><td style="text-align:left">不适用本地磁盘，在LiveCD模式中常用</td></tr><tr><td><strong>网络配置</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>-w或–network</td><td></td><td style="text-align:left">将虚拟机连入虚拟网络中</td></tr><tr><td></td><td>bridge=BRIDGE</td><td style="text-align:left">虚拟网络为名称为BRIDGE的网桥设备</td></tr><tr><td></td><td>network=NAME</td><td style="text-align:left">虚拟网络为名称NAME的网络</td></tr><tr><td></td><td>model</td><td style="text-align:left">GuestOS中看见的虚拟网络设备类型</td></tr><tr><td></td><td>mac</td><td style="text-align:left">配置固定的MAC地址，省略此选项时，MAC地址随机分配，但是无论何种方式，KVM的网络设备的MAC地址前三段必须为52:54:00</td></tr><tr><td>–nonetworks</td><td></td><td style="text-align:left">虚拟机不使用网络</td></tr><tr><td><strong>图形配置</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–graphics</td><td></td><td style="text-align:left">指定图形显示相关的配置，此选项不会配置任何硬件，而是指定虚拟机启动后对其访问的图形界面接口</td></tr><tr><td></td><td>TYPE</td><td style="text-align:left">指定显示类型，可以为vnc，spice、sdl或none</td></tr><tr><td></td><td>port</td><td style="text-align:left">TYPE为vnc或spice时其监听的端口</td></tr><tr><td></td><td>listen</td><td style="text-align:left">TYPE为vnc或spice时其监听的IP地址，默认为127.0.0.1，可以通过修改/etc/libvirt/qemu.conf定义新的默认值</td></tr><tr><td></td><td>password</td><td style="text-align:left">TYPE为vnc或spice时为远程访问监听的服务指定认证密码</td></tr><tr><td>–noautoconsole</td><td></td><td style="text-align:left">禁止自动连接到虚拟机的控制台</td></tr><tr><td><strong>设备选项</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–serial</td><td></td><td style="text-align:left">附加一个串行设备到当前虚拟机，根据设备类型不同，可以使用不同的选项，格式为–serial type,opt1=val1,opt2=val2</td></tr><tr><td></td><td>pty</td><td style="text-align:left">创建伪终端</td></tr><tr><td></td><td>dev,path=HOSTPATH</td><td style="text-align:left">附加主机设备至此虚拟机</td></tr><tr><td>–video</td><td></td><td style="text-align:left">指定显卡设备类型，如cirrus、vga、qxl或vmvga</td></tr><tr><td><strong>虚拟化平台</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>-v或–hvm</td><td></td><td style="text-align:left">当宿主机同时支持全虚和半虚时，使用此选项指定硬件辅助的全虚</td></tr><tr><td>-p或–paravirt</td><td></td><td style="text-align:left">指定使用半虚</td></tr><tr><td>–virt-type</td><td></td><td style="text-align:left">指定使用的hypervisor，如kvm、xen、qemu等，其值可以通过virsh capabilities获得</td></tr><tr><td><strong>其它</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–autostart</td><td></td><td style="text-align:left">指定虚拟机是否在物理机启动后自动启动</td></tr><tr><td>–print-xml</td><td></td><td style="text-align:left">如果虚拟机不需要安装过程(–import、–boot)，则显示生成的XML而不是创建虚拟机。默认情况下，此选项仍会创建虚拟磁盘</td></tr><tr><td>–force</td><td></td><td style="text-align:left">禁止进入命令交互模式，如需回答yes或no，默认自动回答yes</td></tr><tr><td>–dry-run</td><td></td><td style="text-align:left">执行创建虚拟机的整个过程，但不创建虚拟机，改变主机上的设备配置信息及将其创建的需求通知给libvirt；</td></tr><tr><td>-d或–debug</td><td></td><td style="text-align:left">显示debug信息；</td></tr></tbody></table><p><strong>尽管virt-install命令有着类似上述的众多选项，但实际使用中，其必须提供的选项仅包括–name、–ram、–disk（也可是–nodisks）及安装过程相关的选项。此外，有时还需要使用括–connect=CONNCT选项来指定连接至一个非默认的hypervisor。</strong></p><h2 id="图形化界面安装"><a href="#图形化界面安装" class="headerlink" title="图形化界面安装"></a><strong>图形化界面安装</strong></h2><p>这里的使用图形化界面不是通过virt-manager和virt-view工具在图形化服务器或宿主机来安装，而是通过–graphics选项指定vnc或spice方式安装，安装过程中需要宿主机通过vnc client连接图形化接口实现。在我们的KVM实践初步一文中介绍安装ubuntu1204桌面版虚拟机就是采用这种方式。忘了的，可以回顾本站那篇文章。</p><p>现在，我们这里演示的是另一种图形化安装方法，还是通过vnc实现，安装介质使用ubuntu18.10桌面版ISO。如下：</p><p><strong>step1：</strong>我们先查看下当前激活的虚拟网络有哪些，方便后续安装虚拟机时配置网络（图中红色框部分）。同时，我们创建一个qcow2格式的虚拟机磁盘镜像，大小为120G（图中黄色框部分）。</p><p><img src="https://pic.imgdb.cn/item/5ef49b1414195aa59442dd82.jpg" alt></p><p><strong>step2：</strong>通过virt-install工具安装第一个模板虚拟机ubt1810d。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49b2214195aa59442e46a.jpg" alt></p><p>上图中，我们指定了虚拟机当前vcpu为2个，最大可分配4个（黄色框部分），同时指定了虚拟机的镜像磁盘格式为qcow2格式（采用qcow2格式，此项必须明确指定，并且通过size选项指定大小，否则生成的磁盘大小异常，或者在创建镜像磁盘时通过-o选项明确指定也可以），总线类型为virtio类型（图中蓝色框部分）</p><p><strong>step3：</strong>通过vncdisplay命令查询当前虚拟机图形界面接口服务vnc监听的端口号，并通过物理机的vnc client连接开始安装。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49b3a14195aa59442f2a9.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49b4414195aa59442f7e4.jpg" alt></p><p><strong>step4：</strong>完成虚拟机系统安装后，我们通过vnc client登录虚拟机，打开虚拟机的console控制接口，便于后续通过宿主机console连接虚拟机控制台。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49b5414195aa594430124.jpg" alt></p><p><strong>以上，就是通过图形界面接口实现kvm虚拟机的安装过程。一般通过图形界面接口安装，主要用于安装桌面版的操作系统，比如上面的ubuntu desktop版本，windows的各种版本等。</strong>由于ubuntu系统默认不开启root用户的ssh登录，为了方便后续管理，我们可以在ubuntu系统打开ssh的root登录权限。但是，记得首先要设置root的登录密码。这些操作就可以通过宿主机console控制台来完成。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49b7b14195aa5944315ba.jpg" alt></p><h2 id="文本字符界面安装"><a href="#文本字符界面安装" class="headerlink" title="文本字符界面安装"></a><strong>文本字符界面安装</strong></h2><p>上面的图形化安装方式主要借助vnc或spice客户端来实现图形界面安装接口，主要用于桌面操作系统的安装。在实际运维时，一般用于app的虚拟机大部分采用非桌面系统，而且打开vnc或spice监听端口不便于批量安装。这种情况下，就需要通过文本界面实现自动安装。主要通过-x选项打开一个串行接口终端tty和ttyS0实现。但是，如果使用-x选项安装，安装介质必须使用–location选项，而不能使用–cdrom来完成。此时，有两个选择，一是将iso镜像拷贝到宿主机的某个目录，通过-l（–location）选项指定即可，类似–cdrom方式。另一种是将iso挂载到某个目录，通过http、nfs或ftp方式发布，然后在-l选项指定URL即可。我们这里采用第二种方式的http方式进行安装，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49b8a14195aa594431f42.jpg" alt></p><p>上述安装方式，通过-l选项指定安装介质的挂载目录，通过-x选项安装使用kiskstart文件，以及打开串行终端console，通过–nographics选项指定不采用图形界面方式安装。命令执行后，安装界面如下，且通过kickstart文件自动化安装。</p><p><img src="https://pic.imgdb.cn/item/5ef49b9914195aa59443276e.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49ba914195aa594433184.jpg" alt></p><p>以上这种安装方式，也是实际运维中常用的安装方式，同时可以在kickstart文件中指定初始化脚本，这样在系统安装完成后自动完成初始化配置。后续，就可以将此虚拟机作为模板镜像，批量分发。</p><p>但是，如果要将当前虚拟机作为模板镜像，需要删除当前虚拟机中的MAC和UUID，同时清空本地规则和系统规则中网卡配置文件，这样后续通过该虚拟机的模板镜像生成的新的虚拟机就会自动生成新的MAC和网卡名称，而不用通过在XML配置文件中手动指定MAC，实现自动化运维。上面的要求，需要在虚拟机中完成以下配置，然后关机作为后续分发虚拟机的模板。</p><p><img src="https://pic.imgdb.cn/item/5ef49bba14195aa594433a85.jpg" alt></p><p><strong>虚机模板方式安装</strong></p><p>模板方式安装一般有两种方式：一种是通过–import选项导入镜像虚拟机的磁盘来生成一个新的虚拟机，这种方式只能生成一个新虚拟机，当要生成第二个虚拟机时会提示磁盘被占用错误。另一种方式就是利用模板镜像虚拟机的XML文件，生成新的虚拟机的XML配置文件，需要删除源模板镜像虚拟机XML文件中的uuid、mac address，并修改磁盘路径为新虚拟机的磁盘（差分盘或新磁盘都可），而且需要修改XML文件中的虚拟机name为新建虚拟机name。完成后，再通过virsh define预定义一个新虚拟机，然后通过virsh start启动即可。下面，我们就对这两种方式逐一进行验证。</p><p><strong>1）通过–import选项导入现有虚拟机磁盘生成一个新的虚拟机</strong></p><p><img src="https://pic.imgdb.cn/item/5ef49bc714195aa59443426b.jpg" alt></p><p>完成上面的操作后，我们就能立即生成一个新的虚拟机cirros01。然后，我们可以通过virsh console命令登录并执行相关操作。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49bd514195aa594434cc1.jpg" alt></p><p>此时，我们如果需要再通过上面镜像盘生成一个虚拟机时，会提示镜像盘被占用的错误。如下：</p><p>因此，这种方式只能生成一个新虚拟机，一般用于很小的初始化配置测试。所以，在实际运维中，主要通过第二种方式利用模板镜像虚拟机来批量创建虚拟。</p><p><strong>2）通过模板镜像虚机利用virsh-define和virsh-start实现新虚机创建</strong></p><p>首先，我们利用模板镜像虚机磁盘创建一个差分磁盘，用于新虚机的系统盘（也可以创建非差分盘，看实际业务需求而定）。如下，通过-b选项指定backing file为模板镜像虚机的磁盘。</p><p><img src="https://pic.imgdb.cn/item/5ef49bf314195aa594435f1f.jpg" alt></p><p>然后，利用模板镜像虚机的xml文件生成新虚机的xml配置文件。需要删除uuid，mac address配置，修改磁盘存储位置为新创建的虚机差分盘，修改name为新虚机的name。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49c0114195aa5944365b8.jpg" alt></p><p>然后，通过virsh-define定义新虚机，再通过virsh-start启动新虚机，然后通过console控制台进入虚机。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49c0e14195aa594436ce2.jpg" alt></p><p>如下图，可以发现系统为我们自动生成mac和ip，而且mac必然是52:54:00开头。</p><p><img src="https://pic.imgdb.cn/item/5ef49c1e14195aa594437599.jpg" alt></p><p>而且，新生成的虚拟机具备访问外网和宿主机的权利。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49c7114195aa59443a497.jpg" alt></p><h2 id="批量创建KVM虚拟机的脚本"><a href="#批量创建KVM虚拟机的脚本" class="headerlink" title="批量创建KVM虚拟机的脚本"></a><strong>批量创建KVM虚拟机的脚本</strong></h2><p>掌握上面各种创建KVM虚拟机的方法后，我们可以写一个shell脚本来完成批量创建虚拟机的任务。</p><p>首先，我们需要准备创建虚拟机的镜像磁盘，主要有两种方法：一种是拷贝模板虚拟机的镜像磁盘，这种主要用于创建独立虚拟机，优点是不依赖于模板虚拟机磁盘镜像，可以完整保留数据。缺点就是占用存储空间较大。另一种方式就是我们上面的通过建立差分磁盘实现，优点是存储空间占用较小，缺点就是模板镜像磁盘一旦损坏将造成所有依赖虚拟机无法启动。</p><p>完成虚拟磁盘创建后，我们就可以按照上面办法通过模板镜像虚机的XML配置文件批量创建新的虚拟机XML配置文件，并修改里面的NAME、UUID、MAC ADDRESS和DISK PATH等配置项。最终，通过define和start命令完成新虚拟机的定义和启动。</p><p>通过上面的分析，要实现KVM虚拟机批量创建shell脚本，只需要通过两个for循环就能完成。同时，在程序开始要检查程序脚本的执行权限，从管理角度来说非root用户不能执行。还要给脚本进行传参，用来设定需要批量拉起的KVM虚拟机的上限值和下限值，也就是确定批量拉起的虚机个数。那么，传递的参数就必须是2个，不能多也不能少，且两个参数都必须是数字，参数1的值要小于参数2。</p><p>以上，就是我们程序要实现的逻辑，可以采用函数式编程的概念封装成三个函数，实现简单模块化设计。为了讲述清楚，我这里就不封装了。接下来，我们就来一步步写这个脚本。</p><p><strong>Step1：</strong>通过第一个for循环批量创建虚拟机的虚拟磁盘，这里我们采用差分磁盘的方式。如下</p><p><img src="https://pic.imgdb.cn/item/5ef49c8014195aa59443ade6.jpg" alt></p><p>脚本执行后效果如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49c9014195aa59443b796.jpg" alt></p><p><strong>Step2：</strong>通过第二个for循环批量创建虚拟机的XML配置文件，并定义和启动虚拟机。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49c9e14195aa59443bf6b.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49caf14195aa59443c7ee.jpg" alt></p><p>完成上面的替换，在脚本执行virsh defiine和virsh start命令定义新的虚拟机并完成启动。脚本执行后效果如下：</p><p><img src="https://pic.imgdb.cn/item/5ef49ccc14195aa59443d6e7.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef49cdb14195aa59443def4.jpg" alt></p><p>至此，KVM虚拟机的各种创建方法介绍完毕，网上还有一种通过qemu-kvm命令创建虚拟机的方法，与通过virt-install方法创建大同小异，且红帽官方并不建议这种方法。因此，只要掌握virt-install方法创建虚拟机即可。</p><p><img src="https://pic.imgdb.cn/item/5ef49ce914195aa59443e678.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;virt-instal工具简介&quot;&gt;&lt;a href=&quot;#virt-instal工具简介&quot; class=&quot;headerlink&quot; title=&quot;virt-instal工具简介&quot;&gt;&lt;/a&gt;&lt;strong&gt;virt-instal工具简介&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;virt-install是一个命令行工具，它能够为KVM、Xen或其它支持libvrit API的hypervisor创建虚拟机并完成GuestOS安装；此外，它能够基于串行控制台、VNC或SDL支持文本或图形安装界面。安装过程可以使用本地的安装介质如CDROM，也可以通过网络方式如NFS、HTTP或FTP服务实现。对于通过网络安装的方式，virt-install可以自动加载必要的文件以启动安装过程而无须额外提供引导工具。当然，virt-install也支持PXE方式的安装过程，也能够直接使用现有的磁盘映像直接启动安装过程。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>KVM管理工具libvirt</title>
    <link href="https://kkutysllb.cn/2020/03/01/KVM%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7libvirt/"/>
    <id>https://kkutysllb.cn/2020/03/01/KVM管理工具libvirt/</id>
    <published>2020-03-01T08:29:40.000Z</published>
    <updated>2020-06-25T12:09:57.622Z</updated>
    
    <content type="html"><![CDATA[<p>通过前面两篇文章的介绍，相信大家对KVM虚拟机的原理、软件架构、运行机制和部署方式等有所了解。但是，那些东西只能算是非常基础入门的东西。尤其我们介绍的部署KVM虚拟机时用到的virt-install命令，不仅参数很多，很难记忆，而且要想用好virt-<em>系列工具首先需要对libvirt工具有更深刻的了解。因为，virt-</em>系列工具其实是对libvirt工具的封装调用，而libvirt工具又是对底层qemu工具的封装调用，其目的都是为了使命令更加友好与用户交互。本篇文章就详细介绍这几种与libvirt相关的管理工具，为后续各种配置打下良好基础。<a id="more"></a></p><h2 id="libvirt管理工具"><a href="#libvirt管理工具" class="headerlink" title="libvirt管理工具"></a><strong>libvirt管理工具</strong></h2><p>提到KVM的管理工具，就不得不提大名鼎鼎的libvirt，因为libvirt是目前使用最为广泛的对KVM虚拟机进行管理的工具和应用程序接口，而且一些常用的虚拟机管理工具（如virsh、virt-install、virt-manager等）和云计算框架平台（如OpenStack、ZStack、OpenNebula、Eucalyptus等）都在底层使用libvirt的应用程序接口，也就是说<strong>libvirt实际上是一个连接底层Hypervisor与上层应用的中间适配层。</strong></p><p><img src="https://pic.imgdb.cn/item/5ef4919514195aa5943d6dbc.jpg" alt></p><p>如上图，<strong>libvirt作为一个中间适配层，可以让底层Hypervisor对上层用户空间的管理工具是完全透明的，</strong>也就是说上层管理应用无需关注底层的差别，只需要调用libvirt去完成，因此只需要对接libvirt的对外开放api接口即可。同时，<strong>libvirt对底层多种不同的Hypervisor的支持是通过一种基于驱动的架构来实现的，类似neutron中的ml2层。libvirt对不同的Hypervisor提供了不同的驱动。</strong>比如：对Xen有xen_dirver.c驱动文件，对vmware有vmware_dirver.c驱动文件，对kvm有qemu_driver.c驱动文件等等。。。</p><p><strong>在libvirt中涉及节点Node、Hypervisor、域Domain等几个概念，其逻辑关系如下：</strong></p><p><img src="https://pic.imgdb.cn/item/5ef491b814195aa5943d941a.jpg" alt></p><blockquote><ul><li><strong>节点（Node）是一个物理机器</strong>，上面可能运行着多个虚拟客户机。Hypervisor和Domain都运行在节点上。</li><li><strong>Hypervisor也称虚拟机监控器（VMM），如KVM、Xen、VMware、Hyper-V等，是虚拟化中的一个底层软件层</strong>，它可以虚拟化一个节点让其运行多个虚拟机（不同虚拟机可能有不同的配置和操作系统）。</li><li><strong>域（Domain）是在Hypervisor上运行的一个虚拟机操作系统实例。</strong>域也被称为实例（instance，如在亚马逊的AWS云计算服务中客户机就被称为实例）、客户机操作系统（guest OS）、虚拟机，它们都是指同一个概念。</li></ul></blockquote><p>libvirt相关的配置文件都在/etc/libvirt/目录之中，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef491ca14195aa5943da7b7.jpg" alt></p><p>主要涉及四个文件：</p><p><strong>1）/etc/libvirt/libvirt.conf：</strong>用于配置一些常用libvirt连接（通常是远程连接）的别名。比如：uri_aliases = [  “remote1=qemu+ssh:<a href="mailto://root@192.168.101.252" target="_blank" rel="noopener">//root@192.168.101.252</a>/system” ]，有这个别名后，就可以在用virsh等工具或自己写代码调用libvirt API时使用这个别名，比如在Python可以这样调用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conn = libvirt.openReadOnly(<span class="string">'remote1'</span>)</span><br></pre></td></tr></table></figure><p><strong>2）/etc/libvirt/libvirtd.conf：</strong>是libvirt的守护进程libvirtd的配置文件，被修改后需要让libvirtd重新加载配置文件（或重启libvirtd）才会生效。在文件的每一行中使用“配置项=值”（如tcp_port=”16509”）这样key-value格式来设置。我们上篇文章介绍的说监听tcp链接和端口，就是在这里配置。</p><p><strong>3）/etc/libvirt/qemu.conf：</strong>是libvirt对QEMU的驱动的配置文件，包括VNC、SPICE等，以及连接它们时采用的权限认证方式的配置，也包括内存大页、SELinux、Cgroups等相关配置。</p><p><strong>4）/etc/libvirt/qemu/目录：</strong>存放的是使用QEMU驱动的域的配置文件，也就是kvm虚拟机的配置文件以及networks目录存放默认网络配置文件。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef491df14195aa5943db62b.jpg" alt></p><p><strong>要让某个节点能够利用libvirt进行管理（无论是本地还是远程管理），都需要在这个节点上运行libvirtd这个守护进程，以便让其他上层管理工具可以连接到该节点，libvirtd负责执行其他管理工具发送给它的虚拟化管理操作指令</strong>，比如：virsh、virt-manager、nova-compute等等。在CentOS 7.x系统中，libvirtd作为一个系统服务存在，可以利用systemctl工具进行启动、停止、重启、重载等操作。另外，libvirtd守护进程的启动或停止，并不会直接影响正在运行中的客户机。libvirtd在启动或重启完成时，只要客户机的XML配置文件是存在的，libvirtd会自动加载这些客户的配置，获取它们的信息。</p><p><strong>libvirtd除了是系统服务外，本身还是一个可执行程序，可以通过一些选项完成系统进程的启动。</strong>比如：-d选项可以让libvirtd作为守护进程（daemon）在后台运行。-f选项可以指定libvirtd的配置文件启动服务，而不使用默认的配置文件。-l选项开启配置文件中指定的TCP/IP连接，监听TCP socket和服务端口。-p指定进程PID，不使用系统默认分配的PID号等等。</p><h2 id="解读虚拟机的XML配置文件"><a href="#解读虚拟机的XML配置文件" class="headerlink" title="解读虚拟机的XML配置文件"></a><strong>解读虚拟机的XML配置文件</strong></h2><p>在使用libvirt对虚拟化系统进行管理时，很多地方都是以XML文件作为配置文件的，包括虚拟机的配置、宿主机网络接口配置、网络过滤、各个客户机的磁盘存储配置、磁盘加密、宿主机和虚拟机的CPU特性等等。所以，对XML文件进行解析，对后续使用libvirtd管理KVM虚拟机理解的更为深刻。</p><p>我们首先来看下，我们前一篇文章创建的虚拟机ubt1204d的XML配置文件如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]# cat /etc/libvirt/qemu/ubt1204d.xml </span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE</span></span><br><span class="line"><span class="comment">OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:</span></span><br><span class="line"><span class="comment">  virsh edit ubt1204d</span></span><br><span class="line"><span class="comment">or other application using the libvirt API.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">domain</span> <span class="attr">type</span>=<span class="string">'kvm'</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ubt1204d<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">uuid</span>&gt;</span>4c4797b9-776c-44e4-8ef5-3ad445092d0f<span class="tag">&lt;/<span class="name">uuid</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">memory</span> <span class="attr">unit</span>=<span class="string">'KiB'</span>&gt;</span>2097152<span class="tag">&lt;/<span class="name">memory</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">currentMemory</span> <span class="attr">unit</span>=<span class="string">'KiB'</span>&gt;</span>2097152<span class="tag">&lt;/<span class="name">currentMemory</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">vcpu</span> <span class="attr">placement</span>=<span class="string">'static'</span>&gt;</span>2<span class="tag">&lt;/<span class="name">vcpu</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">os</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">type</span> <span class="attr">arch</span>=<span class="string">'x86_64'</span> <span class="attr">machine</span>=<span class="string">'pc-i440fx-rhel7.0.0'</span>&gt;</span>hvm<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">boot</span> <span class="attr">dev</span>=<span class="string">'hd'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">os</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">features</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">acpi</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">apic</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">features</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">cpu</span> <span class="attr">mode</span>=<span class="string">'custom'</span> <span class="attr">match</span>=<span class="string">'exact'</span> <span class="attr">check</span>=<span class="string">'partial'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">model</span> <span class="attr">fallback</span>=<span class="string">'allow'</span>&gt;</span>Broadwell-IBRS<span class="tag">&lt;/<span class="name">model</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">cpu</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">clock</span> <span class="attr">offset</span>=<span class="string">'utc'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">timer</span> <span class="attr">name</span>=<span class="string">'rtc'</span> <span class="attr">tickpolicy</span>=<span class="string">'catchup'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">timer</span> <span class="attr">name</span>=<span class="string">'pit'</span> <span class="attr">tickpolicy</span>=<span class="string">'delay'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">timer</span> <span class="attr">name</span>=<span class="string">'hpet'</span> <span class="attr">present</span>=<span class="string">'no'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">clock</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">on_poweroff</span>&gt;</span>destroy<span class="tag">&lt;/<span class="name">on_poweroff</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">on_reboot</span>&gt;</span>restart<span class="tag">&lt;/<span class="name">on_reboot</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">on_crash</span>&gt;</span>destroy<span class="tag">&lt;/<span class="name">on_crash</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pm</span>&gt;</span></span><br><span class="line"> 。。。。省略若干行。。。</span><br><span class="line"><span class="tag">&lt;/<span class="name">domain</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如上，可以看到在该虚拟机的XML文件中，所有有效配置都在<domain>和</domain>标签之间，这表明该配置文件是一个域的配置。而XML文档中的注释是在两个特殊的标签之间，如&lt;！–注释–&gt;。</p><p>通过libvirt启动客户机，经过文件解析和命令参数的转换，最终也会调用qemu命令行工具来实际完成客户机的创建。用这个XML配置文件启动的客户机，它的qemu命令行参数是非常详细、非常冗长的一行，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef491f514195aa5943dc61e.jpg" alt></p><p>下面，我们就逐个模块来解析虚拟机的XML配置文件。</p><h3 id="CPU的配置"><a href="#CPU的配置" class="headerlink" title="CPU的配置"></a><strong>CPU的配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4920a14195aa5943dd3d2.jpg" alt></p><p>上面的XML文件中关于CPU的配置项如上图所示，vcpu标签，表示客户机中vCPU的个数，这里为2。features标签，表示Hypervisor为客户机打开或关闭CPU或其他硬件的特性，这里打开了ACPI、APIC等特性。而cpu标签中定义了CPU的基础特性，在创建虚拟机时，libvirt自动检测硬件平台，默认使用Broadwell类型的CPU分配给虚拟机。在CPU模型中看见的Broadwell-IBRS，可以在/usr/share/libvirt/cpu_map.xml文件查看其具体信息，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4921914195aa5943dddb3.jpg" alt></p><p><strong>对于CPU模型的配置，有以下3种模式。</strong></p><p>1）custom模式：就是这里示例中表示的，基于某个基础的CPU模型，再做个性化的设置。</p><p>2）host-model模式：根据物理CPU的特性，选择一个与之最接近的标准CPU型号，如果没有指定CPU模式，默认也是使用这种模式。xml配置文件为：<cpu mode="host-model">。</cpu></p><p>3）host-passthrough模式：直接将物理CPU特性暴露给虚拟机使用，在虚拟机上看到的完全就是物理CPU的型号。xml配置文件为：<cpu mode="host-passthrough">。</cpu></p><p>对vCPU的分配，可以有更细粒度的配置，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;domain&gt;</span><br><span class="line">  ...</span><br><span class="line">  &lt;vcpu placement=<span class="string">'static'</span> cpuset=<span class="string">"1-4,^3,6"</span> current=<span class="string">"1"</span>&gt;2&lt;/vcpu&gt;</span><br><span class="line">  ...</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure><p>cpuset表示允许到哪些物理CPU上执行，这里表示客户机的两个vCPU被允许调度到1、2、4、6号物理CPU上执行（^3表示排除3号）；而current表示启动客户机时只给1个vCPU，最多可以增加到使用2个vCPU。除了这种方式外，libvirt还提供cputune标签来对CPU的分配进行更多调节，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;domain&gt;</span><br><span class="line">  ...</span><br><span class="line">  &lt;cputune&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"0"</span> cpuset=<span class="string">"1"</span>/&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"1"</span> cpuset=<span class="string">"2,3"</span>/&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"2"</span> cpuset=<span class="string">"4"</span>/&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"3"</span> cpuset=<span class="string">"5"</span>/&gt;</span><br><span class="line">    &lt;emulatorpin cpuset=<span class="string">"1-3"</span>/&gt;</span><br><span class="line">    &lt;shares&gt;2048&lt;/shares&gt;</span><br><span class="line">    &lt;period&gt;1000000&lt;/period&gt;</span><br><span class="line">    &lt;quota&gt;-1&lt;/quota&gt;</span><br><span class="line">    &lt;emulator_period&gt;1000000&lt;/emulator_period&gt;</span><br><span class="line">    &lt;emulator_quota&gt;-1&lt;/emulator_quota&gt;</span><br><span class="line">  &lt;/cputune&gt;</span><br><span class="line">  ...</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure><p>还记得我们在介绍DPDK的亲和性技术不？这里就是<strong>设置亲和性特性的调优配置</strong>，其中vcpupin标签表示将虚拟CPU绑定到某一个或多个物理CPU上，如“&lt;vcpupin vcpu=”2”cpuset=”4”/&gt;”表示客户机2号虚拟CPU被绑定到4号物理CPU上；“<emulatorpin cpuset="1-3">”表示将QEMU emulator绑定到1~3号物理CPU上。在不设置任何vcpupin和cpuset的情况下，虚拟机的vCPU可能会被调度到任何一个物理CPU上去运行。而“<shares>2048</shares>”表示虚拟机占用CPU时间的加权配置，一个配置为2048的域获得的CPU执行时间是配置为1024的域的两倍。如果不设置shares值，就会使用宿主机系统提供的默认值。</emulatorpin></p><p>除了亲和性绑定外，<strong>还有NUMA架构的调优设置，可以配置虚拟机的NUMA拓扑，以及让虚拟机针对宿主机NUMA特性做相应的策略设置等，主要在标签和标签中完成配置。</strong>NUMA特性配置需要在真实物理服务器上完成，手头暂时没有对应环境，也就不列举配置项了。但是，能够对NUMA进行配置的前提是了解NUMA的原理，所以理解前面DPDK技术和计算虚拟化中NUMA技术原理是重点。</p><h3 id="内存的配置"><a href="#内存的配置" class="headerlink" title="内存的配置"></a><strong>内存的配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4923214195aa5943deb81.jpg" alt></p><p>在KVM虚拟机的XML配置文件中，内存的大小配置如上，大小为2,097,152KB（即2GB），memory标签中的内存表示虚拟机最大可使用的内存，currentMemory标签中的内存表示启动时分配给虚拟机使用的内存。在使用QEMU/KVM时，一般将二者设置为相同的值。</p><p>另外，还记得我们讲内存虚拟化时，提到的内存气球技术不？在KVM虚拟机创建时，默认采用内存气球技术给虚拟机提供虚拟内存，它的配置项在<devices></devices>标签对中的memballoon子标签中，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4924214195aa5943df32b.jpg" alt></p><p>如上图，该配置将为虚拟机分配一个使用virtio-balloon驱动的内存气球设备，以便实现虚拟机内存的ballooning调节。该设备在客户机中的PCI设备编号为0000:00:06.0，也就是设备的I/O空间地址。</p><h3 id="虚拟机启动项配置"><a href="#虚拟机启动项配置" class="headerlink" title="虚拟机启动项配置"></a><strong>虚拟机启动项配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4925414195aa5943dfc9a.jpg" alt></p><p>如上图，这样的配置表示客户机类型是hvm类型，HVM（hardware virtual machine，硬件虚拟机）原本是Xen虚拟化中的概念，它表示在硬件辅助虚拟化技术（Intel VT或AMD-V等）的支持下不需要修改客户机操作系统就可以启动客户机。因为KVM一定要依赖于硬件虚拟化技术的支持，所以<strong>在KVM中，客户机类型应该总是hvm</strong>，操作系统的架构是x86_64，机器类型是pc-i440fx-rhel7.0.0，这个机器类型是libvirt中针对RHEL 7系统的默认类型，也可以根据需要修改为其他类型。</p><p>boot选项用于设置虚拟机启动时的设备，这里只有hd（即硬盘）一种，表示从虚拟机硬盘启动，如果有两种及以上，就与物理机中BIOS设置一样，按照从上到下的顺序先后启动。</p><h3 id="网络的配置"><a href="#网络的配置" class="headerlink" title="网络的配置"></a><strong>网络的配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4926414195aa5943e057d.jpg" alt></p><p>如上图，上面虚拟机XML文件的网络时配置是一种<strong>NAT方式的网络配置</strong>，这里type=’network’和<source network="default">就是表示使用NAT的方式，并使用默认的网络配置，虚拟机将会分到192.168.122.0/24网段中的一个IP地址，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4927614195aa5943e1061.jpg" alt></p><p>使用NAT网络配置的前提是宿主机中要开启DHCP和DNS服务，一般libvirtd进程默认使用dnsmasq同时提供DHCP和DNS服务，在宿主机中通过以下命令可以查看：</p><p><img src="https://pic.imgdb.cn/item/5ef4928814195aa5943e1cb8.jpg" alt></p><p>这里的NAT网络的配置在/etc/libvirt/qemu/networks/defaule.xml中配置，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4929614195aa5943e23d0.jpg" alt></p><p>如上，这里配置就是提供分布式虚拟交换机virbr0，用于KVM虚拟机的虚拟接入。详细配置在前一篇文章有介绍，这里不再赘述。由于使用的Linux bridge作为虚拟交换机，因此可以在宿主机中查看该网桥的实际端口，指令如下：</p><p><img src="https://pic.imgdb.cn/item/5ef492a614195aa5943e2b93.jpg" alt></p><p>上图中的vnet0就是KVM虚拟机ubt1204d的虚拟网卡，这样就实现了虚拟机ubt1204d与宿主机之间的网络互通。</p><p>除了NAT网络模式外，还可以通过自建一个Linux bridge的网桥，在创建虚拟机时，–network选项中指定自建的网桥，这样虚拟机安装完毕后，就会自动分配自建的网桥IP地址段中的一个地址，这种方式称为<strong>桥接模式</strong>。同理，这种方式也需要开启DHCP和DNS服务。通过自建网桥方式，在虚拟机XML文件的配置字段如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">interface</span> <span class="attr">type</span>=<span class="string">'bridge'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mac</span> <span class="attr">address</span>=<span class="string">'52:54:00:e9:e0:3b'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span> <span class="attr">bridge</span>=<span class="string">'br0'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">model</span> <span class="attr">type</span>=<span class="string">'virtio'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">address</span> <span class="attr">type</span>=<span class="string">'pci'</span> <span class="attr">domain</span>=<span class="string">'0x0000'</span> <span class="attr">bus</span>=<span class="string">'0x00'</span> <span class="attr">slot</span>=<span class="string">'0x03'</span> <span class="attr">function</span>=<span class="string">'0x0'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">interface</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上述配置表示，是用bridge为虚拟机提供网络服务，mac address指的是虚拟机的MAC地址，<source bridge="br0">表示使用宿主机中的br0网络接口来建立网桥，这个网络接口需要在宿主机中创建配置文件，类似普通网卡的配置，并制定一个物理网卡与该网桥进行绑定，作为上联接口。<model type="virtio">表示在虚拟机中使用virtio-net驱动的网卡设备，也配置了该网卡在虚拟机中的PCI设备编号为0000:00:03.0。</model></p><p>除了上述两种常见的配置外，还有一种<strong>用户模式网络</strong>的配置，类似Virtual Box中的内部网络模式。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef492b714195aa5943e34b0.jpg" alt></p><p>其在虚拟机的XML字段描述如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">interface</span> <span class="attr">type</span>=<span class="string">'user'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">mac</span> <span class="attr">address</span>=<span class="string">"00:11:22:33:44:55"</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">interface</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如上，type=’user’表示该客户机的网络接口是用户模式网络，是完全由QEMU软件模拟的一个网络协议栈，也就是一个纯用户态内部虚拟交换机。因为没有虚拟接口与宿主机互通，所以宿主机无法与这样的虚拟机通信，这种网络模式下只有同一个内部虚拟交换机上不同虚拟机才能互通。</p><p>还记得我们在介绍I/O虚拟化时，讲过网卡的硬件直通技术VMDq和SR-IOV吗？在KVM虚拟机中也可以<strong>绑定这类硬件直通网卡</strong>，在其XML文件中目前有两种方式：<strong>新方式通过标签来绑定</strong>，在其中指定驱动的名称为vfio，并指定硬件的I/O地址即可，但是这种新方式目前只支持SR-IOV方式。由于我们条件限制，特意找了个网上的配置给大家曹侃，示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;interface <span class="built_in">type</span>=<span class="string">'hostdev'</span>&gt;</span><br><span class="line">    &lt;driver name=<span class="string">'vfio'</span>/&gt;</span><br><span class="line">    &lt;<span class="built_in">source</span>&gt;</span><br><span class="line">    &lt;address <span class="built_in">type</span>=<span class="string">'pci'</span> domain=<span class="string">'0x0000'</span> bus=<span class="string">'0x08'</span> slot=<span class="string">'0x10'</span> <span class="keyword">function</span>= <span class="string">'0x0'</span>/&gt;</span><br><span class="line">    &lt;/<span class="built_in">source</span>&gt;</span><br><span class="line">  &lt;mac address=<span class="string">'52:54:00:6d:90:02'</span>&gt;</span><br><span class="line">  &lt;/interface&gt;</span><br></pre></td></tr></table></figure><p>如上，用<driver name="vfio">指定使用哪一种分配方式（默认是VFIO，如果使用较旧的传统的device assignment方式，这个值可配为’kvm’），用<source>标签来指示将宿主机中的哪个VF分配给宿主机使用，还可使用<mac address="52：54：00：6d：90：02">来指定在客户机中看到的该网卡设备的MAC地址。</mac></driver></p><p>由于新方式支持的直通网卡类型较少，为了支持更多的硬件直通设备，一般还是采用老方式通过<hostdev>标签来绑定。<strong>这种方式不仅支持有SR-IOV功能的高级网卡的VF的直接分配，也支持无SR-IOV功能的普通PCI或PCI-e网卡的直接分配。但是，这种方式并不支持对直接分配的网卡在客户机中的MAC地址的设置，在客户机中网卡的MAC地址与宿主机中看到的完全相同。</strong>同样，我们在网上找了个配置示例给大家参考：</hostdev></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">hostdev</span> <span class="attr">mode</span>=<span class="string">'subsystem'</span> <span class="attr">type</span>=<span class="string">'pci'</span> <span class="attr">managed</span>=<span class="string">'yes'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">address</span> <span class="attr">domain</span>=<span class="string">'0x0000'</span> <span class="attr">bus</span>=<span class="string">'0x08'</span> <span class="attr">slot</span>=<span class="string">'0x00'</span> <span class="attr">function</span>=<span class="string">'0x0'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">hostdev</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如上，表示将宿主机中的PCI 0000:08:00.0设备直接分配给客户机使用。</p><h3 id="存储的配置"><a href="#存储的配置" class="headerlink" title="存储的配置"></a><strong>存储的配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef492cb14195aa5943e3ea4.jpg" alt></p><p>如上图，每个存储设备都由一对<disk></disk>标签来描述，上述配置我们当前的虚拟机有两个存储设备，一个虚拟机硬盘，另一个是虚拟机光驱。</p><p>在虚拟机硬盘和光驱设备描述中，类型均是file，表示虚拟机硬盘使用文件方式。除此之外，还有<strong>block、dir或network取值</strong>，分别表示块设备、目录或网络文件系统作为虚拟机磁盘的来源。后面device属性表示让虚拟机如何来使用该磁盘设备，其取值为floppy、disk、cdrom或lun中的一个，分别表示软盘、硬盘、光盘和LUN（逻辑存储单元），默认值为disk（硬盘）。</p><p><strong>子标签用于定义Hypervisor如何为该磁盘提供驱动</strong>，它的name属性用于指定宿主机中使用的后端驱动名称，QEMU/KVM仅支持name=’qemu’，但是它支持的类型type可以是多种，包括raw、qcow2、qed、bochs等。除了这两个属性外，还有cache属性（我们没有设置），它表示在宿主机中打开该磁盘时使用的缓存方式，可以配置为default、none、writethrough、writeback、directsync和unsafe，其具体含义详见本站DPDK系列文章。</p><p><strong>子标签表示磁盘的位置</strong>，当<disk>标签的type属性为file时，应该配置为<source file="/root/ubt1204d.img">这样的模式，而当type属性为block时，应该配置为<source dev="/dev/sdbN">这样的模式。</disk></p><p><strong>子标签表示将磁盘暴露给虚拟机时的总线类型和设备名称。dev属性表示在客户机中该磁盘设备的逻辑设备名称</strong>，而<strong>bus属性表示该磁盘设备被模拟挂载的总线类型</strong>，bus属性的值可以为ide、scsi、virtio、xen、usb、sata等。如果省略了bus属性，libvirt则会根据dev属性中的名称来“推测”bus属性的值，比如，sda会被推测是scsi，而vda被推测是virtio。</p><p><strong>子标签表示该磁盘设备在虚拟机中的I/O总线地址</strong>，这个标签在前面网络配置中也是多次出现的，如果该标签不存在，libvirt会自动分配一个地址。</p><h3 id="域的配置"><a href="#域的配置" class="headerlink" title="域的配置"></a><strong>域的配置</strong></h3><p>我们前面介绍过，在libvirtd进程中，域、实例和Gust OS是一个概念，就是我们常说的虚拟机。所以，在KVM虚拟机的XML配置文件中，<domain>标签是范围最大、最基本的标签，是其他所有标签的根标签。如下：</domain></p><p><img src="https://pic.imgdb.cn/item/5ef492e114195aa5943e4a10.jpg" alt></p><p>在<domain>标签中可以配置两个属性：一个是type，用于表示Hypervisor的类型，可选的值为xen、kvm、qemu、lxc、kqemu、VMware中的一个；另一个是id，其值是一个数字，用于在该宿主机的libvirt中唯一标识一个运行着的客户机，如果不设置id属性，libvirt会按顺序分配一个最小的可用ID。这个系统自动的分配的ID可以通过如下指令查看：</domain></p><p><img src="https://pic.imgdb.cn/item/5ef492f314195aa5943e5504.jpg" alt></p><p>如上图，我们知道系统给我们当前ubt1204d虚拟机分配的最小可用ID是2。</p><h3 id="域的元数据配置"><a href="#域的元数据配置" class="headerlink" title="域的元数据配置"></a><strong>域的元数据配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4930214195aa5943e5e17.jpg" alt></p><p>在域的XML文件中，有一部分是用于配置域的元数据，即meta data。元数据的概念我们不陌生，在讲解存储虚拟化时介绍过，它主要用来描述数据的属性。在KVM虚拟机中域的元数据就表示域的属性，主要用于区别其他的域。其中，name用于表示该虚拟机的名称，uuid是唯一标识该虚拟机的UUID。在同一个宿主机上，各个虚拟机的名称和UUID都必须是唯一的。除此之外，还有其他的配置，在后续讲解操作实例时遇到了我们还会介绍，这里不再列举。</p><h3 id="QEMU模拟器配置"><a href="#QEMU模拟器配置" class="headerlink" title="QEMU模拟器配置"></a><strong>QEMU模拟器配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4931314195aa5943e668b.jpg" alt></p><p>在KVM虚拟机的XML配置文件中，需要制定使用的设备模型的模拟器。如上图，在emulator标签中配置模拟器的绝对路径为/usr/libexec/qemu-kvm。如果我们自己下载最新的QEMU源码编译了一个QEMU模拟器，要使用的话，需要将这里修改为/usr/local/bin/qemu-system-x86_64。不过，创建虚拟机时，<strong>可能会遇到error: internal error Process exited while reading console log output错误</strong>，这是因为自己编译的QEMU模拟器不支持配置文件中的pc-i440fx-rhel7.0.0机器类型，因此，需要同步修改如下选项：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">type</span> <span class="attr">arch</span>=<span class="string">'x86_64'</span> <span class="attr">machine</span>=<span class="string">'pc'</span>&gt;</span>hvm<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="图形显示方式配置"><a href="#图形显示方式配置" class="headerlink" title="图形显示方式配置"></a><strong>图形显示方式配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4932314195aa5943e6faa.jpg" alt></p><p>如上图，表示通过VNC的方式连接到客户机，其VNC端口为libvirt自动分配，也就是用VNC客户端模拟虚拟机的显示器。除此之外，也支持其他多种类型的图形显示方式，以下示例代码就表示就配置了SDL、VNC、RDP、SPICE等多种客户机显示方式。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'sdl'</span> <span class="attr">display</span>=<span class="string">':0.0'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'vnc'</span> <span class="attr">port</span>=<span class="string">'5904'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">listen</span> <span class="attr">type</span>=<span class="string">'address'</span> <span class="attr">address</span>=<span class="string">'1.2.3.4'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">graphics</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'rdp'</span> <span class="attr">autoport</span>=<span class="string">'yes'</span> <span class="attr">multiUser</span>=<span class="string">'yes'</span> /&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'desktop'</span> <span class="attr">fullscreen</span>=<span class="string">'yes'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'spice'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">listen</span> <span class="attr">type</span>=<span class="string">'network'</span> <span class="attr">network</span>=<span class="string">'rednet'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">graphics</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="虚拟机的声卡和显卡配置"><a href="#虚拟机的声卡和显卡配置" class="headerlink" title="虚拟机的声卡和显卡配置"></a><strong>虚拟机的声卡和显卡配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4933614195aa5943e787c.jpg" alt></p><p>如上图，<strong>标签表示的是显卡配置</strong>，其中<model>子标签表示为虚拟机模拟的显卡的类型，它的类型（type）属性可以为vga、cirrus、vmvga、xen、vbox、qxl中的一个，vram属性表示虚拟显卡的显存容量（单位为KB），heads属性表示显示屏幕的序号。在我们的虚拟机中，显卡的配置为cirrus类型、显存为16384（即16MB）、使用在第1号屏幕上。</model></p><p>由于我们的虚拟机中没有配置声卡，也就没有<strong>标签</strong>，即使有也非常简单，只需要了解他的model属性即可，也就是声卡的类型，常用的选项有es1370、sb16、ac97和ich6。</p><h3 id="串口和控制台配置"><a href="#串口和控制台配置" class="headerlink" title="串口和控制台配置"></a><strong>串口和控制台配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4934714195aa5943e8191.jpg" alt></p><p>如上图，设置了虚拟机的编号为0的串口（即/dev/ttyS0），使用宿主机中的伪终端（pty），由于这里没有指定使用宿主机中的哪个伪终端，因此libvirt会自己选择一个空闲的伪终端（可能为/dev/pts/下的任意一个），如下伪终端均为字符型设备：</p><p><img src="https://pic.imgdb.cn/item/5ef4935e14195aa5943e8c64.jpg" alt></p><p>除了系统默认指定外，也可以加上<source path="/dev/pts/1">配置来明确指定使用宿主机中的哪一个虚拟终端。在部署安装虚拟机virt-install命令增加–extra-args ‘console=ttyS0,115200n8 serial’来指定。</p><p>通常情况下，控制台（console）配置在客户机中的类型为’serial’，此时，如果没有配置串口（serial），则会将控制台的配置复制到串口配置中，如果已经配置了串口（我们前面安装的虚拟机ubt1204d就是如此），则libvirt会忽略控制台的配置项。同时，为了让控制台有输出信息并且能够与虚拟机交互，也需在虚拟机中配置将信息输出到串口。比如，在Linux虚拟机内核的启动行中添加“console=ttyS0”这样的配置。</p><h3 id="输入设备配置"><a href="#输入设备配置" class="headerlink" title="输入设备配置"></a><strong>输入设备配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4937114195aa5943e95f0.jpg" alt></p><p>如上图，这里的配置会让QEMU模拟PS2接口的鼠标和键盘，还提供了tablet这种类型的设备，即模拟USB总线类型的键盘和鼠标，并能让光标可以在客户机获取绝对位置定位。</p><h3 id="PCI控制器配置"><a href="#PCI控制器配置" class="headerlink" title="PCI控制器配置"></a><strong>PCI控制器配置</strong></h3><p><img src="https://pic.imgdb.cn/item/5ef4938014195aa5943e9d81.jpg" alt></p><p>如上图，libvirt会根据虚拟机的不同架构，默认会为虚拟机模拟一些必要的PCI控制器，这类PCI控制器不需要在XML文件中指定，而上图中的需要显式指定都是特殊的PCI控制器。这里显式指定了4个USB控制器、1个pci-root和1个idel控制器。libvirt默认还会为虚拟机分配一些必要的PCI设备，如PCI主桥（Host bridge）、ISA桥等。使用上面的XML配置文件启动虚拟机，在客户机中查看到的PCI信息如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4939014195aa5943ea56a.jpg" alt></p><h2 id="使用libvirt-API进行虚拟化管理"><a href="#使用libvirt-API进行虚拟化管理" class="headerlink" title="使用libvirt API进行虚拟化管理"></a><strong>使用libvirt API进行虚拟化管理</strong></h2><p>要使用libvirt API进行虚拟化管理，就必须先建立到Hypervisor的连接，有了连接才能管理节点、Hypervisor、域、网络等虚拟化要素。对于libvirt连接，可以简单理解为C/S架构模式，服务器端运行Hypervisor，客户端通过各种协议去连接服务器端的Hypervisor，然后进行相应的虚拟化管理。前面提到过，要实现这种连接，<strong>前提条件是libvirtd这个守护进行必须处于运行状态。但是，这里面有个例外，那就是VMware ESX/ESXi就不需要在服务器端运行libvirtd，依然可以通过libvirt客户端以另外的方式连接到VMware。</strong></p><p><strong>为了区分不同的连接，libvirt使用了在互联网应用中广泛使用的URI（Uniform Resource Identifier，统一资源标识符）来标识到某个Hypervisor的连接。</strong>libvirt中连接的标识符URI，其本地URI和远程URI是有一些区别的，具体如下：</p><p><strong>1）在libvirt的客户端使用本地的URI连接本系统范围内的Hypervisor，本地URI的一般格式如下：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver[+transport]:///[path][?extral-param]</span><br></pre></td></tr></table></figure><blockquote><p>其中，driver是连接Hypervisor的驱动名称（如qemu、xen、xbox、lxc等），transport是选择该连接所使用的传输方式，可以为空；path是连接到服务器端上的某个路径，？extral-param是可以额外添加的一些参数（如Unix domain sockect的路径）。</p></blockquote><p><strong>2）libvirt可以使用远程URI来建立到网络上的Hypervisor的连接。远程URI和本地URI是类似的，只是会增加用户名、主机名（或IP地址）和连接端口来连接到远程的节点。远程URI的一般格式如下：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver[+transport]://[user@][host][:port]/[path][?extral-param]</span><br></pre></td></tr></table></figure><blockquote><p>其中，transport表示传输方式，其取值可以是ssh、tcp、libssh2等；user表示连接远程主机使用的用户名，host表示远程主机的主机名或IP地址，port表示连接远程主机的端口。其余参数的意义与本地URI中介绍的完全一样。</p></blockquote><p>无论本地URI还是远程URI，在libvirt中KVM使用QEMU驱动，而QEMU驱动是一个多实例的驱动，它提供了一个root用户的实例system和一个普通用户的实例session，来设定客户端连接到服务器端后，操作权限的范围，类似Linux系统中的root用户与普通用户的区别。使用root用户实例连接的客户端，拥有最大权限，可以查询和控制整个节点范围虚拟机以及相关设备等系统资源；使用普通用户实例连接的客户端，只拥有服务器端对应用户的操作权限。那么，我们通过这一点也就知道<strong>libvirtd也是具备分权分域虚拟化管理能力的。</strong></p><p>有了上面的知识储备，我们就可以使用URI建立到Hypervisor的连接，比如，我们在252机器上，通过SSH方式连接掉251机器上，查看251机器上的KVM虚拟机信息，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef493a314195aa5943eaee3.jpg" alt></p><p>也可以在251机器上创建本地URI连接，进行管理，大家参照自行练习，我就懒得截图了，谁让我佛系嘛。。。</p><p>我们前面提到过，virsh等工具都是对libvirt的封装和调用，所以我们上面的指令看似简单，但是对应libvirt底层就不是那么回事了。在libvirt的底层是由<strong>virConnectOpen函数</strong>来建立到Hypervisor的连接的，这个函数需要一个URI作为参数，当传递给virConnectOpen的URI为空值（NULL）时，libvirt会依次根据如下3条规则去决定使用哪一个URI。</p><p><strong>1）试图使用LIBVIRT_DEFAULT_URI这个环境变量。</strong></p><p><strong>2）试用使用客户端的libvirt配置文件中的uri_default参数的值。</strong></p><p><strong>3）依次尝试用每个Hypervisor的驱动去建立连接，直到能正常建立连接后即停止尝试。</strong></p><p>如果这3条规则都不能够让客户端libvirt建立到Hypervisor的连接，就会报出建立连接失败的错误信息<strong>（“failed to connect to the hypervisor”）。</strong></p><p>除了针对QEMU、Xen、LXC等真实Hypervisor的驱动之外，<strong>libvirt自身还提供了一个名叫“test”的傀儡Hypervisor及其驱动程序。</strong>test Hypervisor是在libvirt中<strong>仅仅用于测试和命令学习的目的</strong>，因为在本地的和远程的Hypervisor都连接不上（或无权限连接）时，test这个Hypervisor却一直都会处于可用状态。使用virsh连接到test Hypervisor的示例操作如下：</p><p><img src="https://pic.imgdb.cn/item/5ef493b414195aa5943eb7e9.jpg" alt></p><p>如上图，输入help后，就会出现各种命令行指令，新手可以那这个test虚拟机来进行指令练习和学习，不会影响正常的虚拟机，类似Linux中namespace对所有资源拷贝一份然后与真实资源隔离。</p><p>下面，我们通过Python来调用libvirt API查询虚拟机信息，在使用Python调用libvirt API之前，需要确定系统是否安装了libvirt-Python，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef493c414195aa5943ec07c.jpg" alt></p><p>上图结果表示系统已经安装libvirt-Python。否则，需要手动安装或自行编译安装相应基础包。</p><p>完成上面的确认后，我们写一个Python小程序脚本GetinfoDm.py，通过调用libvirt的Python API来查询虚拟机的一些信息。代码如下：</p><p><img src="https://pic.imgdb.cn/item/5ef493d514195aa5943ec93b.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef493e614195aa5943ed285.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef493f714195aa5943edba1.jpg" alt></p><p>上面的脚本只是简单地调用libvirt Python API获取一些信息，需要注意的是“import libvirt”语句引入了libvirt.py这个API文件，然后才能够使用libvirt.openReadOnly、conn.lookupByName等libvirt中的方法。在本示例中，引入的libvirt.py这个API文件的绝对路径是/usr/lib64/python2.7/site-packages/libvirt.py，它实际调用的是/usr/lib64/python2.7/site-packages/libvirtmod.so这个共享库文件。运行上面的脚本后，结果如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4940914195aa5943ee4ee.jpg" alt></p><p>通过上面的示例，我们知道无论是virsh等命令还是Python等高级语言，在对KVM虚拟机进行操作时都是调用libvirt的API库函数来完成，这就说明libvirt API是libvirt管理虚拟机的核心。在libvirt中，外部应用接口API函数大致分为8类，是libvirt实现虚拟化管理的基石。在本文的最后，我们通过一张表对其中最常用的6类API进行总结，方便我们后续编写自动化脚本时查询。</p><table><thead><tr><th><strong>库函数类别</strong></th><th><strong>功能说明</strong></th><th><strong>常用函数</strong></th></tr></thead><tbody><tr><td>连接Hypervisor相关API，以virConnect开头的系列函数</td><td>只有在与Hypervisor建立连接之后，才能进行虚拟机管理操作，所以连接Hypervisor的API是其他所有API使用的前提条件。与Hypervisor建立的连接为其他API的执行提供了路径，是其他虚拟化管理功能的基础。</td><td>virConnetcOpen()：建立一个连接，返回一个virConnectPtr对象。virConnetcReadOnly()：建立一个只读连接，只能使用查询功能。virConnectOpenAuth()：根据认证建立安全连接。virConnectGetCapabilities()：返回对Hypervisor和驱动的功能描述XML字符串。virConnectListDomains()：返回一系列域标识符，只返回活动域信息。</td></tr><tr><td>域管理API，以virDomain开头的系列函数</td><td>要管理域，首先要获取virDomainPtr这个域对象，然后才能对域进行操作。</td><td>virDomainLookupByID(virConnectPtr conn,int id)：根据域的id值到conn连接上去查找相应的域。virDomainLookupByName(virConnectPtr conn,string name)：根据域的名称去conn连接上查找相应的域。virDomainLookupByUUID(virConnectPtr conn,string uuid)：根据域的UUID去conn连接上查找相应的域。virDomainGetHostname()：获取相应域的主机名。virDomainGetinfo()：获取相应域的信息。virDomainGetVcpus()：获取相应域vcpu信息。virDomainCreate()：创建域。virDomainSuspend()：挂起域。virDomainResume()：恢复域等等。。。</td></tr><tr><td>节点管理API，以virNode开头的系列函数</td><td>节点管理的多数函数都需要使用一个连接Hypervisor的对象作为其中的一个传入参数，以便可以查询或修改该连接上的节点信息。</td><td>virNodeGetInfo()：获取节点的物理硬件信息。virNodeGetCPUStats()：获取节点上各个CPU的使用统计信息virNodeGetMemoryStats()：获取节点上内存使用统计信息virNodeGetFreeMemory()：获取接上空闲内存信息virNodeSetMemoryParameters()：设置节点上内存调度参数virNodeSuspendForDurarion()：控制节点主机运行</td></tr><tr><td>网络管理API，以virNetwork开头的系列函数和部分virInterface开头系列函数</td><td>libvirt首先需要创建virNetworkPtr对象，然后才能查询或控制虚拟网络。</td><td>virNetworkGetName()：获取网络名称。virNetworkGetBridgeName()：获取网桥名称。virNetworkGetUUID()：获取网络UUID标识。virNetWorkGetXMLDesc()：获取网络以XML格式描述信息。virNetworkIsActive()：查询网络是否在用。virNetworkCreateXML()：根据XML格式创建一个网络。virNetworkDestroy()：注销一个网络。virNetworkUpdate()：更新一个网络。virInterfaceCreate()：创建一个网络端口。。。。等等</td></tr><tr><td>存储卷管理API，以virStorageVol开头的系列函数</td><td>libvirt对存储卷的管理，首先需要创建virStorageVolPtr这个存储卷对象，然后才能对其进行查询或控制操作。</td><td>virStorageVolLookupByKey()：根据全局唯一键值获取一个存储卷对象。virStorageVolLookupByName()：根据名称在一个存储资源池获取一个存储卷对象。virStorageVolLookupByPath()：根据节点上路径获取一个存储卷对象。virStorageVolGetInfo()：查询某个存储卷的使用信息。virStorageVolGetName()：获取存储卷的名称。。。。等等</td></tr><tr><td>存储池管理API，以virStoragePool开头的系列函数</td><td>libvirt对存储池（pool）的管理包括对本地的基本文件系统、普通网络共享文件系统、iSCSI共享文件系统、LVM分区等的管理。libvirt需要基于virStoragePoolPtr这个存储池对象才能进行查询和控制操作。</td><td>virStoragePoolLookupByName()：可以根据存储池的名称来获取一个存储池对象。virStoragePoolLookupByVolume()：可以根据一个存储卷返回其对应的存储池对象。virStoragePoolCreateXML()：可以根据XML描述来创建一个存储池（默认已激活）virStoragePoolDefineXML()：可以根据XML描述信息静态地定义一个存储池（尚未激活）virStoragePoolCreate()：可以激活一个存储池。virStoragePoolIsActive()：可以查询存储池状态是否处于使用中。。。。等等</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过前面两篇文章的介绍，相信大家对KVM虚拟机的原理、软件架构、运行机制和部署方式等有所了解。但是，那些东西只能算是非常基础入门的东西。尤其我们介绍的部署KVM虚拟机时用到的virt-install命令，不仅参数很多，很难记忆，而且要想用好virt-&lt;em&gt;系列工具首先需要对libvirt工具有更深刻的了解。因为，virt-&lt;/em&gt;系列工具其实是对libvirt工具的封装调用，而libvirt工具又是对底层qemu工具的封装调用，其目的都是为了使命令更加友好与用户交互。本篇文章就详细介绍这几种与libvirt相关的管理工具，为后续各种配置打下良好基础。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>KVM实践初步</title>
    <link href="https://kkutysllb.cn/2020/03/01/KVM%E5%AE%9E%E8%B7%B5%E5%88%9D%E6%AD%A5/"/>
    <id>https://kkutysllb.cn/2020/03/01/KVM实践初步/</id>
    <published>2020-03-01T05:17:00.000Z</published>
    <updated>2020-06-25T12:17:30.041Z</updated>
    
    <content type="html"><![CDATA[<p>在x86-64架构的处理器中，KVM需要的硬件辅助虚拟化分别为Intel的虚拟化技术（Intel VT）和AMD的AMD-V技术。在前面我们介绍过，CPU不仅要在硬件上支持VT技术，还要在BIOS中将其功能打开，KVM才能使用到。目前，大多数服务器和PC的BIOS都默认已经打开VT。至于如何在VMware Workstations虚拟机和物理服务器上打开，详见本站《KVM到底是个啥？》一文。<a id="more"></a></p><h2 id="部署和安装KVM"><a href="#部署和安装KVM" class="headerlink" title="部署和安装KVM"></a><strong>部署和安装KVM</strong></h2><p><strong>KVM的部署和安装主要有两种方式：一种源码编译安装，另一种就是通过CentOS的YUM工具安装。</strong>作为学习实验途径，我们这里主要介绍YUM工具安装方式。至于源码编译安装一般属于生产研发人员的操作，这里我们只给一些关键提示，有兴趣的同学可自行研究。</p><p><strong>1）源码编译安装方式。</strong>KVM作为Linux内核的一个module，是从Linux内核版本2.6.20开始的，所以要编译KVM，你的Linux操作系统内核必须在2.6.20版本以上，也就是CentOS 6.x和CentOS 7.x都天生支持，但是CentOS 5.x以下版本需要先升级内核。</p><p><strong>下载KVM源代码，主要通过以下三种方式：</strong></p><ol><li>下载KVM项目开发中的代码仓库kvm.git。</li><li>下载Linux内核的代码仓库linux.git。</li><li>使用git clone命令从github托管网站上下载KVM的源代码。</li></ol><p>根据上面三种途径下载源代码后就可以通过make install的方式编译安装了，编译安装完成后还需要根据KVM官方指导手册进行相关的配置。。。我们这里不展开，大家可自行搜索源码方式安装。<strong>需要注意一点儿的是，除了下载KVM的源码外，还需要同时下载QEMU的源码进行编译安装，因为它俩是一辈子的好基友嘛。。。</strong></p><p><strong>2）YUM工具安装。</strong>首先，需要查看 CPU是否支持VT技术，就可以判断是否支持KVM。然后，再进行相关工具包的安装。最后，加载模块，启动libvirtd守护进程即可。具体步骤如下：</p><p><strong>Step1：</strong>确认服务器是否支持硬件虚拟化技术，如果有返回结果，也就是结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的，否则就是不支持。如下图所示，表示服务器支持Intel的VT-x技术，且有4个CPU。</p><p><img src="https://pic.imgdb.cn/item/5ef4944a14195aa5943f0675.jpg" alt></p><p><strong>Step2：</strong>确认系统关闭SELinux。如下图所示，表示系统已经关闭Linux。</p><p><img src="https://pic.imgdb.cn/item/5ef4945d14195aa5943f10e7.jpg" alt></p><p>如果没有关闭，可以使用如下命令永久关闭：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># setenforce 0   # 临时关闭</span></span><br><span class="line"><span class="comment"># 永久关闭</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># sed -i 's/^SELINUXTYPE=.*/SELINUXTYPE=disabled/g' /etc/selinux/config</span></span><br></pre></td></tr></table></figure><p><strong>Step3：</strong>安装rpm软件包。</p><p>由于KVM是嵌入到Linux内核中的一个模块，我们这里首先安装用户安装用户空间QEMU与内核空间KMV交互的软件协议栈QEMU-KVM，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum -y install qemu-kvm</span></span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef4946e14195aa5943f19bf.jpg" alt></p><p>如上图，不仅完成qemu-kvm（红色框部分）安装，还同步安装qemu-img、qemu-kvm-comm两个依赖包的安装（黄色框部分）。注意：由于我已经安装过，上面提示是updated，没有安装的话，应该是installed。</p><p>完成QEMU-KVM软件协议栈的安装后，我们安装libvirt<em>、virt-</em>等管理工具，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y libvirt* virt-*</span></span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef4947d14195aa5943f2181.jpg" alt></p><p>如上图，将KVM的管理工具libvirt系列软件包和virt-系列软件包（红色框部分）及相关依赖软件包（黄色框部分）进行了安装。</p><p>最后，需要安装一个二层分布式虚拟交换机，用于KVM虚拟机的虚拟接入，可以选择OVS，也可以选择Linux bridge或是其他商用的分布式交换机如VMware、华为FC等。我们这里只出于学习的目的，选择安装Linux bridge即可。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y bridge-utils</span></span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef4948e14195aa5943f29fb.jpg" alt></p><p>如上图，提示我们系统已经安装该软件包且是最新版本（红色框部分），如果你没有安装过，系统会直接安装。</p><p><strong>3）加载KVM模块，使得Linux Kernel变成一个Hypervisor。</strong>首先，加载KVM内核模块，然后查看KVM内核模块的加载情况即可。具体步骤如下：</p><p>首先，通过modprobe指令加载kvm内核模块，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># modprobe kvm</span></span><br></pre></td></tr></table></figure><p>然后，通过lsmod指令查看kvm模块的加载情况，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># lsmod | grep kvm</span></span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef4949f14195aa5943f3315.jpg" alt></p><p><strong>4）启动libvirtd守护进程，并设置开机启动。</strong>因为libvirt管理工具，是用于KVM虚机全生命周期的统一管理工具，它由外部统一API接口、守护进程libvirtd和CLI工具virsh三部分组成，其中守护进程libvirtd用于所有虚机的全面管理。同时，其他管理工具virt-*、openstack等都是调用libvirt的外部统一API接口完成KVM的虚机的管理。所以，我们需要启动libvirtd守护进程，并设置开机启动。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl enable libvirtd &amp;&amp; systemctl start libvirtd &amp;&amp; systemctl status libvirtd</span></span><br></pre></td></tr></table></figure><p><img src="https://pic.imgdb.cn/item/5ef494ae14195aa5943f3a52.jpg" alt></p><p>这里注意一下，一般情况我们单机玩KVM虚拟机时，libvirtd守护进程默认监听的是UNIX domain socket套接字，并没有监听TCP socket套接字。为了同时让libvirtd监听TCP socket套接字，需要修改/etc/libvirt/libvirtd.conf文件中，将tls和tcp，以及tcp监听端口前面的注释取消。然后重新通过libvirtd的原生命令加载配置文件，使其生效。而系统默认的systemctl命令由于不支持–listen选项，所以不能使用。参考代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl stop libvirtd </span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># libvirtd -d --listen</span></span><br></pre></td></tr></table></figure><p>通过netstat命令查看libvirtd监听的tcp端口为16509，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef494c414195aa5943f4666.jpg" alt></p><p>最后，进行tcp链接验证，如果连接成功，表示服务启动正常且tcp监听正常。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef494d214195aa5943f4e3f.jpg" alt></p><h2 id="安装第一个KVM虚拟机"><a href="#安装第一个KVM虚拟机" class="headerlink" title="安装第一个KVM虚拟机"></a><strong>安装第一个KVM虚拟机</strong></h2><p>安装虚拟机之前，我们需要创建一个镜像文件或者磁盘分区等，来存储虚拟机中的系统和文件。首先，我们利用<strong>qemu-img工具</strong>创建一个镜像文件。<strong>这个工具不仅能创建虚拟磁盘，还能用于后续虚拟机镜像管理。</strong>比如，我们要创建一个raw格式的磁盘，具体代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># qemu-img create -f raw ubuntu1204.img 20G </span></span><br><span class="line">Formatting <span class="string">'ubuntu1204.img'</span>, fmt=raw size=21474836480</span><br></pre></td></tr></table></figure><p>如上，表示在当前目录下(/root）创建了一个20G大小的，raw格式的虚拟磁盘，名称为：ubuntu1204.img。虽然，我们看它的大小时20G，实际上并不占用任何存储空间，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef494e714195aa5943f5876.jpg" alt></p><p>这是因为qemu-img聪明地为你按实际需求分配文件的实际大小，它将随着image实际的使用而增大。如果想一开始就分配实际大小为20G的空间，不仅要使用raw格式磁盘，还需加上-o preallocation=full参数选项，这样创建速度会很慢，但是创建的磁盘实实在在占用20G空间。我们这里用创建一个5G磁盘来演示，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef494f714195aa5943f5fe3.jpg" alt></p><p>除raw格式以外，qemu-img还支持创建其他格式的image文件，比如qcow2，甚至是其他虚拟机用到的文件格式，比如VMware的vmdk、vdi、Hyper-v的vhd等，不同的文件格式会有不同的“-o”选项。为了演示我们的第一个虚拟机，我们现在创建一个qcow2格式的虚拟磁盘用作虚拟机的系统磁盘，大小规划40G，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4950714195aa5943f6864.jpg" alt></p><p>上面创建的虚拟磁盘实际就是KVM虚拟机后续的系统盘，在创建完虚拟机磁盘后，我们将要安装的系统镜像盘上传到当前目录下或者通过光驱挂载也可。我们这里为了安装速度问题，采用上传到宿主机本地的方式，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4951714195aa5943f70e8.jpg" alt></p><p><strong>注意：这里的宿主机指的是我们的VMware虚拟机，这里的虚拟机指的是我们在VMware虚拟机中创建的KVM虚拟机，千万别搞混了。</strong></p><p>然后，就是我们的关键一步，通过virt-install命令安装虚拟机，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># virt-install --name ubt1204d \</span></span><br><span class="line">--virt-type kvm \</span><br><span class="line">--ram 2048 --vcpus=2 \</span><br><span class="line">--disk path=/root/ubt1204d.img,format=qcow2,size=40 \</span><br><span class="line">--network network=default,model=virtio \</span><br><span class="line">--graphics vnc,listen=192.168.101.251 --noautoconsole \</span><br><span class="line">--cdrom /root/ubuntu-12.04.5-desktop-amd64.iso </span><br><span class="line"></span><br><span class="line">Starting install...</span><br><span class="line">Domain installation still <span class="keyword">in</span> progress. You can reconnect to </span><br><span class="line">the console to complete the installation process.</span><br></pre></td></tr></table></figure><p>如上，上面的指令表达的意思为：–name指定虚拟机的名称，–virt-type指定虚拟机的类型为kvm，–ram指定给虚拟机分配的虚拟内存大小为2GB，–vcpus指定给虚拟机分配的虚拟cpu为2个，–disk指定虚拟机的系统盘，就是我们刚才创建的虚拟磁盘，–network指定虚拟机使用的虚拟交换机，这里使用系统的默认配置，默认配置文件为/etc/libvirt/qemu/networks/default.xml，其详细信息如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4952714195aa5943f78ef.jpg" alt></p><p>上图中网络模式采用nat方式就表示虚拟机可以通过网桥访问internet或者外部网络。–graphics指定虚拟机的模拟显示器界面，这里采用vnc方式，并监听宿主机地址192.168.101.251。–cdrom选项指定虚拟机的安装镜像配置，也就是系统安装盘iso的位置。</p><p>上面虚拟机正式启动后，可以通过本地机器的vnc客户端连接到KVM虚拟机，作为虚拟机的模拟显示器。为此，首先需要查看VNC连接的端口，代码如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4953614195aa5943f813c.jpg" alt></p><p>如上图黄色框部分，通过本地机器VNC客户端连接目标机器192.168.101.251的0端口就能模拟虚拟机的显示器，进一步完成图形化安装配置，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4954514195aa5943f8e43.jpg" alt></p><p>连接成功后，与真实物理机装系统的操作一致，可以使用键盘、鼠标完成各类安装配置操作。虚拟机模拟显示器的界面如下：</p><p><img src="https://pic.imgdb.cn/item/5ef4955514195aa5943f9948.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef4956b14195aa5943fa477.jpg" alt></p><p><img src="https://pic.imgdb.cn/item/5ef4957b14195aa5943fac22.jpg" alt></p><p>由于我们安装的是ubuntu12.04的桌面版系统，所以完成虚拟机的安装后，后续每次使用虚拟机都需要使用VNC客户端进行连接操作。如下：</p><p><img src="https://pic.imgdb.cn/item/5ef495b114195aa5943fc647.jpg" alt></p><p>同时，我们可以通过virsh命令工具查看本机上所有虚拟机的状态，如下：</p><p><img src="https://pic.imgdb.cn/item/5ef495c914195aa5943fd184.jpg" alt></p><p>最后，提醒一下：我们在自动化运维中介绍过通过kicstart或cobbler批量安装宿主机操作系统，这种方式也是可以在KVM虚拟机安装系统中使用，通过在virt-install命令增加–extra-args选项就可实现。但是，一般我们不这么玩，在云计算和虚拟化场景下，均是通过手动安装一台模板虚拟机，将其系统盘转换为模板镜像格式文件，然后通过批量分发虚拟机的方式（就是我们在存储虚拟化中讲的链接克隆和快照方式）完成虚拟机批量部署操作，电信云中通过VNFD描述的多台虚拟机资源部署也是同样的方式完成，只不过里面借助了OpenStack的编排引擎。这种方式我们在后续介绍KVM虚拟镜像格式实操一文中会详细介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在x86-64架构的处理器中，KVM需要的硬件辅助虚拟化分别为Intel的虚拟化技术（Intel VT）和AMD的AMD-V技术。在前面我们介绍过，CPU不仅要在硬件上支持VT技术，还要在BIOS中将其功能打开，KVM才能使用到。目前，大多数服务器和PC的BIOS都默认已经打开VT。至于如何在VMware Workstations虚拟机和物理服务器上打开，详见本站《KVM到底是个啥？》一文。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
</feed>
