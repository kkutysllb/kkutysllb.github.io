<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一花一菩提，一云一世界</title>
  
  <subtitle>佛系ICT人士技术博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://kkutysllb.cn/"/>
  <updated>2020-03-03T09:49:25.913Z</updated>
  <id>https://kkutysllb.cn/</id>
  
  <author>
    <name>kkutysllb</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-03-03-认证管理服务Keystone</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E8%AE%A4%E8%AF%81%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Keystone/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-认证管理服务Keystone/</id>
    <published>2020-03-03T09:24:21.000Z</published>
    <updated>2020-03-03T09:49:25.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a><strong>概述</strong></h2><p>Keystone作为OpenStack中的一个独立的提供安全认证的模块，作为OpenStack的身份管理服务（Identity Service）主要负责OpenStack用户的身份认证、令牌管理、提供访问资源的服务目录，以及基于用户角色的访问控制。用户访问系统的用户名及密码是否正确、令牌的颁发、服务端点的注册，以及该用户是否具有访问特定资源的权限等，这些都离不开Keystone服务的参与。无论是公有云还是私有云，都会开放接口给众多的用户。为保证系统安全和用户的安全，就需要有完善的安全认证服务，无非包含几个方面：用户认证、服务认证和口令认证。Keystone提供的认证服务是双向的，既对用户进行合法性认证，对用户的权限进行了限制，又对提供给用户的服务进行认证，保证提供给用户的服务是安全可靠的。<a id="more"></a></p><p>在OpenStack的整体框架结构中，Keystone与Horizon类似，相当于一个服务总线与Nova、Glance、Horizon、Swift、Cinder及Neutron等其他核心服务全互联，其他核心服务通过Keystone来注册其服务的入口Endpoint，针对这些服务的任何调用都需要经过Keystone的身份认证，并获得服务的Endpoint来进行访问。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nvJOFgxfrMLJ.png?imageslim" alt="mark"></p><p>Keystone服务作为独立的身份认证模块，首次出现来OpenStack的Essex版本，提供身份认证、服务发现和分布式多用户授权，OpenID Connect，SAML和SQL。它不依赖于其他任何服务，相反其他服务的访问都需要依赖它来提供访问安全机制。</p><h2 id="Keystone的基本概念"><a href="#Keystone的基本概念" class="headerlink" title="Keystone的基本概念"></a><strong>Keystone的基本概念</strong></h2><p>作为 OpenStack 的基础支持服务，Keystone主要完成<strong>身份认证、令牌管理、服务管理、端点注册</strong>和<strong>访问控制</strong>5大功能，如下</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pSosOiKw4diQ.png?imageslim" alt="mark"></p><p>总结起来，Keystone服务主要完成以下三件工作：</p><ul><li>管理用户及其权限（Identity、Policy）；</li><li>维护 OpenStack Services 的 服务目录和入口（catalog、Endpoint）；</li><li>Authentication（认证）和 Authorization（鉴权）（Identity、Token）；</li></ul><p>以上三件工作彼此之间不是相互隔离独立的，反而是有紧密的联系。用户访问云服务时，首先提供用户凭证（Credentials），Keystone完成认证（Identity）后返回给用户一个临时身份凭证（Token），同时根据的用户的权限（Policy）返回用户可以访问的服务和权限（Endpoint、Catalog）。随后，用户就使用Keystone分配的临时身份凭证（Token）按照服务的入口（Endpoint）去访问相关的核心服务并进行相关的资源操作。各核心服务接收到用户的访问请求后，会使用用户的临时身份凭证（Token）去Keystone进行认证（Identity），同时根据权限列表（Policy）对用户的操作进行授权。通过后，才允许用户进行服务访问并进行相关资源操作。</p><p>要想理解Keystone服务的机制，首先要掌握Keystone的服务架构，并根据架构去归类理解Keystone服务的对象模型的各个概念，最后贯穿起来去拉通整个认证服务的流程。Keystone服务的架构如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/13r3TYX1E1dg.png?imageslim" alt="mark"></p><p>从Keystone服务的整体架构来看，整个服务架构分为入口访问模块，核心服务模块、后端存储模块和扩展插件模块，其中：</p><ul><li><strong>入口访问模块：</strong>主要由两个子组件构成，即Keystone API和Middleware。Keystone API接收外部的REST访问请求，而Middleware模块是一个安全认证的中间件，主要用来缓存Token，减轻核心服务模块的认证压力。当用户首次访问时，会通过Keystone API去Middleware模块验证Token的签名，由于此时用户并没有被分配Token，所以Middleware模块会向核心服务模块Keystone Service的Identity子模块进行认证，认证通过后Token子模块会给用户分配一个Token并缓存到Middleware模块中（对Token进行签名），后续用户使用该Token认证时，Middleware模块会代替Identity模块完成认证（Token有效期内），从而减轻Keystone Service核心模块的访问压力。</li><li><strong>核心服务模块：</strong>Keystone Service，由不同的子模块构成，不同子模块提供不同身份认证服务。比如：Identity子模块对用户凭证进行合法性认证，Token子模块用于给用户分配一个临时身份凭证，Assignment子模块用于分配用户的资源配额，Policy子模块用户指定用户访问资源的权限，Catalog子模块用户指定用户可以访问的服务目录等等。</li><li><strong>后端存储模块：</strong>Keystone Backends，主要用来存储实现不同核心服务的数据，不同核心服务数据可以使用不同后端存储来存储。支持的后端存储有：LDAP、SQL、Template、Rules、KVS等。</li><li><strong>扩展插件模块：</strong>Kesytone Plugins，针对不同的身份认证服务提供不同的扩展插件。比如基于密码的认证，通过Password插件完成，基于Token的认证基于Token插件完成等等。</li></ul><p>了解Keystone服务的整体架构后，我们再来看看Keystone服务的对象模型。如下图所示，所谓的对象就是Keystone服务管理的对象，而对象模型就是指Keystone服务管理的几类对象的集合及其子集。其顶层的对象模型就是Service和Policy，而Service对象模型的子集则包括：Identity、Resource、Assignment、Token和Catalog等多个子对象模型。因此，要想彻底掌握Keystone的作用，就需要重点理清这些对象模型的概念及其子集的对应关系，以及对象模型在整个Keystone服务中的作用。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7JJjqyQPt78D.png?imageslim" alt="mark"></p><ul><li><strong>Policy：</strong>安全访问策略，每个OpenStack服务都在相关的策略文件中定义其资源的访问策略（Policy） 。类似于Linux中的权限管理，不同角色的用户或用户组将会拥有不同的操作权限。在OpenStack中也可以针对每种服务（内部或外部）设定不同用户或用户组的访问策略，从而实现不同用户或用户组对访问服务的分权分域管理。各服务的访问策略文件是一个JSON格式的文件，并不是必须的，可以由管理员自行手动创建和配置，配置后无需重启服务即刻生效，一般放在服务配置目录下，即/etc/SERVICE_NAME/policy.json。</li><li><strong>Service：</strong>服务，在一个或多个端点（Endpoint）上公开的一组内部服务。包括：Identity、Resource、Assignment、Token、Catalog等。除内部服务外，Keystone还负责与OpenStack其他服务（ Service ）进行交互，例如计算，存储或镜像，提供一个或多个端点，用户可以通过这些端点访问资源并执行操作。因此，Keystone提供Service对象模型，可以简单理解为Service=内部服务+外部服务。</li><li><strong>Identity：</strong>提供身份凭证数据以及用户和用户组的数据。它有两个子集，分别是User和Group。User就是指单个OpenStack服务的使用者，必须属于某个特定域（Domain），用户名不是全局唯一，但是在其归属域（Domain）内是唯一的。Group是借鉴了Linux操作系统中Group的概念，在M版本之后才引入，作用就是把多个用户作为一个整体进行管理。Group也必须属于某个特定域（Domain），与User一样，在全局范围内Group不是唯一的，但是在其归属域（Domain）内必须唯一。通常情况下，用户和用户组数据由Identity服务管理，允许它处理与这些数据关联的所有CRUD操作。如果使用LDAP进行认证，则Keystone的Identity服务就是LDAP认证的前端，User和Group的管理由权威后端LDAP负责，Keystone的Identity此时只是作为认证的中继。</li><li><strong>Resource：</strong>资源，提供有关项目（Project）和域（Domain）的数据。它也有两个子集，分别是Project和Domain。Project就是以前OpenStack中tenant，也就租户，在N版本以后改为项目Project，是OpenStack中资源拥有者的基本单元，OpenStack中所有的资源都属于特定的项目。Project必须属于某个特定域，如果创建Project时没有指定Domain，则它默认属于Default域。与User和Group一样，Project也并非全局唯一，但是在其归属域（Domain）必须唯一。Domain这个概念也是Keystone v3版本开始引入，可以将其理解为资源的集合，也就是说某一资源必须属于一个特定域。为了将Domain内部的资源进一步细分使用权限和使用者，因此在Domain中可以定义所有归属Project的资源配额，在Project中又可以定义所有归属User和Group的资源配额，在Group中可以集中定义所有归属User的权限、角色和资源配额。Domain只可以通过命令行CLI或API接口创建，无法通过Web UI创建。</li><li><strong>Assignment：</strong>角色分配，提供有关角色（Role）和角色分配（Assignment Role）的数据。Role规定最终用户可以获得的授权级别，角色Role可以在域Domain或项目Project级别授予，也可以在单个用户User或组Group级别分配角色，角色名称在该角色的归属域中必须是唯一的。角色分配Role Assignment是一个三元组，包括：Role、Resource和Identity等服务，也就是说OpenStack通过Identity、Resource和Role三类对象模型决定某个用户User拥有哪些资源，对拥有的资源能够执行哪些操作。</li><li><strong>Catalog：</strong>服务目录，用于提供查询端点（Endpoint）的端点注册表，以便外部访问OpenStack的服务，因此它只有Endpoint一个子集。Endpoint本质上就是一个URL，拥有三种类型，分别是public、admin和internal，分别提供给不同的User，用于其访问OpenStack的服务。比如，提供给普通用户使用的就是public类型的Endpoint，提供给管理员使用的就是admin类型的Endpoint，而提供给OpenStack内部其他服务的就是internal类型的Endpoint。</li><li><strong>Token：</strong>令牌服务，提供用户访问服务的凭证，代表着用户的账户信息。Token一般包含User信息，Scope信息（Project、Domain或者Tenant），Role信息，本质上就是代替用户的一个凭证。用户首次访问OpenStack时，均需要将用户名和密码发给Keystone，Keystone将其转换为有时效性的Token发给用户，用户将其缓存在本地客户端，后续访问其他服务，Token将代替用户进行。因此，Token必须和用户绑定。客户端在调用POST/Tokens后拿到返回结果，如果用户名和密码验证成功，则拿到诸如User、Project、Token、metadata、catalog等数据。从catalog中找到要访问Service的Endpoint，让后在Headers中放入｛X-Auth-Token:token_id｝信息，就向该Service发起访问请求。Service的WSGI APP在收到该Request请求后，首先验证该token_id是否有效，该验证一般都使用一个keystonemiddlerware的中间件来完成，如下图所示：</li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/YkBLatIvmo2w.png?imageslim" alt="mark"></p><p>在OpenStack中Token支持的类型有：UUID、PKI、PKIZ和Fernet集中，OpenStack官方社区目前默认采用Fernet类型Token。几种Token类型对比如下：</p><table><thead><tr><th><strong>Token 类型</strong></th><th><strong>UUID</strong></th><th><strong>PKI</strong></th><th><strong>PKIZ</strong></th><th><strong>Fernet</strong></th></tr></thead><tbody><tr><td>大小</td><td>32 Byte</td><td>KB 级别</td><td>KB 级别</td><td>约 255 Byte</td></tr><tr><td>支持本地认证</td><td>不支持</td><td>支持</td><td>支持</td><td>不支持</td></tr><tr><td>Keystone 负载</td><td>大</td><td>小</td><td>小</td><td>大</td></tr><tr><td>存储于数据库</td><td>是</td><td>是</td><td>是</td><td>否</td></tr><tr><td>携带信息</td><td>无</td><td>user, catalog 等</td><td>user, catalog 等</td><td>user 等</td></tr><tr><td>涉及加密方式</td><td>无</td><td>非对称加密</td><td>非对称加密</td><td>对称加密(AES)</td></tr><tr><td>是否压缩</td><td>否</td><td>否</td><td>是</td><td>否</td></tr></tbody></table><p>UUID类型的令牌，token_id只是一个纯粹的32位十六进制字符串；PKI类型的令牌token_id使用完整性和加密算法对token_data进行加密；PKIZ类型的令牌实际上就是对PKI加密后的令牌再进行压缩，压缩率约50%左右；Fernet类型的令牌是一个JSON格式的字符串，约255字节大小，在我们的实验环境中，通过API方式得到的Fernet类型的Token令牌如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/7mEF4DbSHYGc.png?imageslim" alt="mark"></p><p>上图中，令牌信息部分包含了Token当前的时间，有效截止时间，审计id，验证的方法等内容。在账户部分包含了生成该Token的账户id信息、账户名、域信息等内容。Token令牌类型的选择涉及多个因素，包括Keystone server的负载、region数量、安全因素、维护成本以及令牌本身的成熟度。Region的数量影响PKI/PKIZ令牌的大小。从安全的角度上看，UUID无需维护密钥，PKI需要妥善保管Keystone server上的私钥，Fernet需要周期性的更换密钥。因此，从安全、维护成本和成熟度上看，UUID &gt; PKI/PKIZ &gt; Fernet 。采用何种类型Token，目前华为在自有FusionSphere OpenStack解决方案的建议如下：</p><ul><li><strong>Keystone server 负载低，region少于3个，采用UUID令牌。</strong></li><li><strong>Keystone server 负载高，region少于3个，采用PKI/PKIZ令牌。</strong></li><li><strong>Keystone server 负载低，region大于或等于3个，采用UUID令牌。</strong></li><li><strong>Keystone server 负载高，region大于或等于3个，目前OpenStack新版本默认采用Fernet令牌。</strong></li></ul><p>除了上述对象模型外，OpenStack还有Region、AZ、HA（Host Aggregate）的资源模型概念（这些其实属于计算资源池的概念）。Region是物理位置上划分的资源使用范围，可以简单理解一个数据中心DC就是一个Region，而AZ是Region内部从故障隔离角度划分的一个资源使用范围，其本质上是一组主机构成的资源池，资源池与资源池之间故障隔离（电源、网络、布线等），主要的划分依据还是动力故障隔离。HA（Host Aggregate）主机组是从物理资源的规格角度划分的资源使用范围，比如同是Dell 骁龙处理器的主机可以划分为一个HA，同是万兆网卡的主机可以划分一个HA等等。从资源使用者角度来看，其使用资源的范围从大到小的排序为：Region&gt;AZ&gt;HA&gt;Domain&gt;Project&gt;Group&gt;User，整个Keystone的对象模型的分配关系如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/DxQuPR6hUSHT.png?imageslim" alt="mark"></p><h2 id="Keystone的认证方式"><a href="#Keystone的认证方式" class="headerlink" title="Keystone的认证方式"></a><strong>Keystone的认证方式</strong></h2><p>Keystone的认证方式主要包括三种：<strong>基于令牌的认证方式、基于外部的认证方式</strong>和<strong>基于本地的认证方式</strong>。<strong>生产环境中，最常用的是基于令牌的认证方式</strong>，需要重点学习。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Bvgfm9TbYhPJ.png?imageslim" alt="mark"></p><ul><li><strong>基于令牌的认证方式：</strong>最常用的认证方式，认证请求发送时在HTTP/HTTPS头部添加一个X-Auth-Token的头域，Keystone检查该头域中token_id值，并与数据库SQL中的令牌值比对验证。</li><li><strong>基于外部的认证方式：</strong>采用集成第三方的认证系统，客户端在认证请求头中天剑remote_user信息，Keystone的Identity服务作为接收该认证请求的前端，将认证请求转发给后端的外部认证系统进行认证，并将认证结果返回给客户端。在这种认证方式中，Keystone作为认证的中继服务单元。</li><li><strong>基于本地的认证方式：</strong>这是默认的认证方式，即我们采用的用户名和密码认证。</li></ul><h3 id="基于令牌的认证方式—UUID令牌类型"><a href="#基于令牌的认证方式—UUID令牌类型" class="headerlink" title="基于令牌的认证方式—UUID令牌类型"></a><strong>基于令牌的认证方式—UUID令牌类型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/TQWMdYM4Gjzl.png?imageslim" alt="mark"></p><p><strong>该方案由于UUID令牌长度为固定32位十六进制字符串，不携带其他信息，OpenStack其他服务API收到该令牌后会向Keystone验证，并获得用户的其他信息，因此Keystone必须实现令牌的存储和认证，所有的验证均由Keystone完成</strong>，故采用这种认证方式的实际生产环境中部署Keystone服务的节点负荷较大。总体上，采用UUID类型的令牌认证共需要5步完成：</p><ul><li><strong>Step1：</strong>客户端发起认证请求，在请求的HTTP/HTTPS的Body体中携带用户名和密码信息；</li><li><strong>Step2：</strong>Keystone通过认证后，给用户返回一个token信息，用于该用户后续访问其他服务的身份凭证，该凭证并非永久有效，而是有一个有效期限，默认为1小时。</li><li><strong>Step3：</strong>客户端后续访问其他OpenStack服务时，比如访问nova-api，会在请求的HTTP/HTTPS的Header部分增加一个key-value格式的头域X-Auth-Token，其值就是Keystone返回给用户token_id。</li><li><strong>Step4：</strong>其他服务接收到用户的访问请求后，会将请求消息头部的token_id发送给Keystone进行验证。</li><li><strong>Step5：</strong>验证通过后，Keystone会返回该用户的三元组信息（User、Project、Role）给其他服务API，其他服务根据本地Policy的设定决定该用户使用其资源的权限。</li></ul><h3 id="基于令牌的认证方式—PKI令牌类型"><a href="#基于令牌的认证方式—PKI令牌类型" class="headerlink" title="基于令牌的认证方式—PKI令牌类型"></a><strong>基于令牌的认证方式—PKI令牌类型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kuvMCI3PHG8r.png?imageslim" alt="mark"></p><p><strong>该方案需要部署一台证书服务器（可以由部署Keystone服务的节点兼任），证书服务器将用户的私钥保存在Keystone本地，将用户的公钥分发给OpenStack的其他服务。这种认证方式客户的认证由OpenStack各个服务利用公钥完成，部署Keystone服务节点的负荷较小。</strong>总体上，采用PKI类型令牌认证需要4步完成：</p><ul><li><strong>Step1：</strong>客户端发起认证请求，在请求的HTTP/HTTPS的Body体中携带用户名和密码信息。</li><li><strong>Step2：</strong>Keystone利用本地的私钥对用户认证请求中的用户名和密码进行认证，通过后返回Token给客户端，在Token中携带签名后安全key值。</li><li><strong>Step3：</strong>客户端后续访问其他OpenStack服务时，比如访问nova-api，会在请求的HTTP/HTTPS的Header部分增加一个key-value格式的头域X-Auth-Token，其值就是Keystone返回给用户前面后的安全key值。</li><li><strong>Step4：</strong>其他OpenStack服务利用本地存储的公钥对token进行验证，通过后根据本地Policy的设定决定该用户使用其资源的权限。</li></ul><h3 id="基于令牌的认证方式—PKIZ令牌类型"><a href="#基于令牌的认证方式—PKIZ令牌类型" class="headerlink" title="基于令牌的认证方式—PKIZ令牌类型"></a><strong>基于令牌的认证方式—PKIZ令牌类型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Pb1acFCnbrU6.png?imageslim" alt="mark"></p><p><strong>该方案与PKI认证方案的流程类似，主要考虑到PKI认证方式的HTTP/HTTPS header头域过大（达到8K字节），为了减小管理平面带宽压力，故对PKI认证的头部采用压缩机制，而解压缩则在OpenStack的其他服务中进行。即与PKI认证方式相比，在Keystone返回的Token是一个压缩过的携带签名证书的Token-Key，而在OpenStack的其他服务首先需要对该token进行解压缩，然后再通过公钥进行认证。</strong>流程请参考PKI认证方式，这里不再赘述。</p><h3 id="基于令牌的认证方式—Fernet"><a href="#基于令牌的认证方式—Fernet" class="headerlink" title="基于令牌的认证方式—Fernet"></a><strong>基于令牌的认证方式—Fernet</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/WsQhs3fmf5CX.png?imageslim" alt="mark"></p><p><strong>该方案基于Fernet（一种对称加密算法）的对用户信息进行加密而生成的令牌，与用户相关的所有的认证信息都保存在令牌中，故令牌本身就包含了用户的三元组信息（User、Project、Role），也因此Fernet令牌无需持久化，但是一旦Fernet令牌被攻破，那么就可以做所有该用户能做到的事情，因此采用该令牌类型的认证方式其令牌一般都有一个有效期，默认为1小时，当有效期到期后，需要重新申请令牌。</strong>与UUID、PKI、PKIZ令牌类型认证方式相比，Fernet类型的令牌值无需存储在后端数据库中，只需要放在内存中即可。相比UUID方式，Keystone数据库的眼里不大，无需查表；相比PKI/PKIZ方式，HTTP/HTTPS的header头部较小，占用管理平面带宽较小。采用该令牌类型的认证方式，如果Keystone服务采用高可用集群部署，需要在所有部署Keystone服务的节点共享秘钥用于Token的解密和认证。总体上，该流程也需要5步：</p><ul><li><strong>Step1：</strong>客户端发起认证请求到Keystone服务器，请求HTTP/HTTPS的Body体中携带用户名和密码信息。</li><li><strong>Step2：</strong>Keystone验证通过后，利用对称加密算法AES对返回给客户端的Token进行加密，该加密后的Token中携带用户的三元组信息（User、Project、Role）。</li><li><strong>Step3：</strong>客户端在后续访问其他OpenStack服务的API时，会在HTTP/HTTPS的Header头部携带X-Auth-Token头域，其值就是加密后的token_id，也就是上面Token令牌示意图中的adjust_ids值。</li><li><strong>Step4：</strong>其他OpenStack服务的API将该token_id发送给Keystone进行验证。</li><li><strong>Step5：</strong>Keystone首先完成token_id的解密，验证通过后，将用户的三元组信息（User、Project、Role）发给其他OpenStack服务，其他服务根据本地Policy的设定决定该用户使用其资源的权限。</li></ul><h2 id="基于角色的访问控制—RBAC"><a href="#基于角色的访问控制—RBAC" class="headerlink" title="基于角色的访问控制—RBAC"></a><strong>基于角色的访问控制—RBAC</strong></h2><p>在上面分析过程中，Keystone服务只用于验证Token是否有效，而验证通过后用户资源的操作权限是通过各服务的Policy来决定。至于Policy通过什么设定来决定？首先需要了解下Policy文件的格式，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/L3qGbNTxqchN.png?imageslim" alt="mark"></p><p>上图是Neutron服务的Policy文件的一部分，可以看出该文件采用JSON格式，每一对｛｝内部通过key-value格式的键值对来定义服务的访问规则，每一条规则（Rule）就是一个键值对key-value。比如，上图中创建子网create_subnet这个操作，只有具备admin角色或该子网归属network的属主才能执行，那么可以推理出执行创建的子网的操作只有管理员admin，用户neutron或该子网归属network的租户project才具有权限，不符合上述条件的其他用户执行该命令时都会返回鉴权失败错误码403 Forbidden。而创建带VLAN ID的子网create_subnet:segment_id这个操作，只有具备admin角色的用户（管理员admin或用户neutron）才能执行，其他不符合条件的用户执行该操作都会返回鉴权失败错误码403 Forbidden。因此，通过我们的推理，就可以理解Policy模块在权限检查中所起的作用，以及整个基于角色的访问控制流程RBAC，而policy.json文件就是RBAC流程实现的关键，如下图：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/FGV9IgAfTKI7.png?imageslim" alt="mark"></p><p>上图中，用户发起操作请求Request到OpenStack服务的api进程，OpenStack服务的api进程首先去Keystone的Token模块进行Token有效性验证。验证通过后，返回的Token上下文中携带用户的三元组信息（User、Project、Role），并且Keystone的Policy模块调用OpenStack各服务本地Policy.json文件，根据用户操作请求Request的action结合用户的三元组信息判断用户该操作是否合法？不通过，则直接向客户端返回403 Forbidden；通过，则将验证结果返回给OpenStack的其他相关服务，OpenStack的其他相关服务按照用户的操作请求执行下一步操作Next。</p><p>从上面的分析中，我们可以知道Keystone的Policy模块根据各服务的policy.json文件来检查用户的操作权限，其检测时主要需要三方面的数据：</p><ul><li><strong>各服务的policy.json策略配置文件；</strong></li><li><strong>Auth_token添加到HTTP/HTTPS头部的token数据；</strong></li><li><strong>用户的请求资源数据；</strong></li></ul><p><strong>最后，放上一张Keystone的流程示意图总结Keystone如何实现认证和权限控制，如下：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/AX4TU0f9fJuG.png?imageslim" alt="mark"></p><h2 id="Keystone服务的实战操作（CLI命令行方式）"><a href="#Keystone服务的实战操作（CLI命令行方式）" class="headerlink" title="Keystone服务的实战操作（CLI命令行方式）"></a><strong>Keystone服务的实战操作（CLI命令行方式）</strong></h2><p>这里只展示Keystone服务的命令CLI操作方式，至于Web UI的操作太过于简单，这里不再赘述。需要注意一点，<strong>Keystone的资源模型Domain只有命令行方式可以操作。</strong></p><h3 id="域Domain的实战操作"><a href="#域Domain的实战操作" class="headerlink" title="域Domain的实战操作"></a>域Domain的实战操作</h3><p><strong>步骤1：</strong>执行以下命令，导入admin-openrc.sh的环境变量，进入管理员视角。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/cU8q6GyP2svT.png?imageslim" alt="mark"></p><p><strong>步骤 2：</strong>执行以下命令，查看OpenStack域相关命令的用法。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain -h</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/cmxe7xSDRaUg.png?imageslim" alt="mark"></p><p>各子命令具体用法可通过如下方式进行查看：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ziDE22a3WQfT.png?imageslim" alt="mark"></p><p><strong>步骤 3：</strong>执行以下命令，创建域“Demo_Domain”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain create --<span class="built_in">enable</span> --description <span class="string">"This is a temporary test domain"</span> Demo_Domain</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/mGV6W7MOOREH.png?imageslim" alt="mark"></p><p><strong>步骤 4：</strong>执行以下命令，查看OpenStack域列表信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/NjnELDXYU2nQ.png?imageslim" alt="mark"></p><p><strong>步骤 5：</strong>执行以下命令，查看刚刚创建的Demo_Domain域的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain show 3c660f1059b94bc3864074200cc12981 <span class="comment">#&lt;===这里我用了域的id信息进行查询，也可以使用域名，因为一个Region中域名唯一</span></span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/QJanlBOwyI9i.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>执行以下命令，设置刚刚创建的Demo_Domain为非激活态，再次查看刚刚创建的Demo_Domain域的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain <span class="built_in">set</span> --<span class="built_in">disable</span> Demo_Domain</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/z5fQJfKVSN2y.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，删除刚刚创建的Demo_Domain为非激活态，再次查看域列表信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack domain delete Demo_Domain</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6YUCRjcRuawq.png?imageslim" alt="mark"></p><h3 id="角色role、用户user及用户组group的相关操作实战"><a href="#角色role、用户user及用户组group的相关操作实战" class="headerlink" title="角色role、用户user及用户组group的相关操作实战"></a><strong>角色role、用户user及用户组group的相关操作实战</strong></h3><p><strong>步骤1：</strong>执行以下命令，查看OpenStack角色相关命令的用法（后续其他资源的操作指令使用帮助查询与此类似，不再赘述）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role -h</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/TczbUnrISbdh.png?imageslim" alt="mark"></p><p>上图列出了角色role的相关操作包括CRUD，角色的分配，角色的移除等等。。。</p><p><strong>步骤2：</strong>执行以下命令，查看OpenStack角色列表信息</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/RaL5t0HjPz1i.png?imageslim" alt="mark"></p><p>上图中展示了当前OpenStack中角色有member、user、heat_stack_owner、admin、heat_stack_user、creator和reader。</p><p><strong>步骤3：</strong>执行以下命令，创建角色”Kk_Role”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role create --domain kkutysllb Kk_Role &lt;===创建角色时，可以指定角色的归属域，也可以后续通过<span class="built_in">set</span>指令指定，建议在创建时指定</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Tmw3Hq6hFG4X.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，查看新创建角色”Kk_Role”的详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role show Kk_Role</span><br></pre></td></tr></table></figure><p>我们会发现刚刚创建的角色Kk_Role不存在，但是刚刚明明是创建成功的。这是为什么？原因是我们在刚刚创建Kk_Role时制定了角色归属域为kkutysllb，而当前我们直接使用openstack role show Kk_Role显示角色Kk_Role详细信息时，由于没有显式制定归属域参数，所以默认显示的是Default域的角色信息，而Kk_Role并不归属Default域，自然就没有相关信息展示。因此，执行如下命令显示角色Kk_Role的详细信息和域kkutysllb的所有角色列表。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role show --domain kkutysllb Kk_Role</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/pxKRM5cvagLK.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role list --domain kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/CPlJtGATsYQU.png?imageslim" alt="mark"></p><p><strong>步骤5：</strong>执行以下命令，查看域kkutysllb下用户user列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user list --domain kkutysllb &lt;===可以发现没有任何用户信息</span><br></pre></td></tr></table></figure><p><strong>步骤6：</strong>执行以下命令，创建一个域kkutysllb下的租户project</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project create --domain kkutysllb kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/83zcMzxnGm9p.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，创建一个域kkutysllb下的两个用户kk_user01和kk_user02，密码均为123456</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain kkutysllb --project kkutysllb --password-prompt kk_user01</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/fa6F9P9Xyz46.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain kkutysllb --project kkutysllb --password-prompt kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/jiMlsnjz6ywI.png?imageslim" alt="mark"></p><p><strong>步骤8：</strong>执行以下命令，给两个用户kk_user01和kk_user02添加角色Kk_Role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack role add --domain kkutysllb --project-domain kkutysllb --role-domain kkutysllb --user kk_user01 Kk_Role</span><br><span class="line"></span><br><span class="line">openstack role add --domain kkutysllb --project-domain kkutysllb --role-domain kkutysllb --user kk_user02 Kk_Role</span><br></pre></td></tr></table></figure><p><strong>步骤9：执行以下命令，查看两个用户kk_user01和kk_user02的角色分配情况</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack role assignment list --names | grep kk_user</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/MdmI6sh6dabs.png?imageslim" alt="mark"></p><p><strong>步骤10：</strong>执行以下命令，创建一个用户组Kk_Group，将上面创建的两个用户kk_user01和kk_user02加入该组</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack group create --domain kkutysllb Kk_Group</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/GuIwikpUb6Oz.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack group add user --group-domain kkutysllb --user-domain kkutysllb Kk_Group kk_user01 kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Y1VVT6nATWtO.png?imageslim" alt="mark"></p><p><strong>步骤11：</strong>执行以下命令，将kk_user02用户从用户组Kk_Group中移除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack group remove user Kk_Group kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ebNe7qsf6fbm.png?imageslim" alt="mark"></p><p><strong>步骤 12：</strong>执行以下命令，禁用用户“kk_user02”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user <span class="built_in">set</span> --domain kkutysllb --<span class="built_in">disable</span> kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/PmuEbCtoIkhz.png?imageslim" alt="mark"></p><p><strong>步骤13：</strong>执行以下命令，删除用户“kk_user02”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack user delete --domain kkutysllb kk_user02</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/dQka6P2tvGUP.png?imageslim" alt="mark"></p><h3 id="项目project相关操作实战"><a href="#项目project相关操作实战" class="headerlink" title="项目project相关操作实战"></a><strong>项目project相关操作实战</strong></h3><p><strong>步骤1：</strong>执行以下命令，查看域kkutysllb下的项目列表，可以查看到所有的项目</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project list --domain kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/y5XP0Qxg9YK5.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，将项目kkutysllb去激活必过查看该项目详情</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project <span class="built_in">set</span> --<span class="built_in">disable</span> --domain kkutysllb kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/uFnCa3Yeg12u.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，将项目kkutysllb删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack project delete --domain kkutysllb kkutysllb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/ALwtLXVy4LGb.png?imageslim" alt="mark"></p><p><strong>步骤4：</strong>执行以下命令，在Default域下的创建libing用户，并添加admin的角色</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/lUcNgLUipDas.png?imageslim" alt="mark"></p><p><strong>步骤6：</strong>执行以下命令，查看Default域下的admin项目的资源配额</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack quota show admin</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/DdJBxHpMRGlR.png?imageslim" alt="mark"></p><p><strong>步骤7：</strong>执行以下命令，修改项目“admin”的默认配额，如将实例数量修改为“5”，卷数量修改为“5”，网络修改为“10”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack quota <span class="built_in">set</span> --instance 5 --volumes 5 --network 10 admin</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/VlQiHGsEow7d.png?imageslim" alt="mark"></p><h3 id="服务和服务端点的实战"><a href="#服务和服务端点的实战" class="headerlink" title="服务和服务端点的实战"></a><strong>服务和服务端点的实战</strong></h3><p><strong>步骤 1：</strong>执行以下命令，查看OpenStack服务相关命令的用法和子命令的用法</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/CB9H0tABA8zl.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/p89WdumlBs0z.png?imageslim" alt="mark"></p><p><strong>步骤2：</strong>执行以下命令，创建swift服务，服务类型位object-store</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack service create --name swift --description <span class="string">"OpenStack Object Storage"</span> object-store</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yTIzjEwmxUNA.png?imageslim" alt="mark"></p><p><strong>步骤3：</strong>执行以下命令，创建服务“swift”的服务端点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne object-store publick http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/IYQw1t6TOvwy.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne object-store admin http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/bb0TBXG9zq6D.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne object-store internal http://rocky-controller:8080/v1/AUTH_%\(tenant_id\)s</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/nhmgVAUzu1UY.png?imageslim" alt="mark"></p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a><strong>思考</strong></h2><p><strong>1、为什么不能修改admin帐号域归属信息为kkutysllb后，再来展示域kkutysllb内部的角色信息？</strong></p><p><strong>2、在给用户添加角色时，能否同时指定用户的归属域Domain和项目Project？为什么？通过什么操作实现？</strong></p><p><strong>3、用户通过CLI创建一个VM，Keystone如何验证该用户？如何验证该用户具有创建VM的权限？</strong></p><p><strong>4、用户通过Horizon创建一个VM，Keystone如何验证该用户？如何验证该用户具有创建VM的权限？</strong></p><p><strong>5、上述两种创建VM的方式，Keystone的验证流程有何区别？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Keystone作为OpenStack中的一个独立的提供安全认证的模块，作为OpenStack的身份管理服务（Identity Service）主要负责OpenStack用户的身份认证、令牌管理、提供访问资源的服务目录，以及基于用户角色的访问控制。用户访问系统的用户名及密码是否正确、令牌的颁发、服务端点的注册，以及该用户是否具有访问特定资源的权限等，这些都离不开Keystone服务的参与。无论是公有云还是私有云，都会开放接口给众多的用户。为保证系统安全和用户的安全，就需要有完善的安全认证服务，无非包含几个方面：用户认证、服务认证和口令认证。Keystone提供的认证服务是双向的，既对用户进行合法性认证，对用户的权限进行了限制，又对提供给用户的服务进行认证，保证提供给用户的服务是安全可靠的。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-Web UI管理服务Horizon</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-Web-UI%E7%AE%A1%E7%90%86%E6%9C%8D%E5%8A%A1Horizon/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-Web-UI管理服务Horizon/</id>
    <published>2020-03-03T09:07:48.000Z</published>
    <updated>2020-03-03T09:23:50.219Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>OpenStack的用户界面由两部分组成：一是Web界面，二是Shell CLI界面。Horizon负责展现Web仪表盘，用户可以通过浏览器直接操作、管理、运维OpenStack的一些功能。由于OpenStack项目队伍不断壮大，Danshboard并不能展现所有的OpenStack功能，因此，最新的功能一般会先开发Shell命令行，也就是将CLI（Command Line Interface）提供给Linux用户操作。<a id="more"></a></p><p>在OpenStack的7个核心服务中，Horizon服务本质上就是一个Web Server，基于Django框架开发的，与Keystone服务一样，需要与其他提供业务的服务（计算nova、存储cinder、网络neutron等）进行全互联。如下图所示，如果将OpenStack的7个核心服务分为三层，从上到下依次是<strong>入口层、业务层</strong>和<strong>认证层</strong>。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/65YVSq76MuHT.png?imageslim" alt="mark"></p><p>Horizon服务最早出现OpenStack的Essex版本中，提供基于Web的控制界面，使得云管理员和租户/用户能够可视化的管理OpenStack的服务和资源。Horizon唯一依赖的服务就是Keystone认证服务，同时还可以与其他服务结合使用，例如镜像服务、计算服务、网络服务等。并且，Horizon还可以在独立服务（比如对象存储中）的环境中单独使用。</p><h2 id="REST-ful与WSGI介绍"><a href="#REST-ful与WSGI介绍" class="headerlink" title="REST ful与WSGI介绍"></a><strong>REST ful与WSGI介绍</strong></h2><p>OpenStack各个项目都提供了API接口服务，通过endpoint提供访问URL。无论我们通过Horizon访问OpenStack的服务和资源，还是通过命令行CLI访问OpenStack的服务和资源，其本质都是通过各服务/资源的API的endpoint进行访问，即均是基于HTTP/HTTPS协议的Web服务访问。而这些API接口都是基于一种叫做“REST ful”的架构。RESTful是目前流行的一种互联网软件架构，<strong>REST（Representational State Transfer）就是表述状态转移的意思。</strong></p><p>RESTful架构的一个核心概念是<strong>“资源”</strong>（resource）。从RESTful的角度看，网络里的任何东西都是资源，它可以是一段文本、一张图片、一首歌曲、一种服务等，每个资源都对应一个特定的URL（统一资源定位符）并用它进行标示，访问这个URL就可以获得这个资源。资源可以有多种具体表现形式，也就是资源的“表述”（representation），例如，一张图片可以使用JPEG格式，也可以使用PNG格式。URL只是代表了资源的实体，并不能代表它的表现形式。客户端和服务端之间进行互动传递的就只是资源的表述，我们上网的过程就是调用资源的URL，获取它不同表现形式的过程。这个互动只能使用无状态协议HTTP，也就是说，服务端必须保存所有的状态，客户端可以使用HTTP的几个基本操作，包括GET（获取）、POST（创建）、PUT（更新）、DELETE（删除），使服务端上的资源发生状态转化（State Transfer），也就是所谓的“表述性状态转移”。</p><p>OpenStack各个项目都提供了RESTful架构的API作为对外提供的接口，而RESTful架构的核心是资源与资源上的操作，也就是说，OpenStack定义了很多的资源，并实现了针对这些资源的各种操作函数。OpenStack的API服务进程接收到客户端的HTTP请求时，一个所谓的“路由”模块会将请求的URL转换成相应的资源，并路由到合适的操作函数上。这个所谓的路由模块Routes源自于对Rails（Ruby on Rails）路由系统的重新实现，采用MVC（Model-View-Controller）模式，收到浏览器发出的HTTP请求后，路由系统会将这个请求指派到对应的Controller。RESTful只是设计风格而不是标准，Web服务中通常使用基于HTTP的符合RESTful风格的API。而WSGI（Web Server Gateway Interface，Web服务器网关接口）则是Python语言中所定义的Web服务器和Web应用程序或框架之间的通用接口标准。</p><p>WSGI，顾名思义就是Web服务器网关接口。所有客户端访问Web服务，都需要经过该网关接口进行转发。而Web应用的本质就是客户端发送一个HTTP的请求，服务器收到请求生成一个HTML文档，服务器将该HTML文档作为HTTP响应的Body发送给客户端，客户端从HTTP响应的Body体中提取内容并显示出来。像这类生成HTML，接受HTTP请求、解析HTTP请求、发送HTTP响应等代码如果由我们自己来写，则需要耗费大量的时间和精力。正确的做法是底层代码由专门的服务器软件实现，需要一个统一的接口，让程序员专心编写业务层面的代码，这个接口就是WSGI。</p><p><strong>WSGI有两方：服务器方和应用程序方</strong>，如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/OpC56bzccRzi.png?imageslim" alt="mark"></p><p><strong>1）服务器方：</strong>其调用应用程序，给应用程序提供环境信息和回调函数，这个回调函数用来将应用程序设置的HTTP Header和Status等信息传递给服务器方。</p><p><strong>2）应用程序：</strong>用来生成返回的Header、Body和Status，以便返回给服务器方。</p><p>从名称上看，WSGI是一个网关，作用就是在协议之间进行转换。换句话说，WSGI就是一座桥梁，桥梁的一端称为服务端或者网关端，另一端称为应用端或者框架端。当处理一个WSGI请求时，服务端为应用端提供上下文信息和一个回调函数，应用端处理完请求后，使用服务端所提供的回调函数返回相对应请求的响应。</p><p>WSGI的服务方和应用程序之间还可以加入Middleware中间件，从服务端方面看，中间件就是一个WSGI应用；从应用端方面看，中间件则是一个WSGI服务器。这个中间件可以将客户端的HTTP请求，路由给不同的应用对象，然后将应用处理后的结果返回给客户端。如下图示，我们也可以将WSGI中间件理解为服务端和应用端交互的一层包装，经过不同中间件的包装，便具有不同的功能，比如URL路由分发，再比如权限认证。这些不同中间件的组合便形成了WSGI的框架，比如Paste。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/BtdCU1gmj1Jv.png?imageslim" alt="mark"></p><p>OpenStack使用Paste的Deploy组件（<a href="http://pythonpaste.org/deploy/）来完成WSGI服务器和应用的构建，每个项目源码的etc目录下都有一个Paste配置文件，比如Nova中的etc/nova/api-paste.ini，部署时，这些配置文件会被复制到系统/etc/" target="_blank" rel="noopener">http://pythonpaste.org/deploy/）来完成WSGI服务器和应用的构建，每个项目源码的etc目录下都有一个Paste配置文件，比如Nova中的etc/nova/api-paste.ini，部署时，这些配置文件会被复制到系统/etc/</a><project>/目录下。Paste Deploy的工作便是基于这些配置文件。除了Routes与Paste Deploy外，OpenStack中另一个与WSGI密切相关的是WebOb（<a href="http://webob.org/）。WebOb通过对WSGI的请求与响应进行封装，来简化WSGI应用的编写。" target="_blank" rel="noopener">http://webob.org/）。WebOb通过对WSGI的请求与响应进行封装，来简化WSGI应用的编写。</a></project></p><p>WebOb中两个最重要的对象，<strong>一是webob.Request</strong>，对WSGI请求的environ参数进行封装；<strong>一是webob.Response</strong>，包含了标准WSGI响应的所有要素。此外，还有<strong>一个webob.exc对象</strong>，针对HTTP错误代码进行封装。除了这3个对象，<strong>WebOb提供了一个修饰符（decorator），即webob.dec.wsgify</strong>，以便可以不使用原始的WSGI参数和返回格式，而全部使用WebOb替代。</p><p>除了Paste提供的WSGI框架外，为了解决Paste组合框架的Restful API代码过于臃肿，导致项目的可维护性变差的弊端。一些OpenStack的新项目选了Pecan框架来实现Restful API。Pecan是一个轻量级的WSGI网络框架，其设计并不想解决Web世界的所有问题，而是主要集中在对象路由和Restful支持上，并不提供对话（session）和数据库支持，用户可以自由选择其他模块与之组合。</p><h2 id="Horizon服务的实操体验"><a href="#Horizon服务的实操体验" class="headerlink" title="Horizon服务的实操体验"></a><strong>Horizon服务的实操体验</strong></h2><p><strong>Step1：</strong>使用浏览器登录OpenStack Dashboard：<a href="http://rocky-controller/dashboard/，输入**域名，用户名**和**密码**，单击“Sign" target="_blank" rel="noopener">http://rocky-controller/dashboard/，输入**域名，用户名**和**密码**，单击“Sign</a> In”，进入管理员视图Overview主界面。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/M7Y72gWhRjpi.png?imageslim" alt="mark"></p><p><strong>Step2</strong>：登录成功后，默认进入项目概览页面，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Ur5FUimEscnn.png?imageslim" alt="mark"></p><p>后面，将以发放一个简单的cirros虚拟机实例为例，来帮助大家快速熟悉OpenStack Dashboard的基本操作界面。</p><p>首先，要创建一个虚拟机VM实例，需要完成虚拟机规格的定义，在左侧导航栏，选择“Admin &gt; Compute &gt; Flavors”，进入规格列表，单击页面右上角的“Create Flavor”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/0yDa9n0Sl4Uy.png?imageslim" alt="mark"></p><p>弹出创建规格对话框，输入如下信息：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/rFSLxjMBYOzU.png?imageslim" alt="mark"></p><p>然后，需要创建虚拟机VM实例挂载的网络信息，其本质上就是创建虚拟机的虚拟网卡（后面Neutron部分会讲）。在左侧导航栏，选择“Admin &gt; Network &gt; Networks”，进入网络列表，单击页面右上角的“Create Network”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/yDMvo07AVSew.png?imageslim" alt="mark"></p><p>在Network标签页面，填入如下信息，然后点击Next。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/6FOfEuvFdIyk.png?imageslim" alt="mark"></p><p>在Subnet标签页，填写如下信息，然后点击Next。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/kXb3VUDuH6nN.png?imageslim" alt="mark"></p><p>在Subnet Details页面，填写如下信息，然后点击Create按钮。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Y5hLGzsY01JK.png?imageslim" alt="mark"></p><p>由于虚拟机VM实例所需要的镜像cirros在安装Glance服务时已经完成上传，因此完成上述虚拟机VM实例所需的最小资源配置后，下面就可以通过页面拉起一个虚拟机VM实例。</p><p><strong>Step1：</strong>在左侧导航栏，选择“Project &gt; Compute &gt; Instances”，进入虚拟机实例列表，单击页面右上角的“Launch Instance”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/qpUYI9Rf9VHy.png?imageslim" alt="mark"></p><p><strong>Step2</strong>：弹出发放实例对话框，在“Details”页签，输入虚拟机实例的名称“Web_Srv01”，其他保持默认，单击“Next”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/sOvk3ntEsIuS.png?imageslim" alt="mark"></p><p><strong>Step3</strong>：在“Source”页签，“Create New Volume”下方选择“No”，单击“Available” 下方列表中cirros镜像（环境安装完成后，系统默认创建的测试镜像）的上传箭头，“Allocated”下方列表中将显示选择的镜像，其他保持默认，单击“Next”。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/zMyiklwup9Be.png?imageslim" alt="mark"></p><p><strong>Step4：</strong>在“Flavor”页签，单击“Available”下方列表中刚刚创建的规格“Flavor_web”的上传箭头 ，“Allocated”下方列表中将显示选择的规格，单击“Launch Instance”，完成虚拟机实例的创建。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/FA4LMLf22bUh.png?imageslim" alt="mark"></p><p><strong>Step5：</strong>返回虚拟机实例列表，查看创建的虚拟机实例的状态，等待状态变为“Active”，表示虚拟机VM实例启动成功。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/uu2NNbXOLy5I.png?imageslim" alt="mark"></p><p><strong>Step6：</strong>单击虚拟机实例名称“Web_Srv01”，进入虚拟机实例概览页面，可查看虚拟机实例的详细信息，如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Y7T4WkIbKHUc.png?imageslim" alt="mark"></p><p><strong>Step7：</strong>单击“Console”页签，可进入虚拟机实例的终端页面，如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/5NE2aPIKMUyW.png?imageslim" alt="mark"></p><p>以上，就是通过Horizon服务利用Web页面可视化创建一个虚拟机VM实例的全过程。通过上图中Interface页签，log页签还可以查看虚拟机VM实例的网卡信息及操作日志。同时，Horizon页面不仅用来创建虚机，还可以创建Domain、Project、User、UserGroup、Network、Glance、Volume等资源，在后续各服务的实操环节会有展示。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a><strong>思考</strong></h2><p><strong>1、注销admin用户，通过普通用户登录后，观察Project的Overview视图与admin用户下的Overview视图的区别，并分析之间产生差异的原因？</strong></p><p><strong>2、尝试通过命令行CLI的方式完成上述虚拟机VM实例创建的操作。**</strong>（提示：命令行的用法可以通过openstack &lt;资源类型名&gt; &lt;动作&gt; –help方式查看帮助获得，比如：虚机资源类型名为server，网络资源类型名为network，子网资源类型名为subnet，镜像资源类型名为image，规格资源类型名为flavor等等）**</p><p><strong>3、如果在创建虚拟机VM实例的过程中，选择镜像时要求创建卷，那么所创建的VM实例的系统盘由哪个服务创建？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;OpenStack的用户界面由两部分组成：一是Web界面，二是Shell CLI界面。Horizon负责展现Web仪表盘，用户可以通过浏览器直接操作、管理、运维OpenStack的一些功能。由于OpenStack项目队伍不断壮大，Danshboard并不能展现所有的OpenStack功能，因此，最新的功能一般会先开发Shell命令行，也就是将CLI（Command Line Interface）提供给Linux用户操作。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-03-中间件分布式换粗Memcache和Redis</title>
    <link href="https://kkutysllb.cn/2020/03/03/2020-03-03-%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98Memcache%E5%92%8CRedis/"/>
    <id>https://kkutysllb.cn/2020/03/03/2020-03-03-中间件分布式缓存Memcache和Redis/</id>
    <published>2020-03-03T09:00:03.000Z</published>
    <updated>2020-03-03T09:06:13.490Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>缓存系统在OpenStack集群部署中有着非常重要的应用，在开源OpenStack的解决方案中一共使用了两种分布式缓存技术，一种是用于前端API服务访问的Memcache缓存，另一种就是用于后端计量和告警信息上报的Redis缓存。无论是Memcache缓存还是Redis缓存，均是一种分布式缓存。所谓分布式缓存，就是指缓存服务可以部署多个相互独立的服务器节点上，可以彼此分散独立存取数据，减少单节点的数据存取压力，但是各节点之间的数据并非要求一定保持一致性。大多数的OpenStack服务都会使用Memcache或Redis缓存系统存储如Token等临时数据，缓存技术的应用场景一般都是应用在有大并发访问需求的地方，比如使用Web服务的门户网站，淘宝、京东等购物网站等等。<strong>缓存系统可以认为是基于内存的数据库</strong>，相对于后端大型生产数据库MySQL等，基于内存的缓存系统能够提供快速的数据访问操作，从而提高客户端的数据请求访问，并降低后端数据库的访问压力，避免了访问重复数据时数据库查询所带来的频繁磁盘IO和大型关系表查询时的时间开销。<a id="more"></a></p><h2 id="Memcache分布式缓存"><a href="#Memcache分布式缓存" class="headerlink" title="Memcache分布式缓存"></a><strong>Memcache分布式缓存</strong></h2><p>Memcache利用系统内存对客户端经常进行反复读写和访问的数据进行缓存，来减轻后端数据库的访问负载和提高客户端的数据访问效率。Memcache中缓存的数据经过HASH之后被存放到位于内存上的HASH表内，而HASH表中的数据以Key-Value的形式存放。在Memcache集群中，Memcache客户端的API通过32Bit的循环冗余校验码（CRC-32）将存储数据的键值对进行HASH计算后，存储到不同的Memcached节点服务器上，而当Memcached服务器节点的物理内存剩余空间不足时，Memcached将使用最近<strong>最少使用算法（LRU，LeastRecentlyUsed）</strong>对最近不活跃的数据进行清理，从而腾出多余的内存空间供缓存服务使用。在OpenStack的集群部署中，大部分子项目都会用到内存缓存系统进行客户端访问数据的缓存，从而提高访问速率并减轻数据库的负载压力。</p><p>Memcache缓存系统在工作流程的设计上比较简单，其<strong>主要思想就是Memcache的客户端根据用户访问请求中的Key，到Memcached服务器的内存HASH表中获取对应此Key的Value，Memcache客户端取到值之后直接返回给用户，而不用再到数据库中进行数据查询。当然，内存HASH表中不可能预知和存储全部客户端需要的Key-Value值对，因此，如果在Memcache服务器中没有找到与请求中的Key对应的Value，则转向后端数据库进行查询，并将查询到的Value与Key进行HASH后存入Memcached服务器中，从而保证曾经存储过的数据一定能被查询到，并将数据库中查询到的数据缓存到Memcache服务器中，从而提高下一次缓存查询的命中率，</strong>其工作流程如下图所示。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/I9ekUo8D42DH.png?imageslim" alt="mark"></p><p>上述流程中，Memcache的整个工作流程被分为七步实现，但是并非每次查询都需要经历七个步骤，这需要根据是否在缓存中命中请求Key对应的Value来决定。具体流程如下：</p><p><strong>1~2：</strong>用户终端向Web服务器发起请求（Web服务器中部署有Memcache客户端），Web服务器中的Memcache客户端向Memcached服务器发起查询请求，查询目标为用户请求中包含的Key所对应的Value。</p><p><strong>1~2~3~7：</strong>如果Memcache客户端在Memcached服务器中查询到与请求Key对应的Value，则直接返回结果，本次数据查询过程结束，即整个过程不会访问数据库。</p><p><strong>1~2~4~5~7~6：</strong>如果经过步骤1和2没有查询到数据，则表明Memcached服务器没有缓存请求中的Key对应的Value，即本次缓存查询未命中，但是查询到此并不结束，而是转向数据库中继续查询该值，并将查询到的数据返回给客户端，同时将该数据缓存到Memcached服务器的HASH表中，以便客户端再次发起相同请求时，该值在Memcached服务器中能够直接命中，而无需再次查询数据库。</p><p><strong>需要注意的是：</strong>当数据库中存放的永久数据发生更新时，Memcached服务器中缓存的值也需要同时更新。如果Memcached服务器中分配给Memcached使用的内存空间耗尽，则Memcache缓存系统将使用LRU算法和数据的到期失效策略清理非活跃的冷数据，在这个过程中，已达到策略设置中的保存期限的数据将会首先被清除，然后再清除最近最少使用的数据。</p><p><strong>由于Memcache缓存系统中的每个Memcached服务器节点独立存取数据，彼此之间不存在数据的镜像同步机制</strong>，因此，如果某一个Memcached服务节点故障或者重启，则该Memcached节点上缓存在内存中的全部数据均会丢失，丢失缓存数据后，访问将无法在缓存中命中任何Key，所有Key的访问都需要到数据库重新查询一遍，同时将查询的数据再缓存到Memcached服务器的内存缓存中，即Memcached服务器每重启一次，其储存的数据就要被重新缓存一次。同时，Memcache缓存系统以Key-Value为单元进行数据存储，能够存储的数据Key尺寸大小为250字节，能够存储的Value尺寸大小为1MB，超过这个值不允许存储。并且Memcache缓存系统的内存存储单元是按照Chunk来分配的，这意味着不可能所有存储的Value数据大小都正好等于一个Chunk的大小，因此必然会造成内存数据碎片而浪费存储空间。</p><p>Memcache集群的配置极为简单，服务器端只需运行Memcached服务，客户端只需在应用程序的配置文件中指定要访问的Memcached服务器节点IP地址和端口组成的列表。因为Memcache集群无需在Memcached服务器端进行配置，数据的分布存储完全取决于Memcache客户端的节点选取算法。如下图所示，当应用程序向Memcache集群存储数据时，Memcache客户端会根据请求存储数据的Key-Value值对，利用一定算法从Memcached集群存储节点中选取某一个Memcached服务器节点来存储数据，当算法选定该节点后，对应Key的Value就会被存储到该节点中。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/Idtaxk8yWcu6.png?imageslim" alt="mark"></p><p>Memcache客户端在选取缓存节点时所采用的算法对数据的最终存储位置有着决定性的作用，而节点选取算法也是Memcache缓存系统中最重要的部分。在通常使用时，Memcache缓存系统中最常使用的两个算法分别是<strong>余数HASH算法</strong>和<strong>一致性HASH算法</strong>。</p><p><strong>余数HASH算法就是将需要存储的Key-Value值对的Key字符通过HASH算法后得到HASH Code，这里的HASH Code是一个数字，而HASH算法的作用就是将一个字符串通过HASH后得到一个数值。通常，相同的字符串经过相同的HASH算法后一定会得到相同的HASH Code，因此，存储和提取相同Key的Memcached目标节点一定是同一个节点。</strong>比如Key经过HASH之后得到的HASH Code为100，Memcache集群缓存系统中有3个Memcached节点，那么余数HASH就是将HASH Code 100除以节点数目3，然后取余得到的结果，这里结果为1，这样选取到的Memcached服务器节点为编号为1，即Memcached1节点。因为不同的Key值得到的HASH Code是不同的，这样取余后得到的结果是比较随机均匀的，数据就会随机存储到不同的Memcached服务器节点上，因此，如果在一个系统架构中，Memcached服务器节点数目比较固定，使用过程中无需扩容节点，则余数HASH算法可以很好地满足Memcache集群中的节点选取过程。然后，当需要扩容节点时，比如当增加3个Memcached节点后，现在Memcached服务器节点数目为6，HASH Code仍然为100，可余数HASH的结果却为2，而不是之前的1，即Memcache客户端在提取这个Key的值Value时，算法将会路由到Memcached2节点去，但是扩容之前该Key的Value是存储在Memcached1节点中的，因此客户端将不能在缓存系统命中此Key。</p><p>为了解决这种扩容引起的缓存命中失效问题，引入一致性HASH算法。其实，引入一致性HASH并不能完全解决缓存命中失效的问题，但是可以大大降低缓存命中失效发生的概率，而且随着缓存节点数量逐渐增多，这种缓存命中失效带来的IO瓶颈问题几乎可以忽略不计。一致性HASH通过将Memcached节点和Key字符串HASH后放到一个HASH环上，然后Key字符串的HASH值再以顺时针的方式找到距其最近的节点HASH值，该节点HASH值对应的节点便是Key的Value存储节点（一致性HASH在分布式存储部分会详细讲解，这里简单提一下）。当集群中增加缓存节点，只有HASH Code靠近增加节点HASH值的数据会受影响，其余节点仍保持原来的缓存映射关系，所以一致性HASH相比余数HASH在节点变化的场景下，缓存命中失效的概率要低很多。对于Memcache缓存系统而言，究竟选择余数HASH还是一致性HASH，需要根据Memcache缓存系统的实际使用情况而定。</p><p>如前所述，Memcache集群只能解决数据访问的瓶颈问题，并不能解决数据的高可用问题。如果要<strong>实现Memcache集群的数据HA，就必须借助第三方工具来实现。Memcache缓存系统最常使用的HA方式是Memcached节点代理，其中最常使用的代理软件为Magent</strong>。通过Magent缓存代理，可以防止缓存系统的单点故障，同时将缓存代理服务器配置为主备模式，即可实现代理服务器的高可用。在配置有Magent代理的Memcache缓存系统中，当客户端发起缓存数据请求时，客户端将首先连接到缓存代理服务器，然后缓存代理服务器再连接到Memcached服务器，Magent缓存代理服务器可以连接多台后端Memcached服务器，进而可以将相同的缓存数据同时存储到多个Memcached服务器，最终实现缓存数据的同步存储。如下图所示，实现Memcache服务的数据HA非常简单，只需在两台同样配置的服务器上分别安装memcache和magent软件，稍做简单配置即可实现Magent代理的高可用缓存系统。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/V9JQA8Gipqj2.png?imageslim" alt="mark"></p><h2 id="Redis分布式缓存"><a href="#Redis分布式缓存" class="headerlink" title="Redis分布式缓存"></a><strong>Redis分布式缓存</strong></h2><p>Redis，俗称远程字典服务器（Remote Dictionary Server，Redis），是一款由ANSI C语言编写、遵守BSD协议、支持网络访问、基于系统内存的开源软件，同时，Redis也是支持日志型和Key-Value键值对类型数据持久化的数据库。由于Redis中存储的Value值类型可以是字符串（String）、哈希（HASH）、列表（List）、集合（Sets）和有序集合（Sorted Sets）等类型，因此<strong>Redis通常也被称为数据结构服务器</strong>。在Redis中，这些数据类型都支持Push/Pop、Add/Remove以及针对集合的交集、并集和差集等丰富的数据操作，并且这些操作都具有保持数据一致性的原子性，而且Redis还支持各种不同的数据排序方式。</p><p>与Memcache缓存系统类似，为了保证高效的数据存取效率，Redis的数据都被缓存在内存中，但是Redis与Memcache在处理数据存储方式上有很大的差异，即Redis默认会周期性地把缓存中更新的数据写入磁盘进行永久保存，并将对缓存数据的修改操作追加到日志文件中，由于Redis本质上就是一个基于内存的数据库，因此Redis具备很多如日志记录等通用数据库的共性。<strong>Redis为了实现数据高可用，其利用已存入磁盘中的数据在不同节点之间进行同步操作，从而实现了Master-Slave模式的主从数据高可用，Redis的数据可以从Master服务器向任意数量的Slave服务器上进行同步。</strong></p><p><strong>在Redis缓存系统中，并非所有通过Redis客户端写入的数据都永久性地存储在物理内存中，这是Redis和Memcache缓存系统最为明显的区别，Redis会定期性地将物理内存中的数据写入磁盘，其原理类似内存虚拟化技术中内存置换技术。Redis实现了用虚拟内存（VM，Virtual Memory）机制来扩充逻辑可用的内存空间，当Redis认为物理内存空间使用量达到某个预设值时，Redis将自动进行内存页的交换（Page Swap）操作，从而将缓存中的数据写入磁盘中。</strong></p><p><strong>Redis的VM机制使得Redis可以存储超过系统本身物理内存大小的数据，但是，这并不意味着可以无限减小物理内存并不断增加虚拟内存以满足Redis的整体内存需求</strong>，主要有以下3点约束：</p><p>1）Redis将全部的Key保存在物理内存中，从而可以快速命中Key，并且由于Redis在读取数据时只在物理内存中进行Key的索引，找到Key之后再到物理内存或者虚拟内存中读取对应的Value，因此为Redis预留的物理内存空间至少要满足Key的存储。</p><p>2）过度地减小为Redis预留的物理内存空间，会使得Redis频繁发生Swap操作，极大降低Redis的数据稳定性，并对Redis的数据访问造成响应缓慢等影响。</p><p>3）在Redis将内存中的数据交换到磁盘的时候，提供访问服务的Redis主线程和进行交换操作的子线程会共享这部分内存空间，所以如果此时有请求要更新这部分内存空间的数据，则Redis将会阻塞这个请求操作，直到子线程完成Swap操作后才可以进行数据的更新修改。</p><p>上述的第三点约束是一种默认情况下的机制，即Redis会使用阻塞方式来使I/O请求等待Swap操作将全部所需的Value数据块读回内存，然后才进行下一步操作。这种机制在小量访问对数据访问的影响可以忽略，但是在大并发场景下，其影响直接造成客户体验变差。因此，在大并发场景下，需要配置Redis的IO线程池来解决，使得Redis并发进行磁盘到物理内存的数据读回操作，减少IO阻塞时间。</p><p>在Redis节点重新启动之后，丢失的缓存数据会从磁盘重新加载或者从日志文件中重新恢复，与Memcache相比，天生具备数据持久化功能。<strong>针对两种不同的缓存数据恢复方式，Redis提供了两种数据持久化的方式，分别是Redis数据库方式（RDB，Redis DataBase）和日志文件方式（AOF，Append Only File）</strong>。RDB持久化方式就是将Redis缓存数据进行周期性的快照，并将其写入磁盘等永久性存储介质中，而AOF采用的是与RDB完全不一样的数据持久化方式，在AOF方式下，Redis会将所有对缓存数据库进行数据更新的指令全部记录到日志文件中，待Redis重新启动时，Redis将会把日志文件中的数据更新指令从头到尾重复执行一遍，这是一个重现数据变更的过程，当执行到日志文件末尾时即可实现缓存数据的恢复，AOF恢复方式与DB2数据库的redo日志回滚恢复方式原理类似。在实际的Redis使用过程中，RDB和AOF这两种数据持久化方式可以同时使用，并且这也是官方推荐的数据持久化方式，在两种方式同时开启的情况下，如果Redis数据库重新启动，则会优先采用AOF方式来进行数据恢复，这是因为AOF方式对数据恢复的完整性要高于RDB方式（RDB因为是周期性存储快照数据，所以数据完整性至少丢失一个快照存取周期）。</p><p>在Redis3.0版本发行之前，Redis自身并不具备集群高可用功能，尽管Redis3.0中增加了Redis的Cluster功能，但是，使用Redis Cluster功能的用户相对还是很少，其可靠性和稳定性相对来说还需要进一步验证。目前，针对Redis的集群高可用功能，更多的是借助第三方负载均衡和集群管理软件（Pacemaker+HAProxy/Keepalived）来实现或者是通过对Redis进行二次开发来实现。在借助第三方负载均衡和集群管理软件的方案中，通过负载均衡软件的VRRP协议创建虚拟IP资源以对外提供高可用的Redis虚拟IP，并通过集群管理软件创建Redis的Master-Slave多状态资源，当Redis的Master节点故障后，通过集群管理软件的资源管理功能，利用Redis的Agent脚本将Slave节点提升为Master，同时虚拟IP迁移到新的Master节点继续对外提供服务。</p><p><strong>Redis主从模式的集群能够实现数据高可用的关键，是Redis提供的数据Replication功能</strong>，即数据能够在Redis的主从节点之间进行复制，从而保证了数据的高可用性。其复制是异步复制，在复制的过程中，Master节点是非阻塞的，不会受到Slave节点数据同步的影响，在Slave进行同步的时候，Master继续提供高效的内存数据查询服务。同时，在复制的过程中，Slave节点也是非阻塞的。在数据同步时，通过配置可以允许Slave节点利用之前的数据集提供查询功能活着此时返回一个错误，并提示数据正在同步中。但是不管怎么设置，在Slave同步完成之后，Slave节点的陈旧数据将会被删除，新同步来的数据将会被加载到内存，而在这个删除与加载的过程中，Slave的访问是暂时被禁止的。Redis的Replication功能除了数据冗余之外，还可以将数据的只读查询分散到多个Slave节点，以让Slave节点来负责这类数据的排序操作。</p><p>在Redis的Master-Slave集群模式下，如果配置了一个或多个Slave节点并将其连接到Master节点，则不管Slave节点是第一次连接Master还是重新连接到Master，当连接完成之后，Slave都会发送一个同步命令给Master，Master接到命令后开始在后台进行内存数据快照，并将在快照期间对当前缓存数据进行更新的命令全部缓存起来。当Master上的数据以RDB方式保存完成后，Master将该RDB数据文件传送给Slave，Slave将数据文件写入本地磁盘，然后再将这些来自Master的RDB数据文件加载到内存中，之后Master将数据快照期间缓存的命令发送给Slave，Slave依次执行这些数据变更命令，使得每次同步操作后都能确保自身数据与Master节点缓存数据的一致性。当Slave与Master的连接因某些原因断开后，Slave会自动与Master重新建立连接。如果Master同时收到多个Slave发送的数据同步请求，则Master仅会对缓存数据做一次快照保存，然后将保存的RDB数据文件发送给多个Slave节点。当同步中的Slave与Master断开又重新连接后，Slave将会重新发送同步全部数据的请求，但是从Redis2.8开始，重新连接的Slave可以通过断点续传功能仅同步上次未完成同步的数据请求，其机制是Master和所有的Slave将同步操作的日志记录到内存文件中，同时还记录了数据同步的偏移量和Master此时的运行标志（Run ID），如果数据同步过程被断开，Slave将会重新连接并请求Master继续同步。此时，如果Master的Run ID还没有改变，并且设置的同步偏移量在同步日志中可用，那么同步操作将会从中断点继续同步，如果不能满足这两个条件，则数据同步操作只能重新开始。此外，如果Master的Run ID没有保存到磁盘上，则无法进行续传同步。如果Master节点的磁盘I/O性能较差，而不同的Slave节点不在同一时间点发生同步请求，则数据同步过程必然增加Master的工作负载。为了解决这个问题，<strong>Redis2.8.18中提出了无盘复制（Diskless Replication）的设计</strong>，在这种模式下，Master直接将RDB文件传送到Slave节点，而无须再在本机上进行I/O操作，无盘复制技术的提出极大提升了Master在数据同步复制过程中的响应效率。</p><h2 id="Redis-Cluster集群技术"><a href="#Redis-Cluster集群技术" class="headerlink" title="Redis Cluster集群技术"></a><strong>Redis Cluster集群技术</strong></h2><p>上面提到Redis Cluster是Redis官方在3.0版本以后推出一种集群技术，作为官方的重点推荐，虽然其可靠性和稳定性还有待验证，但是不影响我们理解其工作原理和机制。Redis Cluster是Redis的分布式实现，RedisCluster在设计之初就考虑到了去中心化的问题，因此集群中不存在任何中心控制节点，每一个节点都是功能对等的集群成员，每个节点除了保存有自己的配置元数据外，还保存着整个集群的状态信息。由于每个集群节点之间彼此互联，并且随时保持活动连接，客户端只需连接到集群中的任一个节点，便可获取到存储在其他节点上的数据。机制示意图如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/m6sxxegXy5o3.png?imageslim" alt="mark"></p><p>Redis Cluster中的各个节点之间保持相互连接，并且彼此之间可以通信，客户端随意连接到任何一个集群节点就能将整个Redis集群作为一个整体来访问，同时客户端也无需知道Redis Cluster将其提交的数据存入哪个Redis节点中，数据存储完全由Redis Cluster根据自己的算法来决定。在Redis Cluster中，数据被分散存储到不同的Redis节点上，Redis Cluster并没有采用一致性HASH来分配数据存放到不同的节点，而是采用了一种称为<strong>哈希槽（HASH Slot）</strong>的技术来分配数据。Redis Cluster默认将存储空间分配为16384 Slots，每个节点承担其中的一部分Slots。如果Redis需要扩充Redis节点数，只需重新分配每个节点的槽位。当客户端通过SET命令来保存一个Key-Value键值的时候，Redis Cluster<strong>会采用CRC16（Key）对16384取模来决定该Key应该被存储到哪个Slots中</strong>，具体的计算公式就为：<strong>slot_num=CRC（’Key_name’）％16384</strong>。假设通过公式计算得到的Slot_num为14201，则数据会被存储在14201槽位对应的节点上。为了保证数据的高可用，Redis Cluster用到了Master-Slave的主从数据复制模式，即为每个Master都设置一个或多个Slave，Master负责数据存取，而Slave负责数据的同步复制，当Master故障时，其中的某个Slave会被提升为Master。假设集群中有A、B、C三个主节点，同时分别为其设置了A1、B1、C1三个Slave节点，此时如果B节点故障，则集群会将B1节点选取为Master节点继续提供服务，当B节点恢复之后，它就变成了B1的Slave节点。当然，如果B和B1同时故障，则集群B就不可用了。</p><h2 id="Redis在OpenStack中的应用"><a href="#Redis在OpenStack中的应用" class="headerlink" title="Redis在OpenStack中的应用"></a><strong>Redis在OpenStack中的应用</strong></h2><p>在OpenStack开源云的计费项目Ceilometer中，当进行规模化集群部署时，通常使用Redis插件来为运行在控制节点集群上的多个Ceilometer Agents提供协调机制，Redis插件使用具有Redis后端的Tooz库来为Agents提供一组轮询使用的资源集。在部署了Redis插件之后，Ceilometer Agents的部署可以分布到每个控制节点上，并且这些分布的Agents将会自动加入协调组（Coordination Group）中，在这种分布式集群部署中，通常使用Pacemaker创建Redis-server资源以监控Redis插件进程的运行状态，同时插件会自动配置Redis-sentinel进程来监控Redis集群状态，而Redis-sentinel的主要作用是当Redis集群的Master节点故障时，通过一定的机制重新选取新的Master节点，同时将Ceilometer的代理重新定向到新的Master节点，并进行Redis集群节点之间的数据同步。</p><p>在OpenStack集群高可用部署中，Redis被用作Ceilometer组件的分布式协调组后端缓存，同时集合集群资源管理软件Pacemaker，将Redis配置为Pacemaker的资源以实现Redis-server的高可用Master-Slave模式。此外，还需要为Redis集群配置一个高可用IP地址，高可用IP可以通过Pacemaker的虚拟IP资源实现。在三节点的OpenStack控制节点集群架构中，将其中一个控制节点作为Redis的Master节点，另外两个控制节点作为Redis的Slave节点，Redis和Ceilometer的Central agent在三个控制节点中的部署拓扑如下所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200303/eiMCG194h29S.png?imageslim" alt="mark"></p><p>在集成Pacemaker的集群中，Redis服务被配置成为Pacemaker的多状态资源，即Master/Salve资源组。通过Pacemaker配置的Redis高可用Master/Slave集群中，controller01为Redis的Master节点，controller02和controller03为Redis的Slave节点，同时Redis对外提供服务的虚拟IP地址运行在Master节点，这是通过Pacemaker的Colocation约束设置的，即<strong>Redis的VIP只会运行在Master节点上</strong>。如果Redis的Master节点故障，则Pacemaker将会从两个Slave节点中重新选举一个Mastre节点，而VIP也会相应地自动移动。</p><p>总的来说，memcache缓存技术主要用来存储见到key-value格式的数据，如OpenStack各个服务的API交互数据。如果需要缓存数据类型多样、数据可靠性高的场景，如OpenStack的ceilometer服务提供的计量和告警等数据，则采用Redis这种支持数据持久化且支持多种数据结构的缓存技术更加合适。</p><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、Memcache技术的服务端口号是多少？在OpenStack的最小系统配置下（keystone、nova、neutron、cinder、glance、horizon），控制节点最少需要开启多少个memcache服务进程？</strong></p><p><strong>2、nova服务至少需要开启多少个memcache服务进程？</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;缓存系统在OpenStack集群部署中有着非常重要的应用，在开源OpenStack的解决方案中一共使用了两种分布式缓存技术，一种是用于前端API服务访问的Memcache缓存，另一种就是用于后端计量和告警信息上报的Redis缓存。无论是Memcache缓存还是Redis缓存，均是一种分布式缓存。所谓分布式缓存，就是指缓存服务可以部署多个相互独立的服务器节点上，可以彼此分散独立存取数据，减少单节点的数据存取压力，但是各节点之间的数据并非要求一定保持一致性。大多数的OpenStack服务都会使用Memcache或Redis缓存系统存储如Token等临时数据，缓存技术的应用场景一般都是应用在有大并发访问需求的地方，比如使用Web服务的门户网站，淘宝、京东等购物网站等等。&lt;strong&gt;缓存系统可以认为是基于内存的数据库&lt;/strong&gt;，相对于后端大型生产数据库MySQL等，基于内存的缓存系统能够提供快速的数据访问操作，从而提高客户端的数据请求访问，并降低后端数据库的访问压力，避免了访问重复数据时数据库查询所带来的频繁磁盘IO和大型关系表查询时的时间开销。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-02-中间件消息队列服务RabbitMQ</title>
    <link href="https://kkutysllb.cn/2020/03/02/2020-03-02-%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%9C%8D%E5%8A%A1RabbitMQ/"/>
    <id>https://kkutysllb.cn/2020/03/02/2020-03-02-中间件消息队列服务RabbitMQ/</id>
    <published>2020-03-01T16:02:05.000Z</published>
    <updated>2020-03-01T16:17:30.646Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>OpenStack是由Nova、Neutron、Cinder等多个组件构成的开源云计算项目，各组件之间通过REST接口进相互通信和彼此调用，而<strong>组件之间的REST接口调用，是建立在基于高级消息队列协议（AdvancedMessageQueueProtocol，AMQP）上的RPC通信</strong>。在OpenStack中，AMQP Broker（可以是Qpid Broker，也可以是RabbitMQ Broker）位于OpenStack的任意两个内部组件之间，并使得OpenStack内部组件<strong>以松耦合的方式进行通信</strong>。纵观OpenStack的组件架构设计，其中的消息队列在OpenStack全局架构中扮演着至关重要的组件通信作用，也正是因为基于消息队列的分布式通信技术，才使得OpenStack的部署具有灵活、模块松耦合、架构扁平化和功能节点弹性扩展等特性，所以消息队列在OpenStack的架构设计和实现中扮演着极为核心的消息传递作用。同时，消息队列系统的消息收发性能和消息队列的高可用性也将直接影响OpenStack的集群性能。<a id="more"></a></p><p>在OpenStack的部署过程中，用户可以选择不同的消息队列系统来为OpenStack提供消息服务，但是在众多的消息队列系统中，RabbitMQ是使用最多也是综合性能最接近生产系统要求的消息队列系统。华为FusionSPhere OpenStack的解决方案中，也是采用的RabbitMQ作为消息队列系统。</p><h2 id="AMQP-高级消息队列协议"><a href="#AMQP-高级消息队列协议" class="headerlink" title="AMQP-高级消息队列协议"></a><strong>AMQP-高级消息队列协议</strong></h2><p>AMQP是应用层协议的一个开放标准，专为面向消息的中间件而设计，同时，AMQP也是面向消息、队列、路由、可靠性和安全性而设计的高级消息队列系统。AMQP提供统一消息服务的应用层标准协议，客户端与消息中间件基于此协议传递消息，并且消息传递不受不同客户端/中间件产品和不同开发语言等条件的限制。在消息的传递过程中，消息中间件主要用于组件之间的<strong>解耦</strong>，即<strong>消息生产者不用关系消息由谁来消费，消息消费者也不用关系消息由谁产生</strong>。</p><p>AMQP协议本质上是一种二进制协议，可以让客户端应用与消息中间件之间异步、安全、高效地交互。该协议框架也是分层结构，总体上分为三层，分别是：<strong>Model层、Seesion层</strong>和<strong>Transport层</strong>，消息在Session层与Transport层之间传递，用于执行Model层的Command，本质上就是二进制的封装和解封装过程。</p><ul><li><strong>Model层：</strong>决定了基本域模型所产生的行为，这种行为在AMQP中用“Command”表示。</li><li><strong>Session层：</strong>定义客户端与Broker之间的通信，通信双方都是一个Peer，可互称作Partner，会话层为模型层的Command提供可靠的传输保障。</li><li><strong>Transport层：</strong>专注于数据传送，并与Session保持交互，接受上层的数据并组装成二进制流，数据传送到Receiver后再进行解析并交付给Session层。Session层需要Transport层完成网络异常情况的汇报，顺序传送Command等工作。</li></ul><h2 id="RabbitMQ中的基本概念"><a href="#RabbitMQ中的基本概念" class="headerlink" title="RabbitMQ中的基本概念"></a><strong>RabbitMQ中的基本概念</strong></h2><p>RabbitMQ用Erlang语言开发，是AMQP的开源实现。在RabbitMQ的部署使用和故障排除过程中，将会接触到很多RabbitMQ的基础概念，了解和掌握这些概念，是使用RabbitMQ为集群系统提供消息服务的基础。</p><h3 id="Connection和Channel"><a href="#Connection和Channel" class="headerlink" title="Connection和Channel"></a><strong>Connection和Channel</strong></h3><p><strong>ConnectionFactory、Connection和Channel都是RabbitMQ对外提供的API中最基本的对象。</strong>Connection是RabbitMQ的Socket连接，本质上是一个TCP协议连接，消息的生产者Producer和消费者Consumer都通过Connection建立的TCP链接连接到RabbitMQ Server，因此RabbitMQ服务启动时的初始化过程就是创建一个Connection的连接，而ConnnectionFactory是Connection对象的工厂函数，用来初始化Connection对象。Channel是客户端与RabbitMQ交互消息最重要的一个接口，客户端大部分的业务操作是在Channel这个API接口中完成的，包括定义Queue和Exchange、绑定Queue与Exchange、发布消息等操作。Channel本质上是一个虚拟连接，它建立在TCP连接中，数据流动都是在Channel中进行，通常，程序启动后首先建立TCP连接，接着就是建立Channel对象。RabbitMQ使用Channel而不直接使用TCP，就是因为TCP的建立和拆除系统开销过大，容易引起消息队列系统瓶颈。</p><h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a><strong>Queue</strong></h3><p>Queue是RabbitMQ的内部对象，用于存储消息，Queue可以看成是存储消息的容器。消息的生产者与消费者之间可以是一对多或者多对多的关系，即多个消费者可以订阅同一个Queue，这时，<strong>Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理</strong>。如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/TuHomnveYICk.png?imageslim" alt="mark"></p><h3 id="Message-Acknowledgment"><a href="#Message-Acknowledgment" class="headerlink" title="Message Acknowledgment"></a><strong>Message Acknowledgment</strong></h3><p>RabbitMQ会要求消费者在处理完消息后发送一个回执应答给RabbitMQ Broker，RabbitMQ收到消息回执（Message-Acknowledgment）后才将该消息从Queue中移除。如果RabbitMQ没有收到回执并检测到消费者与RabbitMQ Broker的连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）继续处理。<strong>RabbitMQ的消息处理过程中并不存在Timeout的概念</strong>，即一个消费者处理消息所花的时间再长，只要其连接还存在，RabbitMQ就不会将该消息发送给其他消费者。这种情况会产生另外一个问题，如果开发人员在处理完业务逻辑后，忘记发送回执给RabbitMQ，这将会导致Queue中堆积的消息越来越多，而消费者重新连接到RabbitMQ后会重复消费这些消息并重复执行业务逻辑。因此，<strong>在处理完业务逻辑后一定要向RabbitMQ发送应答回执，否则会造成应用系统存在重大BUG</strong>。</p><h3 id="Message-Durability"><a href="#Message-Durability" class="headerlink" title="Message Durability"></a><strong>Message Durability</strong></h3><p>如果希望即使在RabbitMQ服务重启的情况下，消息也不会丢失，则可以<strong>将Queue与Message均做持久化设置，即MessageDurability</strong>，这样便可保证绝大部分情况下RabbitMQ的消息不会丢失。但是，消息持久化方法依然解决不了小概率丢失消息事件的发生，如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了，如果需要解决这种小概率事件下的消息丢失情况，那么就要用到事务RabbitMQ的高级功能或者将RabbitMQ部署为高可用集群。</p><h3 id="Prefetch-Count"><a href="#Prefetch-Count" class="headerlink" title="Prefetch Count"></a><strong>Prefetch Count</strong></h3><p>如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平分给多个消费者。这时如果每个消费者处理消息的时间不同，就有可能会导致某些消费者一直处于繁忙状态，而另外一些消费者因为处理能力较强而很快就处理完并一直处于空闲状态。要解决这一情况以提高整个消息系统的消息处理效率，可以设置预获取数目（PrefetchCount）来限制Queue每次发送给某个消费者的消息数，默认PrefetchCount=1，则Queue每次给每个消费者发送一条消息，消费者处理完这条消息后Queue会再给该消费者发送下一条消息。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/q4PhV0IJedYz.png?imageslim" alt="mark"></p><h3 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a><strong>Exchange</strong></h3><p>消息投递过程实际上是生产者将消息发送到Exchange，由Exchange将符合转发规则的消息路由到一个或多个Queue中，而将不符合规则的消息直接丢弃。从功能实现上来看，Exchange的功能就像一个路由器，符合路由规则的数据则转发到对应的目标地址，其他数据则被拒绝或者丢弃。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/1uuidxvSy3jq.png?imageslim" alt="mark"></p><h3 id="Routing-Key"><a href="#Routing-Key" class="headerlink" title="Routing Key"></a><strong>Routing Key</strong></h3><p>RoutingKey定义了消息的路由规则，但是，RoutingKey需要与ExchangeType和BindingKey共同使用才能最终决定消息投递到哪个队列中。在实际使用中，ExchangeType与BindingKey通常为预先设定值，而消息生产者在发送消息给Exchange时，需为消息设定相应的RoutingKey，便可决定消息应该投递到哪个Queue。通常，<strong>RabbitMQ为RoutingKey设定的长度限制为255字节。</strong></p><h3 id="Binding和BindingKey"><a href="#Binding和BindingKey" class="headerlink" title="Binding和BindingKey"></a><strong>Binding和BindingKey</strong></h3><p>Binding是RabbitMQ中将Exchange与Queue关联起来的操作，绑定的过程需要用到消息的RoutingKey和绑定自身的BindingKey。BindingKey代表了Queue与Exchange之间的对应关系。消息生产者将消息发送给Exchange时，通常会给消息指定一个RoutingKey。在Exchange中，如果BindingKey与RoutingKey相匹配，则带有该RoutingKey的消息将会被路由到BindingKey所对应的Queue中，即实现了消息到特定队列的投递过程。但是，消息到队列的投递另一个决定因素就是Exchange Type，投递过程会因Exchange Type的不同而有所不同。根据Exchange Type类型不同，在绑定多个Queue到同一个Exchange的时候，Binding操作允许使用相同的BindingKey，也可以不使用BindingKey。比如：在Fanout类型的Exchange中，绑定操作就不会用到BindingKey，而是将消息路由到所有绑定到该Exchange的Queue中。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/4z0qQ8QJccq4.png?imageslim" alt="mark"></p><h3 id="Exchange-Type"><a href="#Exchange-Type" class="headerlink" title="Exchange Type"></a><strong>Exchange Type</strong></h3><p>RabbitMQ使用不同的交换器类型来将不同的消息投递到特定的队列中，不同类型的交换器使用不同的方式进行消息投递，常用的ExchangeType有<strong>Fanout、Direct、Topic</strong>和<strong>Headers</strong>四种。</p><ul><li><strong>Fanout类型：</strong>会把所有发送到该Exchange的消息直接投递到所有与它绑定的Queue中，其功能就像路由交换中的广播，主要作用就是将多个Queue绑定到同一个Exchange中，从而实现消息生产者与队列之间一对多的对应关系。在Fanout类型的Exchange中，消息到队列的投递过程并不依赖消息的RoutingKey和Binding的BindingKey，Exchange仅起到消息传递的作用。</li><li><strong>Direct类型：</strong>Direct仅把RoutingKey与Binding的BindingKey匹配的消息路由到对应的Queue中，其功能就像路由交换中的点到点路由，主要作用就是实现消息的精确匹配投递。</li><li><p><strong>Topic类型：</strong>其与Direct类型的Exchage相似，也是将消息路由到BindingKey与RoutingKey相匹配的Queue中，但Topic采用的匹配规并非完全匹配，而是更加灵活的通配符模式匹配，Topic匹配规则具有如下限定：</p></li><li><ol><li>RoutingKey由点号“.”分隔的字符串组成（通常将句点号“.”分隔开的每一段独立字符串称为一个单词），如值为“stock.usd.nyse”的RoutingKey，其有三个单词组成，而值为“nyse.vmw”的RoutingKey，则由两个单词组成。</li><li>BindingKey与RoutingKey一样也是由句点号“.”分隔的字符串。</li><li>BindingKey中可以包含两种特殊字符“<em>”与“#”，用于做模糊匹配，其中“</em>”用于匹配一个单词，“#”用于匹配零个或多个单词，但是，BindingKey中的匹配最小单位为第一条约束中定义的单词，而不是常见的字母匹配。</li></ol></li></ul><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/Pqx1RBsCsYSp.png?imageslim" alt="mark"></p><p>如上图所示，RoutingKey为“quick.orange.rabbit”的消息会被Exchange同时路由到Q1与Q2队列，因为根据BindingKey的匹配规则，Q1与Q2的BindingKey均与此RoutingKey匹配。RoutingKey为“lazy.brown.fox”的消息只会被路由到Q2队列，因为BindingKey中的“#”字符匹配零个或多个单词，只有Q2中的BindingKey匹配此消息的RoutingKey。同样，根据匹配规则，RoutingKey为“lazy.orange.fox”的消息只会被路由到Q1队列。RoutingKey为“lazy.pink.rabbit”的消息尽管与Q2的两个BindingKey都匹配，但是此消息只会投递给Q2一次。当消息的RoutingKey为“quick.brown.fox”、“orange”或“quick.orange.male.rabbit”时，由于没有任何匹配的BindingKey规则，这些消息将会被Exchange丢弃。</p><p><strong>Topic是一种推送-订阅（Pub-Sub，Publish-Subscribe）模式的模糊路由匹配，由于Topic类型的Exchange具有灵活自动的匹配模式，其在OpenStack的应用场景中是使用最多的Exchange类型。</strong></p><ul><li><strong>Headers类型：</strong>Headers类型的Exchange不依赖于RoutingKey和BindingKey的匹配规则来路由消息，而是根据消息内容中的Headers属性来进行Queue选择。在Headers类型的Exchange中，在绑定Queue与Exchange时通常会设定一组Key-Value键值对，而当消息发送到Exchange时，RabbitMQ会提取此消息的Headers值（其值也是一个Key-Value键值对），并对比其中的键值对是否完全匹配Queue与Exchange绑定时设定的键值对，如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。该类型交换机并不常见。</li></ul><h3 id="Remote-Procedure-Call"><a href="#Remote-Procedure-Call" class="headerlink" title="Remote Procedure Call"></a><strong>Remote Procedure Call</strong></h3><p>在实际的应用场景中，用户很可能需要进行某些同步处理，因此需要同步等待客户端将用户发送的消息处理完成后再进行下一步处理，而这相当于远程过程调用（RPC，Remote Procedure Call），RabbitMQ也支持远程过程调用RPC。<strong>RabbitMQ中实现RPC的机制为：</strong>消息生产者在发送消息时，在消息的属性（AMQP协议中定义了14种消息属性，这些属性会随着消息一起发送）中设置两个值，分别为<strong>ReplyTo</strong>和<strong>CorrelationId</strong>。其中，ReplyTo的值是一个Queue名称，用于告诉消息的消费客户端消息处理完成后将应答消息发送到指定的这个Queue中，CorrelationId表示此次请求的标识号，客户端处理完成后需要将此属性一并返还，消息发送端将根据返回值中的这个id值来判断执行成功或失败的是已经发送出去的那条消息。消息接收客户端收到消息并处理，处理完消息后，将会生成一条应答消息到ReplyTo指定的Queue，同时带上CorrelationId属性，消息发送端之前已订阅了ReplyTo指定的Queue，因此可以从中接收客户端的应答消息，并根据其中的CorrelationId属性分析哪条请求已经被执行，然后根据执行结果进行后续的业务处理。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/qMugcYjPeKea.png?imageslim" alt="mark"></p><h2 id="RabbitMQ的工作原理和集群配置"><a href="#RabbitMQ的工作原理和集群配置" class="headerlink" title="RabbitMQ的工作原理和集群配置"></a><strong>RabbitMQ的工作原理和集群配置</strong></h2><p>RabbitMQ是AMQP的一个开源实现，其主要作用是在分布式集群中进行功能组件之间的异步消息传递，简单的描述就是消息生产者将带有RoutingKey的消息发送至交换器Exchange，Exchange使用BindingKey与Queue进行绑定，然后Exchange将RoutingKey与BindingKey进行匹配对比，并将匹配的消息投递至对应的Queue中。如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/0AXlMvj7dBGY.png?imageslim" alt="mark"></p><p>1）客户端连接至消息队列服务器Broker，在TCP连接中建立一个虚拟Channel。</p><p>2）客户端声明一个Exchange，并设置相关属性。</p><p>3）客户端声明一个Queue，并设置相关属性。</p><p>4）客户端在Exchange和Queue之间建立好绑定关系，并设置BindingKey。</p><p>5）客户端投递带有RoutingKey的消息到Exchange中。</p><p>6）Exchange接收到消息后，根据消息的RoutingKey和已经设置的BindingKey，进行消息路由，将消息投递到一个或多个队列里。</p><p>上图主要由三个环节构成，分别是<strong>发送消息的客户端、解耦消息发送端与接收端的RabbitMQServer</strong>以及<strong>消息的接收客户端</strong>。RabbitMQ Sever也称为Broker Server，其作用主要是负责消息从Producer到Consumer的传递路径，Broker接收从客户端发送过来的消息，然后转发给接收消息的客户端，Broke将发送与接收客户端进行了解耦，从而实现消息发送端与接收端的异步工作。客户端A和B是消息的发送方，即消息的生产者Producer，Producer发出的Messages由Playload和Label两部分数据组成。其中Playload是消息的主体部分，Label是消息的属性部分，属性部分包含了详细的RoutingKey，当消息到达Broker Server后，消息会被RabbitMQ根据消息属性（Label中的RoutingKey）转发到相应的队列中。客户端1、2和3是消息的接收者，即消息消费者Consumers。Broker Server根据消息的Label属性和Consumers对队列（队列已经绑定到Exchange中）的订阅（Subscribe）情况，将消息的Playload主体转发给相应的Consumers。Consumers接收到的消息是没有Label属性的，而仅有消息的主体Playload部分，此外，Consumers也不知道该消息是来自发送消息的客户端A还是B。</p><p>正常情况下，系统中成功安装RabbitMQServer程序后，用户只需启动RabbitMQ服务便可使其正常运行，即RabbitMQ使用自带的默认配置便可提供强大的异步消息传递服务。在某些情况下，用户可能希望自定义RabbitMQ服务，因此RabbitMQ提供了三种自定义配置Broker Server的方式，分别是<strong>环境变量配置方式、配置文件修改方式</strong>和<strong>运行时参数修改方式</strong>。具体的配置方式，可以根据各厂家产品解决方案的描述和RabbitMQ社区相关文档进行了解，这里不再赘述。但是，需要强调一点的是，无论采用哪种配置方式，以下几项参数的配置是必须有的：</p><ul><li><strong>RABBITMQ_NODE_IP_ADDRESS：</strong>该变量主要用于设定RabbitMQServer服务运行时监听的接口IP地址，默认为/etc/hosts配置文件中主机名对应的接口IP地址。</li><li><strong>RABBITMQ_NODE_PORT：</strong>该变量主要用于设置RabbitMQServer服务运行时监听的IP端口号，默认为系统的5672端口。</li><li><strong>RABBITMQ_NODENAME：</strong>该变量表示RabbitMQ集群的节点名称，默认为rabbit@hostname格式，其中“hostname”为当前节点的主机名，对于FQN格式的主机名，如node1.exmple.com，则RabbitMQ节点的名称为默认为rabbit@node1。</li><li><strong>RABBITMQ_USE_LONGNAME：</strong>该变量的定义与RABBITMQ_NODENAME类似，不过此变量代表的是RabbitMQ的长节点名，而RABBITMQ_NODENAME为短节点名称。在RabbitMQ集群的配置中，如果此变量设置为True，则RabbitMQ的节点名称将使用FQN全名，即<a href="mailto:rabbit@node1.exmple.com" target="_blank" rel="noopener">rabbit@node1.exmple.com</a>。</li></ul><p>RabbitMQ Broker是一个或几个运行RabbitMQ应用的Erlang节点的组合，这些节点之间共享Users、VirtualVosts、Queues、Bindings、Exchanges和运行时参数，通常把这些运行RabbitMQ服务的节点组合称为RabbitMQ的集群。在RabbitMQ集群中，RabbitMQ Broker运行所需的元数据和状态信息会自动在集群节点之间进行复制。但是，<strong>在普通的RabbitMQ集群配置中，消息队列Queues不会在多个节点之间复制</strong>，即集群Queues通常只位于集群中的某一个节点上，而其他节点虽然可以看到和访问这个节点上的消息队列，但是不会将该节点上的消息队列复制到其本地。对于普通的RabbitMQ集群模式，假设集群由A和B两个RabbitMQ节点构成，则A、B两个节点都有相同的集群元数据，但是只有A（或者B）节点持有集群消息队列，当消息进入A节点的Queues后，如果Consumer从B节点提取消息，则RabbitMQ会临时在A、B间进行消息传输，把A中的消息取出并经过B发送给Consumer。由于消息集中存放在A节点的队列中，无论Consumer从A或B提取消息，消息总要从A发出，这势必会导致A节点出现性能瓶颈。此外，这种普通集群模式当A节点故障后，B节点无法提取到A节点中还未消费的消息实体，最终因A节点的单点故障而导致整个集群消息系统不可用。解决这个问题一种办法就是将A节点的Queues持久化，尽管可以对A中的Queues做消息持久化，但在A故障后，也必须等待A节点恢复才可继续提供消息传递服务。但是，如果没有消息持久化，则即使A节点恢复，也无法恢复A中的队列消息。<strong>如果要将集群队列Queues进行镜像复制，则需要用到RabbitMQ的HeightAvailable功能。</strong></p><p>在RabbitMQ的集群中，节点通常被划分为两类，即<strong>磁盘（Disk/Disc）节点</strong>和<strong>内存（RAM）节点</strong>。而在多数情况下，集群中的节点均默认是Disk节点，内存节点主要用于具有深度队列和大量Exchanges的集群中以提高消息传递的性能。由于内存节点将消息数据保存到内存中，因此重启内存节点会导致消息丢失。所以，如果存在内存节点，则消息队列必须做持久化，在做了持久化的消息队列中，即使在内存节点上，消息也会被保存到磁盘上，因此重启内存节点也可保证不丢失消息队列。</p><p>如上所述，在默认的RabbitMQ集群中，消息队列Queue不会在集群节点之间进行复制备份，而仅位于集群中的某个节点上，通常该节点是最初声明Queues的节点。与Queues不同，Exchanges和Bindings信息会复制到集群中的每个节点上，因此在默认的RabbitMQ集群配置中，尽管集群的Exchanges和Bindings信息能够避免单点故障，但是由于Queues及其保存的Messages是中心化单点存放的，所以集群中的消息队列仍然具有单点故障而无法实现彻底的高可用，如果拥有Queues的节点发生故障，则虽然整个RabbitMQ集群可以继续提供消息服务，但是即使在消息做了持久化存储，之前位于Queues中还未被取走的消息将会丢失或者暂时不可用。</p><p>为了避免消息队列Queue的单点故障，通常的做法是将队列在节点之间进行镜像复制。在队列镜像模式下，每个镜像的队列由一个Master和一个或多个Slave提供。如果当前的Master故障，则最先成为Slave角色的节点会被选举为新的Master节点。同时，消息生产者投递到Queues中的消息会被复制到所有Slave节点上，并且不论消费者Consumers连接到哪个集群节点，其最终都是到Master节点的Queues中提取消息。又因为Master需要将消息复制到多个Slaves节点，所以<strong>队列镜像模式虽然增加了RabbitMQ集群的高可用性，但是并没有将集群的消息服务负载分散到每个集群节点中。</strong></p><p>实现RabbitMQ集群队列镜像的最主要方式就是RabbitMQ提供的<strong>Policy功能</strong>，Policy功能可以在集群运行的任何时候使用，即可以动态地将一个未镜像的集群消息队列改变为镜像队列。因此，创建镜像队列最简单有效的方式就是先创建一个非镜像的队列，然后通过Policy设置成为镜像队列。但是，刚开始设置的镜像队列与非镜像队列是有区别的：<strong>非镜像队列因为没有额外的镜像操作，所以其运行效率相对要高很多。因此，在RabbitMQ集群的镜像队列设置中，我们可以选择性地对某些队列进行镜像，而其他队列可以不用镜像。</strong>RabbitMQ设置Policy时，最关键的两个参数分别是<strong>ha-mode</strong>和<strong>ha-params</strong>，ha-params参数的值根据ha-mode的取值不同而不同，具体的组合如下：</p><ul><li>ha-mode=all，ha-params=NULL时，将队列镜像到全部集群节点上。</li><li>ha-mode=exactly，ha-params=count时，将队列镜像到count个节点上。如果实际节点数小于count，则镜像到全部节点；如果实际节点数据大于count，则节点孤战过后，重新镜像到count个节点上。</li><li>ha-mode=nodes，ha-params=node name时，将队列镜像到node name指定的节点上。</li></ul><p>示例如下：</p><ul><li>RabbitMQctl set_policy ha-all “^ha.“ ‘{“ha-mode”:”all”}’ #&lt;===将名称全部以“ha.”开头的队列镜像到所有集群节点中</li><li>RabbitMQctl set_policy ha-two “^ha.“ ‘{“ha-mode”:”exactly”,”ha-params”:2,”ha-sync-mode”:”automatic”}’ #&lt;===将名称全部以“ha.”开头的队列镜像到集群中任意两个节点</li><li>RabbitMQctl set_policy ha-nodes “^nodes.“‘{“ha-mode”:”nodes”,”ha-params”:[“rabbit@nodeA”, “rabbit@nodeB”]}’ #&lt;===将名称全部以“nodes.”开头的队列镜像到集群中特定节点上</li></ul><p><strong>队列镜像中的一个特殊情况是独占队列，独占队列在声明队列的连接终止后会被RabbitMQ自动清除，就如程序段中的一个临时变量。因此，针对独占队列没有必要持久化存储。而且，即使队列名称匹配镜像策略的模式，独占队列也不会被RabbitMQ的镜像策略进行镜像。同时，即使对独占队列进行持久化声明，其也不会被持久化。</strong></p><p>在RabbitMQ集群中，设置了镜像策略节点上的队列并非任何时候都是彼此同步的。如果一个节点新加入集群，并且集群Policy将其设为队列镜像的节点，则队列会将此节点当做一个新的Slave节点。但是，此时的新Slave节点上并没有任何的队列，或者说此时新Slave节点上的队列是空的。正常情况下，新加入的Slave节点只会接收晚于其加入镜像队列时间点新增的消息，并最终随着时间推移，新加入的Slave节点中的队列消息会逐步<strong>与原集群队列尾部的消息同步</strong>，并且随着原有队列头部消息被不断消耗，新Slave节点队列中不同步的消息数目会不断减少并最终达到完全同步。因此，基于RabbitMQ的这种队列同步模式，默认设置下，新加入的Slave节点对其加入前已经存在的集群消息并不能形成任何的冗余和高可用，除非对原有集群队列执行显式的同步操作。</p><p>从RabbitMQ3.6开始，RabbitMQ新增了ha-sync-batch-size参数，使得显式的队列同步在速度上有了极大提升，显示队列同步可以有两种实现方式，即<strong>手工同步</strong>和<strong>自动同步</strong>。如果队列被配置为自动同步，则不论新的Slave节点何时加入集群，集群中的队列都会被自动同步到新增的Slave节点。但是，同步过程中的消息队列相应地会变迟钝，这种响应变慢的过程会一直持续到队列同步完成为止。队列自动同步的设置很简单，只需在Policy的镜像设置中指定ha-sync-mode参数，使其值为automatic即可。<strong>ha-sync-mode</strong>参数允许的值是manual和automatic，如果不显式指定automatic，则其值默认为manual。同时，默认情况下，队列在镜像时对消息进行逐条同步，而在RabbitMQ3.6之后，新增了批量同步参数<strong>ha-sync-batch-size</strong>，用户通过设置该参数的值，即可实现批量同步消息，默认值为1。</p><p>RabbitMQ可以工作在<strong>Active/Passive</strong>或者<strong>Active/1active</strong>模式的集群中。当RabbitMQ集群运行在Active/Passive模式时，如果Active节点故障，则正常运行时由Active节点持久化写入磁盘中的消息队列可以被Passive节点恢复。当然，在Active/Passive模式下，如果Active节点的消息队列没有进行持久化操作，则Active节点故障后位于其上的消息就会丢失，尽管Passive节点可以重新提供消息服务，但是之前未被取走的消息却已无法恢复，因此Active/Passive模式下的消息队列必须进行持久化操作。Active/Passive模式的另一可能的问题是，当Active节点故障，并需要提升Passive节点为Active节点以恢复和接管消息时，RabbitMQ集群的消息服务可能会出现短暂中断。<strong>Active/1active高可用模式的本质是将RabbitMQ集群中的队列在集群节点上实现彼此镜像</strong>，在Active/1active模式下，集群中的某一RabbitMQ节点故障后，RabbitMQ服务会自动切换到其他节点，并使用该节点上的镜像队列继续提供消息服务，从而实现消息系统的服务高可用性。在部署OpenStack的高可用集群中，推荐部署的RabbitMQ高可用模式便是Active/1active高可用集群模式。</p><h2 id="RabbitMQ在OpenStack中的应用分析"><a href="#RabbitMQ在OpenStack中的应用分析" class="headerlink" title="RabbitMQ在OpenStack中的应用分析"></a><strong>RabbitMQ在OpenStack中的应用分析</strong></h2><p>在实际应用中，RabbitMQ Broker位于OpenStack的任意两个组件之间，OpenStack的任意两个组件通过RabbitMQ Broker进行松耦合通信。更准确地说，OpenStack的各个组件之间通过远程过程调用RPC来实现彼此的通信，并且组件间的RPC消息传递是基于RabbitMQ的推送-订阅（Publish/Subscribe）模式实现的。OpenStack在消息传递过程中使用到的交换器Exchange的类型主要是Direct、Fanout和Topic类型。OpenStack组件之间关系示意如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/yytpQMWeUkwI.png?imageslim" alt="mark"></p><p>在OpenStack中，默认使用kombu（一种实现了AMQP协议的Python函数库）连接到RabbitMQ服务器，因为消息的收发者都需要一个连接到RabbitMQ服务器的Connetion对象。以nova为例，在Nova的各个连接到RabbitMQ Broker的组件中，其中某个组件可能是消息的发送者Publisher，如Nova-api和Nova-scheduler，也可能是消息的接收者Consumer，如Nova-compute和Nova-network（现已被Neutron代替，但是不影响我们理解AMQP交互）。此外，某个组件在不同的时刻既可以是Publisher，也可以是Consumer。组件发送消息有两种方式，即<strong>同步调用（RPC.CALL）</strong>和<strong>异步调用（RPC.CAST）</strong>，当Nova发起RPC.CALL调用的时候，Nova-api充当的就是消息的Consumer，否则是消息的Publisher。</p><p>在启动Nova的服务时，初始化函数会创建两个队列，其中的一个队列用于接收<strong>RoutingKey格式为“NODE-TYPE.NODE-ID”</strong>的消息，如compute.node1，其中的compute表示该节点为计算节点；另一个队列用于接收R<strong>outingKey格式为”NODE-TYPE”格式的消息</strong>，如compute。比如当Nova的客户端发送“nova stop instance_name”命令到Nova-api时，Nova-api就会将此命令以消息形式投递到第一种队列中（具体消息的路由投递由Exchange操作），从而通过RoutingKey中的NODE-ID（节点主机名）找到目标计算节点，并将命令转发到此Hypervisor主机上以进行实例的停止操作。</p><p>在实际应用中，Nova的各个功能模块根据其逻辑功能可以划分为两类，即<strong>Invoker组件</strong>和<strong>Worker组件</strong>。其中Invoker组件的主要功能是向消息队列中发送系统请求消息，如Nova-api和Nova-Scheduler通常属于Invoker；而Worker模块则负责从消息队列中提取Invoker模块发送的消息并向其返回应答消息，如Nova-Compute和Nova-Network通常属于Worker。<strong>Invoker通过RPC.CALL和RPC.CAST两个进程发送系统请求消息，然后Worker从消息队列中接收消息，并对RPC.CALL做出应答响应，</strong>如下图所示：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/7ow5j9p9PiR4.png?imageslim" alt="mark"></p><ul><li><strong>Topic Publisher：</strong>该对象在Nova中的组件发起RPC.CALL调用时创建，消息发送出去之后便结束，生命周期短暂，主要用于将消息发送到队列系统中，每个Topic Publisher都会连接到Topic类型的Exchange中。</li><li><strong>Direct Consumer：</strong>该对象创建于RPC.CALL调用之后，专门用来接收RPC.CALL调用返回的应答消息，接收到应答消息后便结束。每个Direct Consumer连接到一个以msg_id为BindingKey的专有队列中，同时该队列绑定到一个只接收消息的RoutingKey为msg_id的交换器Exchange中，而这个消息的msg_id被封装在Topic Publisher发出的消息中，并且是由一个专门的UUID生成器所生成。</li><li><strong>Topic Consumer：</strong>该对象在Nova组件服务启动时创建，在服务结束时候销毁，主要用于接收消息队列中的消息。每个Worker都有两个Topic Consumer，一个连接BindingKey为Topic的队列，一个连接BindingKey为Topic.host的队列。每个Topic Consumer通过一个独占或者共享的队列与Topic Publisher连接到同一个Topic类型的Exchange。</li><li><strong>Direct Publisher：</strong>该对象创建于Nova中的RPC.CALL调用返回应答消息时，当Response消息发送完成后便结束，与BindingKey为特定msg_id的Direct类型Exchange连接。</li></ul><p>以Nova客户端发出创建实例请求为例，实例的创建过程伴随着Nova组件之间的消息传递，Topic Publisher（Nova-api）会将消息发送至NODE-TYPE（这里节点类型为compute）的共享队列中，从而全部Topic Consumer（全部运行Nova-compute的节点）都可以提取到共享队列的消息。如果客户端发出的是针对已有实例的Update、Reboot、Stop和Start等操作，由于消息必须转发到拥有该实例的某个特定计算节点上，因此Topic Publisher（Nova-api）会将消息转发至NOTE-TYPE.HOST（compute.hostname）的专有队列中。</p><p>在Openstack的Nova项目中，主要通过两种RPC调用来实现消息传递，即<strong>RPC.CALL和RPC.CAST</strong>。</p><p><strong>1）RPC.CALL属于请求/响应类型的调用</strong>，当请求发送出去以后，需要等待执行结果的响应，这类调用需要指定执行请求的目标对象节点，并等待目标返回的执行结果。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/EVkykmoPYWIo.png?imageslim" alt="mark"></p><p>Step1：初始化一个TopicPublisher，用以将发送消息到RabbitMQ的消息队列。此外，在发送消息之前，需要初始化一个DirectConsumer并以msg_id作为Direct类型Exchange的名称，用于等待消息执行后的应答响应。</p><p>Step2：请求消息被Exchange路由到NODE-TYPE.HOST消息队列，然后，订阅了此队列的相应服务节点（Nova中为compute节点）上的Topic-Consumer会从该队列中获取消息。</p><p>Step3：TopicConsumer从队列中提取消息后，服务结点根据消息内容（调用函数及参数）调用相应函数完成消息请求处理，处理完成之后，DirectPublisher被初始化，并根据请求消息msg_id，将应答消息发送到相应的Exchange和消息队列中。</p><p>Step4：应答消息被位于发起RPC.CALL调用方的DirectConsumer获取到，RPC.CALL调用过程完成。</p><p><strong>2）RPC.CAST属于单向RPC请求</strong>，即只将请求发送出去，无须等待返回执行结果。因此，RPC.CAST调用不关心请求由哪个服务节点完成，只需将请求发送到消息队列即可，而接收消息的队列通常为共享队列，该队列中的消息可以被某一类型的多个节点（如Nova中的全部计算节点）接收。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/LYkO22AfsQ44.png?imageslim" alt="mark"></p><p>Step1：Invoker初始化TopicPublisher，并将消息发送到RabbitMQ消息服务器中。</p><p>Step2：RabbitMQ的交换器将消息转发到NODE-TYPE类型的共享消息队列中，并被相应服务节点（Nova中为Compute节点）的TopicConsumer获取。</p><p>Step3：TopicConsumer提取到订阅队列的消息后，Woker便开始处理消息请求，至此，RPC.CAST的过程已经完成，Invoker不会再等待Woker返回请求消息执行后的结果。</p><h2 id="RabbitMQ集群管理实战"><a href="#RabbitMQ集群管理实战" class="headerlink" title="RabbitMQ集群管理实战"></a><strong>RabbitMQ集群管理实战</strong></h2><h3 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h3><p>现假设三个节点系统中都安装了RabbitMQ-server软件包，RabbitMQ服务已经正常启动，并且RabbitMQ的命令行工具RabbitMQctl已经可以正常使用。由于RabbitMQ节点之间使用Erlang Cookie来建立连接，因此要想让各个RabbitMQ节点之间彼此可以通信，则各个节点需要共享同一个Erlang Cookie。Erlang Cookie是在RabbitMQ-server启动过程中创建的一个随机字符串，在Linux系统中，Cookie保存在/var/lib/RabbitMQ/.erlang.cookie文件中。要让RabbitMQ的各个节点共享同一个Erlang Cookie，最简单的方式就是在某个节点（kvm-server01）中启动RabbitMQ-server，然后将kvm-server01中的/var/lib/RabbitMQ/.erlang.cookie拷贝到kvm-server02和kvm-server03中。另外一种方式就是在启动RabbitMQ-sever过程中或者在RabbitMQctl的命令行中通过参数“-setcookie cookie_str”的方式将特定的Erlang Cookie传入给当前节点的RabbitMQ服务。如果节点之间的Erlang Cookie未能正确匹配，则RabbitMQ的log中会有“Connection attempt from disallowed node”和“Could not auto-Cluster”的记录。我们这里采用第一种方式，如下：</p><p><strong>Step1：</strong> kvm-server01、kvm-server02、kvm-server03三个节点首先停止服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop rabbitmq-server.service</span><br></pre></td></tr></table></figure><p><strong>Step2：</strong>将kvm-server01的erlang.cookie拷贝到kvm-server02和kvm-server03节点上：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/jnWdoCzbWgmb.png?imageslim" alt="mark"></p><p><strong>Step3：</strong>重启三个节点的rabbitmq服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart rabbitmq-server.service</span><br></pre></td></tr></table></figure><p><strong>Step4：</strong>待各个节点的RabbitMQ-server服务启动完成后，便创建了三个独立的RabbitMQ Broker，即每个节点是一个独立的RabbitMQ Broker，通过RabbitMQctl命令行工具可以验证各个节点的Broker的运行状态：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/mVnY7x0BhLyH.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/3HwQgizFzOvp.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/a7xWk1oPueHW.png?imageslim" alt="mark"></p><p><strong><em># 为了实现一个三节点的RabbitMQ集群，通常的做法是首先创建一个两节点的集群，然后再通过新增节点的形式扩展至三节点集群</em></strong></p><p><strong>Step5</strong>：停止kvm-server02上的服务，完成如下操作，并观察节点的服务状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl join_cluster rabbit@kvm-server01</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/mpLKS1WWfFGx.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/wzW0sa1k7WLY.png?imageslim" alt="mark"></p><p><strong>Step6</strong>：停止kvm-server03上的服务，完成如下操作，并观察节点的服务状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl join_cluster rabbit@kvm-server02</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/M2Vi3cVazt3P.png?imageslim" alt="mark"></p><p>至此，三节点的RabbitMQ集群创建完毕，如上图通过RabbitMQctl命令行工具在任何一个节点上均可验证RabbitMQ集群的运行状态，并且在正常情况下，在任一节点上所看到的集群状态应该是一致的。从节点的集群状态输出中可以看到，每个RabbitMQBroker节点都加入到了集群中，并且集群由三个节点组成，每个节点上都正在运行RabbitMQ服务，并且节点默认都是磁盘类型的节点。</p><h3 id="集群节点的启停"><a href="#集群节点的启停" class="headerlink" title="集群节点的启停"></a>集群节点的启停</h3><p>在一个正在运行的RabbitMQ集群中，可以将任何一个或多个节点停止，并且集群中剩下的节点将会正常运行而不会受到某个节点停止的影响。而当停止的节点重新启动后，该节点又会自动加入集群，集群状态自动恢复正常。</p><p><strong>Step1：</strong>在kvm-server01节点上，执行如下操作，停止服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop</span><br></pre></td></tr></table></figure><p><strong>Step2：</strong>在kvm-server02和kvm-server03上观察集群的状态</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/3LjtqFXRA15d.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/msBLuPrWthUp.png?imageslim" alt="mark"></p><p><strong>Step3：</strong>重新启动kvm-server01上的服务，并再次观察kvm-server02和kvm-server03节点上集群运行状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmq-server -detached</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/kO64BRmCx1Jl.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/93uRXP3iJU9g.png?imageslim" alt="mark"></p><p>可以看到，当重新启动kvm-server01节点后，kvm-server01节点会自动加入集群运行，三节点集群状态自动恢复如初，即集群中三个集群成员都在运行，并可同时对外提供消息服务。在RabbitMQ全部集群节点均被停止并需重新重启时，有两点需要特别指出：</p><ul><li><strong>如果整个RabbitMQ集群中的节点都停止，则重启时应该根据节点停止顺序的逆序重新启动节点</strong>。如果首先启动的不是最后停止的节点，则启动过程中会给出30s的等待时间，以等待最后停止的节点变为运行状态，如果在30s内最后停止节点仍然未能激活，则节点启动过程就以失败告终。而<strong>如果最后停止的节点无法启动，则可以使用forget_Cluster_node命令将该节点从集群中移除</strong>，forget_Cluster_node命令的具体使用方式可以参考RabbitMQctl命令行工具的帮助页面。</li><li><strong>如果整个集群节点被同时停止或者发生了掉电等意外情况</strong>导致全部RabbitMQ节点同时关闭，则集群中每个节点都会认为自己不是集群中最后停止的节点，而会认为其他节点将在自己后面停止。如果出现这种情况，<strong>则可使用RabbitMQctl的force_boot命令来重新启动节点</strong>。</li></ul><h3 id="集群节点的移除"><a href="#集群节点的移除" class="headerlink" title="集群节点的移除"></a><strong>集群节点的移除</strong></h3><p>在正常运行的RabbitMQ集群中，当一个节点再也无须加入RabbitMQ集群并作为其成员节点运行的时候，就需要将此节点从集群中移除。要移除集群节点，首先需要停止该节点的RabbitMQ应用，然后重置节点，最后重启该节点的RabbitMQ应用。</p><p><strong>Step1：</strong>执行如下操作，将kvm-server03节点从集群中移除，移除后的节点将作为一个独立节点提供服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl reset</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><p><strong>Step2：</strong>观察三个节点的运行状态</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/3m8oOBIBqHYn.png?imageslim" alt="mark"></p><p>可见，在移除kvm-server03后，集群中只有kvm-server01和kvm-server02两个节点，同时kvm-server03节点中只有一个RabbitMQ Broker在运行。如果集群中某个节点已经失去了响应，且不能通过正常方式将其移除，则可以在本地节点通过forget_Cluster_node命令以远程移除的方式来将此节点从集群中移除。</p><p><strong>Step3：</strong>在kvm-server01节点上停止服务，并在kvm-server02上将kvm-server01节点强制移除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server01</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line"><span class="comment"># server02</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl forget_cluster_node rabbit@kvm-server01</span><br></pre></td></tr></table></figure><p><strong>Step4：</strong>kvm-server01在本地节点以远程方式从集群中移除后，其仍然会认为自己还属于集群节点，因此在启动本地kvm-server01节点的RabbitMQ应用时会报错，在重启之前将其重置即可解决</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/Ctn9zT4qKQOw.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server01</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl reset</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/jfG7pegVyang.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200302/l9FoAwNFS8Pg.png?imageslim" alt="mark"></p><p>如果此时将kvm-server03以同样办法从集群移除，则三个节点将恢复独立运行状态。但是kvm-server02仍然保存有集群的状态信息，即kvm-server02仍然还是集群成员，只不过集群有且只有kvm-server02一个成员。而kvm-server01和kvm-server03已经重新初始化为独立的RabbitMQ Broker，因此这两个节点不会保存任何集群信息，而如果要将kvm-server02也重新初始化为独立的RabbitMQ Broker，需要完成如下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server02</span></span><br><span class="line"></span><br><span class="line">rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">rabbitmqctl reset</span><br><span class="line"></span><br><span class="line">rabbitmqctl start_app</span><br></pre></td></tr></table></figure><h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a><strong>思考：</strong></h2><p><strong>1、在OpenStack中哪些服务之间交互使用了消息队列机制？哪些场景在哪些服务之间或子服务之间的RPC.CALL的调用关系？哪些场景在哪些服务之间或子服务之间是RPC.CAST的调用关系？（从虚拟机全生命周期管理的场景举例即可）</strong></p><p><strong>2、在请详细描述一下虚拟机实例销毁流程中，Nova各个组件之间的RPC调用流程？</strong></p><p><strong>3、在OpenStack中哪些服务或子服务之间没有使用消息队列的交互机制？（从keystone、glance、nova、neutron、cinder、ceilometer、heat等服务中举例即可）</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;OpenStack是由Nova、Neutron、Cinder等多个组件构成的开源云计算项目，各组件之间通过REST接口进相互通信和彼此调用，而&lt;strong&gt;组件之间的REST接口调用，是建立在基于高级消息队列协议（AdvancedMessageQueueProtocol，AMQP）上的RPC通信&lt;/strong&gt;。在OpenStack中，AMQP Broker（可以是Qpid Broker，也可以是RabbitMQ Broker）位于OpenStack的任意两个内部组件之间，并使得OpenStack内部组件&lt;strong&gt;以松耦合的方式进行通信&lt;/strong&gt;。纵观OpenStack的组件架构设计，其中的消息队列在OpenStack全局架构中扮演着至关重要的组件通信作用，也正是因为基于消息队列的分布式通信技术，才使得OpenStack的部署具有灵活、模块松耦合、架构扁平化和功能节点弹性扩展等特性，所以消息队列在OpenStack的架构设计和实现中扮演着极为核心的消息传递作用。同时，消息队列系统的消息收发性能和消息队列的高可用性也将直接影响OpenStack的集群性能。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-OpenStack概述</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-OpenStack%E6%A6%82%E8%BF%B0/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-OpenStack概述/</id>
    <published>2020-03-01T15:44:51.000Z</published>
    <updated>2020-03-01T15:55:06.933Z</updated>
    
    <content type="html"><![CDATA[<h2 id="OpenStack是什么？"><a href="#OpenStack是什么？" class="headerlink" title="OpenStack是什么？"></a><strong>OpenStack是什么？</strong></h2><p>2010年7月，RackSpace和美国国家航空航天局合作，分别贡献出RackSpace云文件平台代码和NASA Nebula平台代码，并以Apache许可证开源发布了OpenStack。OpenStack由此诞生，至此已经走过10个年头，最新发布的版本编号已到达T版本。OpenStack是目前最成熟并且符合生产部署的一套开源云操作系统，全球各大IT，CT厂商，甚至运营商都为此投入了大量的人力、物力和精力。由于其开源的特性，全世界的云计算爱好者也为此贡献了很多。OpenStack是开源云操作系统，可控制整个数据中心的大型计算，存储和网络资源池。用户能够通过Web界面、命令行或API接口配置资源。<a id="more"></a></p><p> <img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/T8UkJRyO7F1f.png?imageslim" alt="mark"></p><h2 id="OpenStack不是虚拟化"><a href="#OpenStack不是虚拟化" class="headerlink" title="OpenStack不是虚拟化"></a><strong>OpenStack不是虚拟化</strong></h2><p>在1959年国际信息处理大会上，Christopher Strachey亲手为虚拟化埋下了种子，他在名为《大型高速计算机中的时间共享》的报告中，提出了“虚拟化”的概念，从此拉开了虚拟化发展的帷幕。到21世纪，VMware的亮相，开启了虚拟化的X86时代，虚拟化的发展进入了一个爆发期。至此，目前业界主流的虚拟化解决方案有开源的KVM、Xen，商业的VMware、Hyper-V、Xen-Server和华为的FusionSphere。虚拟化技术的出现，主要出于提升资源利用率，降低资源投入成本的目的。从这一点上说，显然OpenStack不是虚拟化，因为OpenStack只是系统的控制面，并不包括系统的数据面组件。比如：Hypervisor、存储和网络设等。OpenStack与虚拟化有着本质上的区别，它本身并不提供虚拟化技术，而是对多种虚拟化技术架构进行管理，并能实现异构统一纳管。就像我们的win10操作系统，能够利用驱动程序对底层多种硬件进行统一管理，协调各类硬件的I/O占用。所以，这就是OpenStack被成为Cloud OS的原因，它本身只是个管理层面的系统。反而，虚拟化只是OpenStack的底层具体实现技术之一，但不是核心关注点。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/UnJtqApro3Ai.png?imageslim" alt="mark"></p><h2 id="OpenStack不是云计算"><a href="#OpenStack不是云计算" class="headerlink" title="OpenStack不是云计算"></a><strong>OpenStack不是云计算</strong></h2><p>1983年，Sun提出“网络即是电脑”（“The Network is the Computer”），这被认为是云计算的雏形，而随后计算机技术的迅猛发展以及互联网行业的兴起，似乎都在向这个概念不断靠拢。在这个不断靠拢的过程中，首先写上浓重一笔的是亚马逊。2006年3月，亚马逊推出弹性计算云（Elastic Computing Cloud，EC2），按用户使用的资源进行收费，开启了云计算商业化的元年。云计算概念诞生之初，市场上对其概念有很多种理解，经过一段时间的争论，现在大家一般来说都认可的就是美国标准与技术研究院的它给出的一个最标准的定义。它把云计算定义为一种模式，而不是一种技术。这种模式既可以是商业模式，也可以是服务模式。显然，OpenStack并不是一种模式，它只是实现云计算这种模式的技术之一，但却是构建云计算模式的关键技术，整个云计算模式的内核、骨干、框架、总线均由OpenStack来构建。随着容器云概念的提出，另一种构建云计算的关键技术也随之出现，也就是我们常说的K8S（Kubernetes）。需要明确一点，OpenStack和K8S这两种构建云计算的关键技术既可以独立部署，也可以混合部署，并且在混合部署下丝毫没有违和感。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/8ktXhQI82Mah.png?imageslim" alt="mark"></p><h2 id="OpenStack的体系架构"><a href="#OpenStack的体系架构" class="headerlink" title="OpenStack的体系架构"></a><strong>OpenStack的体系架构</strong></h2><p>在介绍OpenStack的体系架构之前，不得不提一下AWS Cloud。因为，OpenStack最早就是作为AWS Cloud的追随者出现的。AWS由一系列服务组成，去实现IaaS系统所需要的功能。AWS架构由5层组成，自下而上分别是AWS全球基础架构、基础服务、应用平台服务、管理和用户应用程序（华为的FusionSphere解决方案也很类似，不过华为是4层架构）而就服务类型本身而言，AWS主要提供6类服务：计算和网络、存储和内容分发、数据库、分析、部署管理和应用服务。AWS的功能十分强大，而且还在不断发展之中，OpenStack从诞生之初就一直向AWS模仿和学习，同时，OpenStack也提供开放接口去兼容各种AWS服务。做为一个IaaS范畴的云平台，完整的OpenStack系统通过网络将用户和网络背后丰富的硬件资源分离开。OpenStack一方面负责与运行在物理节点上的Hypervisor进行交互，实现对各种硬件资源的管理与控制；另一方面为用户提供一个满足要求的虚拟机。至于OpenStack内部，作为AWS的一个跟随者，它的体系结构里不可避免地体现着前面所介绍的AWS各个组件的痕迹。比如：AWS中最为核心的EC2模块，负责计算资源的管理，以及虚拟机的调度和管理，在OpenStack中对应的就是Nova项目；AWS中的简单存储服务S3，在OpenStack中有Swift项目与其功能相近；AWS中的块存储模块EBS，对应OpenStack的Cinder项目；AWS的验证访问管理服务IAM，对应OpenStack的KeyStone项目；AWS的监控服务CloudWatch，对应OpenStack的Ceilometer项目；AWS有CloudInformation，OpenStack则有Heat项目；AWS支持关系数据库RDS和NoSQL数据库DynamoDB，OpenStack也支持mySQL、postgre和NoSQL数据库MongoDB等等。</p><p>OpenStack共有7个核心组件，分别是计算（Nova）、对象存储（Swift）、认证（Keystone）、用户界面（Horizon）、块存储（Cinder）、网络（Neutron）、镜像服务（Glance）。每个组件都是多个服务的集合，一个服务意味着运行着的一个进程，根据部署Openstack的规模，决定了你是选择将所有服务运行在同一个机器上还是多个机器上。随着OpenStack发展至今，除了上述7个组件，还出现了其他社区高度关注并重点发展的项目，所有组件按照运营层、编排层、服务层和共享层的全景视图如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/E7DVQEyaOWgK.png?imageslim" alt="mark"></p><p>OpenStack的生产环境部署架构如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/vnrx6Ioi2tKJ.png?imageslim" alt="mark"></p><h2 id="OpenStack的关键组件介绍"><a href="#OpenStack的关键组件介绍" class="headerlink" title="OpenStack的关键组件介绍"></a><strong>OpenStack的关键组件介绍</strong></h2><ul><li><p><strong>认证服务Keystone：</strong>首次出现在OpenStack的“Essex”版本中。Keystone提供身份验证，服务发现和分布式多租户授权，支持LDAP，OAuth，OpenID Connect，SAML和SQL。Keystone服务不依赖其他OpenStack服务，为其他OpenStack服务提供认证支持。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/8lq3Q9ciLVil.png?imageslim" alt="mark"></p></li><li><p><strong>操作界面Horizon</strong>：首次出现在OpenStack的“Essex”版本中。Horizon提供基于Web的控制界面，使云管理员和用户能够管理各种OpenStack资源和服务，它依赖于Keystone服务。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/sakeKz2qOqrG.png?imageslim" alt="mark"></p></li><li><strong>镜像服务Glance：</strong>首次出现在OpenStack的“Bexar”版本中。Glance提供发现、注册和检索虚拟机镜像功能，提供的虚拟机实例镜像可以存放在不同地方，例如本地文件系统、对象存储、块存储等。它同样依赖于Keystone服务。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xSKPHXlQfSov.png?imageslim" alt="mark"></li><li><strong>计算服务Nova：</strong>首次出现在OpenStack的“Austin”版本中。Nova提供大规模、可扩展、按需自助服务的计算资源，支持管理裸机，虚拟机和容器。它依赖于Keystone、Neutron和Glance三个服务。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/3pASuW4PJX4E.png?imageslim" alt="mark"></li><li><p><strong>块存储服务Cinder：</strong>首次出现在OpenStack的“Folsom”版本中。Cinder提供块存储服务，为虚拟机实例提供持久化存储，调用不同存储接口驱动，将存储设备转化成块存储池，用户无需了解存储实际部署的位置或设备类型。它依赖于服务Keystone。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/7yjonu5aRi1X.png?imageslim" alt="mark"></p></li><li><p><strong>对象存储服务Swift：</strong>首次出现在OpenStack的“Austin”版本中。Swift提供高度可用、分布式、最终一致的对象存储服务，可以高效、安全且廉价地存储大量数据，非常适合存储需要弹性扩展的非结构化数据。它同样不依赖于其他服务，可以为其他服务提供对象存储服务。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/A7DRSCqdwKC6.png?imageslim" alt="mark"></p></li><li><p><strong>网络服务Neutron：</strong>首次出现在OpenStack的“Folsom”版本中。Neutron负责管理虚拟网络组件，专注于为OpenStack提供网络即服务。它依赖于服务Keystone。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/ASvQMScNvMDe.png?imageslim" alt="mark"></p></li><li><p><strong>编排服务Heat</strong>：首次出现在OpenStack的“Havana”版本中。Heat为云应用程序编排OpenStack基础架构资源，可以提供OpenStack原生Rest API和CloudFormation兼容的查询API。它依赖于服务Keystone。<img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/kDJkQaYA2QYd.png?imageslim" alt="mark"></p></li></ul><h2 id="OpenStack服务间的交互示例"><a href="#OpenStack服务间的交互示例" class="headerlink" title="OpenStack服务间的交互示例"></a><strong>OpenStack服务间的交互示例</strong></h2><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xRveNTHokG5C.png?imageslim" alt="mark"></p><p>以创建虚拟机为例，各组件的配合工作基本流程为：</p><p>1）用户首先接触到的是界面，即Horizon。通过Horizon上的简单界面操作，一个创建虚拟机的请求被发送到OpenStack系统后端。</p><p>2）既然要启动一个虚拟机，就必须指定虚拟机操作系统是什么类型，同时下载启动镜像以供虚拟机启动使用。这件事情就是由Glance来完成的，而此时Glance所管理的镜像有可能存储在Swift上，所以需要与Swift交互得到需要的镜像文件。</p><p>3）在创建虚拟机的时候，自然而然地需要Cinder提供块服务和Neutron提供网络服务，以便该虚拟机有volume可以使用，能被分配到IP地址与外界网络连接，而且之后该虚拟机资源的访问要经过Keystone的认证之后才可以继续。</p><p>4）至此，OpenStack的所有核心组件都参与了这个创建虚拟机的操作。</p><p><strong>建议多思考总结，从生活中熟悉的例子去理解OpenStack各服务之间的交互关系。</strong></p><h2 id="如何学习OpenStack？"><a href="#如何学习OpenStack？" class="headerlink" title="如何学习OpenStack？"></a><strong>如何学习OpenStack？</strong></h2><p>OpenStack需要很强的动手能力，最好能够准备好带虚拟化设备的物理机或者服务器供学习研究使用，动手是最重要的！此外，还须查阅各种关于OpenStack的资料。首先，OpenStack官网是不容错过的。在OpenStack官方网址上，发布了关于OpenStack各种最新的动态。此外，还提供了极为详细的文档，可以说OpenStack的官方文档是所有开源社区写得最好的，没有之一。在官方网址的博客中，提供了各种关于OpenStack的有趣的活动及技术沙龙。</p><p>学习好OpenStack，首先需要顺利地安装OpenStack的各个组件。在安装成功的基础上，学会使用OpenStack系统创建和管理虚拟机、虚拟网络及存储资源。如果需要再深入地研究，那么就需要阅读OpenStack的源代码了。代码的获得主要有两个来源，较为稳定的发行版位于 https:// launchpad. net/。而OpenStack最新的代码，则位于GitHub（https:// github. com/ openstack）。在学习的时候，建议使用launchpad网站上的稳定版本的代码。对OpenStack有了相当了解之后，在学习的过程中，发现一些Bug需要提交Patch的时候，就需要用到GitHub上面最新的代码了。</p><p>学习好OpenStack之后，也可以基于OpenStack做一些有意思的二次开发，无论是开发公有云或者私有云，都变得比较有意思。了解OpenStack的内部机制之后，添加一些自定义的模块或者驱动都是相当容易。也就真正地能够将OpenStack握在手中，为我所用了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;OpenStack是什么？&quot;&gt;&lt;a href=&quot;#OpenStack是什么？&quot; class=&quot;headerlink&quot; title=&quot;OpenStack是什么？&quot;&gt;&lt;/a&gt;&lt;strong&gt;OpenStack是什么？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;2010年7月，RackSpace和美国国家航空航天局合作，分别贡献出RackSpace云文件平台代码和NASA Nebula平台代码，并以Apache许可证开源发布了OpenStack。OpenStack由此诞生，至此已经走过10个年头，最新发布的版本编号已到达T版本。OpenStack是目前最成熟并且符合生产部署的一套开源云操作系统，全球各大IT，CT厂商，甚至运营商都为此投入了大量的人力、物力和精力。由于其开源的特性，全世界的云计算爱好者也为此贡献了很多。OpenStack是开源云操作系统，可控制整个数据中心的大型计算，存储和网络资源池。用户能够通过Web界面、命令行或API接口配置资源。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-CentOS 7手动部署Openstack Rocky版本笔记（含开源NFVO/VNFM）</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-CentOS-7%E6%89%8B%E5%8A%A8%E9%83%A8%E7%BD%B2Openstack-Rocky%E7%89%88%E6%9C%AC%E7%AC%94%E8%AE%B0%EF%BC%88%E5%90%AB%E5%BC%80%E6%BA%90NFVO-VNFM%EF%BC%89/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-CentOS-7手动部署Openstack-Rocky版本笔记（含开源NFVO-VNFM）/</id>
    <published>2020-03-01T12:03:21.000Z</published>
    <updated>2020-03-01T15:41:37.722Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博文记录了在VMware Workstations上手动部署双节点OpenStack Rocky发行版本的部署笔记，包括OpenStack的9大基础核心组件：Keystone、Glance、Nova、Neutron、Cinder、Swift、Heat和Telemetry等服务，以及开源NFVO/VNFM项目tacker，同时将本地OpenStack的环境注册为VIM，构造一个开源的MANO环境。其中，9大核心组件的Swift在对象存储管理服务—Swift文中介绍部署方法，通过单节点双硬盘的方式模拟vNode，同时模拟实际网络云环境中对象存储主要用于数据备份和镜像存储的功能。<a id="more"></a></p><p><strong>需要说明一点：服务器操作系统安装和初始化配置请参考本站Linux相关文章或网上其他文章，本篇博文不会再赘述。</strong></p><h2 id="部署前准备"><a href="#部署前准备" class="headerlink" title="部署前准备"></a>部署前准备</h2><h3 id="节点规划："><a href="#节点规划：" class="headerlink" title="节点规划："></a>节点规划：</h3><p>共两个节点，一个控制节点，一个计算节点。其中，控制节点兼做计算节点和块存储节点，计算节点主要做业务计算。各节点主机名、地址规划如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/YUlu6G8YPgBj.png?imageslim" alt="mark"></p><h3 id="网络规划："><a href="#网络规划：" class="headerlink" title="网络规划："></a>网络规划：</h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/yh3QsDKq24oS.png?imageslim" alt="mark"></p><h3 id="初始化配置要求：以下为要求为基础配置，请自行完成。"><a href="#初始化配置要求：以下为要求为基础配置，请自行完成。" class="headerlink" title="初始化配置要求：以下为要求为基础配置，请自行完成。"></a>初始化配置要求：以下为要求为基础配置，请自行完成。</h3><ol><li>所有节点关闭防火墙和SELinux；</li><li>所有节点开启三层转发功能；</li><li>所有节点修改YUM源和PIP源为阿里云的源； </li><li>所有节点如果存在双网关，需要配置明细路由策略；</li></ol><h2 id="安装时钟同步服务"><a href="#安装时钟同步服务" class="headerlink" title="安装时钟同步服务"></a>安装时钟同步服务</h2><h3 id="控制节点："><a href="#控制节点：" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install chrony -y</span><br></pre></td></tr></table></figure><p>2）配置时钟同步服务文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在/etc/chrony.conf文件中添加修改如下选项</span></span><br><span class="line"></span><br><span class="line">第26行修改：allow 10.28.101.0/24  <span class="comment">#&lt;===修改为管理平面网段</span></span><br></pre></td></tr></table></figure><p>3）配置时钟同步服务开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> chronyd.service &amp;&amp; systemctl start chronyd.service</span><br></pre></td></tr></table></figure><h3 id="计算节点："><a href="#计算节点：" class="headerlink" title="计算节点："></a>计算节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install chrony -y</span><br></pre></td></tr></table></figure><p>2）配置时钟同步服务文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/chrony.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在/etc/chrony.conf文件中添加修改如下选项</span></span><br><span class="line"><span class="comment"># 注释掉3，4，5，6行</span></span><br><span class="line"><span class="comment"># 第7行添加：</span></span><br><span class="line">server rocky-cotroller iburst  <span class="comment">#&lt;===添加控制节点主机名为NTP Server</span></span><br></pre></td></tr></table></figure><p>3）配置时钟同步服务开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> chronyd.service &amp;&amp; systemctl start chronyd.service</span><br></pre></td></tr></table></figure><p>在所有节点执行验证操作：chronyc sources，验证结果如下：</p><p><strong>控制节点：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/8jGtaNw2O91b.png?imageslim" alt="mark"></p><p><strong>计算节点：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/f2Bn9UAxWm6c.png?imageslim" alt="mark"></p><h2 id="安装OpenStack软件包"><a href="#安装OpenStack软件包" class="headerlink" title="安装OpenStack软件包"></a>安装OpenStack软件包</h2><p><strong>控制节点&amp;&amp;计算节点：</strong></p><p>1）添加rocky版本软件仓库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install centos-release-openstack-rocky -y</span><br></pre></td></tr></table></figure><p>2）更新系统：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum upgrade -y</span><br></pre></td></tr></table></figure><p>3）安装OpenStack客户端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install python-openstackclient -y</span><br></pre></td></tr></table></figure><p>4）安装OpenStack安全策略组件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-selinux -y</span><br></pre></td></tr></table></figure><h2 id="安装SQL数据库"><a href="#安装SQL数据库" class="headerlink" title="安装SQL数据库"></a>安装SQL数据库</h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mariadb mariadb-server python2-PyMySQL -y</span><br></pre></td></tr></table></figure><p>2）创建和编辑/etc/my.cnf.d/openstack.cnf文件，创建一个[mysqld]的Session，并将bind-address 密钥设置为控制器节点的管理IP地址，以允许其他节点通过管理网络进行访问。设置其他键以启用有用的选项和UTF-8字符集，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/7oWP90noeSPl.png?imageslim" alt="mark"></p><p>3）配置数据库服务开机启动：vim</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> mariadb.service &amp;&amp; systemctl start mariadb.service</span><br></pre></td></tr></table></figure><p>4）通过执行如下脚本，设置mysql数据root帐号和密码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql_secure_installation</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/mRdLCJ0ofjpE.png?imageslim" alt="mark"></p><h2 id="安装消息队列服务"><a href="#安装消息队列服务" class="headerlink" title="安装消息队列服务"></a><strong>安装消息队列服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install rabbitmq-server -y</span><br></pre></td></tr></table></figure><p>2）配置开机启动消息队列服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> rabbitmq-server.service &amp;&amp; systemctl start rabbitmq-server.service</span><br></pre></td></tr></table></figure><p>3）添加OpenStack用户并添加密码:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl add_user openstack openstack</span><br></pre></td></tr></table></figure><p>4）设置openstack用户的访问权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rabbitmqctl set_permissions openstack <span class="string">".*"</span> <span class="string">".*"</span> <span class="string">".*"</span></span><br></pre></td></tr></table></figure><h2 id="安装分布式缓存memcache"><a href="#安装分布式缓存memcache" class="headerlink" title="安装分布式缓存memcache"></a><strong>安装分布式缓存memcache</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install memcached python-memcached -y</span><br></pre></td></tr></table></figure><p>2）配置/etc/sysconfig/memcached文件，在OPTIONS中添加控制节点主机名：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/sysconfig/memcached&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/4mG8ul2l6rtd.png?imageslim" alt="mark"></p><p>3）配置开机启动memcache服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> memcached.service &amp;&amp; systemctl start memcached.service</span><br></pre></td></tr></table></figure><h2 id="安装分布式键值数据库etcd服务"><a href="#安装分布式键值数据库etcd服务" class="headerlink" title="安装分布式键值数据库etcd服务"></a><strong>安装分布式键值数据库etcd服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install etcd -y</span><br></pre></td></tr></table></figure><p>2）编辑/etc/etcd/etcd.conf文件，并设置ETCD_INITIAL_CLUSTER， ETCD_INITIAL_ADVERTISE_PEER_URLS，ETCD_ADVERTISE_CLIENT_URLS， ETCD_LISTEN_CLIENT_URLS控制器节点，以使经由管理网络通过其他节点的访问的管理IP地址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/etcd/etcd.conf&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xEL8XzCFPVUe.png?imageslim" alt="mark"></p><p>3）配置开机启动ETCD服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> etcd &amp;&amp; systemctl start etcd</span><br></pre></td></tr></table></figure><h2 id="安装Horizon仪表盘服务"><a href="#安装Horizon仪表盘服务" class="headerlink" title="安装Horizon仪表盘服务"></a><strong>安装Horizon仪表盘服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-dashboard -y</span><br></pre></td></tr></table></figure><p>2）编辑 /etc/openstack-dashboard/local_settings 文件并完成以下修改操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/openstack-dashboard/local_settings&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 184行修改为：OPENSTACK_HOST = "rocky-controller"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 38行修改为：ALLOWED_HOSTS = ['*'] #&lt;===生产环境建议根据访问需求修改，不建议修改为*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 160行添加：SESSION_ENGINE = 'django.contrib.sessions.backends.cache'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 166行添加：'LOCATION': 'rocky-controller:11211', #&lt;===注意后面的逗号不能少</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 75行修改为：OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 64、66、67、68、70、97行去掉注释</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 189行修改为：OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 467行修改为：TIME_ZONE = "Asia/Shanghai"</span></span><br></pre></td></tr></table></figure><p>3）在/etc/httpd/conf.d/openstack-dashboard.conf中添加以下行 ：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/X01qyzntGCAk.png?imageslim" alt="mark"></p><p>4）重启web服务和分布式缓存服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart httpd.service memcached.service</span><br></pre></td></tr></table></figure><h2 id="安装keystone服务"><a href="#安装keystone服务" class="headerlink" title="安装keystone服务"></a><strong>安装keystone服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装keystone数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE keystone;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON keystone.* TO <span class="string">'keystone'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'keystone'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON keystone.* TO <span class="string">'keystone'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'keystone'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-keystone httpd mod_wsgi -y</span><br></pre></td></tr></table></figure><p>3）配置/etc/keystone/keystone.conf文件，完成[database]和[token]部分配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/keystone/keystone.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># [database]部分：connection = mysql+pymysql://keystone:keystone@rocky-controller/keystone</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [token]部分：provider = fernet</span></span><br></pre></td></tr></table></figure><p>4）初始化keystone数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"keystone-manage db_sync"</span> keystone</span><br></pre></td></tr></table></figure><p>5）初始化fernet秘钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone</span><br><span class="line"></span><br><span class="line">keystone-manage credential_setup --keystone-user keystone --keystone-group keystone</span><br></pre></td></tr></table></figure><p>6）建立引导服务：（注意修改密码，更改主机名）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keystone-manage bootstrap --bootstrap-password &lt;你自己的admin用户密码&gt; --bootstrap-admin-url http://rocky-controller:5000/v3/ --bootstrap-internal-url http://rocky-controller:5000/v3/ --bootstrap-public-url http://rocky-controller:5000/v3/ --bootstrap-region-id RegionOne</span><br></pre></td></tr></table></figure><p>7）编辑/etc/httpd/conf/httpd.conf文件并配置 ServerName选项以引用控制器节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/httpd/conf/httpd.conf&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/wxGwWnaGk9M3.png?imageslim" alt="mark"></p><p>8）创建到/usr/share/keystone/wsgi-keystone.conf文件的链接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/</span><br></pre></td></tr></table></figure><p>9）启动Apache HTTP服务，并将其配置为在系统启动时启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> httpd.service &amp;&amp; systemctl start httpd.service</span><br></pre></td></tr></table></figure><p>10）创建和编辑admin-openrc文件</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/71Jr2WWTBOWr.png?imageslim" alt="mark"></p><p>11）创建domain、project、user和role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">openstack domain create --description <span class="string">"KkutysLLB-Domain"</span> kkutysllb</span><br><span class="line"></span><br><span class="line">openstack project create --domain default --description <span class="string">"Service Project"</span> service</span><br><span class="line"></span><br><span class="line">openstack project create --domain default --description <span class="string">"Demo Project"</span> kkproject</span><br><span class="line"></span><br><span class="line">openstack user create --domain default --password-prompt kkutysllb</span><br><span class="line"></span><br><span class="line">openstack role create kkrole</span><br><span class="line"></span><br><span class="line">openstack role add --project kkproject --user kkutysllb kkrole</span><br></pre></td></tr></table></figure><p>12）创建和编辑kkutysllb-openrc文件</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/0iAXvVjvsMI7.png?imageslim" alt="mark"></p><p>13）验证admin用户和kkutysllb用户的token</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br><span class="line"></span><br><span class="line">openstack token issue</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> kkutysllb-openrc.sh</span><br><span class="line"></span><br><span class="line">openstack token issue</span><br></pre></td></tr></table></figure><h2 id="安装Glance服务"><a href="#安装Glance服务" class="headerlink" title="安装Glance服务"></a><strong>安装Glance服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装glance数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE glance;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON glance.* TO <span class="string">'glance'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'glance'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON glance.* TO <span class="string">'glance'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'glance'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>;</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建glance用户、角色和服务endpoint</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt glance</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user glance admin</span><br><span class="line"></span><br><span class="line">openstack service create --name glance --description <span class="string">"OpenStack Image"</span> image</span><br></pre></td></tr></table></figure><p>4）创建glance服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne image public http://rocky-controller:9292</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne image internal http://rocky-controller:9292</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne image admin http://rocky-controller:9292</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-glance -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/glance/glance-api.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/glance/glance-api.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[database]部分添加：connection = mysql+pymysql://glance:glance@rocky-controller/glance</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加：</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rokcy-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = Default</span><br><span class="line"></span><br><span class="line">user_domain_name = Default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = glance</span><br><span class="line"></span><br><span class="line">password = glance</span><br><span class="line"></span><br><span class="line">[paste_deploy]部分添加：flavor = keystone</span><br><span class="line"></span><br><span class="line">[glance_store]部分添加：</span><br><span class="line"></span><br><span class="line">stores = file,http</span><br><span class="line"></span><br><span class="line">default_store = file</span><br><span class="line"></span><br><span class="line">filesystem_store_datadir = /var/lib/glance/images/</span><br></pre></td></tr></table></figure><p>7）编辑/etc/glance/glance-registry.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/glance/glance-registry.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[database]部分添加：connection = mysql+pymysql://glance:glance@rocky-controller/glance</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加：</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = Default</span><br><span class="line"></span><br><span class="line">user_domain_name = Default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = glance</span><br><span class="line"></span><br><span class="line">password = glance</span><br><span class="line"></span><br><span class="line">[paste_deploy]部分添加：flavor = keystone</span><br></pre></td></tr></table></figure><p>8）初始化glance数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"glance-manage db_sync"</span> glance</span><br></pre></td></tr></table></figure><p>9）设置开机启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-glance-api.service openstack-glance-registry.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-glance-api.service openstack-glance-registry.service</span><br></pre></td></tr></table></figure><p>10）验证服务，上传镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img</span><br><span class="line"></span><br><span class="line">openstack image create <span class="string">"cirros"</span> --file cirros-0.4.0-x86_64-disk.img --disk-format qcow2 --container-format bare --public</span><br><span class="line"></span><br><span class="line">openstack image list</span><br></pre></td></tr></table></figure><h2 id="安装Nova服务"><a href="#安装Nova服务" class="headerlink" title="安装Nova服务"></a><strong>安装Nova服务</strong></h2><h3 id="控制节点：-1"><a href="#控制节点：-1" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）创建nova数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE nova_api;</span><br><span class="line"></span><br><span class="line">CREATE DATABASE nova;</span><br><span class="line"></span><br><span class="line">CREATE DATABASE nova_cell0;</span><br><span class="line"></span><br><span class="line">CREATE DATABASE placement;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON nova_api.* TO <span class="string">'nova'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON nova_api.* TO <span class="string">'nova'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON nova.* TO <span class="string">'nova'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON nova.* TO <span class="string">'nova'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON nova_cell0.* TO <span class="string">'nova'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON nova_cell0.* TO <span class="string">'nova'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'nova'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON placement.* TO <span class="string">'placement'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'placement'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON placement.* TO <span class="string">'placement'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'placement'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>;</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建nova用户、添加角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt nova</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user nova admin</span><br><span class="line"></span><br><span class="line">openstack service create --name nova --description <span class="string">"OpenStack Compute"</span> compute</span><br></pre></td></tr></table></figure><p>4）创建API服务端点endpoint</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne compute public http://rocky-controller:8774/v2.1</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne compute internal http://rocky-controller:8774/v2.1</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne compute admin http://rocky-controller:8774/v2.1</span><br></pre></td></tr></table></figure><p>5）创建placement用户、添加角色和服务实体，placement用于资源的追踪展示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt placement</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user placement admin</span><br><span class="line"></span><br><span class="line">openstack service create --name placement --description <span class="string">"Placement API"</span> placement</span><br></pre></td></tr></table></figure><p>6）创建placement API服务端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne placement public http://rocky-controller:8778</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne placement internal http://rocky-controller:8778</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne placement admin http://rocky-controller:8778</span><br></pre></td></tr></table></figure><p>7）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api -y</span><br></pre></td></tr></table></figure><p>8）编辑/etc/nova/nova.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/nova/nova.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加：</span><br><span class="line"></span><br><span class="line">enabled_apis = osapi_compute,metadata</span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">my_ip = 10.28.101.81</span><br><span class="line"></span><br><span class="line">use_neutron = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">firewall_driver = nova.virt.firewall.NoopFirewallDriver</span><br><span class="line"></span><br><span class="line">[api_database]部分添加：connection = mysql+pymysql://nova:nova@rocky-controller/nova_api</span><br><span class="line"></span><br><span class="line">[database]部分添加：connection = mysql+pymysql://nova:nova@rocky-controller/nova</span><br><span class="line"></span><br><span class="line">[placement_database]部分添加：connection = mysql+pymysql://placement:placement@rocky-controller/placement</span><br><span class="line"></span><br><span class="line">[api]部分添加：auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加：</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = Default</span><br><span class="line"></span><br><span class="line">user_domain_name = Default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = nova</span><br><span class="line"></span><br><span class="line">password = nova</span><br><span class="line"></span><br><span class="line">[vnc]部分添加：</span><br><span class="line"></span><br><span class="line">enabled = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">server_listen = <span class="variable">$my_ip</span></span><br><span class="line"></span><br><span class="line">server_proxyclient_address = <span class="variable">$my_ip</span></span><br><span class="line"></span><br><span class="line">[glance]部分添加：api_servers = http://rocky-controller:9292</span><br><span class="line"></span><br><span class="line">[oslo_concurrency]部分添加：lock_path = /var/lib/nova/tmp</span><br><span class="line"></span><br><span class="line">[placement]部分添加：</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br><span class="line">project_domain_name = Default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">user_domain_name = Default</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line"></span><br><span class="line">username = placement</span><br><span class="line"></span><br><span class="line">password = placement</span><br></pre></td></tr></table></figure><p>9）修复一个bug，修改 /etc/httpd/conf.d/00-nova-placement-api.conf配置如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Directory</span> /<span class="attr">usr</span>/<span class="attr">bin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">IfVersion</span> &gt;</span>= 2.4&gt;</span><br><span class="line"></span><br><span class="line">   Require all granted</span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  &lt;IfVersion &lt; 2.4&gt;</span><br><span class="line"></span><br><span class="line">   Order allow,deny</span><br><span class="line"></span><br><span class="line">   Allow from all</span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">Directory</span>&gt;</span></span><br></pre></td></tr></table></figure><p>10）重启web服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>11）初始化nova-api和placement数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage api_db sync"</span> nova</span><br></pre></td></tr></table></figure><p>12）注册cell0数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 map_cell0"</span> nova</span><br></pre></td></tr></table></figure><p>13）创建cell1网格：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 create_cell --name=cell1 --verbose"</span> nova</span><br></pre></td></tr></table></figure><p>14）初始化nova数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage db sync"</span> nova</span><br></pre></td></tr></table></figure><p>15）验证nova cell0和cell1是否正确注册：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 list_cells"</span> nova</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/b0ys13tLW2DI.png?imageslim" alt="mark"></p><p>16）配置开机启动服务：（备注：nova-consoleauth服务自18.0版本开始不再使用，建议以后每个单元都部署控制台代理）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-nova-api.service openstack-nova-consoleauth openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-nova-api.service openstack-nova-consoleauth openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service</span><br></pre></td></tr></table></figure><h3 id="计算节点：-1"><a href="#计算节点：-1" class="headerlink" title="计算节点："></a>计算节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-nova-compute -y</span><br></pre></td></tr></table></figure><p>2）拷贝控制节点的nova.conf文件至计算节点，并修改以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/nova/nova.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">scp /etc/nova/nova.conf rocky-compute:/etc/nova/nova.conf</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分修改计算节点管理网口IP：my_ip = 10.28.101.82</span><br><span class="line"></span><br><span class="line">[vnc]部分添加以下配置：novncproxy_base_url = http://rocky-controller:6080/vnc_auto.html</span><br><span class="line"></span><br><span class="line">[libvirt]部分添加以下配置：virt_type = qemu <span class="comment">#&lt;===因为我们环境是虚拟机安装，所以选择qemu，生产环境根据实际情况可以选择KVM、Xen、VMware等</span></span><br></pre></td></tr></table></figure><p>3）配置compute服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> libvirtd.service openstack-nova-compute.service</span><br><span class="line"></span><br><span class="line">systemctl start libvirtd.service openstack-nova-compute.service</span><br></pre></td></tr></table></figure><h3 id="控制节点：-2"><a href="#控制节点：-2" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>2）列出计算主机：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack compute service list --service nova-compute</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xqNVjwPu1fzu.png?imageslim" alt="mark"></p><p>3）在数据库中同步发现的计算主机：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"nova-manage cell_v2 discover_hosts --verbose"</span> nova</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/i024exdk4QJI.png?imageslim" alt="mark"></p><p>4）（可选）在控制节点的nova.conf添加scheduler选项，用于自动发现主机，扫描周期可配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[scheduler]</span><br><span class="line"></span><br><span class="line">discover_hosts_in_cells_interval = 300</span><br></pre></td></tr></table></figure><p>5）重启nova-api和nova-scheduler服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-api.service openstack-nova-scheduler.service</span><br></pre></td></tr></table></figure><h2 id="安装Neutron服务"><a href="#安装Neutron服务" class="headerlink" title="安装Neutron服务"></a><strong>安装Neutron服务</strong></h2><h3 id="控制节点-amp-amp-网络节点："><a href="#控制节点-amp-amp-网络节点：" class="headerlink" title="控制节点&amp;&amp;网络节点："></a>控制节点&amp;&amp;网络节点：</h3><p>1）安装neutron数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE neutron;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON neutron.* TO <span class="string">'neutron'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'neutron'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON neutron.* TO <span class="string">'neutron'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'neutron'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建neutron用户、服务实体、添加角色：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt neutron</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user neutron admin</span><br><span class="line"></span><br><span class="line">openstack service create --name neutron --description <span class="string">"OpenStack Networking"</span> network</span><br></pre></td></tr></table></figure><p>4）创建API的服务端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne network public http://rocky-controller:9696</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne network internal http://rocky-controller:9696</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne network admin http://rocky-controller:9696</span><br></pre></td></tr></table></figure><p><strong><em>选择网络选项二，私有网络</em></strong></p><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables -y</span><br></pre></td></tr></table></figure><p>6）配置neutron.conf文件，添加以下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/neutron.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加：</span><br><span class="line"></span><br><span class="line">core_plugin = ml2</span><br><span class="line"></span><br><span class="line">service_plugins = router</span><br><span class="line"></span><br><span class="line">allow_overlapping_ips = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">notify_nova_on_port_status_changes = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">notify_nova_on_port_data_changes = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[database]部分添加：connection = mysql+pymysql://neutron:neutron@rocky-controller/neutron</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加：</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = neutron</span><br><span class="line"></span><br><span class="line">password = neutron</span><br><span class="line"></span><br><span class="line">[nova]部分添加：</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = nova</span><br><span class="line"></span><br><span class="line">password = nova</span><br><span class="line"></span><br><span class="line">[oslo_concurrency]部分添加：lock_path = /var/lib/neutron/tmp</span><br></pre></td></tr></table></figure><p>7）配置ml2_conf.ini文件，添加以下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/plugins/ml2/ml2_conf.ini&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[ml2]部分添加以下选项：</span><br><span class="line"></span><br><span class="line">type_drivers = flat,vlan,vxlan</span><br><span class="line"></span><br><span class="line">tenant_network_types = vxlan</span><br><span class="line"></span><br><span class="line">extension_drivers = port_security</span><br><span class="line"></span><br><span class="line">mechanism_drivers = openvswitch,l2population</span><br><span class="line"></span><br><span class="line">[ml2_type_vxlan]部分添加如下选项：vni_ranges = 1:1000</span><br><span class="line"></span><br><span class="line">[securitygroup]部分添加如下选项：enable_ipset = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>8）配置openvswitch_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/plugins/ml2/openvswitch_agent.ini&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[agent]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">tunnel_types = vxlan</span><br><span class="line"></span><br><span class="line">l2_population = True</span><br><span class="line"></span><br><span class="line">[ovs]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">bridge_mappings = provider:br-extnet</span><br><span class="line"></span><br><span class="line">local_ip = 99.64.101.81</span><br><span class="line"></span><br><span class="line">[securitygroup]部分添加如下选项：firewall_driver = iptables_hybrid</span><br></pre></td></tr></table></figure><p>9）配置l3_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/l3_agent.ini&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">interface_driver = openvswitch</span><br><span class="line"></span><br><span class="line">external_network_bridge =</span><br></pre></td></tr></table></figure><p>10）配置dhcp_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/dhcp_agent.ini&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">interface_driver = openvswitch</span><br><span class="line"></span><br><span class="line">dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq</span><br><span class="line"></span><br><span class="line">enable_ioslated_metadata = <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>11）配置metadata_agent.ini文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/metadata_agent.ini&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">nova_metadata_host = rocky-controller</span><br><span class="line"></span><br><span class="line">metadata_proxy_shared_secret = neutron</span><br></pre></td></tr></table></figure><p>12）修改nova.conf配置文件，添加neutron模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[neutron]</span><br><span class="line"></span><br><span class="line">url = http://rocky-controller:9696</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = neutron</span><br><span class="line"></span><br><span class="line">password = neutron</span><br><span class="line"></span><br><span class="line">service_metadata_proxy = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">metadata_proxy_shared_secret = neutron</span><br></pre></td></tr></table></figure><p>13）创建ml2_conf.ini文件的软连接，用户网络服务的初始化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini</span><br></pre></td></tr></table></figure><p>14）初始化neutron数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head"</span> neutron</span><br></pre></td></tr></table></figure><p>15）重启nova-api服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-api.service</span><br></pre></td></tr></table></figure><p>16）配置OVS服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openvswitch &amp;&amp; systemctl start openvswitch &amp;&amp; systemctl status openvswitch</span><br></pre></td></tr></table></figure><p>17）添加外部网桥br-extnet和端口port配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/sysconfig/network-scripts/</span><br><span class="line"></span><br><span class="line">touch ifcfg-br-extnet</span><br><span class="line"></span><br><span class="line">cp ifcfg-eth2&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/GrjS1mgnlQQ3.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/BGy3KeErz2xC.png?imageslim" alt="mark"></p><p>18）添加OVS外部网桥，并绑定端口eth2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ovs-vsctl add-br br-extnet</span><br><span class="line"></span><br><span class="line">ovs-vsctl add-port br-extnet eth2</span><br></pre></td></tr></table></figure><p>19）配置neutron服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service</span><br><span class="line"></span><br><span class="line">systemctl start neutron-server.service neutron-openvswitch-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service</span><br></pre></td></tr></table></figure><p>20）检查OVS网桥状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ovs-vsctl show</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/JiQTxeahSdbF.png?imageslim" alt="mark"></p><h3 id="计算节点：-2"><a href="#计算节点：-2" class="headerlink" title="计算节点："></a>计算节点：</h3><p>1）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y openstack-neutron-openvswitch ebtables</span><br></pre></td></tr></table></figure><p>2）修改neutron.conf配置文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/neutron.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = neutron</span><br><span class="line"></span><br><span class="line">password = neutron</span><br></pre></td></tr></table></figure><p>3）修改openvswitch_agent.ini 配置文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/neutron/plugins/ml2/openvswitch_agent.ini&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[ovs]部分添加如下选项：local_ip = 99.64.101.82</span><br><span class="line"></span><br><span class="line">[agent]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">tunnel_types = vxlan</span><br><span class="line"></span><br><span class="line">l2_population = True</span><br></pre></td></tr></table></figure><p>4）修改nova.conf配置文件，添加neutron模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[neutron]</span><br><span class="line"></span><br><span class="line">url = http://rocky-controller:9696</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = neutron</span><br><span class="line"></span><br><span class="line">password = neutron</span><br><span class="line"></span><br><span class="line">service_metadata_proxy = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">metadata_proxy_shared_secret = neutron</span><br></pre></td></tr></table></figure><p>5）配置OVS服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openvswitch &amp;&amp; systemctl start openvswitch &amp;&amp; systemctl status openvswitch</span><br></pre></td></tr></table></figure><p>6）重启nova-compute服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-compute.service</span><br></pre></td></tr></table></figure><p>7）配合开机启动OVS AGENT服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> neutron-openvswitch-agent.service &amp;&amp; systemctl start neutron-openvswitch-agent.service &amp;&amp; systemctl status neutron-openvswitch-agent.service</span><br></pre></td></tr></table></figure><h3 id="控制节点：-3"><a href="#控制节点：-3" class="headerlink" title="控制节点："></a>控制节点：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">验证服务：openstack network agent list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/2F4NMr0x1h55.png?imageslim" alt="mark"></p><h2 id="安装heat服务"><a href="#安装heat服务" class="headerlink" title="安装heat服务"></a><strong>安装heat服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装heat数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE heat;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON heat.* TO <span class="string">'heat'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'heat'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON heat.* TO <span class="string">'heat'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'heat'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>;</span><br></pre></td></tr></table></figure><p>2）获取管理员权限:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建heat用户、添加角色，创建heat和heat-cfn服务实体</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt heat</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user heat admin</span><br><span class="line"></span><br><span class="line">openstack service create --name heat --description <span class="string">"Orchestration"</span> orchestration</span><br><span class="line"></span><br><span class="line">openstack service create --name heat-cfn --description <span class="string">"Orchestration"</span> cloudformation</span><br></pre></td></tr></table></figure><p>4）创建编排服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne orchestration public [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne orchestration internal [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne orchestration admin [http://rocky-controller:8004/v1/%\(tenant_id\)s](http://rocky-controller:8004/v1/%/(tenant_id/)s)</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne cloudformation public http://rocky-controller:8000/v1</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne cloudformation internal http://rocky-controller:8000/v1</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne cloudformation admin http://rocky-controller:8000/v1</span><br></pre></td></tr></table></figure><p>5）创建编排需要的身份服务中的其他信息来管理堆栈</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">openstack domain create --description <span class="string">"Stack projects and users"</span> heat</span><br><span class="line"></span><br><span class="line">openstack user create --domain heat --password-prompt heat_domain_admin</span><br><span class="line"></span><br><span class="line">openstack role add --domain heat --user-domain heat --user heat_domain_admin admin</span><br><span class="line"></span><br><span class="line">openstack role create heat_stack_owner</span><br><span class="line"></span><br><span class="line">openstack role add --project kkproject --user kkutysllb heat_stack_owner</span><br><span class="line"></span><br><span class="line">openstack role create heat_stack_user</span><br></pre></td></tr></table></figure><p>6）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine -y</span><br></pre></td></tr></table></figure><p>7）编辑/etc/heat/heat.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/heat/heat.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[database]部分添加：connection = mysql+pymysql://heat:heat@rocky-controller/heat</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加：</span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">heat_metadata_server_url = http://rocky-controller:8000</span><br><span class="line"></span><br><span class="line">heat_waitcondition_server_url = http://rocky-controller:8000/v1/waitcondition</span><br><span class="line"></span><br><span class="line">stack_domain_admin = heat_domain_admin</span><br><span class="line"></span><br><span class="line">stack_domain_admin_password = heat</span><br><span class="line"></span><br><span class="line">stack_user_domain_name = heat</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加：</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = heat</span><br><span class="line"></span><br><span class="line">password = heat</span><br><span class="line"></span><br><span class="line">[trustee]部分添加：</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">username = heat</span><br><span class="line"></span><br><span class="line">password = heat</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">[clients_keystone]部分添加：auth_uri = http://rocky-controller:5000</span><br></pre></td></tr></table></figure><p>8）初始化heat数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"heat-manage db_sync"</span> heat</span><br></pre></td></tr></table></figure><p>9）配置开机启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-heat-api.service openstack-heat-api-cfn.service openstack-heat-engine.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-heat-api.service openstack-heat-api-cfn.service openstack-heat-engine.service</span><br></pre></td></tr></table></figure><p>10）安装heat-dashboard</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install heat-dashboard</span><br><span class="line"></span><br><span class="line">cp /usr/lib/python2.7/site-packages/heat_dashboard/enabled/_[1-9]*.py /usr/share/openstack-dashboard/openstack_dashboard/enabled/</span><br><span class="line"></span><br><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>11）验证安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br><span class="line"></span><br><span class="line">openstack orchestration service list</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/jHkOh5sX8pwf.png?imageslim" alt="mark"></p><h2 id="安装cinder服务"><a href="#安装cinder服务" class="headerlink" title="安装cinder服务"></a><strong>安装cinder服务</strong></h2><h3 id="控制节点：-4"><a href="#控制节点：-4" class="headerlink" title="控制节点："></a>控制节点：</h3><p>1）安装cinder数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE cinder;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON cinder.* TO <span class="string">'cinder'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'cinder'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON cinder.* TO <span class="string">'cinder'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'cinder'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建cinder用户、v2/v3服务实体，添加角色：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt cinder</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user cinder admin</span><br><span class="line"></span><br><span class="line">openstack service create --name cinderv2 --description <span class="string">"OpenStack Block Storage"</span> volumev2</span><br><span class="line"></span><br><span class="line">openstack service create --name cinderv3 --description <span class="string">"OpenStack Block Storage"</span> volumev3</span><br></pre></td></tr></table></figure><p>4）创建块存储服务API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne volumev2 public http://rocky-controller:8776/v2/%\(project_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne volumev2 internal http://rocky-controller:8776/v2/%\(project_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne volumev2 admin http://rocky-controller:8776/v2/%\(project_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne volumev3 public http://rocky-controller:8776/v3/%\(project_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne volumev3 internal http://rocky-controller:8776/v3/%\(project_id\)s</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne volumev3 admin http://rocky-controller:8776/v3/%\(project_id\)s</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-cinder -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/cinder/cinder.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/cinder/cinder.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">my_ip = 10.28.101.81</span><br><span class="line"></span><br><span class="line">[database]部分添加如下选项：connection = mysql+pymysql://cinder:cinder@rocky-controller/cinder</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_id = default</span><br><span class="line"></span><br><span class="line">user_domain_id = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = cinder</span><br><span class="line"></span><br><span class="line">password = cinder</span><br><span class="line"></span><br><span class="line">[oslo_concurrency]部分添加如下选项：lock_path = /var/lib/cinder/tmp</span><br></pre></td></tr></table></figure><p>7）初始化cinder数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"cinder-manage db sync"</span> cinder</span><br></pre></td></tr></table></figure><p>8）编辑/etc/nova/nova.conf文件并向其中添加cinder模块：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[cinder]</span><br><span class="line"></span><br><span class="line">os_region_name = RegionOne</span><br></pre></td></tr></table></figure><p>9）重启nova-api服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart openstack-nova-api.service</span><br></pre></td></tr></table></figure><p>10）配置块存储服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-cinder-api.service openstack-cinder-scheduler.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service</span><br></pre></td></tr></table></figure><h3 id="存储节点（实验环境控制节点兼任）："><a href="#存储节点（实验环境控制节点兼任）：" class="headerlink" title="存储节点（实验环境控制节点兼任）："></a>存储节点（实验环境控制节点兼任）：</h3><p>1）挂载第二块硬盘，大小200G：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/aP5ra3mE9Rkx.png?imageslim" alt="mark"></p><p>2）针对第二块硬盘新建2个100G的分区：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">'n\np\n1\n\n+100G\nw'</span> | fdisk /dev/sdb</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">'n\np\n2\n\n\nw'</span> | fdisk /dev/sdb</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/zfn0OkzsHNaj.png?imageslim" alt="mark"></p><p>3）格式化两个分区为ext4文件系统格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkfs.ext4 /dev/sdb1</span><br><span class="line"></span><br><span class="line">mkfs.ext4 /dev/sdb2</span><br></pre></td></tr></table></figure><p>4）创建nfs_volume目录，并将第二块/dev/sdb2挂载在其下，同时设置开机自动挂载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir /nfs_volume</span><br><span class="line"></span><br><span class="line">mount -t ext4 /dev/sdb2 /nfs_volume</span><br><span class="line"></span><br><span class="line">df -h | grep /dev/sdb2</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/LUaxH6XKdMHS.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"mount -t ext4 /dev/sdb2 /nfs_volume"</span> &gt;&gt;/etc/rc.d/rc.local</span><br><span class="line"></span><br><span class="line">tail -1 /etc/rc.d/rc.local</span><br><span class="line"></span><br><span class="line">chmod +x /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><p>5）安装lvm2软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install lvm2 device-mapper-persistent-data -y</span><br></pre></td></tr></table></figure><p>6）设置LVM2逻辑卷服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> lvm2-lvmetad.service &amp;&amp; systemctl start lvm2-lvmetad.service</span><br></pre></td></tr></table></figure><p>7）创建物理卷PV、卷组cinder_kklvm01</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pvcreate /dev/sdb1</span><br><span class="line"></span><br><span class="line">vgcreate cinder_kklvm01 /dev/sdb1</span><br><span class="line"></span><br><span class="line">vgdisplay</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Ny0pxXRBVOae.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在devices &#123; &#125;部分添加 filter = [ "a/sdb1/", "r/.*/"]</span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">'141a filter = [ "a/sdb1/", "r/.*/"]'</span> /etc/lvm/lvm.conf <span class="comment">#在141行后添加</span></span><br><span class="line"></span><br><span class="line">systemctl restart lvm2-lvmetad.service</span><br></pre></td></tr></table></figure><p>8）安装NFS服务，作为第二个后端存储：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">yum install nfs-utils rpcbind -y</span><br><span class="line"></span><br><span class="line">mkdir -p /nfs_volume/&#123;cinder_nfs1,cinder_nfs2&#125;</span><br><span class="line"></span><br><span class="line">chown root:cinder /nfs_volume/cinder_nfs1</span><br><span class="line"></span><br><span class="line">chmod a+w /nfs_volume/cinder_nfs1</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'10.28.101.81:/nfs_volume/cinder_nfs1'</span>&gt;/etc/cinder/nfs_shares</span><br><span class="line"></span><br><span class="line">chmod a+w /etc/cinder/nfs_shares</span><br><span class="line"></span><br><span class="line">chown root:cinder /etc/cinder/nfs_shares</span><br></pre></td></tr></table></figure><p>9）配置NFS服务，并启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"/nfs_volume/cinder_nfs1 *(rw,root_squash,sync,anonuid=165,anongid=165)"</span>&gt;/etc/exports</span><br><span class="line"></span><br><span class="line">exportfs -r</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> rpcbind nfs-server &amp;&amp; systemctl restart rpcbind nfs-server</span><br><span class="line"></span><br><span class="line">showmount -e localhost</span><br></pre></td></tr></table></figure><p>10）安装cinder-volume服务软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-cinder targetcli python-keystone -y</span><br></pre></td></tr></table></figure><p>11）编辑/etc/cinder/cinder.conf文件，添加如下配置：（由于实验环境存储节点与控制节点合一，所以只需要添加个别选项即可。如果生产环境需要按照要求严格配置）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]部分增加如下选项：</span><br><span class="line"></span><br><span class="line">log_dir = /var/<span class="built_in">log</span>/cinder</span><br><span class="line"></span><br><span class="line">state_path = /var/lib/cinder</span><br><span class="line"></span><br><span class="line">enabled_backends = lvm,nfs</span><br><span class="line"></span><br><span class="line">glance_api_servers = http://rocky-controller:9292</span><br><span class="line"></span><br><span class="line">[lvm]部分增加如下选项：</span><br><span class="line"></span><br><span class="line">volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver</span><br><span class="line"></span><br><span class="line">iscsi_helper = lioadm</span><br><span class="line"></span><br><span class="line">iscsi_protocol = iscsi</span><br><span class="line"></span><br><span class="line">volume_group = cinder_kklvm01</span><br><span class="line"></span><br><span class="line">iscsi_ip_address = 10.28.101.81</span><br><span class="line"></span><br><span class="line">volumes_dir = <span class="variable">$state_path</span>/volumes</span><br><span class="line"></span><br><span class="line">volume_backend_name = kklvm01</span><br><span class="line"></span><br><span class="line">[nfs]部分增加如下选项：</span><br><span class="line"></span><br><span class="line">volume_driver = cinder.volume.drivers.nfs.NfsDriver</span><br><span class="line"></span><br><span class="line">nfs_shares_config = /etc/cinder/nfs_shares</span><br><span class="line"></span><br><span class="line">nfs_mount_point_base = <span class="variable">$state_path</span>/mnt</span><br><span class="line"></span><br><span class="line">volume_backend_name = kknfs01</span><br></pre></td></tr></table></figure><p>12）配置开机启动cinder-volume服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-cinder-volume.service target.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-cinder-volume.service target.service</span><br></pre></td></tr></table></figure><h2 id="安装barbican服务"><a href="#安装barbican服务" class="headerlink" title="安装barbican服务"></a><strong>安装barbican服务</strong></h2><p><strong>控制节点：</strong></p><p>1）创建barbican数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE barbican;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON barbican.* TO <span class="string">'barbican'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'barbican'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON barbican.* TO <span class="string">'barbican'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'barbican'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建barbican用户、添加角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password-prompt barbican</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user barbican admin</span><br><span class="line"></span><br><span class="line">openstack role create creator</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user barbican creator</span><br><span class="line"></span><br><span class="line">openstack service create --name barbican --description <span class="string">"Key Manager"</span> key-manager</span><br></pre></td></tr></table></figure><p>4）创建barbican服务API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne key-manager public http://rocky-controller:9311</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne key-manager internal http://rocky-controller:9311</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne key-manager admin http://rocky-controller:9311</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-barbican-api -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/barbican/barbican.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/barbican/barbican.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">sql_connection = mysql+pymysql://barbican:barbican@rocky-controller/barbican</span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">db_auto_create = False</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]部分添加如下选项：</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = barbican</span><br><span class="line"></span><br><span class="line">password = barbican</span><br></pre></td></tr></table></figure><p>7）初始化数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su -s /bin/sh -c <span class="string">"barbican-manage db upgrade"</span> barbican</span><br></pre></td></tr></table></figure><p>8）创建/etc/httpd/conf.d/wsgi-barbican.conf文件，添加如下内容：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">Listen 9311</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">VirtualHost</span> *<span class="attr">:9311</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  #ServerName rocky-controller</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  ## Logging</span><br><span class="line"></span><br><span class="line">  ErrorLog "/var/log/httpd/barbican_wsgi_main_error_ssl.log"</span><br><span class="line"></span><br><span class="line">  LogLevel debug</span><br><span class="line"></span><br><span class="line">  ServerSignature Off</span><br><span class="line"></span><br><span class="line">  CustomLog "/var/log/httpd/barbican_wsgi_main_access_ssl.log" combined</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  WSGIApplicationGroup %&#123;GLOBAL&#125;</span><br><span class="line"></span><br><span class="line">  WSGIDaemonProcess barbican-api display-name=barbican-api group=barbican processes=2 threads=8 user=barbican</span><br><span class="line"></span><br><span class="line">  WSGIProcessGroup barbican-api</span><br><span class="line"></span><br><span class="line">  WSGIScriptAlias / "/usr/lib/python2.7/site-packages/barbican/api/app.wsgi"</span><br><span class="line"></span><br><span class="line">  WSGIPassAuthorization On</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">Directory</span> /<span class="attr">usr</span>/<span class="attr">lib</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">IfVersion</span> &gt;</span>= 2.4&gt;</span><br><span class="line"></span><br><span class="line">      Require all granted</span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    &lt;IfVersion &lt; 2.4&gt;</span><br><span class="line"></span><br><span class="line">      Order allow,deny</span><br><span class="line"></span><br><span class="line">      Allow from all</span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">IfVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;/<span class="name">Directory</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">VirtualHost</span>&gt;</span></span><br></pre></td></tr></table></figure><p>9）重启web服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>10）配置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-barbican-api.service &amp;&amp; systemctl restart openstack-barbican-api.service &amp;&amp; systemctl status openstack-barbican-api.service</span><br></pre></td></tr></table></figure><p>11）安装barbican客户端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install python-barbicanclient -y</span><br></pre></td></tr></table></figure><p>12）使用OpenStack CLI存储密钥：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack secret store --name kksecret --payload j4=]d21</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/W85zPGbB7hO4.png?imageslim" alt="mark"></p><p>13）通过检索来确认机密已存储：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack secret get http://localhost:9311/v1/secrets/ba3380a4-bbb6-4666-bba2-6c1390478eb3</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/f5wUTQvEhaBt.png?imageslim" alt="mark"></p><p>14）检索秘钥的有效载荷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack secret get http://localhost:9311/v1/secrets/ba3380a4-bbb6-4666-bba2-6c1390478eb3 --payload</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/3jKKxScdOxkr.png?imageslim" alt="mark"></p><h2 id="安装Mistral服务"><a href="#安装Mistral服务" class="headerlink" title="安装Mistral服务"></a><strong>安装Mistral服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装依赖软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python-devel python-setuptools python-pip libffi-devel libxslt-devel libxml2-devel libyaml-devel openssl-devel</span><br></pre></td></tr></table></figure><p>2）安装Mistra软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install openstack-mistral-api.noarch openstack-mistral-engine.noarch openstack-mistral-executor.noarch openstack-mistral-ui.noarch</span><br></pre></td></tr></table></figure><p>3）创建数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE mistral;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON mistral.* TO <span class="string">'mistral'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'mistral'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON mistral.* TO <span class="string">'mistral'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'mistral'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>4）创建用户、添加角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password=mistral mistral</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user mistral admin</span><br><span class="line"></span><br><span class="line">openstack service create --name mistral --description <span class="string">'OpenStack Workflow service'</span> workflowv2</span><br></pre></td></tr></table></figure><p>5）创建Mistral服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne workflowv2 public http://rocky-controller:8989/v2</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne workflowv2 internal http://rocky-controlle:8989/v2</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne workflowv2 admin http://rocky-controlle:8989/v2</span><br></pre></td></tr></table></figure><p>6）编辑配置/etc/mistral/mistral.conf文件，添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/mistral/mistral.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]</span><br><span class="line"></span><br><span class="line">debug = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">auth_type = keystone</span><br><span class="line"></span><br><span class="line">rpc_backend = rabbit</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line"></span><br><span class="line">connection = mysql+pymysql://mistral:mistral@rocky-controller/mistral</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = default</span><br><span class="line"></span><br><span class="line">user_domain_name = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = mistral</span><br><span class="line"></span><br><span class="line">password = mistral</span><br><span class="line"></span><br><span class="line">[oslo_messaging_rabbit]</span><br><span class="line"></span><br><span class="line">rabbit_host = rocky-controller</span><br><span class="line"></span><br><span class="line">rabbit_port = 5672</span><br><span class="line"></span><br><span class="line">rabbit_hosts = <span class="variable">$rabbit_host</span>:<span class="variable">$rabbit_port</span></span><br><span class="line"></span><br><span class="line">rabbit_userid = openstack</span><br><span class="line"></span><br><span class="line">rabbit_password = openstack</span><br></pre></td></tr></table></figure><p>7）初始化数据库，添加缺省项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mistral-db-manage --config-file /etc/mistral/mistral.conf upgrade head</span><br><span class="line"></span><br><span class="line">mistral-db-manage --config-file /etc/mistral/mistral.conf populate（报错请忽略）</span><br></pre></td></tr></table></figure><p>8）配置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.service</span><br><span class="line"></span><br><span class="line">systemctl restart openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.service</span><br><span class="line"></span><br><span class="line">systemctl status openstack-mistral-api.service openstack-mistral-engine.service openstack-mistral-executor.service</span><br><span class="line"></span><br><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>9）启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mistral-server --server api,engine,executor,notifier --config-file /etc/mistral/mistral.conf</span><br></pre></td></tr></table></figure><h2 id="安装tacker服务"><a href="#安装tacker服务" class="headerlink" title="安装tacker服务"></a><strong>安装tacker服务</strong></h2><p><strong>控制节点：</strong></p><p>1）修改admin-openrc.sh文件如下，增加图中画红框的部分</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/yP3CjMxP5kTN.png?imageslim" alt="mark"></p><p>2）创建tacker数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE tacker;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON tacker.* TO <span class="string">'tacker'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'tacker'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON tacker.* TO <span class="string">'tacker'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'tacker'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>3）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>4）创建tacker用户、角色，服务实体和端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password tacker tacker</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user tacker admin</span><br><span class="line"></span><br><span class="line">openstack service create --name tacker --description <span class="string">"Tacker Project"</span> nfv-orchestration</span><br></pre></td></tr></table></figure><p>5）创建tacker服务API的端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne nfv-orchestration public http://rocky-controller:9890/</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne nfv-orchestration internal http://rocky-controller:9890/</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne nfv-orchestration admin http://rocky-controller:9890/</span><br></pre></td></tr></table></figure><p>6）从github上下载源码，编译安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /app</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /app</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/tacker -b stable/rocky</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> tacker/</span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><p>7）创建日志目录和配置目录，生成服务配置文件并进行修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/<span class="built_in">log</span>/tacker</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/tacker</span><br><span class="line"></span><br><span class="line">. tools/generate_config_file_sample.sh</span><br><span class="line"></span><br><span class="line">cp -rf /app/tacker/etc/tacker/* /etc/tacker/</span><br><span class="line"></span><br><span class="line">cp /app/tacker/etc/tacker/tacker.conf.sample /etc/tacker/tacker.conf</span><br><span class="line"></span><br><span class="line">\<span class="comment"># 修改/etc/tacker/tacker.conf文件如下：</span></span><br><span class="line"></span><br><span class="line">[DEFAULT]</span><br><span class="line"></span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">policy_file = /etc/tacker/policy.json</span><br><span class="line"></span><br><span class="line">debug = True</span><br><span class="line"></span><br><span class="line">use_syslog = False</span><br><span class="line"></span><br><span class="line">bind_host = 10.28.101.81</span><br><span class="line"></span><br><span class="line">bind_port = 9890</span><br><span class="line"></span><br><span class="line">service_plugins = nfvo,vnfm</span><br><span class="line"></span><br><span class="line">state_path = /var/lib/tacker</span><br><span class="line"></span><br><span class="line">[nfvo_vim]</span><br><span class="line"></span><br><span class="line">vim_drivers = openstack</span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line"></span><br><span class="line">memcached_servers = 11211</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_name = Default</span><br><span class="line"></span><br><span class="line">user_domain_name = Default</span><br><span class="line"></span><br><span class="line">username = tacker</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">password = tacker</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http:/rocky-controller:5000</span><br><span class="line"></span><br><span class="line">[agent]</span><br><span class="line"></span><br><span class="line">root_helper = sudo /usr/bin/tacker-rootwrap /etc/tacker/rootwrap.conf</span><br><span class="line"></span><br><span class="line">[database]</span><br><span class="line"></span><br><span class="line">connection = mysql+pymysql://tacker:tacker@rocky-controller:3306/tacker?charset=utf8</span><br><span class="line"></span><br><span class="line">[tacker]</span><br><span class="line"></span><br><span class="line">monitor_driver = ping,http_ping</span><br></pre></td></tr></table></figure><p>8）初始化tacker数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/tacker-db-manage --config-file /etc/tacker/tacker.conf upgrade head</span><br></pre></td></tr></table></figure><p>9）将tacker.service and tacker-conductor.service复制到/etc/systemd/system/下，并重启systemctl守护进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp /app/tacker/etc/systemd/system/tacker.service /etc/systemd/system/</span><br><span class="line"></span><br><span class="line">cp /app/tacker/etc/systemd/system/tacker-conductor.service /etc/systemd/system/</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure><p>10）源码方式安装tacker客户端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /app</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/python-tackerclient -b stable/rocky</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> python-tackerclient</span><br><span class="line"></span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><p>11）安装tacker的dashboard</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /app</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/tacker-horizon -b stable/rocky</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> tacker-horizon</span><br><span class="line"></span><br><span class="line">python setup.py install</span><br><span class="line"></span><br><span class="line">cp tacker_horizon/enabled/* /usr/share/openstack-dashboard/openstack_dashboard/enabled/</span><br><span class="line"></span><br><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>12）新开两个终端（或者让进程在后台运行也可以，我的选择），分别运行tacker的服务和执行体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tacker-server --config-file /etc/tacker/tacker.conf --<span class="built_in">log</span>-file /var/<span class="built_in">log</span>/tacker/takcer.log &amp;</span><br><span class="line"></span><br><span class="line">tacker-conductor --config-file /etc/tacker/tacker.conf --<span class="built_in">log</span>-file /var/<span class="built_in">log</span>/tacker/tacker-conductor.log &amp;</span><br></pre></td></tr></table></figure><p>13）创建vim_config.yaml文件，注册OPS为默认的VIM</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim vim_config.yaml</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/5d1Ys1Y66GUh.png?imageslim" alt="mark"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack vim register --config-file vim_config.yaml --description <span class="string">'kk first vim'</span> --is-default kkvim</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/IBqC7974EMp9.png?imageslim" alt="mark"></p><p>14）验证VIM是否注册成功</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/4vErpVTDDwC2.png?imageslim" alt="mark"></p><p>15）创建简单实例VNFD模版文件和VNFD文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim sample_vnfd.yaml</span><br><span class="line"></span><br><span class="line">openstack vnf descriptor create --vnfd-file sample_vnfd.yaml kk-first-vnfd</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/2ettC6ndmpTt.png?imageslim" alt="mark"></p><p>图太长，未截取全</p><p>16）创建VNF</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack vnf create --vnfd-name kk-first-vnfd sample_vnf01</span><br></pre></td></tr></table></figure><h2 id="安装aodh服务"><a href="#安装aodh服务" class="headerlink" title="安装aodh服务"></a><strong>安装aodh服务</strong></h2><p><strong>控制节点：</strong></p><p>1）安装aodh数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE aodh;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON aodh.* TO <span class="string">'aodh'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'aodh'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON aodh.* TO <span class="string">'aodh'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'aodh'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>2）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>3）创建aodh用户、角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password aodh aodh</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user aodh admin</span><br><span class="line"></span><br><span class="line">openstack service create --name aodh --description <span class="string">"Telemetry"</span> alarming</span><br></pre></td></tr></table></figure><p>4）创建aodh服务的API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne alarming public http://rocky-controller:8042</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne alarming internal http://rocky-controller:8042</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne alarming admin http://rocky-controller:8042</span><br></pre></td></tr></table></figure><p>5）安装软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-aodh-api openstack-aodh-evaluator openstack-aodh-notifier openstack-aodh-listener openstack-aodh-expirer python-aodhclient -y</span><br></pre></td></tr></table></figure><p>6）编辑/etc/aodh/aodh.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/aodh/aodh.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]</span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">[database]</span><br><span class="line"></span><br><span class="line">connection = mysql+pymysql://aodh:aodh@rocky-controller/aodh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">project_domain_id = default</span><br><span class="line"></span><br><span class="line">user_domain_id = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = aodh</span><br><span class="line"></span><br><span class="line">password = aodh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[service_credentials]</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line"></span><br><span class="line">project_domain_id = default</span><br><span class="line"></span><br><span class="line">user_domain_id = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = aodh</span><br><span class="line"></span><br><span class="line">password = aodh</span><br><span class="line"></span><br><span class="line">interface = internalURL</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br></pre></td></tr></table></figure><p>7）初始化aodh数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aodh-dbsync</span><br></pre></td></tr></table></figure><p>8）配置开机启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-aodh-api.service openstack-aodh-evaluator.service openstack-aodh-notifier.service openstack-aodh-listener.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-aodh-api.service openstack-aodh-evaluator.service openstack-aodh-notifier.service openstack-aodh-listener.service</span><br></pre></td></tr></table></figure><h2 id="安装ceilometer服务"><a href="#安装ceilometer服务" class="headerlink" title="安装ceilometer服务"></a><strong>安装ceilometer服务</strong></h2><h3 id="控制节点"><a href="#控制节点" class="headerlink" title="控制节点"></a><strong>控制节点</strong></h3><p>1）获取管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>2）创建ceilometer用户、角色：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password ceilometer ceilometer</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user ceilometer admin</span><br></pre></td></tr></table></figure><p>3）创建gnocchi用户、角色和服务实体：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack user create --domain default --password gnocchi gnocchi</span><br><span class="line"></span><br><span class="line">openstack service create --name gnocchi --description <span class="string">"Metric Service"</span> metric</span><br><span class="line"></span><br><span class="line">openstack role add --project service --user gnocchi admin</span><br></pre></td></tr></table></figure><p>4）创建Metric服务API端点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">openstack endpoint create --region RegionOne metric public http://rocky-controller:8041</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne metric internal http://rocky-controller:8041</span><br><span class="line"></span><br><span class="line">openstack endpoint create --region RegionOne metric admin http://rocky-controller:8041</span><br></pre></td></tr></table></figure><p>5）安装gnocchi软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-gnocchi-api openstack-gnocchi-metricd python-gnocchiclient -y</span><br></pre></td></tr></table></figure><p>6）创建gnocchi数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line">CREATE DATABASE gnocchi;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON gnocchi.* TO <span class="string">'gnocchi'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'gnocchi'</span>;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON gnocchi.* TO <span class="string">'gnocchi'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'gnocchi'</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span>；</span><br></pre></td></tr></table></figure><p>7）编辑/etc/gnocchi/gnocchi.conf文件变添加如下选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置gnocchi功能参数，log地址以及对接redis url端口</span></span><br><span class="line"></span><br><span class="line">[DEFAULT]</span><br><span class="line"></span><br><span class="line">debug = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">verbose = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">log_dir = /var/<span class="built_in">log</span>/gnocchi</span><br><span class="line"></span><br><span class="line">parallel_operations = 4</span><br><span class="line"></span><br><span class="line">coordination_url = redis://rocky-controller:6379</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置gnocchi工作端口信息，host为控制节点管理IP</span></span><br><span class="line"></span><br><span class="line">[api]</span><br><span class="line"></span><br><span class="line">auth_mode = keystone</span><br><span class="line"></span><br><span class="line">host = 10.28.101.81</span><br><span class="line"></span><br><span class="line">port = 8041</span><br><span class="line"></span><br><span class="line">uwsgi_mode = http-socket</span><br><span class="line"></span><br><span class="line">max_limit = 1000</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置ceilometer默认收集测试指标策略</span></span><br><span class="line"></span><br><span class="line">[archive_policy]</span><br><span class="line"></span><br><span class="line">default_aggregation_methods = mean,min,max,sum,std,count</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置允许的访问来源，这里是grafana的地址，允许前端直接访问gnocchi获取计量数据，需要配置允许跨域访问（Keystone中完成）</span></span><br><span class="line"></span><br><span class="line">[cors]</span><br><span class="line"></span><br><span class="line">allowed_origin = http://rocky-controller:3000</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置keystone认证信息，该模块需要另外添加</span></span><br><span class="line"></span><br><span class="line">[keystone_authtoken]</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">www_authenticate_uri = http://rocky-controller:5000</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line"></span><br><span class="line">memcached_servers = rocky-controller:11211</span><br><span class="line"></span><br><span class="line">project_domain_name = Default</span><br><span class="line"></span><br><span class="line">user_domain_name = Default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = gnocchi</span><br><span class="line"></span><br><span class="line">password = gnocchi</span><br><span class="line"></span><br><span class="line">interface = internalURL</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br><span class="line">service_token_roles_required = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置元数据默认存储方式。</span></span><br><span class="line"></span><br><span class="line">[indexer]</span><br><span class="line"></span><br><span class="line">url = mysql+pymysql://gnocchi:gnocchi@rocky-controller/gnocchi</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置gnocchi存储方式以及位置，在这种配置下将其存储到本地文件系统。</span></span><br><span class="line"></span><br><span class="line">[storage]</span><br><span class="line"></span><br><span class="line">coordination_url = redis://rocky-controller:6379</span><br><span class="line"></span><br><span class="line">file_basepath = /var/lib/gnocchi</span><br><span class="line"></span><br><span class="line">driver = file</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置数据库检索的策略</span></span><br><span class="line"></span><br><span class="line">[metricd]</span><br><span class="line"></span><br><span class="line">workers = 4</span><br><span class="line"></span><br><span class="line">metric_processing_delay = 60</span><br><span class="line"></span><br><span class="line">greedy = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">metric_reporting_delay = 120</span><br><span class="line"></span><br><span class="line">metric_cleanup_delay = 300</span><br></pre></td></tr></table></figure><p>8）安装redis软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install redis -y</span><br></pre></td></tr></table></figure><p>9）编辑/etc/redis.conf ，修改以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置redis可以在后台启动：daemonize yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置redis关闭安全模式：protected-mode no</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置redis绑定控制节点主机：bind 10.28.101.81</span></span><br></pre></td></tr></table></figure><p>10）启动redis-server，设置开机启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">redis-server /etc/redis.conf</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"redis-server /etc/redis.conf"</span> &gt;&gt; /etc/rc.local</span><br><span class="line"></span><br><span class="line">chmod a+x /etc/rc.local</span><br></pre></td></tr></table></figure><p>11）安装uwsgi插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install uwsgi-plugin-common uwsgi-plugin-python uwsgi -y</span><br></pre></td></tr></table></figure><p>12）赋予/var/lib/gnocchi文件可读写权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R 777 /var/lib/gnocchi</span><br></pre></td></tr></table></figure><p>13）初始化gnocchi数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gnocchi-upgrade</span><br></pre></td></tr></table></figure><p>14）配置gnocchi服务开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-gnocchi-api.service openstack-gnocchi-metricd.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-gnocchi-api.service openstack-gnocchi-metricd.service</span><br><span class="line"></span><br><span class="line">systemctl status openstack-gnocchi-api.service openstack-gnocchi-metricd.service</span><br></pre></td></tr></table></figure><p>15）安装ceilometer软件包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-ceilometer-notification openstack-ceilometer-central -y</span><br></pre></td></tr></table></figure><p>16）编辑/etc/ceilometer/pipeline.yaml文件并完成以下gnocchi配置：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Yic641begQet.png?imageslim" alt="mark"></p><p>17）编辑/etc/ceilometer/ceilometer.conf文件并完成以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/ceilometer/ceilometer.conf&#123;,.bak&#125;</span><br><span class="line"></span><br><span class="line">[DEFAULT]</span><br><span class="line"></span><br><span class="line">debug = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">auth_strategy = keystone</span><br><span class="line"></span><br><span class="line">transport_url = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line">pipeline_cfg_file = pipeline.yaml</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">[service_credentials]</span><br><span class="line"></span><br><span class="line">auth_type = password</span><br><span class="line"></span><br><span class="line">auth_url = http://rocky-controller:5000/v3</span><br><span class="line"></span><br><span class="line">project_domain_id = default</span><br><span class="line"></span><br><span class="line">user_domain_id = default</span><br><span class="line"></span><br><span class="line">project_name = service</span><br><span class="line"></span><br><span class="line">username = ceilometer</span><br><span class="line"></span><br><span class="line">password = ceilometer</span><br><span class="line"></span><br><span class="line">interface = internalURL</span><br><span class="line"></span><br><span class="line">region_name = RegionOne</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">[notification]</span><br><span class="line"></span><br><span class="line">store_events = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">messaging_urls = rabbit://openstack:openstack@rocky-controller</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">[polling]</span><br><span class="line"></span><br><span class="line">cfg_file = polling.yaml</span><br></pre></td></tr></table></figure><p>13）初始化数据库，在Gnocchi上创建资源，要求Gnocchi已运行并在Keystone配置了endpoint。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceilometer-upgrade</span><br></pre></td></tr></table></figure><p>14）配置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-ceilometer-notification.service openstack-ceilometer-central.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-ceilometer-notification.service openstack-ceilometer-central.service</span><br></pre></td></tr></table></figure><h3 id="计算节点"><a href="#计算节点" class="headerlink" title="计算节点"></a>计算节点</h3><p>1）安装软件包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install openstack-ceilometer-compute openstack-ceilometer-ipmi -y</span><br></pre></td></tr></table></figure><p>2）编辑/etc/ceilometer/ceilometer.conf文件并完成以下操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用scp指令将控制节点的文件拷贝过来即可</span></span><br></pre></td></tr></table></figure><p>3）编辑/etc/nova/nova.conf文件并在以下[DEFAULT]部分配置通知：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line"></span><br><span class="line">instance_usage_audit = True</span><br><span class="line"></span><br><span class="line">instance_usage_audit_period = hour</span><br><span class="line"></span><br><span class="line">[notifications]</span><br><span class="line"></span><br><span class="line">notify_on_state_change = vm_and_task_state</span><br><span class="line"></span><br><span class="line">notification_format = unversioned</span><br><span class="line"></span><br><span class="line">[oslo_messaging_notifications]</span><br><span class="line"></span><br><span class="line">driver = messagingv2</span><br></pre></td></tr></table></figure><p>4）（可选）配置轮询ipmi（虚拟机方式下用不到）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编辑/etc/sudoers文件并包含：</span></span><br><span class="line"></span><br><span class="line"><span class="string">ceilometer</span> <span class="string">ALL</span> <span class="string">=</span> <span class="string">(root)</span> <span class="attr">NOPASSWD:</span> <span class="string">/usr/bin/ceilometer-rootwrap</span> <span class="string">/etc/ceilometer/rootwrap.conf</span> <span class="string">*</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编辑/etc/ceilometer/polling.yaml，添加以下度量项目（注意格式对齐）：</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- name:</span> <span class="string">ipmi</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> interval:</span> <span class="number">300</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> meters:</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">   -</span> <span class="string">hardware.ipmi.temperature</span></span><br></pre></td></tr></table></figure><p>5）完成计算节点安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> openstack-ceilometer-compute.service openstack-ceilometer-ipmi.service</span><br><span class="line"></span><br><span class="line">systemctl start openstack-ceilometer-compute.service openstack-ceilometer-ipmi.service</span><br><span class="line"></span><br><span class="line">systemctl restart openstack-nova-compute.service</span><br></pre></td></tr></table></figure><p><em>———————————————————————-结束—————————————————————————–</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇博文记录了在VMware Workstations上手动部署双节点OpenStack Rocky发行版本的部署笔记，包括OpenStack的9大基础核心组件：Keystone、Glance、Nova、Neutron、Cinder、Swift、Heat和Telemetry等服务，以及开源NFVO/VNFM项目tacker，同时将本地OpenStack的环境注册为VIM，构造一个开源的MANO环境。其中，9大核心组件的Swift在对象存储管理服务—Swift文中介绍部署方法，通过单节点双硬盘的方式模拟vNode，同时模拟实际网络云环境中对象存储主要用于数据备份和镜像存储的功能。
    
    </summary>
    
      <category term="OpenStack" scheme="https://kkutysllb.cn/categories/OpenStack/"/>
    
    
      <category term="云计算" scheme="https://kkutysllb.cn/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-KVM虚拟机网络管理实战</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM虚拟机网络管理实战/</id>
    <published>2020-03-01T11:45:16.000Z</published>
    <updated>2020-03-01T11:58:46.340Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章我们介绍完KVM的存储管理实战后，接下来我们本篇文章就开始主要介绍KVM的网络管理实战。KVM的网络管理模型与OpenStack Neutron中的网络管理非常类似，而在学些OpenStack时，网络服务Neutron往往是很多人初学OpenStack的一个难点。因此，掌握了本篇内容的知识点，对于后续学习OpenStack Neutron部分的内容有事半功倍的效果。<a id="more"></a></p><h2 id="virsh中的网络管理基础"><a href="#virsh中的网络管理基础" class="headerlink" title="virsh中的网络管理基础"></a><strong>virsh中的网络管理基础</strong></h2><p>在介绍KVM中常见的网络模型之前，我们先来看看virsh有哪些网络管理的命令。与存储管理实战和全生命周期管理实战类似，virsh主要提供对节点上的物理网络接口和分配给虚拟机的虚拟网络进行管理的命令。包括：创建节点上的物理接口、编辑节点上物理接口的XML配置文件，查询节点上物理接口、创建虚拟机的虚拟网络、编辑虚拟机的虚拟网络和删除虚拟机的虚拟网络等。常用的命令和作用如下：</p><table><thead><tr><th><strong>virsh网络管理常用命令</strong></th><th></th></tr></thead><tbody><tr><td><strong>命令</strong></td><td><strong>功能描述</strong></td></tr><tr><td>iface-list</td><td>显示出物理主机的网络接口列表</td></tr><tr><td>iface-list<if-name></if-name></td><td>根据网络接口名称查询其对应的MAC地址</td></tr><tr><td>iface-name<mac></mac></td><td>根据MAC地址查询其对应的网络接口名称</td></tr><tr><td>iface-edit<if-name or uuid></if-name></td><td>编辑一个物理主机的网络接口XML配置文件</td></tr><tr><td>iface-dumpxml<ifa-name or uuid></ifa-name></td><td>以XML配置文件转存出一个网络接口的状态信息</td></tr><tr><td>iface-destroy<if-name or uuid></if-name></td><td>关闭宿主机上的一个物理网络接口</td></tr><tr><td>net-list</td><td>列出libvirt管理的虚拟网络</td></tr><tr><td>net-info<net-name or uuid></net-name></td><td>根据名称查询一个虚拟网络的基本信息</td></tr><tr><td>net-uuid<net-name></net-name></td><td>根据名称查询一个虚拟网络的uuid</td></tr><tr><td>net-name<uuid></uuid></td><td>根据uuid查询一个虚拟网络的名称</td></tr><tr><td>net-create&lt;net.xml&gt;</td><td>根据一个网络的xml配置文件创建一个虚拟网络</td></tr><tr><td>net-edit<net-name or uuid></net-name></td><td>编译一个虚拟网络的XML配置文件</td></tr><tr><td>net-dumpxml<net-name or uuid></net-name></td><td>转存出一个虚拟网络的XML配置文件</td></tr><tr><td>net-destroy<net-name or uuid></net-name></td><td>销毁一个虚拟网络</td></tr></tbody></table><p>有了上面的命令基础，我们就可以几个简单验证。比如，我们现在想看下当前节点有哪几个物理接口，可以使用如下命令：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/gVK09BVOIdwD.png?imageslim" alt="mark"></p><p>现在，我们想把br0这个网桥的XML文件导出转存，可以使用下面的命令的实现。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/0iaHWueTUvoG.png?imageslim" alt="mark"></p><p>以后就可以按照上面的xml配置文件，写一个网桥，然后通过iface-define定义，然后再通过iface-start命令启动即可（需要注意MAC不能重复，可以使用random工具随机生成MAC）。</p><p>我们完成物理接口的简单验证后，接下来看看虚拟网络的信息。比如，我们现在需要看下当前节点有哪些虚拟网络，可以通过net-list命令实现。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/dfGCJ0LSoUYi.png?imageslim" alt="mark"></p><p>上图表示当前节点只有一个虚拟网络default，我们通过net-info命令可以看下default网络的详细信息。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/mUC2RgBvA61F.png?imageslim" alt="mark"></p><p>同理，我们也可以将default网络的xml配置转存，便于后续自己编写网络xml文件。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/luc8wwkG2E9M.png?imageslim" alt="mark"></p><h2 id="KVM中常见的网络模型"><a href="#KVM中常见的网络模型" class="headerlink" title="KVM中常见的网络模型"></a><strong>KVM中常见的网络模型</strong></h2><p>有了上面基础认知，下面我们就来看下KVM中常见的4种简单网络模型，分别如下：</p><ul><li><strong>隔离模型：</strong>虚拟机之间组建网络，该模式无法与宿主机通信，无法与其他网络通信，相当于虚拟机只是连接到一台交换机上。其对应OpenStack Neutron中local网络模型。</li><li><strong>路由模型：</strong>相当于虚拟机连接到一台路由器上，由路由器(物理网卡)，统一转发，但是不会改变源地址。</li><li><strong>NAT模型：</strong>在路由模式中，会出现虚拟机可以访问其他主机，但是其他主机的报文无法到达虚拟机，而NAT模式则将源地址转换为路由器(物理网卡)地址，这样其他主机也知道报文来自那个主机，在docker环境中经常被使用。</li><li><strong>桥接模型：</strong>在宿主机中创建一张虚拟网卡作为宿主机的网卡，而物理网卡则作为交换机。</li></ul><p>为了加深大家的理解，我们后面就对上述4种网络模型逐一进行验证。</p><h3 id="隔离网络模型"><a href="#隔离网络模型" class="headerlink" title="隔离网络模型"></a><strong>隔离网络模型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/SrpowLwtXmWy.png?imageslim" alt="mark"></p><p>如上图所示，VM0和VM1都是在宿主机上创建的虚拟机，虚拟机的网卡分为前半段和后半段，前半段位于虚拟机上，后半段在宿主机上，按照图中所示，前半段就是eth0，它是在虚拟机内部看到的网卡名字，而后半段就是vnet0和vnet1，它们是在宿主机上看到的网卡名字。实际上，在VM1上所有发往eth0的数据就是直接发往vnet0，是由vnet0进行数据的传送处理。</p><p>在隔离模式下，宿主机创建一个虚拟交换机vSwitch，然后把vnet0和vnet1接入到该虚拟交换机，交换机也可以叫做bridge，因为vnet0和vnet1在一个网桥内，所以可以互相通信，而虚拟机的eth0是通过后半段进行数据传输，所以只要虚拟机的前半段ip在一个网段内，就可以互相通信，这就是隔离模式。下面，我们就通过实例进行验证。</p><p><strong>Step1：</strong>创建一个网桥br1，且不连接宿主机的任何物理网卡。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/EzWbRhBqFATv.png?imageslim" alt="mark"></p><p><strong>Step2：</strong>我们用centos7.5模板虚拟机镜像直接迅速拉起两个测试虚拟机VM0和VM1。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/5SVFqJ1DhjcC.png?imageslim" alt="mark"></p><p>由于我们没有给网桥配置IP和DHCP分配范围，所以虚拟机没有IP地址。我们可以手动给虚拟机添加IP，VM0的虚拟机地址设置为10.10.101.251。如下</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/NECwB0SOLTDU.png?imageslim" alt="mark"></p><p>同理，给虚拟机VM1设置IP为192.168.101.252。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/OkdaimNVRbpg.png?imageslim" alt="mark"></p><p>此时，我们对VM0与VM1进行ping测试，是可以ping通的。但是，即使两个虚拟机与宿主机（10.10.101.11）在同一个网段，仍然无法ping通。所以，这是一个隔离网络模型。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/POfTTS9vwGVE.png?imageslim" alt="mark"></p><p><strong>Step3：</strong>此时，我们在宿主机上查询网桥br1的挂接信息和虚拟机的虚拟网卡vnet信息。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/jgiBYgnoTbLS.png?imageslim" alt="mark"></p><h3 id="路由网络模型"><a href="#路由网络模型" class="headerlink" title="路由网络模型"></a><strong>路由网络模型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/w0qFPF9nA6dq.png?imageslim" alt="mark"></p><p>在隔离模型的基础上，将宿主机的一块虚拟网卡virnet0加入到虚拟网桥中，这样virnet0就可以和虚拟机通信，通过将虚拟机的默认网关设置为virnet0的IP地址，然后在宿主机中打开IP地址转发，使得虚拟机可以访问宿主机。不过此时虚拟机仅仅可以将报文发送到外部网络，因为外部网络没有路由到虚拟机中，所以外部网络无法将报文回传给虚拟机。</p><p><strong>step1：</strong>在宿主机上用tunctl创建一个虚拟网卡，也就是创建一个tap设备virnet0。忘了tunctl是什么的，请回顾本站Linux原生网络虚拟化内容。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/q5fU8IdAdJf3.png?imageslim" alt="mark"></p><p><strong>step2：</strong>将virnet0加入到网桥br1中，并设置网桥br1为网关，地址为10.10.101.100</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/PP5pKwmcBg5o.png?imageslim" alt="mark"></p><p><strong>step3：</strong>进入虚拟机设置网关为10.10.101.100，也就是配置一条默认路由即可。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/D6O3Kt4h5vjL.png?imageslim" alt="mark"></p><p><strong>step4：</strong>在宿主机中打开ip包转发功能，也就是将宿主机变成一个路由器（详见本站Linux原生网络虚拟化文章内容）。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/qa93buGjEe2M.png?imageslim" alt="mark"></p><p><strong>step5：</strong>此时，我们在虚拟机中尝试ping宿主机，发现可以ping通。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/6AObAhv1AEve.png?imageslim" alt="mark"></p><p>但是，我们无法ping通宿主机的网关，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/V0DB9o8EMXms.png?imageslim" alt="mark"></p><p>这是因为报文发到网关后，网关找不到回包的路由，所以报文无法回复，这时候，我们通过在宿主机上添加一条iptables规则，使得网关可以回包。至于，iptables如何配置，请参见本站Linux常用运维工具分类中的iptable文章。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -s 10.10.101.0/24 -j MASQUERADE</span><br></pre></td></tr></table></figure><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/trsOmmwdBEtg.png?imageslim" alt="mark"></p><p>添加包转发规则后，就可以在虚拟机中ping通宿主机网关了。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/OlQrTgXAN2xI.png?imageslim" alt="mark"></p><p><strong>step6：</strong>但是，我们在宿主机外的机器上仍无法ping通虚拟机。比如，我们在真实的物理机（win10）上还是无法ping通虚拟机10.10.101.251。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/1SNgPBy8qebz.png?imageslim" alt="mark"></p><p>为了解决外部主机ping通虚拟机的问题，我们需要win10上也添加一条路由，将访问虚拟机的数据包发往VMware的虚拟网卡地址192.168.101.11上。完成后，再次进行尝试，发现网络已通，且可以通过win10直接ssh虚拟机vm0。如下：（<strong>这里注意必须指定VMware NAT网卡的地址段，因为在VMware内部只有NAT虚拟网卡有三层转发功能）</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/wN30Lv2NpYPq.png?imageslim" alt="mark"></p><p>通过上述实践，也可以发现路由模型的缺陷，虽然虚拟机能同宿主机通信，也能将数据包发到外部网络，但是外部网络无法回传数据包，要想外部网络能与虚拟机通信，就要添加对应的路由规则。这对于大规模的虚拟环境，这显然是不科学的。</p><h3 id="NAT网络模型"><a href="#NAT网络模型" class="headerlink" title="NAT网络模型"></a><strong>NAT网络模型</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/flrposnswtXy.png?imageslim" alt="mark"></p><p>NAT模型其实就是SNAT的实现，路由中虚拟机能将报文发送给外部主机，但是外部主机因找不到通往虚拟机的路由因而无法回应请求。外部主机能同宿主机通信，所以在宿主机上添加一个NAT转发，从而在外部主机响应能够到达虚拟机，但是不能直接访问虚拟机。这种方式是将虚拟机的IP地址转换为宿主机上的某个地址，从而实现虚拟机与外部网络通信，其实际上只是通过iptables的nat表的POSTROUTING链实现地址转换罢了。如果要实现外部网络直接访问虚拟机，还需要在宿主机上配置DNAT转发，一般不采用这种方式。</p><p>在我们创建KVM虚拟机时，如果使用default网络类型，它就是一个NAT网络。我们可以看下它的XML配置文件内容：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/A07pnS6DlDD2.png?imageslim" alt="mark"></p><p>有了上面的了解，那我们创建一个NAT网络就很轻松了。怎么办？重新写一份？no…no…这不符合互联网时代的要求！拷贝一份，直接修改。。。。同时，我们可以在自定义的nat网络配置文件中，指定两个虚拟机的IP地址。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xXSnPT0Kim31.png?imageslim" alt="mark"></p><p>完成上面的配置后，现在定义一个网络natnet，并设置开启自动启动。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/VDaKsSv46SBd.png?imageslim" alt="mark"></p><p>完成上面配置后，libvirt会自动帮我们创建一个XML文件描述的网桥natbr0，并且将网关IP和MAC按照XML文件的配置自动生成。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/5txshsum9rL5.png?imageslim" alt="mark"></p><p>同时，libvirt会在宿主机的iptables的nat转发表中，自动帮我增加nat网络的数据包转发策略。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/58xdPKsS02zj.png?imageslim" alt="mark"></p><p>完成上面的配置，此时我们需要修改两个虚拟机XML文件的网络配置信息，然后重新定义启动。虚拟机XML配置文件网络信息修改，只需要修改接口类型为network类型，network的上行接口修改为我们自定义的网络名称即可，其它不变。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/02MFu1TNfKrt.png?imageslim" alt="mark"></p><p>然后，我们重新定义虚拟机并启动。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/dVWDaYTW2LBR.png?imageslim" alt="mark"></p><p>现在，我们进入虚拟机查询的虚拟机的IP确实为我们制定的IP地址188.64.100.2，且自动增加了默认路由指向natbr0网桥上联接口地址188.64.100.1，而且虚拟机和宿主机可以互相ping通。如下</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/XFQGKuwbmwvY.png?imageslim" alt="mark"></p><p>而且，我们在虚拟机下可以ping通外部真实物理机win10，但是从真实物理机win10下ping测虚拟机vm0却发现还是ping不通，这就是典型的SNAT原理，也就是虚拟机可以访问外部网络，但是外部网络看不见内网的虚拟机，这样对内网的虚拟机中应用起到保护作用。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Cz9sKqnSIvUB.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Nof4j5mIVeGi.png?imageslim" alt="mark"></p><p>如果，需要虚拟机访问互联网internet，需要给虚拟机中增加DNS解析，nameserver就配置成虚拟网关188.64.100.1即可。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/onq1E9vYC8TV.png?imageslim" alt="mark"></p><p>至此，nat网络模型验证完毕。四个基本模型中还有一个桥接网络模型，这也是网上常见的配置模型，这种模型就是将虚拟机与宿主机放在一个局域网内，可以互访，且同一局域网的其他外部主机也能访问虚拟机。而且，如果宿主机能上互联网，那么互联网的服务器也能访问虚拟机。除非有特殊应用场景，否则一般在实际运维不采用这种方式，因为不安全。这种方式我就懒得举例了，网上的教程很多。最后，祝大家在KVM的世界里愉快的玩耍。。。。。。嘿嘿嘿嘿。。。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章我们介绍完KVM的存储管理实战后，接下来我们本篇文章就开始主要介绍KVM的网络管理实战。KVM的网络管理模型与OpenStack Neutron中的网络管理非常类似，而在学些OpenStack时，网络服务Neutron往往是很多人初学OpenStack的一个难点。因此，掌握了本篇内容的知识点，对于后续学习OpenStack Neutron部分的内容有事半功倍的效果。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-KVM虚拟机存储管理实战（下篇）</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%8B%E7%AF%87%EF%BC%89/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM虚拟机存储管理实战（下篇）/</id>
    <published>2020-03-01T11:31:58.000Z</published>
    <updated>2020-03-01T11:43:38.667Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇中我们介绍qemu-img这个对磁盘镜像操作的命令，包括检查镜像磁盘，创建镜像磁盘、查看镜像磁盘信息、转换镜像磁盘格式、调整镜像磁盘大小以及镜像磁盘的快照链操作。但是，在镜像磁盘快照链操作中，我们也提到了通过qemu-img命令创建快照链只能在虚拟机关机状态下运行。如果虚拟机为运行态，只能通过virsh save vm来保存当前状态。那么，本文就专门讲述在KVM虚拟机中如何通过virsh命令来创建快照链，以及链中快照的相互关系，如何缩短链，如何利用这条链回滚我们的虚拟机到某个状态等等。<a id="more"></a></p><h2 id="什么是虚拟机快照链"><a href="#什么是虚拟机快照链" class="headerlink" title="什么是虚拟机快照链"></a><strong>什么是虚拟机快照链</strong></h2><p>虚拟机快照保存了虚拟机在某个指定时间点的状态（包括操作系统和所有的程序），利用快照，我们可以恢复虚拟机到某个以前的状态。比如：测试软件的时候经常需要回滚系统，以及新手安装OpenStack时为了防止重头再来，每安装成功一个服务就做一次快照等等。</p><p><strong>快照链就是多个快照组成的关系链</strong>，这些快照按照创建时间排列成链，像下面这样。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base-image&lt;--guest1&lt;--snap1&lt;--snap2&lt;--snap3&lt;--snap4&lt;--当前(active)</span><br></pre></td></tr></table></figure><p>如上，base-image是制作好的一个qcow2格式的磁盘镜像文件，它包含有完整的OS以及引导程序。现在，以这个base-image为模板创建多个虚拟机，简单点方法就是每创建一个虚拟机我们就把这个镜像完整复制一份，但这种做法效率底下，满足不了生产需要。这时，就用到了qcow2镜像的<strong>copy-on-write（写时复制）特性。</strong></p><p>qcow2(qemu copy-on-write)格式镜像支持快照，具有创建一个base-image，以及在base-image(backing file)基础上创建多个<strong>copy-on-write overlays镜像</strong>的能力。这里需要解释下backing file和overlay的概念。在上面那条链中，我们为base-image创建一个guest1，那么此时base-image就是guest1的backing file，guest1就是base-image的overlay。同理，为guest1虚拟机创建了一个快照snap1，此时guest1就是snap1的backing file，snap1是guest1的overlay。</p><p><strong>backing files和overlays十分有用，可以快速的创建瘦装备实例，特别是在开发测试过程中可以快速回滚到之前某个状态。</strong>以CentOS系统来说，我们制作了一个qcow2格式的虚拟机镜像，想要以它作为模板来创建多个虚拟机实例，有两种方法实现：</p><p><strong>1）每新建一个实例，把centosbase模板复制一份，创建速度慢。说白了就是复制原始虚拟机。</strong></p><p><strong>2）使用copy-on-write技术(qcow2格式的特性)，创建基于模板的实例，创建速度很快，可以通过查看磁盘文件信息，进行大小比较。也就是我们将存储虚拟化特性中提到的链接克隆。</strong></p><p>如下，我们有一个centosbase的原始镜像(包含完整OS和引导程序)，现在用它作为模板创建多个虚拟机，每个虚拟机都可以创建多个快照组成快照链，但是不能直接为centosbase创建快照。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/NPIK079viCoq.png?imageslim" alt="mark"></p><p>上图中centos1，centos2，centos3等都是基于centosbase模板创建的虚拟机(guest)，接下来做的测试需要用到centos1_sn1、centos1_sn2、centos1_sn3等centos1的快照链实现。也就是说，我们可以只用一个backing files创建多个虚拟机实例(overlays)，然后可以对每个虚拟机实例做多个快照。<strong>这里需要注意：backing files总是只读的文件。换言之，一旦新快照被创建，他的后端文件就不能更改(快照依赖于后端这种状态)。</strong></p><h2 id="virsh命令实现KVM虚拟机的内置快照"><a href="#virsh命令实现KVM虚拟机的内置快照" class="headerlink" title="virsh命令实现KVM虚拟机的内置快照"></a><strong>virsh命令实现KVM虚拟机的内置快照</strong></h2><p>qemu/kvm有三种快照，分别是<strong>内部（保存在硬盘镜像中）/外部（保存为另外的镜像名）/虚拟机状态</strong> ，很多网站上提供的资料和教程也大多是内部快照功能。<strong>内部快照不支持raw格式的镜像文件，所以如果想要使作内部快照，需要先将镜像文件转换成qcow2格式。</strong></p><p>内置快照在虚拟机运行状态和关闭状态都可以创建。在关机状态下，它通过单个qcow2镜像磁盘存储快照时刻的磁盘状态，并没有新磁盘文件产生。在虚机开机状态下，可以同时保存内存状态，设备状态和磁盘状态到一个指定文件中。当需要还原虚拟机状态时，将虚机关机后通过virsh restore命令还原回去。以上这些就是虚拟机的内置快照链操作，一般用于测试场景中的不断的将vm还原到某个起点，然后重新开始部署和测试。下面，我们就来玩玩KVM虚拟机的内置快照链。</p><p><strong>Stpe1：</strong>首先，我们找一台运行的虚拟机，然后通过控制台console登录，建一个空目录flags并写一个测试文件test01作为标记。在后面快照回滚时，可以通过对比该文件查看具体的效果 。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/nWAGt9O5UDnw.png?imageslim" alt="mark"></p><p><strong>Step2：</strong>由于内部快照不支持raw格式的磁盘镜像文件，所以我们首先需要查看下当前虚拟机的磁盘镜像格式，如果不符合要求，需要利用qemu-img命令进行磁盘格式转换。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/zplpEbPdRO01.png?imageslim" alt="mark"></p><p><strong>Step3：</strong>使用snapshot-create-as命令创建第一个快照snap01。其实，还有个类似命令的snapshot-create，它创建的快照名称是系统随机生成的，一般我们使用snap-create-as命令指定快照名创建。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/336Bbxpx8ClA.png?imageslim" alt="mark"></p><p><strong>step4：</strong>使用snapshot-current命令查看当前快照的详细信息。这里，其实查看的是/var/libvirt/qemu/snapshot/centos7.5/下的虚拟机的快照xml配置文件信息。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/12CPYe6xlXAk.png?imageslim" alt="mark"></p><p><strong>step5：</strong>此时，我们再次在虚拟机创建测试内容，在test01文件追加内容456，然后再次创建快照snap02。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/t5HzaNKGnWxA.png?imageslim" alt="mark"></p><p>同时，我们利用qemu-img命令查看虚拟机的磁盘信息，发现创建的快照后磁盘的容量有所增加，这也符合常理。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/99oPzSch7WoT.png?imageslim" alt="mark"></p><p><strong>step6：</strong>此时，我们利用快照回滚虚拟机。在回滚之前最好先关闭虚拟机 virsh shutdown 或virsh destroy ，在不关闭的情况下也可以做回滚。但是，此时如有新数据写入时不过会出现问题。所以，在实际运维中，还是建议先停机再做回滚。如果真的要求不停机回滚，最好加上–force选项表示强制回滚，此时即使有数据写入也会被丢弃。不加–force选项时，老版本的libvirt会报错，但是新版本不会报错，不过不建议这样使用。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/fHn9rJvL5Dwp.png?imageslim" alt="mark"></p><p>此时，我们在虚拟机运行态下，再次利用快照snap02进行回滚。而且，我们没有加–force选项。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/8N8rd3rBJNwz.png?imageslim" alt="mark"></p><p>上面完成快照的创建，回滚验证后，还有个操作时快照的删除的，也就是利用snapshot-delete命令删除，这个没有什么好说的。但是需要提示一点，快照删除后，虚拟机的磁盘大小并不会变小。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/EPS1hCkLn5TG.png?imageslim" alt="mark"></p><p>以上，就是KVM虚拟机的内置快照操作内容，我觉得我是讲明白了。。。关于kvm虚拟机的状态备份，也就是save命令，时间比较长，一般要5－10分钟左右，造成该问题的原因是：save vm保存的是当前客户机系统的运行状态（包括：内存、寄存器、CPU执行等的状态），保存为一个文件，而且要在load vm时可以完全恢复，这个过程比较复杂，如果客户机里面的内存很大、运行的程序很多，save vm比较耗时，也是可以理解的。暂时很难有什么改进方法。</p><p>而往往我们并不需要去备份一个虚拟机当前状态完整的快照，实际运维中中可能只需要对disk做一个快照就OK了。所以，这就要提到外部快照（External snapshot）。</p><h2 id="virsh命令实现KVM虚拟机的外置快照"><a href="#virsh命令实现KVM虚拟机的外置快照" class="headerlink" title="virsh命令实现KVM虚拟机的外置快照"></a><strong>virsh命令实现KVM虚拟机的外置快照</strong></h2><p>KVM的外部快照功能比较实用，可以支持仅对disk进行快照，也支持live snapshot，很多虚拟化云方案中一般也会使用外部快照功能创建快照链。不过，KVM虚拟机要支持外部快照功能，需要qemu版本在1.5.3及以上，否则只能通过下载最新的qemu源码包进行编译安装。可以通过rpm -aq qemu查看当前宿主机的qemu软件包的版本，也可以通过qemu-kvm –version命令来查看。</p><p>这里需要注意一点，centos系统默认将qemu-kvm命令放在/usr/libexec/目录下，所以在当前命令行执行qemu-kvm会提示找不到命令，需要带上命令的全路径执行，也就是/usr/libexec/qemu-kvm –version，或者通过创建软链接的方法，在/usr/bin目录下创建一个qemu-kmv命令的软链接，就可以执行qemu-kvm命令了。（我采用的就是这种办法）</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/wk87s1Xqy0Gh.png?imageslim" alt="mark"></p><p><strong>step1：</strong>在虚拟机关机状态下，我们创建一个外部快照ext_snap01。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/S64SnFwJIrmT.png?imageslim" alt="mark"></p><p>从上面的查询中不难看出centos75.ext_snap01的backing file来自于/kvm_host/vmdisk/centos75.img 。同样，也可以利用下面的命令进行查看：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/l5XTfva57vh0.png?imageslim" alt="mark"></p><p><strong>创建完外部快照后，原磁盘镜像文件会变成只读，新的变更都会写入到新的快照文件中。也就是我们常说链接克隆中创建的差分磁盘。</strong>忘了的，请回顾本站存储虚拟化相关文章。。。。。。</p><p><strong>step2：</strong>我们做完快照后，写一个200M的测试文件到虚拟机centos7.5的系统/opt/目录下，并做如下验证。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xjsFY4X5ia7D.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/6ldOLNFxhOV0.png?imageslim" alt="mark"></p><p>如上图，在未写入测试文件data01前，虚拟机外部快照盘centos75.ext_snap01的大小为5.5M（上图红框部分），在写入data01测试文件后变为207M（上图黄框部分），且虚拟机原始磁盘大小仍为4.3G（上图蓝框部分）。至于，显示大小不准确的问题，是因为我们通过-h选项通过ls命令查看，在转换成可读模式大小时的转换误差导致，并非真实文件大小不准确。其实，用原始字节数表示就没有误差，但是那样反人类啊。。。</p><p>至于外部快照的删除、回滚等操作与内部快照一直，我就懒得演示验证了。。。。<strong>不过，外部快照多了一个快照合并的功能，也就是将差分磁盘中的数据变化写入到原始磁盘镜像中。主要有两种方式：一种是通过blockcommit命令完成，从top文件合并数据到base，也就是从快照文件向backing file合并；另一种是blockpull命令完成，从base文件合并数据到top，也就是从backing file向快照文件合并。</strong>截止目前只能将backing file合并至当前的active的快照镜像中，也就是说还不支持指定top的合并。</p><p>但是，在centos系统执行virsh blockcommit或者virsh blockpull等命令时，都会报QEMU不支持的二进制操作错误或者是版本不支持等错误，这是CentOS内部集成的qemu-kvm版本问题导致。因此，此时有两种解决办法，一种是升级QEMU-KVM，通过源码编译内核完成升级。另一种就是使用qemu-img命令，commit对应上面blockcommit，rebase对应blockpull操作。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/LAprFY73P5cG.png?imageslim" alt="mark"></p><p>通过qemu-img commit命令将快照文件的数据的合并到原始镜像文件中（图中红色框部分），此时不需要指定backing file文件。而且，我们此时查看原始镜像文件大小，发现变大为4.5G（图中黄色框部分）。此时，我们可以删除磁盘快照文件，进入虚拟机查看我们当初的测试文件是否还存在。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/e3AoNybcs49S.png?imageslim" alt="mark"></p><p><strong>以上，commit的操作，如果采用rebase命令，反向合并时，就必须要指定backing file文件。其他操作与commit操作一致，大家自己尝试。。。。。</strong></p><p>在openstack等平台中，使用的快照功能大都是基于外置快照的形式。这里需要特别注意一点，KVM的快照不要和Vmware的快照混为一谈，Vmware的所有快照默认情况下都是基于baseimage，创建的overlays，也就是所有快照的backing file都是baseimage，删除任一个快照对其他快照的使用和还原都不会有影响，而KVM不同，其快照之间存在链式关系，snap01是基于baseimage，snap02是基于snpa01，以此类推。。。基中任一环节出现问题，都会出现无法还原到之间状态。<strong>所以，在做快照的合并，删除等操作前，一定要提前通过qemu-img info –backing-chain查看虚拟机的快照链关系，避免发生不可挽回的数据丢失错误。</strong></p><h2 id="libguestfs-tools工具使用总结"><a href="#libguestfs-tools工具使用总结" class="headerlink" title="libguestfs-tools工具使用总结"></a><strong>libguestfs-tools工具使用总结</strong></h2><p>libguestfs是一组Linux下的C语言的API ，用来访问虚拟机的磁盘映像文件。其项目官网是<a href="http://libguestfs.org/" target="_blank" rel="noopener">http://libguestfs.org/</a> 。该工具包含的软件有virt-cat、virt-df、virt-ls、virt-copy-in、virt-copy-out、virt-edit、guestfs、guestmount、virt-filesystems、virt-partitions等工具，具体用法也可以参看官网。该工具可以在不启动KVM guest主机的情况下，直接查看guest主机内的文内容，也可以直接向img镜像中写入文件和复制文件到外面的物理机，甚至也可以像mount一样，支持挂载操作。</p><p>现在，我们将虚拟机centos7.5关机，然后查看其镜像磁盘使用情况，可以通过virt-df命令实现。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/vxNFpNR7UdgL.png?imageslim" alt="mark"></p><p>同样，我们想查看虚拟机关机态下根分区下详细文件/目录信息，可以使用virt-ls命令实现。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/oLbun1Adras9.png?imageslim" alt="mark"></p><p>此时，我们需要从关机态下的虚拟中拷贝一个文件到宿主机中，可以使用copy-out命令实现。但是，需要带上-d选项指定哪个虚拟机。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/vXD3BX21yX14.png?imageslim" alt="mark"></p><p>既然能从虚拟机中拷贝文件到本地宿主机，自然也能从本地宿主机拷贝文件到虚拟机，通过copy-in命令就能实现。其实，这些命令工具的用法和Linux的原始命令非常类似，所以熟悉Linux常用命令的使用是必须要掌握的技能。</p><p>接下来，我们查看下虚拟机的文件系统以及分区信息，通过filesystems命令实现。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/wboPUznGRrAh.png?imageslim" alt="mark"></p><p>完成虚拟机文件系统和分区信息的查询后，我们可以通过guestmount命令，将虚拟机centos7.4的系统盘离线挂载到宿主机的/mnt分区，与挂载光驱操作类似，但是我们可以设置挂载镜像的操作方式。比如：只读、只写、读写等。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/YEQaPKROgRGo.png?imageslim" alt="mark"></p><p>以上是挂载linux系统的镜像磁盘，如果需要挂载windows虚拟机的磁盘，需要额外安装ntfs -3g的工具来识别windows系统的NTFS文件系统格式。</p><p>实际应用中的KVM主机也会遇到像物理机一样的情况，如系统崩溃、无法引导等情况。物理机出现该情况时，我们可以通过光盘引导、单用户模式、PE引导、修复或升级安装等方式获取系统内的文件和数据，KVM中同样也可以使用上述方法，既可以上面的利用libguestfs-tools的工具进行挂载修改，也可以通过linux系统原生的mount -o loop方式进行挂载修改。下面，我们就通过mount命令对raw格式和qcow2格式磁盘进行挂载做个演示。</p><h2 id="raw磁盘镜像的挂载"><a href="#raw磁盘镜像的挂载" class="headerlink" title="raw磁盘镜像的挂载"></a><strong>raw磁盘镜像的挂载</strong></h2><p>由于raw格式简单原始，其通常做为多种格式互相转换的中转格式，所以对raw格式的磁盘挂载操作时需重点掌握。raw格式的分区挂载也有两种方法：<strong>一种是通过计算偏移量offset方式挂载。另一种是通过kpartx分区映射方式实现挂载。</strong></p><p><strong>计算偏移量offset方式的思路为找出分区的开始位置，使用mount命令的offset参数偏移掉前面不需要的，即可得到真正的分区。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/e5gq81Q51C9g.png?imageslim" alt="mark"></p><p>上图中，我们可以通过fdisk -lu命令查看磁盘镜像的分区信息，可以发现centos74.img的磁盘一共有3个分区，每个分区都有对应的起始扇区和结束扇区编号，且每个扇区的大小为512字节。通过这些信息，我们就可以计算分区的offset值，从而实现找到真正的分区内容存放位置。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/PgMvXHo4SVO4.png?imageslim" alt="mark"></p><p>然后，通过mount -o loop，offset=xxxx命令用其实扇区的偏移字节数实现分区挂载，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/i8cdp36fBT1F.png?imageslim" alt="mark"></p><p>除了上述计算偏移量的方法外，还可以通过<strong>kpartx分区映射方式实现挂载。首先通过kpartx工具对磁盘镜像做个分区映射。如下：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/9rcuCx8iBlz8.png?imageslim" alt="mark"></p><p>上图中，通过-av选项添加磁盘镜像并将映射结果显示出来（图中红色框部分），然后就可以知道磁盘镜像有3个分区，分别映射为loop0p1、loop0p2和loop0p3（图中黄色框部分）。完成映射后，我们就可以像挂载普通磁盘或光驱那样对映射分区进行挂载。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/cRizJI8Dp0sh.png?imageslim" alt="mark"></p><p>注意映射的设备的文件位置在/dev/mapper目录下。我们可以看下该目录下的具体信息，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/YCWFNKC5z2Ml.png?imageslim" alt="mark"></p><p>上图中可以很清楚的发现上面的映射分区是通过软链接的方式建立的与磁盘内部分区的映射关系。通过磁盘映射方式下实现挂载后，记得使用完成不仅要卸载挂载点，还需要删除映关系。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/5tcFLsNAGzqB.png?imageslim" alt="mark"></p><p>上图中可以很清楚的发现上面的映射分区是通过软链接的方式建立的与磁盘内部分区的映射关系。通过磁盘映射方式下实现挂载后，记得使用完成不仅要卸载挂载点，还需要删除映关系。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/cqqzg7k8QgoX.png?imageslim" alt="mark"></p><p>以上就是raw格式镜像挂载，需要注意的如果虚拟机使用了LVM逻辑卷，那么针对逻辑卷的挂载操作需要使用losetup工具完成，具体可以查询使用方法，非常简单这里就不在赘述了。而qcow2格式的镜像的挂载不能通过kaprtx直接映射，可以先转换成raw的格式进行处理，也可以通过libguestfs-tools工具处理，还可以使用qemu-nbd直接挂载。就速度上而言qemu-nbd的速度肯定是最快的。不过由于centos/redhat原生内核和rpm源里并不含有对nbd模块的支持及qemu-nbd（在fedora中包含在qemu-common包里）工具，所以想要支持需要编译重新编译内核并安装qemu-nbd包 。</p><p>至此，KVM虚拟机的存储管理实战全部介绍完毕，后面我们进入KVM虚拟机的网络管理实战。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇中我们介绍qemu-img这个对磁盘镜像操作的命令，包括检查镜像磁盘，创建镜像磁盘、查看镜像磁盘信息、转换镜像磁盘格式、调整镜像磁盘大小以及镜像磁盘的快照链操作。但是，在镜像磁盘快照链操作中，我们也提到了通过qemu-img命令创建快照链只能在虚拟机关机状态下运行。如果虚拟机为运行态，只能通过virsh save vm来保存当前状态。那么，本文就专门讲述在KVM虚拟机中如何通过virsh命令来创建快照链，以及链中快照的相互关系，如何缩短链，如何利用这条链回滚我们的虚拟机到某个状态等等。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-KVM虚拟机存储管理实战（上篇）</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%8A%E7%AF%87%EF%BC%89/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM虚拟机存储管理实战（上篇）/</id>
    <published>2020-03-01T11:17:55.000Z</published>
    <updated>2020-03-01T11:30:34.588Z</updated>
    
    <content type="html"><![CDATA[<p>我们上一篇介绍了KVM虚拟机的全生命周期管理实战，如果与OpenStack的实操对应，正好与nova服务的实操相符。在云计算体系中，运维管理主要涉及计算nova、存储cinder和网络neutron三部分。因此，本篇文章我们主要介绍KVM的存储管理实战。KVM的存储选项有多种，包括虚拟磁盘文件、基于文件系统的存储和基于设备的存储。同样，也有对应的virsh管理命令。<a id="more"></a></p><h2 id="KVM的存储选项和常用管理命令"><a href="#KVM的存储选项和常用管理命令" class="headerlink" title="KVM的存储选项和常用管理命令"></a><strong>KVM的存储选项和常用管理命令</strong></h2><p>为实现KVM存储管理，可以使用LVM逻辑卷和创建存储池。储存池Storage Pool 是宿主机上可以看到的一片存储空间，可以是多种型。而逻辑卷Volume 是在 存储池Storage Pool 中划分出的一块空间，宿主机将 Volume 分配给虚拟机，Volume 在虚拟机中看到的就是一块硬盘。</p><h3 id="虚拟磁盘文件"><a href="#虚拟磁盘文件" class="headerlink" title="虚拟磁盘文件"></a><strong>虚拟磁盘文件</strong></h3><p>当系统创建KVM虚拟机的时候，默认使用虚拟磁盘文件作为后端存储。安装后，虚拟机认为在使用真实的磁盘，但实际上看到的是用于模拟硬盘的虚拟磁盘文件。就是因为加了一层额外的文件系统层，所以系统的I/O读写性能会降低，但是基于文件系统的虚拟磁盘可以用于其他虚拟机，且具备快照、链接克隆、弹性扩缩容等特性，方便虚拟机的迁移。因此，可以说是各有利弊，根据实际使用场景来选择。</p><h3 id="基于文件系统的KVM存储"><a href="#基于文件系统的KVM存储" class="headerlink" title="基于文件系统的KVM存储"></a><strong>基于文件系统的KVM存储</strong></h3><p>在安装KVM宿主机时，<strong>可选文件系统为dir（directory）或fs（formatted block storage）</strong>作为初始KVM存储格式。默认为dir，通过指定本地文件系统中的一个目录用于创建磁盘镜像文件。而fs选项可以允许指定某个格式化文件系统的分区，把它作为专用的磁盘镜像文件存储。<strong>两种KVM存储选项之间最主要的区别在于：fs文件系统不需要挂载到某个特定的目录。</strong>两种选项所指定的文件系统，都可以是本地文件系统或位于SAN上某个物理宿主机上的网络文件系统。后者具备数据共享的优势，可以很轻易地实现多个主机同时访问。</p><p><strong>还有一种基于文件的磁盘存储方式是netfs</strong>，可以指定一个网络文件系统的名称，如NFS，用这种方式作为KVM存储与SAN类似，可以访问到位于其它服务器上的虚拟磁盘，同时也可以多台宿主机共享访问磁盘文件。</p><p>但是，所有的这些基于文件系统的KVM存储方式都有一个缺点：<strong>文件系统固有缺陷。</strong>因为虚拟机的磁盘文件不能直接读取或写入KVM存储设备，而是写入宿主机OS之上的文件系统。这也就意味着在访问和写入文件时中间增加了额外一层，因此会降低性能。所以，如果只是单纯要求性能好，就需要考虑考虑基于设备的存储。</p><h3 id="基于设备的KVM存储"><a href="#基于设备的KVM存储" class="headerlink" title="基于设备的KVM存储"></a><strong>基于设备的KVM存储</strong></h3><p>另外一种KVM存储的方式就是使用基于设备的方式。<strong>共支持四种不同的物理存储：磁盘、iSCSI、SCSI和lvm逻辑盘。</strong>磁盘方式指直接读写硬盘设备。iSCSI和SCSI方式可选，取决于通过SCSI或iSCSI地址与磁盘设备连接。这种KVM存储方式的<strong>优势在于磁盘的名称固定</strong>，不依赖宿主机OS搜索到磁盘设备的顺序。但是，这种连接磁盘的方式也有<strong>缺点：灵活性不足</strong>。虚拟磁盘的大小很难改变，而且这种连接方式的KVM存储不支持快照。</p><p><strong>如果要提升基于设备的KVM存储灵活性，可以使用LVM。</strong>LVM的优势在于可以使用快照，但是快照并不是KVM虚拟化的特性，而是LVM自带的特性。</p><p>LVM可以把所有存储放到一个卷组里，该卷组是物理磁盘设备的一个抽象，所以如果超出可用磁盘空间最大值，还可以向卷组中添加新的设备，增加的空间在逻辑卷中直接可以使用。使用LVM使得磁盘空间分配更加灵活，而且增加和删除存储也更为容易。LVM除了在单机场景下使用外，还可以在多机场景下使用。在多宿主机环境中，可以在SAN上创建逻辑卷，且如果使用Cluster LVM（集群LVM），可以很容易的配置成多个主机同时访问某个逻辑卷。详见本站Linux常用运维工具分类中LVM逻辑卷一文。</p><p>virsh也可以对节点上的存储池和存储卷进行管理。virsh常用于存储池和存储卷的管理命令如下：</p><table><thead><tr><th><strong>命令</strong></th><th><strong>功能描述</strong></th></tr></thead><tbody><tr><td>pool-list</td><td>显示libvirt管理的存储池</td></tr><tr><td>pool-define &lt;pool.xml&gt;</td><td>根据xml文件定义一个存储池</td></tr><tr><td>pool-define-as <pool-name>&lt;–type\</pool-name></td><td>target&gt;</td><td>定义一个存储池，指定pool名，并指定pool类型和存储路径</td></tr><tr><td>pool-build <pool-name></pool-name></td><td>构建一个存储池</td></tr><tr><td>pool-start <pool-name></pool-name></td><td>激活一个存储池</td></tr><tr><td>pool-autostart <pool-name></pool-name></td><td>设置存储池开机自动运行</td></tr><tr><td>pool-info<pool-name></pool-name></td><td>根据一个存储池名称查询其基本信息</td></tr><tr><td>pool-uuid<pool-name></pool-name></td><td>根据存储池名称查询其uuid信息</td></tr><tr><td>pool-create&lt;pool.xml&gt;</td><td>根据xml配置文件信息创建一个存储池</td></tr><tr><td>pool-edit<pool-name or uuid></pool-name></td><td>编辑一个存储池的xml配置文件</td></tr><tr><td>pool-destroy<pool-name or uuid></pool-name></td><td>关闭一个存储池</td></tr><tr><td>pool-delete<pool-name or uuid></pool-name></td><td>删除一个存储池，不可恢复</td></tr><tr><td>vol-list<pool-name or uuid></pool-name></td><td>查询一个存储池中的存储卷的列表</td></tr><tr><td>vol-name<vol-key-or-path></vol-key-or-path></td><td>查询一个存储卷的名称</td></tr><tr><td>vol-path –pool<pool><vol-name or key></vol-name></pool></td><td>查询一个存储卷的路径</td></tr><tr><td>vol-create&lt;vol.xml&gt;</td><td>根据xml配置创建一个存储卷</td></tr><tr><td>vol-create-as &lt;–pool\</td><td>name\</td><td>capacity\</td><td>allocation\</td><td>format&gt;</td><td>创建一个卷，指定归属pool，卷名、预分配大小、占用大小、磁盘格式</td></tr><tr><td>vol-clone<vol-name-path><name></name></vol-name-path></td><td>克隆一个存储卷</td></tr><tr><td>vol-delete<vol-name-or-key-or-path></vol-name-or-key-or-path></td><td>删除一个存储卷</td></tr><tr><td>vol-pool <vol-name or vol-path></vol-name></td><td>根据存储卷名或路径查询归属存储池信息</td></tr></tbody></table><p>与全生命周期管理一样，可以通过<strong>virsh help|egrep ‘(pool*|vol*)’</strong>查看全量存储管理命令。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/dYHpv93mgEcD.png?imageslim" alt="mark"></p><h2 id="KVM虚拟机的存储管理实战"><a href="#KVM虚拟机的存储管理实战" class="headerlink" title="KVM虚拟机的存储管理实战"></a><strong>KVM虚拟机的存储管理实战</strong></h2><h3 id="存储池和存储卷的信息查询"><a href="#存储池和存储卷的信息查询" class="headerlink" title="存储池和存储卷的信息查询"></a><strong>存储池和存储卷的信息查询</strong></h3><p>有了上面的命令知识储备，我们<strong>通过pool-list命令首先看下当前宿主机（192.168.101.251）上存储池信息。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/zG5zjC4gTh5t.png?imageslim" alt="mark"></p><p>上图中表示当前宿主机上有两个KVM虚拟机存储池，一个是网络文件系统NFS的存储池，名称为nfsdir，另一个是本地文件系统存储池，名称为root。其实，这两个存储池是在我们前面创建虚拟机通过disk选项制定虚拟机系统盘时，默认创建的。我们可以<strong>通过pool-info命令查看对应存储池的详细信息。</strong>如下：</p><p><strong>1）nfsdir存储池信息</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/ITfhwM16cxKU.png?imageslim" alt="mark"></p><p><strong>2）root存储池信息</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/NPcOag6cou8s.png?imageslim" alt="mark"></p><p><strong>这里需要明确一个概念，上图中的存储池的总空间大小并不是创建的存储池空间大小，而是存储池所在的磁盘分区的大小，由于我们前面在创建虚拟机时指定的虚拟机系统盘文件都在系统根分区下创建（/root and /mydata/nfsdir/），所以这里的存储池总大小为根分区的大小。这一点可以通过查看根分区大小来确认。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/TUS6nVINfLps.png?imageslim" alt="mark"></p><p>掌握了存储池信息查看后，我们可以通<strong>过vol-list命令查看该存储池下有哪些存储卷</strong>。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/G0fVydWk8EMq.png?imageslim" alt="mark"></p><p>上图中表示nfsdir存储池下有一个存储卷cto67s.img，它位于/mydata/nfsdir目录下。接下来，我们还可以<strong>通过vol-info命令查看该存储卷的详细信息。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/VvIEWorSV62M.png?imageslim" alt="mark"></p><p>如上图，在查看存储卷的详细信息时，需要通过–pool选项制定该存储卷对应的存储池名称。上图中表示cto67s.img存储卷的类型为file，总大小为2.28GB。</p><h3 id="存储池和存储卷的增、删、改实战"><a href="#存储池和存储卷的增、删、改实战" class="headerlink" title="存储池和存储卷的增、删、改实战"></a><strong>存储池和存储卷的增、删、改实战</strong></h3><p>上面查看存储池和存储卷的命令只能浏览大致的一些信息，其实，我们还可以通过<strong>命令pool-deumpxml查看具体的存储卷xml配置详细信息。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/63o2lSECGcwr.png?imageslim" alt="mark"></p><p>上图中显示的存储池nfsdir的xml配置信息，从配置信息中我们可以知道存储池nfsdir的类型type、name、uuid、空间的大小、路径path、权限、归属的用户和用户组等信息。同理，我们如果<strong>想查看某个存储卷的xml配置信息，可以使用vol-dumpxml命令实现</strong>。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/nGc6wTxakDqX.png?imageslim" alt="mark"></p><p>在上图中，我们不仅知道存储卷的类型type、name、key、空间大小、路径、磁盘格式、文件权限，归属用户&amp;用户组，还知道该存储卷的访问时间，修改时间和状态改变时间。</p><p>有了上面的知识储备后，我们就可以通过xml文件创建一个存储池和一个存储卷。我们首先创建一个存储池POOLB，xml配置内容如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/WmpajpncXoiV.png?imageslim" alt="mark"></p><p>如上图，我们只需要定义要创建的存储池类型、名称、路径、权限和归属用户及用户组等信息，其他如uuid、空间大小等信息会在存储池创建后，系统自动生成。由于我们定义的存储池类型为dir目录类型，因此实际上空间大小等信息就是存储池目录所在的磁盘分区大小信息。</p><p>有了上面存储池的xml配置文件后，就可以<strong>通过pool-create命令来创建一个存储池poolB了</strong>。需要注意，<strong>由于我们定义的存储池类型为dir目录类型，所以需要提前在xml文件指定的路径下创建目标目录，否则会提示创建存储池失败。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/NbOSmXya6kwC.png?imageslim" alt="mark"></p><p>在/mydata目录下创建poolB子目录后，结果如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/yf8eCEtKQw7G.png?imageslim" alt="mark"></p><p>上图中，我们完成存储池创建后，在该存储池中创建一个卷data01.img。同理，我们还是<strong>通过xml文件来创建，首先创建卷的xml配置文件data01.xml</strong>。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/FYTSKq9rR0zT.png?imageslim" alt="mark"></p><p>然后，我们通过<strong>命令vol-cretae命令来创建卷data01.img</strong>。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/CLNpj7QALhOY.png?imageslim" alt="mark"></p><p>完成存储池和卷的创建验证后，我们现在来验证删除存储池和卷。既然我们创建的时候，先创建存储池，再创建卷。那么，删除的时候自然是反着来，是不是就是先删除卷再删除存储池。如果不删除存储池中的卷就直接删除存储池行不行？答案是可以的。。。所以，<strong>我们既可以先通过vol-delete命令删除卷，然后通过pool-destroy命令删除池。也可以直接通过pool-destroy命令删除池。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/KJTiv638md4O.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/BWOEhxd34swK.png?imageslim" alt="mark"></p><p>在删除池时还有一个pool-delete命令，这个命令请慎用。因为一旦通过它删除池就无法恢复。所以，我们一般使用pool-destroy命令来销毁一个池，一旦需要时还可以通过pool-start命令恢复。即使真的不需要这个池，也可以通过find+rm命令来删除。</p><p>大家可能发现了，通过上面的方式创建的存储池是不能自动开始的。其实，KVM的virsh命令还有另一种方式来创建存储池和存储卷。就是<strong>通过pool-define-as命令先定义一个存储池，然后通过pool-build命令来构建一个存储池，再通过pool-start命令激活存储池，最后通过pool-autostart命令设置该存储池开机自动运行。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/9z6NWD2j00R8.png?imageslim" alt="mark"></p><p>其实，通过xml配置文件也能通过这种方式创建。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/usFDPTkxLtDa.png?imageslim" alt="mark"></p><p>完成上面pool的创建后，我们也可以<strong>通过vol-create-as命令来创建一个卷。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Keej1y6rqA1O.png?imageslim" alt="mark"></p><p>如上图，通过制定归属pool，卷名、预分配大小、占用大小和磁盘格式，通过vol-create-as命令完成一个卷的创建。</p><p>删除卷和池的方式与上面类似，这里就不再举例了。存储池和卷的其他命令大家可以参考virsh help的信息自行研究。接下来，我们来看一个经常使用qemu-img命令，也就是KVM存储磁盘管理命令。</p><h2 id="qemu-img命令实战"><a href="#qemu-img命令实战" class="headerlink" title="qemu-img命令实战"></a><strong>qemu-img命令实战</strong></h2><p>镜像算不算存储？在我初学OpenStack时，因为镜像管理服务是glance，存储管理服务是cinder和swift，所以当初单纯以为的镜像不算存储。后来通过深入了解和研究，发现其实镜像也是一种存储，可以把它简单理解为“<strong>虚拟机的元数据</strong>”，既然是元数据，当然是存储。</p><p>我们都知道，在创建KVM虚拟机时，首先需要通过qemu-img命令去创建一个虚拟系统盘。这个qemu-img命令就是是QEMU的磁盘镜像管理工具，在完成qemu-kvm源码编译或rpm包安装后就会默认编译好qemu-img这个二进制文件。qemu-img也是QEMU/KVM使用过程中一个比较重要的工具，我们下来就对其常用用法总结验证下。</p><p>qemu-img支持非常多种的文件格式，可以通过“qemu-img-h”查看其命令帮助得到，它支持20多种格式：file，quorum，blkverify，luks，dmg，sheepdog，parallels，nbd，vpc，bochs，blkdebug，qcow2，vvfat，qed，host_cdrom，cloop，vmdk，host_device，qcow，vdi，null-aio，blkreplay，null-co，raw等。</p><p><strong>qemu-img工具的命令行基本用法为：qemu-img command[command options]</strong>，也就是说qemu-img工具通过后面命令和命令选项实现各种磁盘镜像管理功能。它支持的命令分为如下几种：</p><h3 id="检查镜像磁盘的数据一致性check-f-fmt-filename"><a href="#检查镜像磁盘的数据一致性check-f-fmt-filename" class="headerlink" title="检查镜像磁盘的数据一致性check[-f fmt]filename"></a><strong>检查镜像磁盘的数据一致性check[-f fmt]filename</strong></h3><p><strong>该命令用于对磁盘镜像文件进行一致性检查，查找镜像文件中的错误</strong>，目前，仅支持对<strong>“qcow2”、”qed”、”vdi”格式文件的检查</strong>。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/PLFUHsWQ0hao.png?imageslim" alt="mark"></p><p>如上图，提示该磁盘镜像没有错误，并且其中显示了该镜像磁盘在物理盘的I/O偏移地址等存储信息。在上面的命令中，我们使用<strong>-f参数指定镜像磁盘的格式</strong>为qcow2，它和qed都是QEMU支持的磁盘镜像格式，qed是qcow2的增强磁盘文件格式，避免了qcow2格式的一些缺点，也提高了性能，不过目前还不够成熟。而另一种磁盘镜像格式vdi（Virtual Disk Image）是Oracle的VirtualBox虚拟机中的存储格式。最后的<strong>filename参数是磁盘镜像文件的名称（包括路径）。</strong></p><h3 id="创建磁盘镜像文件create-f-fmt-o-options-filename-size"><a href="#创建磁盘镜像文件create-f-fmt-o-options-filename-size" class="headerlink" title="创建磁盘镜像文件create[-f fmt][-o options]filename[size]"></a><strong>创建磁盘镜像文件create[-f fmt][-o options]filename[size]</strong></h3><p><strong>该命令用于创建一个格式为fmt，大小为size，文件名为filename（包含路径）的镜像文件。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/aTYY5SMb5j9c.png?imageslim" alt="mark"></p><p>上图中，我们创建了一个raw格式的，大小为20G的镜像磁盘metdata001.raw，位于/mydata/poolB目录下。其实，可以根据文件格式fmt的不同，添加一个或多个选项（options）来附加对该文件的各种功能设置，可以使用”-o?”来查询某种格式文件支持哪些选项，在”-o”选项中各个选项用逗号来分隔。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/ItIeSS8ManLv.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/2OJq9v3fd1sx.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/nU73iiRqgSMK.png?imageslim" alt="mark"></p><p>如上图，如果“-o”选项中使用了backing_file这个选项来指定其后端镜像文件，那么这个创建的镜像文件仅记录与后端镜像文件的差异部分。后端镜像文件不会被修改，除非在QEMU monitor中使用“commit”命令或者使用“qemu-img commit”命令去手动提交这些改动。这种情况下，size参数不是必须需的，其值默认为后端镜像文件的大小。另外，镜像文件的大小（size）也并非必须写在命令的最后，它也可以被写在“-o”选项中作为其中一个选项。而且，直接使用“-b backfile”参数也与“-o backing_file=backfile”效果等价。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/ElGTNtzfNLXd.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Ls79woJMCvqJ.png?imageslim" alt="mark"></p><h3 id="修改磁盘镜像文件格式convert-c-f-fmt-O-output-fmt-o-options-filename-filename2-…-output-filename"><a href="#修改磁盘镜像文件格式convert-c-f-fmt-O-output-fmt-o-options-filename-filename2-…-output-filename" class="headerlink" title="修改磁盘镜像文件格式convert[-c][-f fmt][-O output_fmt][-o options]filename[filename2[…]]output_filename"></a><strong>修改磁盘镜像文件格式convert[-c][-f fmt][-O output_fmt][-o options]filename[filename2[…]]output_filename</strong></h3><p><strong>该命令用于将fmt格式的filename镜像文件根据options选项转换为格式为output_fmt的名为output_filename的镜像文件。</strong>比如我们先创建test01的qcwo2格式磁盘文件，然后将其转换为raw格式文件。<strong>如下：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/v5Yn7VyrPqqt.png?imageslim" alt="mark"></p><p>如上图，将创建的qcow2格式的test01文件转换为raw格式的文件（蓝色框部分）。<strong>这个命令一般在做虚拟机模板镜像文件时经常用到，将非raw格式的磁盘文件转换为raw格式的后缀为.img的镜像模板文件，用于后续的批量虚拟机发放。同时，它支持不同格式的镜像文件之间的转换，比如可以用VMware用的vmdk格式文件转换为qcow2文件，这对从其他虚拟化方案转移到KVM上的用户非常有用。</strong>一般来说，输入文件格式fmt由qemu-img工具自动检测到，而输出文件格式output_fmt根据自己需要来指定，默认会被转换为与raw文件格式（且默认使用稀疏文件的方式存储以节省存储空间）。</p><p>命令中，“-c”参数是对输出的镜像文件进行压缩，<strong>不过只有qcow2和qcow格式的镜像文件才支持压缩，而且这种压缩是只读的，如果压缩的扇区被重写，则会被重写为未压缩的数据。</strong>同样可以使用“-o options”来指定各种选项，如：后端镜像、文件大小、是否加密等等。<strong>使用backing_file选项来指定后端镜像，让生成的文件是copy-on-write的增量文件，这时必须让转换命令中指定的后端镜像与输入文件的后端镜像的内容是相同的，尽管它们各自后端镜像的目录、格式可能不同。</strong></p><p><strong>如果使用qcow2、qcow、cow等作为输出文件格式来转换raw格式的镜像文件（非稀疏文件格式），镜像转换还可以起到将镜像文件转化为更小的镜像，</strong>因为它可以将空的扇区删除使之在生成的输出文件中并不存在。</p><h3 id="查看镜像磁盘信息info-f-fmt-filename"><a href="#查看镜像磁盘信息info-f-fmt-filename" class="headerlink" title="查看镜像磁盘信息info [-f fmt] filename"></a><strong>查看镜像磁盘信息info [-f fmt] filename</strong></h3><p><strong>该命令用于显示镜像磁盘的详细信息。</strong>如果文件是使用稀疏文件的存储方式，也会显示出它的本来分配的大小以及实际已占用的磁盘空间大小。如果文件中存放有客户机快照，快照的信息也会被显示出来。比如，我们想查看cto67s.img这个镜像磁盘的信息。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/tr2WT3X7xVkr.png?imageslim" alt="mark"></p><p>这个命令比较简单，聪明如你，一学就会。。。。。。</p><h3 id="镜像磁盘的快照命令集snapshot-l-a-snapshot-c-snapshot-d-snapshot-filename"><a href="#镜像磁盘的快照命令集snapshot-l-a-snapshot-c-snapshot-d-snapshot-filename" class="headerlink" title="镜像磁盘的快照命令集snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename"></a><strong>镜像磁盘的快照命令集snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename</strong></h3><p><strong>这是一组命令，“-l” 选项是查询并列出镜像文件中的所有快照，“-a snapshot”是让镜像文件使用某个快照，“-c snapshot”是创建一个快照，“-d”是删除一个快照。</strong>比如，我们现在看当下宿主机中有没有虚拟机磁盘cto74d.img的快照，可以使用-l选项查看，结果发现没有。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Pm7gYMCYePzk.png?imageslim" alt="mark"></p><p>此时，我们使用-c选项对该磁盘创建一个快照，此时需要注意<strong>raw格式的磁盘文件不支持快照，因此对raw格式的磁盘创建快照时，需要将其转换为qcow2格式。同时，无论是使用快照还是创建快照都需要在关闭虚拟机的情况下进行，如果虚拟机时运行状态需要使用另一个命令virsh save vm。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/URzTm3p0biWo.png?imageslim" alt="mark"></p><p>上述快照创建成功后，我们在当前目录下查看是否新的镜像文件产生。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/ldLmPKm89VBP.png?imageslim" alt="mark"></p><p>发现并没有新的镜像文件产生，说明通过qemu-img该步并不会创建一个新的镜像，但是磁盘镜像的快照确实存在，因为通过-l选项可以查看。这样，我们在需要的时候，就可以使用<strong>-a选项利用快照恢复磁盘</strong>。同样，如果我们不需要快照，可以通过<strong>-d选项将其删除</strong>。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/jLNB8ibNBzWf.png?imageslim" alt="mark"></p><h3 id="修改磁盘镜像文件的大小resize-filename-size"><a href="#修改磁盘镜像文件的大小resize-filename-size" class="headerlink" title="修改磁盘镜像文件的大小resize filename[+|-]size"></a><strong>修改磁盘镜像文件的大小resize filename[+|-]size</strong></h3><p><strong>该命令用于改变镜像文件的大小。</strong>“+”和“-”分别表示增加和减少镜像文件的大小，size也支持K、M、G、T等单位的使用。比如，我们现在将cto67s.img磁盘镜像大小增大5G。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/l5O0PCK54IUU.png?imageslim" alt="mark"></p><p>在上图中，源磁盘大小为40G（红色框部分），通过resize命令增大5G后（黄色框部分），变成了45G（蓝色框部分）。<strong>需要注意的是：使用resize命令时需要小心（做好备份），如果失败，可能会导致镜像文件无法正常使用，而造成数据丢失。同时，缩小镜像的大小之前，需要在虚拟机中保证其中的文件系统有空余空间，否则数据会丢失。另外，qcow2格式文件不支持缩小镜像的操作。</strong></p><p>至此，KVM虚拟机的存储管理实战上篇介绍完了。我们主要讲述了如何利用virsh命令行工具来管理KVM虚拟机的存储池以及存储卷等操作，以及介绍了qemu-img这个常用的命令几种用法等内容。这些内容是下一篇通过virsh命令完成快照链操作的基础，因此需要重点掌握。光说不练假把式，IT的东西就需要大量实践才能掌握，这也是IT领域没有科技创建只有最佳实践的原因。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们上一篇介绍了KVM虚拟机的全生命周期管理实战，如果与OpenStack的实操对应，正好与nova服务的实操相符。在云计算体系中，运维管理主要涉及计算nova、存储cinder和网络neutron三部分。因此，本篇文章我们主要介绍KVM的存储管理实战。KVM的存储选项有多种，包括虚拟磁盘文件、基于文件系统的存储和基于设备的存储。同样，也有对应的virsh管理命令。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-KVM虚拟机全生命周期管理实战</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%85%A8%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM虚拟机全生命周期管理实战/</id>
    <published>2020-03-01T10:44:01.000Z</published>
    <updated>2020-03-01T11:02:23.739Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇我们介绍了KVM最重要的管理工具libvirt，它是KVM其他管理工具的基础，处于KVM管理架构的中间适配层。本篇我们主要要介绍libvirt的命令行管理工具的virsh，它也是将libvirt作为基础，通过封包调用实现的。所以，在看本篇内容之前，最好将上一篇的内容做个预习。<a id="more"></a></p><h2 id="libvirt命令行工具virsh"><a href="#libvirt命令行工具virsh" class="headerlink" title="libvirt命令行工具virsh"></a><strong>libvirt命令行工具virsh</strong></h2><p>virsh通过调用libvirt API实现虚拟化管理，与virt-manager工具类似，都是管理虚拟化环境中的虚拟机和Hypervisor的工具，只不过virsh是命令行文本方式的，也是更常用的方式。</p><p>在使用virsh命令行进行虚拟化管理操作时，可以使用两种工作模式：交互模式和非交互模式。交互模式是连接到Hypervisor上，然后输入一个命令得到一个返回结果，直到输入quit命令退出。非交互模式是直接在命令上通过建立URI连接，然后执行一个或多个命令，执行完后将命令的输出结果返回到终端上，然后自动断开连接。两种操作模式的截图如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/fNyrRSVA9glH.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/TUGt0wEfs2pj.png?imageslim" alt="mark"></p><p>我们经常在本地使用virsh命令，这是一种特殊的交互模式，也是最常用的模式，它本质上就是默认连接到本节点的Hypervisor上。</p><p>libvirt中实现的功能和最新的QEMU/KVM中的功能相比有一定的滞后性，因此virsh只是实现了对QEMU/KVM中的大多数而不是全部功能的调用。同时，由于virsh还实现了对Xen、VMware等其他Hypervisor的支持，因此有部分功能对QEMU/KVM无效。下面，我们还是按照“边验证边理解”的套路，对virsh常用的命令进行分类整理说明。</p><h2 id="域管理（虚拟机管理）命令"><a href="#域管理（虚拟机管理）命令" class="headerlink" title="域管理（虚拟机管理）命令"></a><strong>域管理（虚拟机管理）命令</strong></h2><p>virsh的最重要的功能之一就是实现对域（虚拟机）的管理，但是与虚拟机相关的命令是很多的，包括后面的网络管理、存储管理也都有很多是对域（虚拟机）的管理。为了简单起见，本文使用“<id>”来表示一个域的唯一标识（而不专门指定为“<id or name uuid>”这样冗长的形式）。常用的virsh域管理命令如下：</id></id></p><table><thead><tr><th><strong>virsh中的虚拟机管理命令</strong></th><th></th></tr></thead><tbody><tr><td>命令</td><td>功能描述</td></tr><tr><td>list</td><td>获取当前节点上多有虚拟机的列表</td></tr><tr><td>domstate<id></id></td><td>获取一个虚拟机的运行状态</td></tr><tr><td>dominfo<id></id></td><td>获取一个虚拟机的基本信息</td></tr><tr><td>domid<name or uuid></name></td><td>根据虚拟机的名称或UUID返回虚拟机的ID</td></tr><tr><td>domname<id or uuid></id></td><td>根据虚拟机的ID或UUID返回虚拟机的名称</td></tr><tr><td>dommemstat<id></id></td><td>获取一个虚拟机的内存使用情况的统计信息</td></tr><tr><td>setmem<id> <mem-size></mem-size></id></td><td>设置一个虚拟机的大小（默认单位的KB）</td></tr><tr><td>vcpuinfo<id></id></td><td>获取一个虚拟机的vCPU的基本信息</td></tr><tr><td>vcpupin<id><vcpu><pcpu></pcpu></vcpu></id></td><td>将一个虚拟机的vCPU绑定到某个物理CPU上运行</td></tr><tr><td>setvcpus<id><vcpu-num></vcpu-num></id></td><td>设置一个虚拟机的vCPU的个数</td></tr><tr><td>vncdisplay<id></id></td><td>获取一个虚拟机的VNC连接IP地址和端口</td></tr><tr><td>create&lt;dom.xml&gt;</td><td>根据虚拟机的XML配置文件创建一个虚拟机</td></tr><tr><td>define&lt;dom.xml&gt;</td><td>定义一个虚拟机但不启动，此时虚拟机处于预定义状态</td></tr><tr><td>start<id></id></td><td>启动一个预定义的虚拟机</td></tr><tr><td>suspend<id></id></td><td>暂停一个虚拟机</td></tr><tr><td>resume<id></id></td><td>恢复一个虚拟机</td></tr><tr><td>shutdown<id></id></td><td>对一个虚拟机下电关机</td></tr><tr><td>reboot<id></id></td><td>让一个虚拟机重启</td></tr><tr><td>reset<id></id></td><td>与reboot的区别是，它是强制一个虚拟机重启，相当于在物理机上长按reset按钮，可能会循环虚拟机系统盘的文件系统</td></tr><tr><td>destroy<id></id></td><td>立即销毁一个虚拟机，相当于物理机上直接拔电源</td></tr><tr><td>save<id>&lt;file.img&gt;</id></td><td>保存一个运行中的虚拟机状态到一个文件中</td></tr><tr><td>restore&lt;file.img&gt;</td><td>从一个被保存的文件中恢复一个虚拟机的运行</td></tr><tr><td>migrate<id>&lt;dest_uri&gt;</id></td><td>将一个虚拟机迁移到另一个目标地址</td></tr><tr><td>dump<id>&lt;core.file&gt;</id></td><td>coredump一个虚拟机保存到一个文件</td></tr><tr><td>dumpxml<id></id></td><td>以XML格式转存出一个虚拟机的信息到标准输出中</td></tr><tr><td>attach-device<id>&lt;device.xml&gt;</id></td><td>向一个虚拟机添加xml文件中的设备，也就是热插拔</td></tr><tr><td>detach-device<id>&lt;device.xml&gt;</id></td><td>将一个XML文件中的设备从虚拟机中移除</td></tr><tr><td>console<id></id></td><td>连接到一个虚拟机的控制台</td></tr></tbody></table><p>上表中，只是列出常用的几个KVM虚拟机全生命周期管理命令，如果想查找全量KVM虚拟机全量生命周期管理命令，可以使用命令帮助。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/9yVENoFcQwhX.png?imageslim" alt="mark"></p><p>如上图，由于输出太长，我们只截取了一部分。上图中每个命令后面都有详细的文字说明来描述命令的用途。</p><h2 id="虚拟机全生命周期管理实战"><a href="#虚拟机全生命周期管理实战" class="headerlink" title="虚拟机全生命周期管理实战"></a><strong>虚拟机全生命周期管理实战</strong></h2><h3 id="虚拟机状态信息查询"><a href="#虚拟机状态信息查询" class="headerlink" title="虚拟机状态信息查询"></a><strong>虚拟机状态信息查询</strong></h3><p>有了上面的命令储备后，我们下来在环境中进行实操验证。首先，我们通过<strong>list命令，查看下当前节点的虚拟机个数</strong>，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/4gsgTjd4d1Ep.png?imageslim" alt="mark"></p><p>如上图，本节点有2个虚拟机，当前状态均为shut off，也就是没有启动。那么，我们现在<strong>通过start命令，将上述两个虚拟机同时启动</strong>，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/LbwDCAd424XA.png?imageslim" alt="mark"></p><p>启动后，我们可以<strong>通过domstate命令查看虚拟机当前的状态</strong>，并且<strong>通过dominfo命令查看虚拟机的详细信息</strong>，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/esJBKFdwGiKl.png?imageslim" alt="mark"></p><p>同时，我们可以<strong>通过dommemstate和vcpuinfo命令，查看虚拟机的虚拟内存信息及vCPU信息。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/RLux9MLg8mKj.png?imageslim" alt="mark"></p><h3 id="虚拟机vCPU与vMEM管理操作"><a href="#虚拟机vCPU与vMEM管理操作" class="headerlink" title="虚拟机vCPU与vMEM管理操作"></a><strong>虚拟机vCPU与vMEM管理操作</strong></h3><p>如上图，通过上面的vCPU的详细信息，我们发现cto74d的两个vCPU都与pCPU0默认绑定。那么，我们现在<strong>可以通过vcpupin命令将vCPU1与pCPU1进行绑定。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/8XPMr9oRPeMj.png?imageslim" alt="mark"></p><p>同时，我们发现cto74d虚拟机只有2个vCPU。现在，我们需要给它增加到4个vCPU怎么办？在KVM中，可以<strong>通过setvcpus命令完成cto74d虚拟机的vCPU在线热添加。</strong>但是，在热添加之前，我们需要明确一个概念，那就是<strong>虚拟机的vCPU最大可分配数与当前vCPU数的区别</strong>。为了讲明白这个概念，我们还是先来看虚拟机cto74d的XML配置文件，可以<strong>通过dumpxml命令将虚拟机XML配置文件输出到屏幕上。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/A0Bjs5ILCXn4.png?imageslim" alt="mark"></p><p>上图中表示cto74d虚拟机的vCPU最大可分配数为2。而且，我们还可以<strong>通过指令emulatorpin查看虚拟机当前的vCPU使用情况。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/YLvs5uu9lgBJ.png?imageslim" alt="mark"></p><p>上图中表示虚拟机cto74d当前使用的vCPU个数也为2。所以，在这种配置要求下，自然无法通过指令将cto74d虚拟机的vCPU数量调整为4个。为了实现我们的需求，首先需要<strong>通过shutdown命令将虚拟机下电</strong>，然后修改配置文件如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/4VWpRaG4AmYb.png?imageslim" alt="mark"></p><p>上图中，红色框部分表示我们将虚拟机下电，黄色框部分我们将虚拟机的vCPU配置修改为：最大支持4个vCPU，当前使用2个vCPU。我们可以查看XML文件验证下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/6zNyvuUoag6r.png?imageslim" alt="mark"></p><p>完成上述修改后，我们需要<strong>通过define命令重新定义下cto74d这个虚拟机</strong>，然后通过start命令启动虚拟机。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/CKrHPluyFt1j.png?imageslim" alt="mark"></p><p>现在，我们具备条件后，就可以使用命令setvcpus命令在线对cto74d虚拟机的vCPU进行热添加，添加到4个。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/CxDRz9nrNYsF.png?imageslim" alt="mark"></p><p>在完成vCPU的热插后，那么vCPU是否可以热拔呢？<strong>答案是不可以</strong>，因为当vCPU被分配给虚拟机使用时，虚拟机中的程序进程就会占用刚分配的vCPU，此时如果我们进行热拔操作，前提是需要将上面的进程迁移到别的vCPU，但是我们是无法确切知道有哪些进程的。所以，vCPU自然不支持热拔操作。那么，既然KVM支持CPU和内存的虚拟化，内存是否也支持在线调整呢？答案是必须支持。我们首先通过dominfo命令看下当前虚拟机的内存情况。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/PRyNutTXm0SG.png?imageslim" alt="mark"></p><p>上图表示，虚拟机当前最大内存和可用内存都是4194304KB，也就是4GB大小。我们可以通过<strong>通过setmem指令设置虚拟机的可用内存。</strong>与vCPU概念类似，<strong>这种方式支持的在线调整范围只能小于当前虚拟机的最大内存配额，也就是4GB以内。否则，需要关机，先调整虚拟机支持的最大内存，然后再调整虚拟机支持的可用内存。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/wEdflEs6oMM2.png?imageslim" alt="mark"></p><p>从上图，我们发现，内存在线调整内存是可以减小的，即在线将虚拟机原内存从4G调整为3G。另外，<strong>setmaxmem指令用于调整虚拟机的最大可支持内存，只能在虚拟机下电后执行。</strong>否则会报如下错误：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/LVRtyuOYOcVX.png?imageslim" alt="mark"></p><h3 id="虚拟机磁盘和网卡的热插拔"><a href="#虚拟机磁盘和网卡的热插拔" class="headerlink" title="虚拟机磁盘和网卡的热插拔"></a><strong>虚拟机磁盘和网卡的热插拔</strong></h3><p>完成了vCPU和内存的在线调整实战后，我们接下来玩玩热插拔。先来玩热插拔磁盘，<strong>在KVM中通过virsh管理工具向虚拟机热插拔磁盘，可以使用attach-disk命令。</strong>为了实现上述需求，首先我们需要创建一个虚拟磁盘文件，然后通过attach-disk命令将其挂载到虚拟机中，然后通过控制台进入虚拟机对新挂载的磁盘进行格式化，并创建对应目录对其挂载。最后，我们向挂载目录中写入测试文件测试。同时，需注意向虚拟机挂载raw格式qcow2格式的磁盘是不一样的。</p><p>我们先来热插raw格式的磁盘，操作过程如下：</p><p><strong>Step1：创建一个虚拟磁盘cto74d_data.raw，磁盘格式为raw，大小为5G</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/OCgetrISBfwv.png?imageslim" alt="mark"></p><p><strong>Step2：为虚拟机热插磁盘，需要注意指定磁盘的位置要使用绝对路径，并设置虚拟盘符为vdb</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/S5QOTUYPNi1o.png?imageslim" alt="mark"></p><p><strong>Step3：通过控制台进入虚拟机cto74d，查看磁盘信息</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/5yVMDua18bFh.png?imageslim" alt="mark"></p><p><strong>Step4：对新挂载的磁盘格式化，并创建一个/data目录进行挂载</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/iD8ljAitUc6O.png?imageslim" alt="mark"></p><p><strong>Step5：写入一个大文件进行测试</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/GAu17SrIoRO4.png?imageslim" alt="mark"></p><p><strong>Step6：我们在宿主机上通过domblklist命令查看虚拟机的磁盘信息</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/CdbV1xieK9zt.png?imageslim" alt="mark"></p><p>如上图，可以看见当前虚拟机有两块磁盘，一块系统盘，一块数据盘。我们查看虚拟机XML文件信息进行验证。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/oaJm85WftQvP.png?imageslim" alt="mark"></p><p>以上就是raw格式磁盘的热插操作过程，如果想热插qcow2格式的磁盘（后缀为<em>.qcow2）或想热插qcow2格式的镜像文件（后缀为</em>.img），在Step1创建磁盘时，需要通过<strong>-o size选项指定预分配大小</strong>，否则系统会默认挂载一个几百K大小虚拟盘，这样会导致空间不够。同事，还需要通过preallocation选项指定磁盘格式为metadata，也就是元数据格式。除此之外，这两种类型磁盘的热插操作与raw格式磁盘一致。</p><p>上面完成了磁盘的热插，那么<strong>要实现磁盘的热拔呢？可以通过detach-disk这个命令完成。</strong>但是，这时需要注意一点：<strong>使用virsh指令删除磁盘会直接强制将虚拟机中磁盘删除，如果磁盘已经挂载使用，要停止该磁盘的写操作，否则会造成数据丢失，拔掉的磁盘并没有删除，仍然存储在当时创建的位置，需要使用可以再挂载使用。</strong>因此，在热拔磁盘之前，我们需要先进入虚拟机将对该磁盘的写入进程全部暂停，然后将磁盘从挂载点卸载，最后回到宿主机通过detach-disk命令完成磁盘的热拔。这里，大家自己玩吧，我就不陪着演示了。。。。。</p><p>磁盘的热插拔我们验证了，接下来我们玩网卡的热插拔。<strong>在KVM虚拟机中，要实现网卡的热插，可以使用attach-interface命令完成。同理，热拔的命令就是detach-interface。</strong>首先，我们通过domiflist命令查看当前虚拟机的网络和网卡信息，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/l5PQmvz25OML.png?imageslim" alt="mark"></p><p>如上图，黄色框部分就表示当前虚拟机cto74d的网络信息，类型（type）为network，源设备（source）为default。如果在创建KVM虚拟机时，网络配置是自定义网桥，那么这里的类型（type）为bridge，源设备（source）为自定义网桥名称，如br0，</p><p>有了上面的知识储备后，下面我们<strong>通过attach-interface命令来热插一个网卡</strong>，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Vy0jiitQreAe.png?imageslim" alt="mark"></p><p>同时，我们可以<strong>通过domifaddr命令查看新增加的网卡动态获得的IP地址</strong>，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/R4cBVmR8R43C.png?imageslim" alt="mark"></p><p>接下来，我们尝试下在宿主机通过SSH方式远程连接vnet2的地址是否成功。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/D0vPFQXRc26s.png?imageslim" alt="mark"></p><p>上图表示，可以通过vnet2的地址192.168.122.230远程连接虚拟机cto74d，表明我们新增加的网卡有效且地址分配正常（这个地址是通过DHCP方式动态分配的），通过输入密码就可以远程登录。</p><p>搞定网卡的热插后，我们接下来自然要实现网卡的热拔。与磁盘类似，在对网卡热拔时，首先需要停止网卡的工作，这样后续数据包就不会转发到该网卡上，不到导致网络丢包。而且，在对网卡热拔时，我们要根据网卡的MAC地址来实现，而不能通过网卡名称，否则libvirt内部会将该网卡名称归属的同一个二层设备上所有端口销毁。<strong>对网卡热拔的命令就是detach-ifinterface。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/vQCDbEPgABXf.png?imageslim" alt="mark"></p><p>上图中，通过–mac参数指定mac地址删除，就不会发生误删除事件。</p><h2 id="虚拟机的热迁移"><a href="#虚拟机的热迁移" class="headerlink" title="虚拟机的热迁移"></a><strong>虚拟机的热迁移</strong></h2><p>接下来，我们再来玩一个更高级的操作，就是<strong>KVM虚拟机的热迁移</strong>。在KVM中虚拟机的迁移分为热迁移和冷迁移。每一种迁移方式又细分为基于本地存储和基于共享存储两种。</p><p>两种方式下的冷迁移均比较简单，就是将原虚拟机的磁盘文件和配置文件拷贝至目标服务器，然后通过配置文件定义一个新的虚拟机启动即可。而基于本地存储的热迁移与此类似，也需要在目标主机创建同一个存储文件，通过暴露TCP端口，基于socket完成迁移，同样比较简单，但是过程很慢。有兴趣的可参考<a href="https://www.cnblogs.com/lyhabc/p/6104555.html这篇文章。" target="_blank" rel="noopener">https://www.cnblogs.com/lyhabc/p/6104555.html这篇文章。</a></p><p>我们这里演示的是<strong>基于共享存储的热迁移（电信云采用分布式存储，每一个计算组规划区都是一个共享存储池）</strong>，它实现的前提就是需要源目服务器之间有共享存储。因此，为了演示这个操作，我们需要在源主机（192.168.101.251）和目标主机（192.168.101.252）之间通过NFS方式建立共享文件系统，也可以使用GFS2集群文件系统来实现。至于，NFS网络共享文件系统如何配置，请参见本站的Linux常用运维工具分类中的NFS网络文件共享系统一文，这里不再赘述。</p><p>整个实验的拓扑如下，源主机和目标主机都作为NFS的客户端与NFS服务器（192.168.101.11）之间互通，实现数据的共享存储。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/zCAm9gf8p9n7.png?imageslim" alt="mark"></p><p><strong>Step1：首先确保两个NFS客户端服务器与NFS服务器端共享目录正常，且两个客户端服务器上存放虚拟机磁盘的目录一致。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/HgoivxdhLga0.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/rSaQdVAzV6F8.png?imageslim" alt="mark"></p><p><strong>Step2：在251服务器首先将虚拟机磁盘创建在/mydata/nfsdir下，格式为qcow2格式，然后去252服务器的对应目录下检查确保文件共享正常。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/NDzzCdfmkSOD.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/wngwtCqhKtgs.png?imageslim" alt="mark"></p><p><strong>Step3：在251服务器创建虚拟机cto67s，并启动。</strong>如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/XLRkoENtnjf4.png?imageslim" alt="mark"></p><p>在安装过程中，我们手动通过图形化方式安装，因此需要查询vnc图形化界面的连接地址和服务端口。<strong>可以通过vncdisplay命令查看虚拟机的VNC客户端连接地址及服务端口</strong>，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/iS3MgSR2soep.png?imageslim" alt="mark"></p><p>然后，还是通过vnc界面安装虚拟机，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xE60sntllmev.png?imageslim" alt="mark"></p><p>完成，虚拟机OS安装后，需要重启生效。然后，源主机（192.168.101.251）中通过start命令，启动虚拟机cto67s。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/YNuKDAIkgcPD.png?imageslim" alt="mark"></p><p><strong>Step4：开始虚拟机cto67s的热迁移。</strong>如下：</p><p>如上图，当前虚拟机cto67s在源主机（192.168.101.251）处于开机运行状态，为了清晰说明热迁移的过程，我们需要知道虚拟机cto67s的ip地址，这样在虚拟机热迁移时，由于内存热数据的迭代拷贝，会有一个暂停-恢复-暂停-恢复的过程，反映在网络测就会出抖动或丢包。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/ENTb8klo7Hoz.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/9z3VTf9gx7QF.png?imageslim" alt="mark"></p><p>完成上述的准备工作后，我们<strong>通过migrate命令开始进行虚拟机热迁移</strong>。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/JH3k3bXx8vJn.png?imageslim" alt="mark"></p><p><strong>如上图，我们期望的热迁移失败啦。。。。这是为啥？</strong>我们看上图红色框部分的报错内容，<strong>提示：域名解析失败。</strong>其实，在迁移期间，目标主机上运行的libvirtd会从其希望接收迁移数据的地址和端口创建URI，并将其发送回在源主机上运行的libvirtd。在我们的例子中，目标主机（192.168.122.252）的名称设置为“c7-test02”。出于某种原因，在该主机上运行的libvirtd无法将该名称解析为可以发回的IP地址，因此它发送了主机名，希望源libvirtd在解析名称时会更成功。但是，源主机的libvirt因为也没有配置这个主机名的解析，所以提示：Name or service not know。</p><p>有了上面的分析，那就简单了，无非就是在源主机和目的主机上配置主机名和IP的解析就可解决上述问题。这里，有两种配置方法：一种是在/etc/resolve中配置DNS解析，但是不建议这样配置，因为这里一般是配置公网域名解析的地方。另一种就是在/etc/hosts中配置主机名与IP的映射关系。我们采用第二种方法解决。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/pOLQ9JHHMtYk.png?imageslim" alt="mark"></p><p>完成上面的配置后，我们再次尝试热迁移。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/tpVeYi8s5QxT.png?imageslim" alt="mark"></p><p>同时，我们在源主机和目标主机分别查看虚拟机cto67s的状态。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/PwzLx1VP9W9Y.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/p0kNJc95gQ0S.png?imageslim" alt="mark"></p><p>此时，虽然虚拟机cto67s已经在目标主机上启动，但是目标主机上还没有虚拟机cto67s的配置文件。所以需要根据当前虚拟机的状态，创建配置文件并定义虚拟机。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/tOd4XoLcPp6z.png?imageslim" alt="mark"></p><p>最后，我们从目标主机上进入虚拟机cto67s进行验证，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/sKXOckTYzt8U.png?imageslim" alt="mark"></p><p>至此，KVM虚拟机热迁移演示完成。这里需要提示一下，由于我们创建的虚拟机都是nat网络模式的，这样在迁移后，在源主机ping虚拟机测试会提示目标不可达，但是在虚拟机内部ping源主机可以ping通，只是迁移过程中会有1-2个ICMP包丢失或抖动。如果，非要在宿主机上ping测试虚拟机来观察迁移过程中的丢包或抖动过程，可以将虚拟机的网络模式改为桥接bridge。而且，源主机和目标主机必须归属同一个网桥bridge。</p><p>以上，就是我们KVM虚拟机全生命周期管理实战的全部内容，至于虚拟机的挂起、恢复等操作都比较简单，我也就懒得举例演示，各位可以自行玩玩，挺有意思的，真的。。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇我们介绍了KVM最重要的管理工具libvirt，它是KVM其他管理工具的基础，处于KVM管理架构的中间适配层。本篇我们主要要介绍libvirt的命令行管理工具的virsh，它也是将libvirt作为基础，通过封包调用实现的。所以，在看本篇内容之前，最好将上一篇的内容做个预习。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-KVM虚拟机的各种安装方法</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%90%84%E7%A7%8D%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM虚拟机的各种安装方法/</id>
    <published>2020-03-01T10:19:09.000Z</published>
    <updated>2020-03-01T10:37:39.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="virt-instal工具简介"><a href="#virt-instal工具简介" class="headerlink" title="virt-instal工具简介"></a><strong>virt-instal工具简介</strong></h2><p>virt-install是一个命令行工具，它能够为KVM、Xen或其它支持libvrit API的hypervisor创建虚拟机并完成GuestOS安装；此外，它能够基于串行控制台、VNC或SDL支持文本或图形安装界面。安装过程可以使用本地的安装介质如CDROM，也可以通过网络方式如NFS、HTTP或FTP服务实现。对于通过网络安装的方式，virt-install可以自动加载必要的文件以启动安装过程而无须额外提供引导工具。当然，virt-install也支持PXE方式的安装过程，也能够直接使用现有的磁盘映像直接启动安装过程。<a id="more"></a></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/8h7pLTIGjpIA.png?imageslim" alt="mark"></p><p>virt-install命令有许多选项，这些选项大体可分为下面几大类，同时对每类中的常用选项也做出简单说明。</p><table><thead><tr><th>普通选项</th><th></th><th style="text-align:left"></th></tr></thead><tbody><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>-n或–name</td><td></td><td style="text-align:left">虚拟机名称，全局唯一</td></tr><tr><td>-r或–ram</td><td></td><td style="text-align:left">虚拟机内存大小，单位为MB</td></tr><tr><td>–vcpus</td><td>maxvcpu，sockets，cores，threads</td><td style="text-align:left">vCPU的个数及相关配置</td></tr><tr><td>–cpu</td><td></td><td style="text-align:left">CPU模式及特性，可以使用qemu-kvm -cpu ? 来获取支持的类型</td></tr><tr><td>安装方法选项</td><td></td><td style="text-align:left"></td></tr><tr><td>选项</td><td>子选项</td><td style="text-align:left">说明</td></tr><tr><td>-c或–cdrom</td><td></td><td style="text-align:left">光盘安装介质</td></tr><tr><td>-l或–location</td><td></td><td style="text-align:left">安装源URL，支持FPT，HTTP及NFS，如:<a href="ftp://172.0.6.1/pub" target="_blank" rel="noopener">ftp://172.0.6.1/pub</a></td></tr><tr><td>–pxe</td><td></td><td style="text-align:left">基于PXE完成安装</td></tr><tr><td>–livecd</td><td></td><td style="text-align:left">把光盘当做启动引导CD</td></tr><tr><td>–os-type</td><td></td><td style="text-align:left">操作系统类型，如linux、windows或unix等</td></tr><tr><td>–os-variant</td><td></td><td style="text-align:left">某类型操作系统的发行版，如rhel7、Ubuntud等</td></tr><tr><td>-x或–extra-args</td><td></td><td style="text-align:left">根据–location指定的方式安装GuestOS时，用于传递给内核的参数选项，例如指定kickstart文件的位置。</td></tr><tr><td>–boot</td><td></td><td style="text-align:left">指定安装过程完成后的配置选项，如指定引导设备的次序、使用指定的而非安装kernel/initrd来引导系统。比如：–boot cdrom、hd、network：指定引导次序分别为光驱、硬盘和网络；–boot kernel=KERNEL,initrd=INITRD,kernel_args=”console=/dev/ttyS0”：指定启动系统的内核及initrd文件，并创建一个模拟终端</td></tr><tr><td><strong>存储配置</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–disk</td><td></td><td style="text-align:left">指定存储设备的路径及其属性</td></tr><tr><td></td><td>device</td><td style="text-align:left">设备类型，如cdrom、disk或floppy等，默认为disk</td></tr><tr><td></td><td>bus</td><td style="text-align:left">磁盘总线类型，可以为ide、scsi、usb、virtio或xen</td></tr><tr><td></td><td>perms</td><td style="text-align:left">访问权限，如rw、ro或sh（共享可读写），默认为rw</td></tr><tr><td></td><td>size</td><td style="text-align:left">新建磁盘镜像大小，单位为GB</td></tr><tr><td></td><td>cache</td><td style="text-align:left">缓存类型，可以为none、writethrouth及writeback</td></tr><tr><td></td><td>format</td><td style="text-align:left">磁盘镜像格式，如raw、qcow2、vmdk等</td></tr><tr><td></td><td>sparse</td><td style="text-align:left">磁盘镜像的存储数据格式为稀疏格式，即不立即分配指定大小的空间</td></tr><tr><td>–nodisks</td><td></td><td style="text-align:left">不适用本地磁盘，在LiveCD模式中常用</td></tr><tr><td><strong>网络配置</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>-w或–network</td><td></td><td style="text-align:left">将虚拟机连入虚拟网络中</td></tr><tr><td></td><td>bridge=BRIDGE</td><td style="text-align:left">虚拟网络为名称为BRIDGE的网桥设备</td></tr><tr><td></td><td>network=NAME</td><td style="text-align:left">虚拟网络为名称NAME的网络</td></tr><tr><td></td><td>model</td><td style="text-align:left">GuestOS中看见的虚拟网络设备类型</td></tr><tr><td></td><td>mac</td><td style="text-align:left">配置固定的MAC地址，省略此选项时，MAC地址随机分配，但是无论何种方式，KVM的网络设备的MAC地址前三段必须为52:54:00</td></tr><tr><td>–nonetworks</td><td></td><td style="text-align:left">虚拟机不使用网络</td></tr><tr><td><strong>图形配置</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–graphics</td><td></td><td style="text-align:left">指定图形显示相关的配置，此选项不会配置任何硬件，而是指定虚拟机启动后对其访问的图形界面接口</td></tr><tr><td></td><td>TYPE</td><td style="text-align:left">指定显示类型，可以为vnc，spice、sdl或none</td></tr><tr><td></td><td>port</td><td style="text-align:left">TYPE为vnc或spice时其监听的端口</td></tr><tr><td></td><td>listen</td><td style="text-align:left">TYPE为vnc或spice时其监听的IP地址，默认为127.0.0.1，可以通过修改/etc/libvirt/qemu.conf定义新的默认值</td></tr><tr><td></td><td>password</td><td style="text-align:left">TYPE为vnc或spice时为远程访问监听的服务指定认证密码</td></tr><tr><td>–noautoconsole</td><td></td><td style="text-align:left">禁止自动连接到虚拟机的控制台</td></tr><tr><td><strong>设备选项</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–serial</td><td></td><td style="text-align:left">附加一个串行设备到当前虚拟机，根据设备类型不同，可以使用不同的选项，格式为–serial type,opt1=val1,opt2=val2</td></tr><tr><td></td><td>pty</td><td style="text-align:left">创建伪终端</td></tr><tr><td></td><td>dev,path=HOSTPATH</td><td style="text-align:left">附加主机设备至此虚拟机</td></tr><tr><td>–video</td><td></td><td style="text-align:left">指定显卡设备类型，如cirrus、vga、qxl或vmvga</td></tr><tr><td><strong>虚拟化平台</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>-v或–hvm</td><td></td><td style="text-align:left">当宿主机同时支持全虚和半虚时，使用此选项指定硬件辅助的全虚</td></tr><tr><td>-p或–paravirt</td><td></td><td style="text-align:left">指定使用半虚</td></tr><tr><td>–virt-type</td><td></td><td style="text-align:left">指定使用的hypervisor，如kvm、xen、qemu等，其值可以通过virsh capabilities获得</td></tr><tr><td><strong>其它</strong></td><td></td><td style="text-align:left"></td></tr><tr><td><strong>选项</strong></td><td><strong>子选项</strong></td><td style="text-align:left"><strong>说明</strong></td></tr><tr><td>–autostart</td><td></td><td style="text-align:left">指定虚拟机是否在物理机启动后自动启动</td></tr><tr><td>–print-xml</td><td></td><td style="text-align:left">如果虚拟机不需要安装过程(–import、–boot)，则显示生成的XML而不是创建虚拟机。默认情况下，此选项仍会创建虚拟磁盘</td></tr><tr><td>–force</td><td></td><td style="text-align:left">禁止进入命令交互模式，如需回答yes或no，默认自动回答yes</td></tr><tr><td>–dry-run</td><td></td><td style="text-align:left">执行创建虚拟机的整个过程，但不创建虚拟机，改变主机上的设备配置信息及将其创建的需求通知给libvirt；</td></tr><tr><td>-d或–debug</td><td></td><td style="text-align:left">显示debug信息；</td></tr></tbody></table><p><strong>尽管virt-install命令有着类似上述的众多选项，但实际使用中，其必须提供的选项仅包括–name、–ram、–disk（也可是–nodisks）及安装过程相关的选项。此外，有时还需要使用括–connect=CONNCT选项来指定连接至一个非默认的hypervisor。</strong></p><h2 id="图形化界面安装"><a href="#图形化界面安装" class="headerlink" title="图形化界面安装"></a><strong>图形化界面安装</strong></h2><p>这里的使用图形化界面不是通过virt-manager和virt-view工具在图形化服务器或宿主机来安装，而是通过–graphics选项指定vnc或spice方式安装，安装过程中需要宿主机通过vnc client连接图形化接口实现。在我们的KVM实践初步一文中介绍安装ubuntu1204桌面版虚拟机就是采用这种方式。忘了的，可以回顾本站那篇文章。</p><p>现在，我们这里演示的是另一种图形化安装方法，还是通过vnc实现，安装介质使用ubuntu18.10桌面版ISO。如下：</p><p><strong>step1：</strong>我们先查看下当前激活的虚拟网络有哪些，方便后续安装虚拟机时配置网络（图中红色框部分）。同时，我们创建一个qcow2格式的虚拟机磁盘镜像，大小为120G（图中黄色框部分）。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Vz0l4nn8rfrG.png?imageslim" alt="mark"></p><p><strong>step2：</strong>通过virt-install工具安装第一个模板虚拟机ubt1810d。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/g23WFSzoMi86.png?imageslim" alt="mark"></p><p>上图中，我们指定了虚拟机当前vcpu为2个，最大可分配4个（黄色框部分），同时指定了虚拟机的镜像磁盘格式为qcow2格式（采用qcow2格式，此项必须明确指定，并且通过size选项指定大小，否则生成的磁盘大小异常，或者在创建镜像磁盘时通过-o选项明确指定也可以），总线类型为virtio类型（图中蓝色框部分）</p><p><strong>step3：</strong>通过vncdisplay命令查询当前虚拟机图形界面接口服务vnc监听的端口号，并通过物理机的vnc client连接开始安装。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/CCcvSaBXLyzV.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Mg1jNeuPheOz.png?imageslim" alt="mark"></p><p><strong>step4：</strong>完成虚拟机系统安装后，我们通过vnc client登录虚拟机，打开虚拟机的console控制接口，便于后续通过宿主机console连接虚拟机控制台。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/GSiPEXUgO8hK.png?imageslim" alt="mark"></p><p><strong>以上，就是通过图形界面接口实现kvm虚拟机的安装过程。一般通过图形界面接口安装，主要用于安装桌面版的操作系统，比如上面的ubuntu desktop版本，windows的各种版本等。</strong>由于ubuntu系统默认不开启root用户的ssh登录，为了方便后续管理，我们可以在ubuntu系统打开ssh的root登录权限。但是，记得首先要设置root的登录密码。这些操作就可以通过宿主机console控制台来完成。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/mODBkfX9JvPd.png?imageslim" alt="mark"></p><h2 id="文本字符界面安装"><a href="#文本字符界面安装" class="headerlink" title="文本字符界面安装"></a><strong>文本字符界面安装</strong></h2><p>上面的图形化安装方式主要借助vnc或spice客户端来实现图形界面安装接口，主要用于桌面操作系统的安装。在实际运维时，一般用于app的虚拟机大部分采用非桌面系统，而且打开vnc或spice监听端口不便于批量安装。这种情况下，就需要通过文本界面实现自动安装。主要通过-x选项打开一个串行接口终端tty和ttyS0实现。但是，如果使用-x选项安装，安装介质必须使用–location选项，而不能使用–cdrom来完成。此时，有两个选择，一是将iso镜像拷贝到宿主机的某个目录，通过-l（–location）选项指定即可，类似–cdrom方式。另一种是将iso挂载到某个目录，通过http、nfs或ftp方式发布，然后在-l选项指定URL即可。我们这里采用第二种方式的http方式进行安装，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/poIofAW7g8rO.png?imageslim" alt="mark"></p><p>上述安装方式，通过-l选项指定安装介质的挂载目录，通过-x选项安装使用kiskstart文件，以及打开串行终端console，通过–nographics选项指定不采用图形界面方式安装。命令执行后，安装界面如下，且通过kickstart文件自动化安装。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/3VzJdfq3iWea.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/yd9HUluqTuxw.png?imageslim" alt="mark"></p><p>以上这种安装方式，也是实际运维中常用的安装方式，同时可以在kickstart文件中指定初始化脚本，这样在系统安装完成后自动完成初始化配置。后续，就可以将此虚拟机作为模板镜像，批量分发。</p><p>但是，如果要将当前虚拟机作为模板镜像，需要删除当前虚拟机中的MAC和UUID，同时清空本地规则和系统规则中网卡配置文件，这样后续通过该虚拟机的模板镜像生成的新的虚拟机就会自动生成新的MAC和网卡名称，而不用通过在XML配置文件中手动指定MAC，实现自动化运维。上面的要求，需要在虚拟机中完成以下配置，然后关机作为后续分发虚拟机的模板。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/3LCymbL4DjcE.png?imageslim" alt="mark"></p><p><strong>虚机模板方式安装</strong></p><p>模板方式安装一般有两种方式：一种是通过–import选项导入镜像虚拟机的磁盘来生成一个新的虚拟机，这种方式只能生成一个新虚拟机，当要生成第二个虚拟机时会提示磁盘被占用错误。另一种方式就是利用模板镜像虚拟机的XML文件，生成新的虚拟机的XML配置文件，需要删除源模板镜像虚拟机XML文件中的uuid、mac address，并修改磁盘路径为新虚拟机的磁盘（差分盘或新磁盘都可），而且需要修改XML文件中的虚拟机name为新建虚拟机name。完成后，再通过virsh define预定义一个新虚拟机，然后通过virsh start启动即可。下面，我们就对这两种方式逐一进行验证。</p><p><strong>1）通过–import选项导入现有虚拟机磁盘生成一个新的虚拟机</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xiMr145MYNS2.png?imageslim" alt="mark"></p><p>完成上面的操作后，我们就能立即生成一个新的虚拟机cirros01。然后，我们可以通过virsh console命令登录并执行相关操作。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/RzAabqMfbimw.png?imageslim" alt="mark"></p><p>此时，我们如果需要再通过上面镜像盘生成一个虚拟机时，会提示镜像盘被占用的错误。如下：</p><p>因此，这种方式只能生成一个新虚拟机，一般用于很小的初始化配置测试。所以，在实际运维中，主要通过第二种方式利用模板镜像虚拟机来批量创建虚拟。</p><p><strong>2）通过模板镜像虚机利用virsh-define和virsh-start实现新虚机创建</strong></p><p>首先，我们利用模板镜像虚机磁盘创建一个差分磁盘，用于新虚机的系统盘（也可以创建非差分盘，看实际业务需求而定）。如下，通过-b选项指定backing file为模板镜像虚机的磁盘。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/OBslPPz9qRyz.png?imageslim" alt="mark"></p><p>然后，利用模板镜像虚机的xml文件生成新虚机的xml配置文件。需要删除uuid，mac address配置，修改磁盘存储位置为新创建的虚机差分盘，修改name为新虚机的name。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/cV37uADblJG4.png?imageslim" alt="mark"></p><p>然后，通过virsh-define定义新虚机，再通过virsh-start启动新虚机，然后通过console控制台进入虚机。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/HmLaBCUbe4OL.png?imageslim" alt="mark"></p><p>如下图，可以发现系统为我们自动生成mac和ip，而且mac必然是52:54:00开头。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/fkQjaiYjEbrI.png?imageslim" alt="mark"></p><p>而且，新生成的虚拟机具备访问外网和宿主机的权利。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Kl5gCrHopMBl.png?imageslim" alt="mark"></p><h2 id="批量创建KVM虚拟机的脚本"><a href="#批量创建KVM虚拟机的脚本" class="headerlink" title="批量创建KVM虚拟机的脚本"></a><strong>批量创建KVM虚拟机的脚本</strong></h2><p>掌握上面各种创建KVM虚拟机的方法后，我们可以写一个shell脚本来完成批量创建虚拟机的任务。</p><p>首先，我们需要准备创建虚拟机的镜像磁盘，主要有两种方法：一种是拷贝模板虚拟机的镜像磁盘，这种主要用于创建独立虚拟机，优点是不依赖于模板虚拟机磁盘镜像，可以完整保留数据。缺点就是占用存储空间较大。另一种方式就是我们上面的通过建立差分磁盘实现，优点是存储空间占用较小，缺点就是模板镜像磁盘一旦损坏将造成所有依赖虚拟机无法启动。</p><p>完成虚拟磁盘创建后，我们就可以按照上面办法通过模板镜像虚机的XML配置文件批量创建新的虚拟机XML配置文件，并修改里面的NAME、UUID、MAC ADDRESS和DISK PATH等配置项。最终，通过define和start命令完成新虚拟机的定义和启动。</p><p>通过上面的分析，要实现KVM虚拟机批量创建shell脚本，只需要通过两个for循环就能完成。同时，在程序开始要检查程序脚本的执行权限，从管理角度来说非root用户不能执行。还要给脚本进行传参，用来设定需要批量拉起的KVM虚拟机的上限值和下限值，也就是确定批量拉起的虚机个数。那么，传递的参数就必须是2个，不能多也不能少，且两个参数都必须是数字，参数1的值要小于参数2。</p><p>以上，就是我们程序要实现的逻辑，可以采用函数式编程的概念封装成三个函数，实现简单模块化设计。为了讲述清楚，我这里就不封装了。接下来，我们就来一步步写这个脚本。</p><p><strong>Step1：</strong>通过第一个for循环批量创建虚拟机的虚拟磁盘，这里我们采用差分磁盘的方式。如下</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/05H8pcgg7tWr.png?imageslim" alt="mark"></p><p>脚本执行后效果如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/qxi9Lh37MYeC.png?imageslim" alt="mark"></p><p><strong>Step2：</strong>通过第二个for循环批量创建虚拟机的XML配置文件，并定义和启动虚拟机。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/duowiSJnM12D.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/IPjfNbXTAktd.png?imageslim" alt="mark"></p><p>完成上面的替换，在脚本执行virsh defiine和virsh start命令定义新的虚拟机并完成启动。脚本执行后效果如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/tTEnd55F6vDR.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Y0wbcGDO6FIF.png?imageslim" alt="mark"></p><p>至此，KVM虚拟机的各种创建方法介绍完毕，网上还有一种通过qemu-kvm命令创建虚拟机的方法，与通过virt-install方法创建大同小异，且红帽官方并不建议这种方法。因此，只要掌握virt-install方法创建虚拟机即可。</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/lI8SNxelT4gE.png?imageslim" alt="mark"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;virt-instal工具简介&quot;&gt;&lt;a href=&quot;#virt-instal工具简介&quot; class=&quot;headerlink&quot; title=&quot;virt-instal工具简介&quot;&gt;&lt;/a&gt;&lt;strong&gt;virt-instal工具简介&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;virt-install是一个命令行工具，它能够为KVM、Xen或其它支持libvrit API的hypervisor创建虚拟机并完成GuestOS安装；此外，它能够基于串行控制台、VNC或SDL支持文本或图形安装界面。安装过程可以使用本地的安装介质如CDROM，也可以通过网络方式如NFS、HTTP或FTP服务实现。对于通过网络安装的方式，virt-install可以自动加载必要的文件以启动安装过程而无须额外提供引导工具。当然，virt-install也支持PXE方式的安装过程，也能够直接使用现有的磁盘映像直接启动安装过程。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-KVM管理工具libvirt</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7libvirt/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM管理工具libvirt/</id>
    <published>2020-03-01T08:29:40.000Z</published>
    <updated>2020-03-01T10:14:01.716Z</updated>
    
    <content type="html"><![CDATA[<p>通过前面两篇文章的介绍，相信大家对KVM虚拟机的原理、软件架构、运行机制和部署方式等有所了解。但是，那些东西只能算是非常基础入门的东西。尤其我们介绍的部署KVM虚拟机时用到的virt-install命令，不仅参数很多，很难记忆，而且要想用好virt-<em>系列工具首先需要对libvirt工具有更深刻的了解。因为，virt-</em>系列工具其实是对libvirt工具的封装调用，而libvirt工具又是对底层qemu工具的封装调用，其目的都是为了使命令更加友好与用户交互。本篇文章就详细介绍这几种与libvirt相关的管理工具，为后续各种配置打下良好基础。<a id="more"></a></p><h2 id="libvirt管理工具"><a href="#libvirt管理工具" class="headerlink" title="libvirt管理工具"></a><strong>libvirt管理工具</strong></h2><p>提到KVM的管理工具，就不得不提大名鼎鼎的libvirt，因为libvirt是目前使用最为广泛的对KVM虚拟机进行管理的工具和应用程序接口，而且一些常用的虚拟机管理工具（如virsh、virt-install、virt-manager等）和云计算框架平台（如OpenStack、ZStack、OpenNebula、Eucalyptus等）都在底层使用libvirt的应用程序接口，也就是说<strong>libvirt实际上是一个连接底层Hypervisor与上层应用的中间适配层。</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/wGjIlOxyI2PX.png?imageslim" alt="mark"></p><p>如上图，<strong>libvirt作为一个中间适配层，可以让底层Hypervisor对上层用户空间的管理工具是完全透明的，</strong>也就是说上层管理应用无需关注底层的差别，只需要调用libvirt去完成，因此只需要对接libvirt的对外开放api接口即可。同时，<strong>libvirt对底层多种不同的Hypervisor的支持是通过一种基于驱动的架构来实现的，类似neutron中的ml2层。libvirt对不同的Hypervisor提供了不同的驱动。</strong>比如：对Xen有xen_dirver.c驱动文件，对vmware有vmware_dirver.c驱动文件，对kvm有qemu_driver.c驱动文件等等。。。</p><p><strong>在libvirt中涉及节点Node、Hypervisor、域Domain等几个概念，其逻辑关系如下：</strong></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/jqoBUHL5wLt6.png?imageslim" alt="mark"></p><blockquote><ul><li><strong>节点（Node）是一个物理机器</strong>，上面可能运行着多个虚拟客户机。Hypervisor和Domain都运行在节点上。</li><li><strong>Hypervisor也称虚拟机监控器（VMM），如KVM、Xen、VMware、Hyper-V等，是虚拟化中的一个底层软件层</strong>，它可以虚拟化一个节点让其运行多个虚拟机（不同虚拟机可能有不同的配置和操作系统）。</li><li><strong>域（Domain）是在Hypervisor上运行的一个虚拟机操作系统实例。</strong>域也被称为实例（instance，如在亚马逊的AWS云计算服务中客户机就被称为实例）、客户机操作系统（guest OS）、虚拟机，它们都是指同一个概念。</li></ul></blockquote><p>libvirt相关的配置文件都在/etc/libvirt/目录之中，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/qrwqK8rHepb9.png?imageslim" alt="mark"></p><p>主要涉及四个文件：</p><p><strong>1）/etc/libvirt/libvirt.conf：</strong>用于配置一些常用libvirt连接（通常是远程连接）的别名。比如：uri_aliases = [  “remote1=qemu+ssh:<a href="mailto://root@192.168.101.252" target="_blank" rel="noopener">//root@192.168.101.252</a>/system” ]，有这个别名后，就可以在用virsh等工具或自己写代码调用libvirt API时使用这个别名，比如在Python可以这样调用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conn = libvirt.openReadOnly(<span class="string">'remote1'</span>)</span><br></pre></td></tr></table></figure><p><strong>2）/etc/libvirt/libvirtd.conf：</strong>是libvirt的守护进程libvirtd的配置文件，被修改后需要让libvirtd重新加载配置文件（或重启libvirtd）才会生效。在文件的每一行中使用“配置项=值”（如tcp_port=”16509”）这样key-value格式来设置。我们上篇文章介绍的说监听tcp链接和端口，就是在这里配置。</p><p><strong>3）/etc/libvirt/qemu.conf：</strong>是libvirt对QEMU的驱动的配置文件，包括VNC、SPICE等，以及连接它们时采用的权限认证方式的配置，也包括内存大页、SELinux、Cgroups等相关配置。</p><p><strong>4）/etc/libvirt/qemu/目录：</strong>存放的是使用QEMU驱动的域的配置文件，也就是kvm虚拟机的配置文件以及networks目录存放默认网络配置文件。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/snLnPbiMzGiN.png?imageslim" alt="mark"></p><p><strong>要让某个节点能够利用libvirt进行管理（无论是本地还是远程管理），都需要在这个节点上运行libvirtd这个守护进程，以便让其他上层管理工具可以连接到该节点，libvirtd负责执行其他管理工具发送给它的虚拟化管理操作指令</strong>，比如：virsh、virt-manager、nova-compute等等。在CentOS 7.x系统中，libvirtd作为一个系统服务存在，可以利用systemctl工具进行启动、停止、重启、重载等操作。另外，libvirtd守护进程的启动或停止，并不会直接影响正在运行中的客户机。libvirtd在启动或重启完成时，只要客户机的XML配置文件是存在的，libvirtd会自动加载这些客户的配置，获取它们的信息。</p><p><strong>libvirtd除了是系统服务外，本身还是一个可执行程序，可以通过一些选项完成系统进程的启动。</strong>比如：-d选项可以让libvirtd作为守护进程（daemon）在后台运行。-f选项可以指定libvirtd的配置文件启动服务，而不使用默认的配置文件。-l选项开启配置文件中指定的TCP/IP连接，监听TCP socket和服务端口。-p指定进程PID，不使用系统默认分配的PID号等等。</p><h2 id="解读虚拟机的XML配置文件"><a href="#解读虚拟机的XML配置文件" class="headerlink" title="解读虚拟机的XML配置文件"></a><strong>解读虚拟机的XML配置文件</strong></h2><p>在使用libvirt对虚拟化系统进行管理时，很多地方都是以XML文件作为配置文件的，包括虚拟机的配置、宿主机网络接口配置、网络过滤、各个客户机的磁盘存储配置、磁盘加密、宿主机和虚拟机的CPU特性等等。所以，对XML文件进行解析，对后续使用libvirtd管理KVM虚拟机理解的更为深刻。</p><p>我们首先来看下，我们前一篇文章创建的虚拟机ubt1204d的XML配置文件如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]# cat /etc/libvirt/qemu/ubt1204d.xml </span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE</span></span><br><span class="line"><span class="comment">OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:</span></span><br><span class="line"><span class="comment">  virsh edit ubt1204d</span></span><br><span class="line"><span class="comment">or other application using the libvirt API.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">domain</span> <span class="attr">type</span>=<span class="string">'kvm'</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ubt1204d<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">uuid</span>&gt;</span>4c4797b9-776c-44e4-8ef5-3ad445092d0f<span class="tag">&lt;/<span class="name">uuid</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">memory</span> <span class="attr">unit</span>=<span class="string">'KiB'</span>&gt;</span>2097152<span class="tag">&lt;/<span class="name">memory</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">currentMemory</span> <span class="attr">unit</span>=<span class="string">'KiB'</span>&gt;</span>2097152<span class="tag">&lt;/<span class="name">currentMemory</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">vcpu</span> <span class="attr">placement</span>=<span class="string">'static'</span>&gt;</span>2<span class="tag">&lt;/<span class="name">vcpu</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">os</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">type</span> <span class="attr">arch</span>=<span class="string">'x86_64'</span> <span class="attr">machine</span>=<span class="string">'pc-i440fx-rhel7.0.0'</span>&gt;</span>hvm<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">boot</span> <span class="attr">dev</span>=<span class="string">'hd'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">os</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">features</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">acpi</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">apic</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">features</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">cpu</span> <span class="attr">mode</span>=<span class="string">'custom'</span> <span class="attr">match</span>=<span class="string">'exact'</span> <span class="attr">check</span>=<span class="string">'partial'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">model</span> <span class="attr">fallback</span>=<span class="string">'allow'</span>&gt;</span>Broadwell-IBRS<span class="tag">&lt;/<span class="name">model</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">cpu</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">clock</span> <span class="attr">offset</span>=<span class="string">'utc'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">timer</span> <span class="attr">name</span>=<span class="string">'rtc'</span> <span class="attr">tickpolicy</span>=<span class="string">'catchup'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">timer</span> <span class="attr">name</span>=<span class="string">'pit'</span> <span class="attr">tickpolicy</span>=<span class="string">'delay'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">timer</span> <span class="attr">name</span>=<span class="string">'hpet'</span> <span class="attr">present</span>=<span class="string">'no'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">clock</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">on_poweroff</span>&gt;</span>destroy<span class="tag">&lt;/<span class="name">on_poweroff</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">on_reboot</span>&gt;</span>restart<span class="tag">&lt;/<span class="name">on_reboot</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">on_crash</span>&gt;</span>destroy<span class="tag">&lt;/<span class="name">on_crash</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">pm</span>&gt;</span></span><br><span class="line"> 。。。。省略若干行。。。</span><br><span class="line"><span class="tag">&lt;/<span class="name">domain</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如上，可以看到在该虚拟机的XML文件中，所有有效配置都在<domain>和</domain>标签之间，这表明该配置文件是一个域的配置。而XML文档中的注释是在两个特殊的标签之间，如&lt;！–注释–&gt;。</p><p>通过libvirt启动客户机，经过文件解析和命令参数的转换，最终也会调用qemu命令行工具来实际完成客户机的创建。用这个XML配置文件启动的客户机，它的qemu命令行参数是非常详细、非常冗长的一行，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/CY30dlu4how6.png?imageslim" alt="mark"></p><p>下面，我们就逐个模块来解析虚拟机的XML配置文件。</p><h3 id="CPU的配置"><a href="#CPU的配置" class="headerlink" title="CPU的配置"></a><strong>CPU的配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/fVMgeJCQfpSJ.png?imageslim" alt="mark"></p><p>上面的XML文件中关于CPU的配置项如上图所示，vcpu标签，表示客户机中vCPU的个数，这里为2。features标签，表示Hypervisor为客户机打开或关闭CPU或其他硬件的特性，这里打开了ACPI、APIC等特性。而cpu标签中定义了CPU的基础特性，在创建虚拟机时，libvirt自动检测硬件平台，默认使用Broadwell类型的CPU分配给虚拟机。在CPU模型中看见的Broadwell-IBRS，可以在/usr/share/libvirt/cpu_map.xml文件查看其具体信息，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/b4YvgfUANeYv.png?imageslim" alt="mark"></p><p><strong>对于CPU模型的配置，有以下3种模式。</strong></p><p>1）custom模式：就是这里示例中表示的，基于某个基础的CPU模型，再做个性化的设置。</p><p>2）host-model模式：根据物理CPU的特性，选择一个与之最接近的标准CPU型号，如果没有指定CPU模式，默认也是使用这种模式。xml配置文件为：<cpu mode="host-model">。</cpu></p><p>3）host-passthrough模式：直接将物理CPU特性暴露给虚拟机使用，在虚拟机上看到的完全就是物理CPU的型号。xml配置文件为：<cpu mode="host-passthrough">。</cpu></p><p>对vCPU的分配，可以有更细粒度的配置，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;domain&gt;</span><br><span class="line">  ...</span><br><span class="line">  &lt;vcpu placement=<span class="string">'static'</span> cpuset=<span class="string">"1-4,^3,6"</span> current=<span class="string">"1"</span>&gt;2&lt;/vcpu&gt;</span><br><span class="line">  ...</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure><p>cpuset表示允许到哪些物理CPU上执行，这里表示客户机的两个vCPU被允许调度到1、2、4、6号物理CPU上执行（^3表示排除3号）；而current表示启动客户机时只给1个vCPU，最多可以增加到使用2个vCPU。除了这种方式外，libvirt还提供cputune标签来对CPU的分配进行更多调节，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;domain&gt;</span><br><span class="line">  ...</span><br><span class="line">  &lt;cputune&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"0"</span> cpuset=<span class="string">"1"</span>/&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"1"</span> cpuset=<span class="string">"2,3"</span>/&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"2"</span> cpuset=<span class="string">"4"</span>/&gt;</span><br><span class="line">    &lt;vcpupin vcpu=<span class="string">"3"</span> cpuset=<span class="string">"5"</span>/&gt;</span><br><span class="line">    &lt;emulatorpin cpuset=<span class="string">"1-3"</span>/&gt;</span><br><span class="line">    &lt;shares&gt;2048&lt;/shares&gt;</span><br><span class="line">    &lt;period&gt;1000000&lt;/period&gt;</span><br><span class="line">    &lt;quota&gt;-1&lt;/quota&gt;</span><br><span class="line">    &lt;emulator_period&gt;1000000&lt;/emulator_period&gt;</span><br><span class="line">    &lt;emulator_quota&gt;-1&lt;/emulator_quota&gt;</span><br><span class="line">  &lt;/cputune&gt;</span><br><span class="line">  ...</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure><p>还记得我们在介绍DPDK的亲和性技术不？这里就是<strong>设置亲和性特性的调优配置</strong>，其中vcpupin标签表示将虚拟CPU绑定到某一个或多个物理CPU上，如“&lt;vcpupin vcpu=”2”cpuset=”4”/&gt;”表示客户机2号虚拟CPU被绑定到4号物理CPU上；“<emulatorpin cpuset="1-3">”表示将QEMU emulator绑定到1~3号物理CPU上。在不设置任何vcpupin和cpuset的情况下，虚拟机的vCPU可能会被调度到任何一个物理CPU上去运行。而“<shares>2048</shares>”表示虚拟机占用CPU时间的加权配置，一个配置为2048的域获得的CPU执行时间是配置为1024的域的两倍。如果不设置shares值，就会使用宿主机系统提供的默认值。</emulatorpin></p><p>除了亲和性绑定外，<strong>还有NUMA架构的调优设置，可以配置虚拟机的NUMA拓扑，以及让虚拟机针对宿主机NUMA特性做相应的策略设置等，主要在标签和标签中完成配置。</strong>NUMA特性配置需要在真实物理服务器上完成，手头暂时没有对应环境，也就不列举配置项了。但是，能够对NUMA进行配置的前提是了解NUMA的原理，所以理解前面DPDK技术和计算虚拟化中NUMA技术原理是重点。</p><h3 id="内存的配置"><a href="#内存的配置" class="headerlink" title="内存的配置"></a><strong>内存的配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/w94y5iYjDP4j.png?imageslim" alt="mark"></p><p>在KVM虚拟机的XML配置文件中，内存的大小配置如上，大小为2,097,152KB（即2GB），memory标签中的内存表示虚拟机最大可使用的内存，currentMemory标签中的内存表示启动时分配给虚拟机使用的内存。在使用QEMU/KVM时，一般将二者设置为相同的值。</p><p>另外，还记得我们讲内存虚拟化时，提到的内存气球技术不？在KVM虚拟机创建时，默认采用内存气球技术给虚拟机提供虚拟内存，它的配置项在<devices></devices>标签对中的memballoon子标签中，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/DP278JT2GOCE.png?imageslim" alt="mark"></p><p>如上图，该配置将为虚拟机分配一个使用virtio-balloon驱动的内存气球设备，以便实现虚拟机内存的ballooning调节。该设备在客户机中的PCI设备编号为0000:00:06.0，也就是设备的I/O空间地址。</p><h3 id="虚拟机启动项配置"><a href="#虚拟机启动项配置" class="headerlink" title="虚拟机启动项配置"></a><strong>虚拟机启动项配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/5lDtAgD0aEy7.png?imageslim" alt="mark"></p><p>如上图，这样的配置表示客户机类型是hvm类型，HVM（hardware virtual machine，硬件虚拟机）原本是Xen虚拟化中的概念，它表示在硬件辅助虚拟化技术（Intel VT或AMD-V等）的支持下不需要修改客户机操作系统就可以启动客户机。因为KVM一定要依赖于硬件虚拟化技术的支持，所以<strong>在KVM中，客户机类型应该总是hvm</strong>，操作系统的架构是x86_64，机器类型是pc-i440fx-rhel7.0.0，这个机器类型是libvirt中针对RHEL 7系统的默认类型，也可以根据需要修改为其他类型。</p><p>boot选项用于设置虚拟机启动时的设备，这里只有hd（即硬盘）一种，表示从虚拟机硬盘启动，如果有两种及以上，就与物理机中BIOS设置一样，按照从上到下的顺序先后启动。</p><h3 id="网络的配置"><a href="#网络的配置" class="headerlink" title="网络的配置"></a><strong>网络的配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/nQLC4zzrVT2X.png?imageslim" alt="mark"></p><p>如上图，上面虚拟机XML文件的网络时配置是一种<strong>NAT方式的网络配置</strong>，这里type=’network’和<source network="default">就是表示使用NAT的方式，并使用默认的网络配置，虚拟机将会分到192.168.122.0/24网段中的一个IP地址，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xphYfqupGzTt.png?imageslim" alt="mark"></p><p>使用NAT网络配置的前提是宿主机中要开启DHCP和DNS服务，一般libvirtd进程默认使用dnsmasq同时提供DHCP和DNS服务，在宿主机中通过以下命令可以查看：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/HBYkH6sKspvM.png?imageslim" alt="mark"></p><p>这里的NAT网络的配置在/etc/libvirt/qemu/networks/defaule.xml中配置，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/HAMuf8fjtNTx.png?imageslim" alt="mark"></p><p>如上，这里配置就是提供分布式虚拟交换机virbr0，用于KVM虚拟机的虚拟接入。详细配置在前一篇文章有介绍，这里不再赘述。由于使用的Linux bridge作为虚拟交换机，因此可以在宿主机中查看该网桥的实际端口，指令如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/M13kauIhsHCE.png?imageslim" alt="mark"></p><p>上图中的vnet0就是KVM虚拟机ubt1204d的虚拟网卡，这样就实现了虚拟机ubt1204d与宿主机之间的网络互通。</p><p>除了NAT网络模式外，还可以通过自建一个Linux bridge的网桥，在创建虚拟机时，–network选项中指定自建的网桥，这样虚拟机安装完毕后，就会自动分配自建的网桥IP地址段中的一个地址，这种方式称为<strong>桥接模式</strong>。同理，这种方式也需要开启DHCP和DNS服务。通过自建网桥方式，在虚拟机XML文件的配置字段如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">interface</span> <span class="attr">type</span>=<span class="string">'bridge'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mac</span> <span class="attr">address</span>=<span class="string">'52:54:00:e9:e0:3b'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span> <span class="attr">bridge</span>=<span class="string">'br0'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">model</span> <span class="attr">type</span>=<span class="string">'virtio'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">address</span> <span class="attr">type</span>=<span class="string">'pci'</span> <span class="attr">domain</span>=<span class="string">'0x0000'</span> <span class="attr">bus</span>=<span class="string">'0x00'</span> <span class="attr">slot</span>=<span class="string">'0x03'</span> <span class="attr">function</span>=<span class="string">'0x0'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">interface</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上述配置表示，是用bridge为虚拟机提供网络服务，mac address指的是虚拟机的MAC地址，<source bridge="br0">表示使用宿主机中的br0网络接口来建立网桥，这个网络接口需要在宿主机中创建配置文件，类似普通网卡的配置，并制定一个物理网卡与该网桥进行绑定，作为上联接口。<model type="virtio">表示在虚拟机中使用virtio-net驱动的网卡设备，也配置了该网卡在虚拟机中的PCI设备编号为0000:00:03.0。</model></p><p>除了上述两种常见的配置外，还有一种<strong>用户模式网络</strong>的配置，类似Virtual Box中的内部网络模式。如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/qa4KV3fAwACj.png?imageslim" alt="mark"></p><p>其在虚拟机的XML字段描述如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">interface</span> <span class="attr">type</span>=<span class="string">'user'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">mac</span> <span class="attr">address</span>=<span class="string">"00:11:22:33:44:55"</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">interface</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如上，type=’user’表示该客户机的网络接口是用户模式网络，是完全由QEMU软件模拟的一个网络协议栈，也就是一个纯用户态内部虚拟交换机。因为没有虚拟接口与宿主机互通，所以宿主机无法与这样的虚拟机通信，这种网络模式下只有同一个内部虚拟交换机上不同虚拟机才能互通。</p><p>还记得我们在介绍I/O虚拟化时，讲过网卡的硬件直通技术VMDq和SR-IOV吗？在KVM虚拟机中也可以<strong>绑定这类硬件直通网卡</strong>，在其XML文件中目前有两种方式：<strong>新方式通过标签来绑定</strong>，在其中指定驱动的名称为vfio，并指定硬件的I/O地址即可，但是这种新方式目前只支持SR-IOV方式。由于我们条件限制，特意找了个网上的配置给大家曹侃，示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;interface <span class="built_in">type</span>=<span class="string">'hostdev'</span>&gt;</span><br><span class="line">    &lt;driver name=<span class="string">'vfio'</span>/&gt;</span><br><span class="line">    &lt;<span class="built_in">source</span>&gt;</span><br><span class="line">    &lt;address <span class="built_in">type</span>=<span class="string">'pci'</span> domain=<span class="string">'0x0000'</span> bus=<span class="string">'0x08'</span> slot=<span class="string">'0x10'</span> <span class="keyword">function</span>= <span class="string">'0x0'</span>/&gt;</span><br><span class="line">    &lt;/<span class="built_in">source</span>&gt;</span><br><span class="line">  &lt;mac address=<span class="string">'52:54:00:6d:90:02'</span>&gt;</span><br><span class="line">  &lt;/interface&gt;</span><br></pre></td></tr></table></figure><p>如上，用<driver name="vfio">指定使用哪一种分配方式（默认是VFIO，如果使用较旧的传统的device assignment方式，这个值可配为’kvm’），用<source>标签来指示将宿主机中的哪个VF分配给宿主机使用，还可使用<mac address="52：54：00：6d：90：02">来指定在客户机中看到的该网卡设备的MAC地址。</mac></driver></p><p>由于新方式支持的直通网卡类型较少，为了支持更多的硬件直通设备，一般还是采用老方式通过<hostdev>标签来绑定。<strong>这种方式不仅支持有SR-IOV功能的高级网卡的VF的直接分配，也支持无SR-IOV功能的普通PCI或PCI-e网卡的直接分配。但是，这种方式并不支持对直接分配的网卡在客户机中的MAC地址的设置，在客户机中网卡的MAC地址与宿主机中看到的完全相同。</strong>同样，我们在网上找了个配置示例给大家参考：</hostdev></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">hostdev</span> <span class="attr">mode</span>=<span class="string">'subsystem'</span> <span class="attr">type</span>=<span class="string">'pci'</span> <span class="attr">managed</span>=<span class="string">'yes'</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">address</span> <span class="attr">domain</span>=<span class="string">'0x0000'</span> <span class="attr">bus</span>=<span class="string">'0x08'</span> <span class="attr">slot</span>=<span class="string">'0x00'</span> <span class="attr">function</span>=<span class="string">'0x0'</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">hostdev</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如上，表示将宿主机中的PCI 0000:08:00.0设备直接分配给客户机使用。</p><h3 id="存储的配置"><a href="#存储的配置" class="headerlink" title="存储的配置"></a><strong>存储的配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/FoYTc5zdpz2w.png?imageslim" alt="mark"></p><p>如上图，每个存储设备都由一对<disk></disk>标签来描述，上述配置我们当前的虚拟机有两个存储设备，一个虚拟机硬盘，另一个是虚拟机光驱。</p><p>在虚拟机硬盘和光驱设备描述中，类型均是file，表示虚拟机硬盘使用文件方式。除此之外，还有<strong>block、dir或network取值</strong>，分别表示块设备、目录或网络文件系统作为虚拟机磁盘的来源。后面device属性表示让虚拟机如何来使用该磁盘设备，其取值为floppy、disk、cdrom或lun中的一个，分别表示软盘、硬盘、光盘和LUN（逻辑存储单元），默认值为disk（硬盘）。</p><p><strong>子标签用于定义Hypervisor如何为该磁盘提供驱动</strong>，它的name属性用于指定宿主机中使用的后端驱动名称，QEMU/KVM仅支持name=’qemu’，但是它支持的类型type可以是多种，包括raw、qcow2、qed、bochs等。除了这两个属性外，还有cache属性（我们没有设置），它表示在宿主机中打开该磁盘时使用的缓存方式，可以配置为default、none、writethrough、writeback、directsync和unsafe，其具体含义详见本站DPDK系列文章。</p><p><strong>子标签表示磁盘的位置</strong>，当<disk>标签的type属性为file时，应该配置为<source file="/root/ubt1204d.img">这样的模式，而当type属性为block时，应该配置为<source dev="/dev/sdbN">这样的模式。</disk></p><p><strong>子标签表示将磁盘暴露给虚拟机时的总线类型和设备名称。dev属性表示在客户机中该磁盘设备的逻辑设备名称</strong>，而<strong>bus属性表示该磁盘设备被模拟挂载的总线类型</strong>，bus属性的值可以为ide、scsi、virtio、xen、usb、sata等。如果省略了bus属性，libvirt则会根据dev属性中的名称来“推测”bus属性的值，比如，sda会被推测是scsi，而vda被推测是virtio。</p><p><strong>子标签表示该磁盘设备在虚拟机中的I/O总线地址</strong>，这个标签在前面网络配置中也是多次出现的，如果该标签不存在，libvirt会自动分配一个地址。</p><h3 id="域的配置"><a href="#域的配置" class="headerlink" title="域的配置"></a><strong>域的配置</strong></h3><p>我们前面介绍过，在libvirtd进程中，域、实例和Gust OS是一个概念，就是我们常说的虚拟机。所以，在KVM虚拟机的XML配置文件中，<domain>标签是范围最大、最基本的标签，是其他所有标签的根标签。如下：</domain></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/UXx9Dd3nzIPP.png?imageslim" alt="mark"></p><p>在<domain>标签中可以配置两个属性：一个是type，用于表示Hypervisor的类型，可选的值为xen、kvm、qemu、lxc、kqemu、VMware中的一个；另一个是id，其值是一个数字，用于在该宿主机的libvirt中唯一标识一个运行着的客户机，如果不设置id属性，libvirt会按顺序分配一个最小的可用ID。这个系统自动的分配的ID可以通过如下指令查看：</domain></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/legpRcaSLtn5.png?imageslim" alt="mark"></p><p>如上图，我们知道系统给我们当前ubt1204d虚拟机分配的最小可用ID是2。</p><h3 id="域的元数据配置"><a href="#域的元数据配置" class="headerlink" title="域的元数据配置"></a><strong>域的元数据配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/xYn55vkak6E7.png?imageslim" alt="mark"></p><p>在域的XML文件中，有一部分是用于配置域的元数据，即meta data。元数据的概念我们不陌生，在讲解存储虚拟化时介绍过，它主要用来描述数据的属性。在KVM虚拟机中域的元数据就表示域的属性，主要用于区别其他的域。其中，name用于表示该虚拟机的名称，uuid是唯一标识该虚拟机的UUID。在同一个宿主机上，各个虚拟机的名称和UUID都必须是唯一的。除此之外，还有其他的配置，在后续讲解操作实例时遇到了我们还会介绍，这里不再列举。</p><h3 id="QEMU模拟器配置"><a href="#QEMU模拟器配置" class="headerlink" title="QEMU模拟器配置"></a><strong>QEMU模拟器配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/QH4QOpXOzfni.png?imageslim" alt="mark"></p><p>在KVM虚拟机的XML配置文件中，需要制定使用的设备模型的模拟器。如上图，在emulator标签中配置模拟器的绝对路径为/usr/libexec/qemu-kvm。如果我们自己下载最新的QEMU源码编译了一个QEMU模拟器，要使用的话，需要将这里修改为/usr/local/bin/qemu-system-x86_64。不过，创建虚拟机时，<strong>可能会遇到error: internal error Process exited while reading console log output错误</strong>，这是因为自己编译的QEMU模拟器不支持配置文件中的pc-i440fx-rhel7.0.0机器类型，因此，需要同步修改如下选项：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">type</span> <span class="attr">arch</span>=<span class="string">'x86_64'</span> <span class="attr">machine</span>=<span class="string">'pc'</span>&gt;</span>hvm<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="图形显示方式配置"><a href="#图形显示方式配置" class="headerlink" title="图形显示方式配置"></a><strong>图形显示方式配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/Mi5BVcKVmKlX.png?imageslim" alt="mark"></p><p>如上图，表示通过VNC的方式连接到客户机，其VNC端口为libvirt自动分配，也就是用VNC客户端模拟虚拟机的显示器。除此之外，也支持其他多种类型的图形显示方式，以下示例代码就表示就配置了SDL、VNC、RDP、SPICE等多种客户机显示方式。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'sdl'</span> <span class="attr">display</span>=<span class="string">':0.0'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'vnc'</span> <span class="attr">port</span>=<span class="string">'5904'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">listen</span> <span class="attr">type</span>=<span class="string">'address'</span> <span class="attr">address</span>=<span class="string">'1.2.3.4'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">graphics</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'rdp'</span> <span class="attr">autoport</span>=<span class="string">'yes'</span> <span class="attr">multiUser</span>=<span class="string">'yes'</span> /&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'desktop'</span> <span class="attr">fullscreen</span>=<span class="string">'yes'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">'spice'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">listen</span> <span class="attr">type</span>=<span class="string">'network'</span> <span class="attr">network</span>=<span class="string">'rednet'</span>/&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">graphics</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="虚拟机的声卡和显卡配置"><a href="#虚拟机的声卡和显卡配置" class="headerlink" title="虚拟机的声卡和显卡配置"></a><strong>虚拟机的声卡和显卡配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/YQFixPdlVdie.png?imageslim" alt="mark"></p><p>如上图，<strong>标签表示的是显卡配置</strong>，其中<model>子标签表示为虚拟机模拟的显卡的类型，它的类型（type）属性可以为vga、cirrus、vmvga、xen、vbox、qxl中的一个，vram属性表示虚拟显卡的显存容量（单位为KB），heads属性表示显示屏幕的序号。在我们的虚拟机中，显卡的配置为cirrus类型、显存为16384（即16MB）、使用在第1号屏幕上。</model></p><p>由于我们的虚拟机中没有配置声卡，也就没有<strong>标签</strong>，即使有也非常简单，只需要了解他的model属性即可，也就是声卡的类型，常用的选项有es1370、sb16、ac97和ich6。</p><h3 id="串口和控制台配置"><a href="#串口和控制台配置" class="headerlink" title="串口和控制台配置"></a><strong>串口和控制台配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/9EjaHgAIYw4e.png?imageslim" alt="mark"></p><p>如上图，设置了虚拟机的编号为0的串口（即/dev/ttyS0），使用宿主机中的伪终端（pty），由于这里没有指定使用宿主机中的哪个伪终端，因此libvirt会自己选择一个空闲的伪终端（可能为/dev/pts/下的任意一个），如下伪终端均为字符型设备：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/21FdHV1A0gdp.png?imageslim" alt="mark"></p><p>除了系统默认指定外，也可以加上<source path="/dev/pts/1">配置来明确指定使用宿主机中的哪一个虚拟终端。在部署安装虚拟机virt-install命令增加–extra-args ‘console=ttyS0,115200n8 serial’来指定。</p><p>通常情况下，控制台（console）配置在客户机中的类型为’serial’，此时，如果没有配置串口（serial），则会将控制台的配置复制到串口配置中，如果已经配置了串口（我们前面安装的虚拟机ubt1204d就是如此），则libvirt会忽略控制台的配置项。同时，为了让控制台有输出信息并且能够与虚拟机交互，也需在虚拟机中配置将信息输出到串口。比如，在Linux虚拟机内核的启动行中添加“console=ttyS0”这样的配置。</p><h3 id="输入设备配置"><a href="#输入设备配置" class="headerlink" title="输入设备配置"></a><strong>输入设备配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/08Q7OMzwqtV9.png?imageslim" alt="mark"></p><p>如上图，这里的配置会让QEMU模拟PS2接口的鼠标和键盘，还提供了tablet这种类型的设备，即模拟USB总线类型的键盘和鼠标，并能让光标可以在客户机获取绝对位置定位。</p><h3 id="PCI控制器配置"><a href="#PCI控制器配置" class="headerlink" title="PCI控制器配置"></a><strong>PCI控制器配置</strong></h3><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/N0WjE2QuYzp5.png?imageslim" alt="mark"></p><p>如上图，libvirt会根据虚拟机的不同架构，默认会为虚拟机模拟一些必要的PCI控制器，这类PCI控制器不需要在XML文件中指定，而上图中的需要显式指定都是特殊的PCI控制器。这里显式指定了4个USB控制器、1个pci-root和1个idel控制器。libvirt默认还会为虚拟机分配一些必要的PCI设备，如PCI主桥（Host bridge）、ISA桥等。使用上面的XML配置文件启动虚拟机，在客户机中查看到的PCI信息如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/KR29xi7KYtPR.png?imageslim" alt="mark"></p><h2 id="使用libvirt-API进行虚拟化管理"><a href="#使用libvirt-API进行虚拟化管理" class="headerlink" title="使用libvirt API进行虚拟化管理"></a><strong>使用libvirt API进行虚拟化管理</strong></h2><p>要使用libvirt API进行虚拟化管理，就必须先建立到Hypervisor的连接，有了连接才能管理节点、Hypervisor、域、网络等虚拟化要素。对于libvirt连接，可以简单理解为C/S架构模式，服务器端运行Hypervisor，客户端通过各种协议去连接服务器端的Hypervisor，然后进行相应的虚拟化管理。前面提到过，要实现这种连接，<strong>前提条件是libvirtd这个守护进行必须处于运行状态。但是，这里面有个例外，那就是VMware ESX/ESXi就不需要在服务器端运行libvirtd，依然可以通过libvirt客户端以另外的方式连接到VMware。</strong></p><p><strong>为了区分不同的连接，libvirt使用了在互联网应用中广泛使用的URI（Uniform Resource Identifier，统一资源标识符）来标识到某个Hypervisor的连接。</strong>libvirt中连接的标识符URI，其本地URI和远程URI是有一些区别的，具体如下：</p><p><strong>1）在libvirt的客户端使用本地的URI连接本系统范围内的Hypervisor，本地URI的一般格式如下：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver[+transport]:///[path][?extral-param]</span><br></pre></td></tr></table></figure><blockquote><p>其中，driver是连接Hypervisor的驱动名称（如qemu、xen、xbox、lxc等），transport是选择该连接所使用的传输方式，可以为空；path是连接到服务器端上的某个路径，？extral-param是可以额外添加的一些参数（如Unix domain sockect的路径）。</p></blockquote><p><strong>2）libvirt可以使用远程URI来建立到网络上的Hypervisor的连接。远程URI和本地URI是类似的，只是会增加用户名、主机名（或IP地址）和连接端口来连接到远程的节点。远程URI的一般格式如下：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver[+transport]://[user@][host][:port]/[path][?extral-param]</span><br></pre></td></tr></table></figure><blockquote><p>其中，transport表示传输方式，其取值可以是ssh、tcp、libssh2等；user表示连接远程主机使用的用户名，host表示远程主机的主机名或IP地址，port表示连接远程主机的端口。其余参数的意义与本地URI中介绍的完全一样。</p></blockquote><p>无论本地URI还是远程URI，在libvirt中KVM使用QEMU驱动，而QEMU驱动是一个多实例的驱动，它提供了一个root用户的实例system和一个普通用户的实例session，来设定客户端连接到服务器端后，操作权限的范围，类似Linux系统中的root用户与普通用户的区别。使用root用户实例连接的客户端，拥有最大权限，可以查询和控制整个节点范围虚拟机以及相关设备等系统资源；使用普通用户实例连接的客户端，只拥有服务器端对应用户的操作权限。那么，我们通过这一点也就知道<strong>libvirtd也是具备分权分域虚拟化管理能力的。</strong></p><p>有了上面的知识储备，我们就可以使用URI建立到Hypervisor的连接，比如，我们在252机器上，通过SSH方式连接掉251机器上，查看251机器上的KVM虚拟机信息，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/SUe9wilEued2.png?imageslim" alt="mark"></p><p>也可以在251机器上创建本地URI连接，进行管理，大家参照自行练习，我就懒得截图了，谁让我佛系嘛。。。</p><p>我们前面提到过，virsh等工具都是对libvirt的封装和调用，所以我们上面的指令看似简单，但是对应libvirt底层就不是那么回事了。在libvirt的底层是由<strong>virConnectOpen函数</strong>来建立到Hypervisor的连接的，这个函数需要一个URI作为参数，当传递给virConnectOpen的URI为空值（NULL）时，libvirt会依次根据如下3条规则去决定使用哪一个URI。</p><p><strong>1）试图使用LIBVIRT_DEFAULT_URI这个环境变量。</strong></p><p><strong>2）试用使用客户端的libvirt配置文件中的uri_default参数的值。</strong></p><p><strong>3）依次尝试用每个Hypervisor的驱动去建立连接，直到能正常建立连接后即停止尝试。</strong></p><p>如果这3条规则都不能够让客户端libvirt建立到Hypervisor的连接，就会报出建立连接失败的错误信息<strong>（“failed to connect to the hypervisor”）。</strong></p><p>除了针对QEMU、Xen、LXC等真实Hypervisor的驱动之外，<strong>libvirt自身还提供了一个名叫“test”的傀儡Hypervisor及其驱动程序。</strong>test Hypervisor是在libvirt中<strong>仅仅用于测试和命令学习的目的</strong>，因为在本地的和远程的Hypervisor都连接不上（或无权限连接）时，test这个Hypervisor却一直都会处于可用状态。使用virsh连接到test Hypervisor的示例操作如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/ttAD7lHmoun9.png?imageslim" alt="mark"></p><p>如上图，输入help后，就会出现各种命令行指令，新手可以那这个test虚拟机来进行指令练习和学习，不会影响正常的虚拟机，类似Linux中namespace对所有资源拷贝一份然后与真实资源隔离。</p><p>下面，我们通过Python来调用libvirt API查询虚拟机信息，在使用Python调用libvirt API之前，需要确定系统是否安装了libvirt-Python，如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/AD0xmtrjyewQ.png?imageslim" alt="mark"></p><p>上图结果表示系统已经安装libvirt-Python。否则，需要手动安装或自行编译安装相应基础包。</p><p>完成上面的确认后，我们写一个Python小程序脚本GetinfoDm.py，通过调用libvirt的Python API来查询虚拟机的一些信息。代码如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/dsVNKOX5O0hr.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/iqg6hu3pXQtr.png?imageslim" alt="mark"></p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/bWFTEUGEs0qT.png?imageslim" alt="mark"></p><p>上面的脚本只是简单地调用libvirt Python API获取一些信息，需要注意的是“import libvirt”语句引入了libvirt.py这个API文件，然后才能够使用libvirt.openReadOnly、conn.lookupByName等libvirt中的方法。在本示例中，引入的libvirt.py这个API文件的绝对路径是/usr/lib64/python2.7/site-packages/libvirt.py，它实际调用的是/usr/lib64/python2.7/site-packages/libvirtmod.so这个共享库文件。运行上面的脚本后，结果如下：</p><p><img src="http://q6icfngd4.bkt.clouddn.com/blog/20200301/tvkIgmXBSR65.png?imageslim" alt="mark"></p><p>通过上面的示例，我们知道无论是virsh等命令还是Python等高级语言，在对KVM虚拟机进行操作时都是调用libvirt的API库函数来完成，这就说明libvirt API是libvirt管理虚拟机的核心。在libvirt中，外部应用接口API函数大致分为8类，是libvirt实现虚拟化管理的基石。在本文的最后，我们通过一张表对其中最常用的6类API进行总结，方便我们后续编写自动化脚本时查询。</p><table><thead><tr><th><strong>库函数类别</strong></th><th><strong>功能说明</strong></th><th><strong>常用函数</strong></th></tr></thead><tbody><tr><td>连接Hypervisor相关API，以virConnect开头的系列函数</td><td>只有在与Hypervisor建立连接之后，才能进行虚拟机管理操作，所以连接Hypervisor的API是其他所有API使用的前提条件。与Hypervisor建立的连接为其他API的执行提供了路径，是其他虚拟化管理功能的基础。</td><td>virConnetcOpen()：建立一个连接，返回一个virConnectPtr对象。virConnetcReadOnly()：建立一个只读连接，只能使用查询功能。virConnectOpenAuth()：根据认证建立安全连接。virConnectGetCapabilities()：返回对Hypervisor和驱动的功能描述XML字符串。virConnectListDomains()：返回一系列域标识符，只返回活动域信息。</td></tr><tr><td>域管理API，以virDomain开头的系列函数</td><td>要管理域，首先要获取virDomainPtr这个域对象，然后才能对域进行操作。</td><td>virDomainLookupByID(virConnectPtr conn,int id)：根据域的id值到conn连接上去查找相应的域。virDomainLookupByName(virConnectPtr conn,string name)：根据域的名称去conn连接上查找相应的域。virDomainLookupByUUID(virConnectPtr conn,string uuid)：根据域的UUID去conn连接上查找相应的域。virDomainGetHostname()：获取相应域的主机名。virDomainGetinfo()：获取相应域的信息。virDomainGetVcpus()：获取相应域vcpu信息。virDomainCreate()：创建域。virDomainSuspend()：挂起域。virDomainResume()：恢复域等等。。。</td></tr><tr><td>节点管理API，以virNode开头的系列函数</td><td>节点管理的多数函数都需要使用一个连接Hypervisor的对象作为其中的一个传入参数，以便可以查询或修改该连接上的节点信息。</td><td>virNodeGetInfo()：获取节点的物理硬件信息。virNodeGetCPUStats()：获取节点上各个CPU的使用统计信息virNodeGetMemoryStats()：获取节点上内存使用统计信息virNodeGetFreeMemory()：获取接上空闲内存信息virNodeSetMemoryParameters()：设置节点上内存调度参数virNodeSuspendForDurarion()：控制节点主机运行</td></tr><tr><td>网络管理API，以virNetwork开头的系列函数和部分virInterface开头系列函数</td><td>libvirt首先需要创建virNetworkPtr对象，然后才能查询或控制虚拟网络。</td><td>virNetworkGetName()：获取网络名称。virNetworkGetBridgeName()：获取网桥名称。virNetworkGetUUID()：获取网络UUID标识。virNetWorkGetXMLDesc()：获取网络以XML格式描述信息。virNetworkIsActive()：查询网络是否在用。virNetworkCreateXML()：根据XML格式创建一个网络。virNetworkDestroy()：注销一个网络。virNetworkUpdate()：更新一个网络。virInterfaceCreate()：创建一个网络端口。。。。等等</td></tr><tr><td>存储卷管理API，以virStorageVol开头的系列函数</td><td>libvirt对存储卷的管理，首先需要创建virStorageVolPtr这个存储卷对象，然后才能对其进行查询或控制操作。</td><td>virStorageVolLookupByKey()：根据全局唯一键值获取一个存储卷对象。virStorageVolLookupByName()：根据名称在一个存储资源池获取一个存储卷对象。virStorageVolLookupByPath()：根据节点上路径获取一个存储卷对象。virStorageVolGetInfo()：查询某个存储卷的使用信息。virStorageVolGetName()：获取存储卷的名称。。。。等等</td></tr><tr><td>存储池管理API，以virStoragePool开头的系列函数</td><td>libvirt对存储池（pool）的管理包括对本地的基本文件系统、普通网络共享文件系统、iSCSI共享文件系统、LVM分区等的管理。libvirt需要基于virStoragePoolPtr这个存储池对象才能进行查询和控制操作。</td><td>virStoragePoolLookupByName()：可以根据存储池的名称来获取一个存储池对象。virStoragePoolLookupByVolume()：可以根据一个存储卷返回其对应的存储池对象。virStoragePoolCreateXML()：可以根据XML描述来创建一个存储池（默认已激活）virStoragePoolDefineXML()：可以根据XML描述信息静态地定义一个存储池（尚未激活）virStoragePoolCreate()：可以激活一个存储池。virStoragePoolIsActive()：可以查询存储池状态是否处于使用中。。。。等等</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过前面两篇文章的介绍，相信大家对KVM虚拟机的原理、软件架构、运行机制和部署方式等有所了解。但是，那些东西只能算是非常基础入门的东西。尤其我们介绍的部署KVM虚拟机时用到的virt-install命令，不仅参数很多，很难记忆，而且要想用好virt-&lt;em&gt;系列工具首先需要对libvirt工具有更深刻的了解。因为，virt-&lt;/em&gt;系列工具其实是对libvirt工具的封装调用，而libvirt工具又是对底层qemu工具的封装调用，其目的都是为了使命令更加友好与用户交互。本篇文章就详细介绍这几种与libvirt相关的管理工具，为后续各种配置打下良好基础。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2020-03-01-KVM实践初步</title>
    <link href="https://kkutysllb.cn/2020/03/01/2020-03-01-KVM%E5%AE%9E%E8%B7%B5%E5%88%9D%E6%AD%A5/"/>
    <id>https://kkutysllb.cn/2020/03/01/2020-03-01-KVM实践初步/</id>
    <published>2020-03-01T05:17:00.000Z</published>
    <updated>2020-03-01T07:11:56.033Z</updated>
    
    <content type="html"><![CDATA[<p>在x86-64架构的处理器中，KVM需要的硬件辅助虚拟化分别为Intel的虚拟化技术（Intel VT）和AMD的AMD-V技术。在前面我们介绍过，CPU不仅要在硬件上支持VT技术，还要在BIOS中将其功能打开，KVM才能使用到。目前，大多数服务器和PC的BIOS都默认已经打开VT。至于如何在VMware Workstations虚拟机和物理服务器上打开，详见本站《KVM到底是个啥？》一文。<a id="more"></a></p><h2 id="部署和安装KVM"><a href="#部署和安装KVM" class="headerlink" title="部署和安装KVM"></a><strong>部署和安装KVM</strong></h2><p><strong>KVM的部署和安装主要有两种方式：一种源码编译安装，另一种就是通过CentOS的YUM工具安装。</strong>作为学习实验途径，我们这里主要介绍YUM工具安装方式。至于源码编译安装一般属于生产研发人员的操作，这里我们只给一些关键提示，有兴趣的同学可自行研究。</p><p><strong>1）源码编译安装方式。</strong>KVM作为Linux内核的一个module，是从Linux内核版本2.6.20开始的，所以要编译KVM，你的Linux操作系统内核必须在2.6.20版本以上，也就是CentOS 6.x和CentOS 7.x都天生支持，但是CentOS 5.x以下版本需要先升级内核。</p><p><strong>下载KVM源代码，主要通过以下三种方式：</strong></p><ol><li>下载KVM项目开发中的代码仓库kvm.git。</li><li>下载Linux内核的代码仓库linux.git。</li><li>使用git clone命令从github托管网站上下载KVM的源代码。</li></ol><p>根据上面三种途径下载源代码后就可以通过make install的方式编译安装了，编译安装完成后还需要根据KVM官方指导手册进行相关的配置。。。我们这里不展开，大家可自行搜索源码方式安装。<strong>需要注意一点儿的是，除了下载KVM的源码外，还需要同时下载QEMU的源码进行编译安装，因为它俩是一辈子的好基友嘛。。。</strong></p><p><strong>2）YUM工具安装。</strong>首先，需要查看 CPU是否支持VT技术，就可以判断是否支持KVM。然后，再进行相关工具包的安装。最后，加载模块，启动libvirtd守护进程即可。具体步骤如下：</p><p><strong>Step1：</strong>确认服务器是否支持硬件虚拟化技术，如果有返回结果，也就是结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的，否则就是不支持。如下图所示，表示服务器支持Intel的VT-x技术，且有4个CPU。</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/clipboard.png" alt></p><p><strong>Step2：</strong>确认系统关闭SELinux。如下图所示，表示系统已经关闭Linux。</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301142932.png" alt></p><p>如果没有关闭，可以使用如下命令永久关闭：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># setenforce 0   # 临时关闭</span></span><br><span class="line"><span class="comment"># 永久关闭</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># sed -i 's/^SELINUXTYPE=.*/SELINUXTYPE=disabled/g' /etc/selinux/config</span></span><br></pre></td></tr></table></figure><p><strong>Step3：</strong>安装rpm软件包。</p><p>由于KVM是嵌入到Linux内核中的一个模块，我们这里首先安装用户安装用户空间QEMU与内核空间KMV交互的软件协议栈QEMU-KVM，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum -y install qemu-kvm</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144136.png" alt></p><p>如上图，不仅完成qemu-kvm（红色框部分）安装，还同步安装qemu-img、qemu-kvm-comm两个依赖包的安装（黄色框部分）。注意：由于我已经安装过，上面提示是updated，没有安装的话，应该是installed。</p><p>完成QEMU-KVM软件协议栈的安装后，我们安装libvirt<em>、virt-</em>等管理工具，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y libvirt* virt-*</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144259.png" alt></p><p>如上图，将KVM的管理工具libvirt系列软件包和virt-系列软件包（红色框部分）及相关依赖软件包（黄色框部分）进行了安装。</p><p>最后，需要安装一个二层分布式虚拟交换机，用于KVM虚拟机的虚拟接入，可以选择OVS，也可以选择Linux bridge或是其他商用的分布式交换机如VMware、华为FC等。我们这里只出于学习的目的，选择安装Linux bridge即可。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y bridge-utils</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144434.png" alt></p><p>如上图，提示我们系统已经安装该软件包且是最新版本（红色框部分），如果你没有安装过，系统会直接安装。</p><p><strong>3）加载KVM模块，使得Linux Kernel变成一个Hypervisor。</strong>首先，加载KVM内核模块，然后查看KVM内核模块的加载情况即可。具体步骤如下：</p><p>首先，通过modprobe指令加载kvm内核模块，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># modprobe kvm</span></span><br></pre></td></tr></table></figure><p>然后，通过lsmod指令查看kvm模块的加载情况，如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># lsmod | grep kvm</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144605.png" alt></p><p><strong>4）启动libvirtd守护进程，并设置开机启动。</strong>因为libvirt管理工具，是用于KVM虚机全生命周期的统一管理工具，它由外部统一API接口、守护进程libvirtd和CLI工具virsh三部分组成，其中守护进程libvirtd用于所有虚机的全面管理。同时，其他管理工具virt-*、openstack等都是调用libvirt的外部统一API接口完成KVM的虚机的管理。所以，我们需要启动libvirtd守护进程，并设置开机启动。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl enable libvirtd &amp;&amp; systemctl start libvirtd &amp;&amp; systemctl status libvirtd</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144720.png" alt></p><p>这里注意一下，一般情况我们单机玩KVM虚拟机时，libvirtd守护进程默认监听的是UNIX domain socket套接字，并没有监听TCP socket套接字。为了同时让libvirtd监听TCP socket套接字，需要修改/etc/libvirt/libvirtd.conf文件中，将tls和tcp，以及tcp监听端口前面的注释取消。然后重新通过libvirtd的原生命令加载配置文件，使其生效。而系统默认的systemctl命令由于不支持–listen选项，所以不能使用。参考代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl stop libvirtd </span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># libvirtd -d --listen</span></span><br></pre></td></tr></table></figure><p>通过netstat命令查看libvirtd监听的tcp端口为16509，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144827.png" alt></p><p>最后，进行tcp链接验证，如果连接成功，表示服务启动正常且tcp监听正常。如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301144908.png" alt></p><h2 id="安装第一个KVM虚拟机"><a href="#安装第一个KVM虚拟机" class="headerlink" title="安装第一个KVM虚拟机"></a><strong>安装第一个KVM虚拟机</strong></h2><p>安装虚拟机之前，我们需要创建一个镜像文件或者磁盘分区等，来存储虚拟机中的系统和文件。首先，我们利用<strong>qemu-img工具</strong>创建一个镜像文件。<strong>这个工具不仅能创建虚拟磁盘，还能用于后续虚拟机镜像管理。</strong>比如，我们要创建一个raw格式的磁盘，具体代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># qemu-img create -f raw ubuntu1204.img 20G </span></span><br><span class="line"></span><br><span class="line">Formatting <span class="string">'ubuntu1204.img'</span>, fmt=raw size=21474836480</span><br></pre></td></tr></table></figure><p>如上，表示在当前目录下(/root）创建了一个20G大小的，raw格式的虚拟磁盘，名称为：ubuntu1204.img。虽然，我们看它的大小时20G，实际上并不占用任何存储空间，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145048.png" alt></p><p>这是因为qemu-img聪明地为你按实际需求分配文件的实际大小，它将随着image实际的使用而增大。如果想一开始就分配实际大小为20G的空间，不仅要使用raw格式磁盘，还需加上-o preallocation=full参数选项，这样创建速度会很慢，但是创建的磁盘实实在在占用20G空间。我们这里用创建一个5G磁盘来演示，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145129.png" alt></p><p>除raw格式以外，qemu-img还支持创建其他格式的image文件，比如qcow2，甚至是其他虚拟机用到的文件格式，比如VMware的vmdk、vdi、Hyper-v的vhd等，不同的文件格式会有不同的“-o”选项。为了演示我们的第一个虚拟机，我们现在创建一个qcow2格式的虚拟磁盘用作虚拟机的系统磁盘，大小规划40G，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145211.png" alt></p><p>上面创建的虚拟磁盘实际就是KVM虚拟机后续的系统盘，在创建完虚拟机磁盘后，我们将要安装的系统镜像盘上传到当前目录下或者通过光驱挂载也可。我们这里为了安装速度问题，采用上传到宿主机本地的方式，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145248.png" alt></p><p><strong>注意：这里的宿主机指的是我们的VMware虚拟机，这里的虚拟机指的是我们在VMware虚拟机中创建的KVM虚拟机，千万别搞混了。</strong></p><p>然后，就是我们的关键一步，通过virt-install命令安装虚拟机，代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># virt-install --name ubt1204d \</span></span><br><span class="line">--virt-type kvm \</span><br><span class="line">--ram 2048 --vcpus=2 \</span><br><span class="line">--disk path=/root/ubt1204d.img,format=qcow2,size=40 \</span><br><span class="line">--network network=default,model=virtio \</span><br><span class="line">--graphics vnc,listen=192.168.101.251 --noautoconsole \</span><br><span class="line">--cdrom /root/ubuntu-12.04.5-desktop-amd64.iso </span><br><span class="line"></span><br><span class="line">Starting install...</span><br><span class="line">Domain installation still <span class="keyword">in</span> progress. You can reconnect to </span><br><span class="line">the console to complete the installation process.</span><br></pre></td></tr></table></figure><p>如上，上面的指令表达的意思为：–name指定虚拟机的名称，–virt-type指定虚拟机的类型为kvm，–ram指定给虚拟机分配的虚拟内存大小为2GB，–vcpus指定给虚拟机分配的虚拟cpu为2个，–disk指定虚拟机的系统盘，就是我们刚才创建的虚拟磁盘，–network指定虚拟机使用的虚拟交换机，这里使用系统的默认配置，默认配置文件为/etc/libvirt/qemu/networks/default.xml，其详细信息如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145417.png" alt></p><p>上图中网络模式采用nat方式就表示虚拟机可以通过网桥访问internet或者外部网络。–graphics指定虚拟机的模拟显示器界面，这里采用vnc方式，并监听宿主机地址192.168.101.251。–cdrom选项指定虚拟机的安装镜像配置，也就是系统安装盘iso的位置。</p><p>上面虚拟机正式启动后，可以通过本地机器的vnc客户端连接到KVM虚拟机，作为虚拟机的模拟显示器。为此，首先需要查看VNC连接的端口，代码如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145502.png" alt></p><p>如上图黄色框部分，通过本地机器VNC客户端连接目标机器192.168.101.251的0端口就能模拟虚拟机的显示器，进一步完成图形化安装配置，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145535.png" alt></p><p>连接成功后，与真实物理机装系统的操作一致，可以使用键盘、鼠标完成各类安装配置操作。虚拟机模拟显示器的界面如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145608.png" alt></p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145737.png" alt></p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145805.png" alt></p><p>由于我们安装的是ubuntu12.04的桌面版系统，所以完成虚拟机的安装后，后续每次使用虚拟机都需要使用VNC客户端进行连接操作。如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145843.png" alt></p><p>同时，我们可以通过virsh命令工具查看本机上所有虚拟机的状态，如下：</p><p><img src="https://raw.githubusercontent.com/kkutysllb/PicGO-Deb/master/img/20200301145921.png" alt></p><p>最后，提醒一下：我们在自动化运维中介绍过通过kicstart或cobbler批量安装宿主机操作系统，这种方式也是可以在KVM虚拟机安装系统中使用，通过在virt-install命令增加–extra-args选项就可实现。但是，一般我们不这么玩，在云计算和虚拟化场景下，均是通过手动安装一台模板虚拟机，将其系统盘转换为模板镜像格式文件，然后通过批量分发虚拟机的方式（就是我们在存储虚拟化中讲的链接克隆和快照方式）完成虚拟机批量部署操作，电信云中通过VNFD描述的多台虚拟机资源部署也是同样的方式完成，只不过里面借助了OpenStack的编排引擎。这种方式我们在后续介绍KVM虚拟镜像格式实操一文中会详细介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在x86-64架构的处理器中，KVM需要的硬件辅助虚拟化分别为Intel的虚拟化技术（Intel VT）和AMD的AMD-V技术。在前面我们介绍过，CPU不仅要在硬件上支持VT技术，还要在BIOS中将其功能打开，KVM才能使用到。目前，大多数服务器和PC的BIOS都默认已经打开VT。至于如何在VMware Workstations虚拟机和物理服务器上打开，详见本站《KVM到底是个啥？》一文。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-27-打造文件实时同步架构之sersync篇</title>
    <link href="https://kkutysllb.cn/2019/06/27/2019-06-27-%E6%89%93%E9%80%A0%E6%96%87%E4%BB%B6%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E4%B9%8Bsersync%E7%AF%87/"/>
    <id>https://kkutysllb.cn/2019/06/27/2019-06-27-打造文件实时同步架构之sersync篇/</id>
    <published>2019-06-27T09:30:01.000Z</published>
    <updated>2019-06-27T10:15:51.408Z</updated>
    
    <content type="html"><![CDATA[<p>前文讲述的rsync主要用于较大量数据同步，一般简单的服务器数据传输会使用ftp/sftp等方式，但是这样的方式效率不高，不支持差异化增量同步也不支持实时传输。针对数据实时同步需求大多数人会选择rsync+inotify-tools的解决方案，但是这样的方案也存在一些缺陷，（下文会具体指出），sersync是国人基于前两者开发的工具，不仅保留了优点同时还强化了实时监控，文件过滤，简化配置等功能，帮助用户提高运行效率，节省时间和网络资源。<a id="more"></a></p><p><img src="https://i.loli.net/2019/06/27/5d148d0bbe06c11169.jpg" alt></p><h2 id="inotify-rsync模式"><a href="#inotify-rsync模式" class="headerlink" title="inotify+rsync模式"></a><strong>inotify+rsync模式</strong></h2><p>如果要实现定时同步数据，可以在客户端将rsync加入定时任务，但是定时任务的同步时间粒度并不能达到实时同步的要求。在Linux kernel 2.6.13后提供了inotify文件系统监控机制。通过rsync+inotify组合可以实现实时同步。</p><p>inotify实现工具有几款：inotify本身、sersync、lsyncd。其中sersync是金山的周洋开发的工具，克服了inotify的缺陷，且提供了几个插件作为可选工具。此处先介绍inotify的用法以及它的缺陷，通过其缺陷引出sersync，并介绍其用法。</p><h3 id="安装inotify-tools"><a href="#安装inotify-tools" class="headerlink" title="安装inotify-tools"></a><strong>安装inotify-tools</strong></h3><p>inotify由inotify-tools包提供。在安装inotify-tools之前，请<strong>确保内核版本高于2.6.13</strong>，且在/proc/sys/fs/inotify目录下有以下三项，这表示系统支持inotify监控，关于这3项的意义，下文会简单解释。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l /proc/sys/fs/inotify/</span></span><br><span class="line">total 0</span><br><span class="line">-rw-r--r-- 1 root root 0 Jun  3 18:30 max_queued_events</span><br><span class="line">-rw-r--r-- 1 root root 0 Jun  3 18:30 max_user_instances</span><br><span class="line">-rw-r--r-- 1 root root 0 Jun  3 18:30 max_user_watches</span><br></pre></td></tr></table></figure><p>epel源上提供了inotify-tools工具，可以先安装epel-release源，然后通过yum方式安装，也可以通过下载源码编译方式安装。由于我们部署了本地yum源，所以选择第一种方式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y epel-release</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># yum install -y inotify-tools</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/27/5d14922a46f2577518.jpg" alt></p><p><strong>inotify-tools工具只提供了两个命令</strong>：<strong>inotifywait和inotifywatch</strong>。如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rpm -ql inotify-tools</span></span><br><span class="line">/usr/bin/inotifywait</span><br><span class="line">/usr/bin/inotifywatch</span><br></pre></td></tr></table></figure><p>其中，<strong>inotifywait命令用于等待文件发生变化，所以可以实现监控(watch)的功能，该命令是inotify的核心命令。</strong>inotifywatch用于收集文件系统的统计数据，例如发生了多少次inotify事件，某文件被访问了多少次等等，一般用不上。inotify相关的内核参数如下：</p><p><strong>1）/proc/sys/fs/inotify/max_queued_events：</strong>调用inotify_init时分配到inotify instance中可排队的event数的最大值，超出的事件被丢弃，但会触发队列溢出Q_OVERFLOW事件。</p><p><strong>2）/proc/sys/fs/inotify/max_user_instances：</strong>每一个real user可创建的inotify instances数量的上限。</p><p><strong>3）/proc/sys/fs/inotify/max_user_watches：</strong>每个inotify实例相关联的watches的上限，即每个inotify实例可监控的最大目录、文件数量。如果监控的文件数目巨大，需要根据情况适当增加此值。</p><h3 id="inotify的不足"><a href="#inotify的不足" class="headerlink" title="inotify的不足"></a><strong>inotify的不足</strong></h3><p>inotify是监控工具，监控目录或文件的变化，然后触发一系列的操作。假如有一台站点发布服务器A，还有3台web服务器B/C/D，目的是让服务器A上存放站点的目录中有文件变化时，自动触发同步将它们推到web服务器上，这样能够让web服务器最快的获取到最新的文件。需要搞清楚的是，监控的是A上的目录，推送到的是B/C/D服务器，所以在站点发布服务器A上装好inotify工具。除此之外，一般还在web服务器BCD上将rsync配置为daemon运行模式，让其在873端口上处于监听状态(并非必须，即使是sersync也非必须如此)。也就是说，<strong>对于rsync来说，监控端是rsync的客户端，其他的是rsync的服务端。</strong></p><p>以上描述的只是最可能的使用情况，并非一定需要如此。况且，inotify是独立的工具，它和rsync无关，它只是为rsync提供一种比较好的实时同步方式而已。</p><p>虽然inotify已经整合到了内核中，在应用层面上也常拿来辅助rsync实现实时同步功能，但是inotify因其设计太过细致从而使得它配合rsync并不完美，所以需要尽可能地改进inotify+rsync脚本或者使用sersync工具。</p><p>另外，<strong>inotify存在bug—</strong>当向监控目录下拷贝复杂层次目录(多层次目录中包含文件)，或者向其中拷贝大量文件时，inotify经常会随机性地遗漏某些文件。这些遗漏掉的文件由于未被监控到，所有监控的后续操作都不会执行，例如不会被rsync同步。实际上，上面描述的问题<strong>不是inotify的缺陷，而是inotify-tools包中inotifywait工具的缺陷。</strong></p><h2 id="Sersync模式"><a href="#Sersync模式" class="headerlink" title="Sersync模式"></a><strong>Sersync模式</strong></h2><p>sersync主要用于服务器同步，web镜像等功能。目前使用的比较多的同步解决方案是inotify-tools+rsync ，另外一个是google开源项目Openduckbill（依赖于inotify- tools），这两个都是基于脚本语言编写的。相比较上面两个项目，sersync优点是：</p><p>1）sersync是使用c++编写，而且对linux系统文件系统产生的临时文件和重复的文件操作进行过滤，所以在结合rsync同步的时候，<strong>节省了运行时耗和网络资源</strong>。</p><p>2）sersync<strong>配置起来很简单</strong>，其中bin目录下已经有基本上静态编译的二进制文件，配合bin目录下的xml配置文件直接使用即可。</p><p>3）相比较其他脚本开源项目，<strong>使用多线程进行同步</strong>，尤其在同步较大文件时，能够保证多个服务器实时保持同步状态。</p><p>4）<strong>有出错处理机制</strong>，通过失败队列对出错的文件重新同步，如果仍旧失败，则按设定时长对同步失败的文件重新同步。</p><p>5）<strong>自带crontab功能</strong>，只需在xml配置文件中开启，即可按您的要求，隔一段时间整体同步一次。无需再额外配置crontab功能。</p><p>6）<strong>具备socket与http插件扩展</strong>，满足您二次开发的需要。</p><p><strong>sersync架构图如下所示，其架构说明如下：</strong></p><p><img src="https://i.loli.net/2019/06/27/5d149291a3ebd68291.jpg" alt></p><p>1 ) 线程组线程是等待线程队列的守护线程，当事件队列中有事件产生的时候，线程组守护线程就会逐个唤醒同步线程。当队列中 Inotify 事件较多的时候，同步线程就会被全部唤醒一起工作。这样设计的目的是为了能够同时处理多个 Inotify 事件，从而提升服务器的并发同步能力。同步线程的最佳数量=核数 x 2 + 2。</p><p>2 ) 之所以称之为线程组线程，是因为每个线程在工作的时候，会根据服务器上新写入文件的数量去建立子线程，子线程可以保证所有的文件与各个服务器同时同步。当要同步的文件较大的时候，这样的设计可以保证每个远程服务器都可以同时获得需要同步的文件。</p><p>3 ) 服务线程的作用有三个：</p><ul><li>处理同步失败的文件，将这些文件再次同步，对于再次同步失败的文件会生成 rsync_fail_log.sh 脚本，记录失败的事件。</li><li>每隔10个小时执行 rsync_fail_log.sh 脚本一次，同时清空脚本。</li><li>crontab功能，可以每隔一定时间，将所有路径整体同步一次。</li></ul><p>4 ) 过滤队列的建立是为了过滤短时间内产生的重复的inotify信息，例如：在删除文件夹的时候，inotify就会同时产生删除文件夹里的文件与删除文件夹的事件，通过过滤队列，当删除文件夹事件产生的时候，会将之前加入队列的删除文件的事件全部过滤掉，这样只产生一条删除文件夹的事件，从而减轻了同步的负担。同时对于修改文件的操作的时候，会产生临时文件的重复操作。</p><h3 id="Sersync安装"><a href="#Sersync安装" class="headerlink" title="Sersync安装"></a><strong>Sersync安装</strong></h3><p><strong>安装规划如下：</strong></p><ul><li>服务器A（主服务器）：192.168.101.11</li><li>服务器B（从服务器/备份服务器）:192.168.101.251</li><li>rsync默认TCP端口为873</li></ul><p><strong>Step1：服务器B上安装rsync</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]# yum install -y rsync</span><br></pre></td></tr></table></figure><p><strong>Step2：配置/etc/rsyncd.conf</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># cat /etc/rsyncd.conf </span></span><br><span class="line"><span class="comment">#服务器B上的rsyncd.conf文件内容</span></span><br><span class="line">uid=root</span><br><span class="line">gid=root</span><br><span class="line"><span class="comment">##最大连接数</span></span><br><span class="line">max connections=36000</span><br><span class="line"><span class="comment">##默认为true，修改为no，增加对目录文件软连接的备份 </span></span><br><span class="line">use chroot=no</span><br><span class="line"><span class="comment">##定义日志存放位置</span></span><br><span class="line"><span class="built_in">log</span> file=/var/<span class="built_in">log</span>/rsyncd.log</span><br><span class="line"><span class="comment">##忽略无关错误</span></span><br><span class="line">ignore errors = yes</span><br><span class="line"><span class="comment">##设置rsync服务端文件为读写权限</span></span><br><span class="line"><span class="built_in">read</span> only = no </span><br><span class="line"><span class="comment">##认证的用户名与系统帐户无关在认证文件做配置，如果没有这行则表明是匿名</span></span><br><span class="line">auth users = rsync</span><br><span class="line"><span class="comment">##密码认证文件，格式(虚拟用户名:密码）</span></span><br><span class="line">secrets file = /etc/rsync.pass</span><br><span class="line"><span class="comment">##这里是认证的模块名，在client端需要指定，可以设置多个模块和路径</span></span><br><span class="line">[rsync]</span><br><span class="line"><span class="comment">##自定义注释</span></span><br><span class="line">comment  = rsync</span><br><span class="line"><span class="comment">##同步到B服务器的文件存放的路径</span></span><br><span class="line">path=/app/data/site/</span><br><span class="line">[img]</span><br><span class="line">comment  = img</span><br><span class="line">path=/app/data/site/img</span><br></pre></td></tr></table></figure><p><strong>Step3：创建rsync认证文件，可以设置多个，每行一个用户名:密码，注意中间以“:”分割</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># echo "rsync:rsync" &gt; /etc/rsync.pass</span></span><br></pre></td></tr></table></figure><p><strong>Step4：设置文件所有者和读写权限</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># chmod 600 /etc/rsyncd.conf </span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># chmod 600 /etc/rsync.pass</span></span><br></pre></td></tr></table></figure><p><strong>Step5：启动服务器B上rsync服务，并查询监听端口</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># systemctl enable rsyncd &amp;&amp; systemctl start rsyncd</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/rsyncd.service to /usr/lib/systemd/system/rsyncd.service.</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># netstat -an | grep 873</span></span><br><span class="line">tcp        0      0 0.0.0.0:873             0.0.0.0:*               LISTEN     </span><br><span class="line">tcp6       0      0 :::873                  :::*                    LISTEN     </span><br><span class="line">[root@c7-test01 ~]<span class="comment"># lsof -i tcp:873</span></span><br><span class="line">COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME</span><br><span class="line">rsync   1960 root    4u  IPv4  25279      0t0  TCP *:rsync (LISTEN)</span><br><span class="line">rsync   1960 root    5u  IPv6  25280      0t0  TCP *:rsync (LISTEN)</span><br></pre></td></tr></table></figure><p><strong>以下安装步骤是在服务器A上执行！！！</strong></p><p><strong>Step6：在服务器A上安装rsync服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># yum install -y rsync</span></span><br></pre></td></tr></table></figure><p><strong>Step7：安装inotify-tools工具</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># yum install -y inotify-tools</span></span><br></pre></td></tr></table></figure><p><strong>Step8：安装sersync软件包</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/sersync/sersync2.5.4_64bit_binary_stable_final.tar.gz</span><br><span class="line">[root@c7-server01 <span class="built_in">local</span>]<span class="comment"># tar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz </span></span><br><span class="line">GNU-Linux-x86/</span><br><span class="line">GNU-Linux-x86/sersync2</span><br><span class="line">GNU-Linux-x86/confxml.xml</span><br><span class="line">[root@c7-server01 <span class="built_in">local</span>]<span class="comment"># mv GNU-Linux-x86 sersync</span></span><br><span class="line">[root@c7-server01 <span class="built_in">local</span>]<span class="comment"># cd sersync/</span></span><br></pre></td></tr></table></figure><p><strong>Step9：配置密码文件并修改权限为600。这个密码用于访问服务器B，需要的密码和上面服务器B的密码必须一致：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 sersync]<span class="comment"># echo "rsync" &gt; /app/local/sersync/user.pass</span></span><br><span class="line">[root@c7-server01 sersync]<span class="comment"># chmod 600 /app/local/sersync/user.pass</span></span><br></pre></td></tr></table></figure><p><strong>Step10：配置confxml.xml文件</strong></p><p><img src="https://i.loli.net/2019/06/27/5d14966e1becf95654.jpg" alt></p><p><strong>Step11：在服务器A上运行sersync服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 sersync]<span class="comment"># cd /app/local/sersync/</span></span><br><span class="line">[root@c7-server01 sersync]<span class="comment"># ./sersync2 -r -d</span></span><br><span class="line">-d:启用守护进程模式</span><br><span class="line">-r:在监控前，将监控目录与远程主机用rsync命令推送一遍</span><br><span class="line">-n: 指定开启守护线程的数量，默认为10个</span><br><span class="line">-o:指定配置文件，默认使用confxml.xml文件</span><br></pre></td></tr></table></figure><h2 id="效果验证"><a href="#效果验证" class="headerlink" title="效果验证"></a><strong>效果验证</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在服务器A（192.168.101.11）的/tmp目录创建10个文件，比如：tets01..test10</span></span><br><span class="line">[root@c7-server01 sersync]<span class="comment"># touch /tmp/test&#123;01..10&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在服务器B（192.168.101.251）的/app/data/site/目录下查看效果</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/27/5d1496e9acb8536499.jpg" alt></p><p>最后，在服务器A上设置sersync开机启动，整体文件实时同步的架构部署完毕。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"/app/local/sersync/sersync2 -r -d -o /opt/sersync/confxml.xml &gt;/app/local/sersync/rsync.log 2&gt;&amp;1 &amp;"</span> &gt;&gt; /etc/rc.local</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前文讲述的rsync主要用于较大量数据同步，一般简单的服务器数据传输会使用ftp/sftp等方式，但是这样的方式效率不高，不支持差异化增量同步也不支持实时传输。针对数据实时同步需求大多数人会选择rsync+inotify-tools的解决方案，但是这样的方案也存在一些缺陷，（下文会具体指出），sersync是国人基于前两者开发的工具，不仅保留了优点同时还强化了实时监控，文件过滤，简化配置等功能，帮助用户提高运行效率，节省时间和网络资源。
    
    </summary>
    
      <category term="Linux常用运维工具" scheme="https://kkutysllb.cn/categories/Linux%E5%B8%B8%E7%94%A8%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="https://kkutysllb.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-27-基础正则的五朵金花</title>
    <link href="https://kkutysllb.cn/2019/06/27/2019-06-27-%E5%9F%BA%E7%A1%80%E6%AD%A3%E5%88%99%E7%9A%84%E4%BA%94%E6%9C%B5%E9%87%91%E8%8A%B1/"/>
    <id>https://kkutysllb.cn/2019/06/27/2019-06-27-基础正则的五朵金花/</id>
    <published>2019-06-27T05:28:03.000Z</published>
    <updated>2019-06-27T06:33:57.372Z</updated>
    
    <content type="html"><![CDATA[<p>在前一篇《正则表达式基础入门》中，我们总结了跟<strong>“位置匹配”</strong>的相关的正则，它其实也是基础正则五朵金花之一，我们只是在入门介绍中单独拿出来进行科普。这篇文章我们就来掰扯掰扯基础正则的其它四朵金花。<a id="more"></a></p><h2 id="基础正则的连续次数匹配"><a href="#基础正则的连续次数匹配" class="headerlink" title="基础正则的连续次数匹配"></a><strong>基础正则的连续次数匹配</strong></h2><p>连续次数匹配是什么意思？空口白话不容易说清楚，我们还是举个例子来说明。为了说明连续次数匹配的，我们首先得重新创建一个测试文件，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1454b93712376409.jpg" alt></p><p>现在，我们想查找测试文件中哪些行包含的连续的两个字母a。聪明如你，肯定想到了如下的查找办法：</p><p><img src="https://i.loli.net/2019/06/27/5d1454e06833880580.jpg" alt></p><p>如上图，我们通过匹配“aa”就能将包含连续两个a的行搜索出来（注意：上面的条件是指至少包含两个连续的a）。那么，我们现在需要找出连续10个a的行呢？简单。。。查找“aaaaaaaaaa”不就行了。但是，如果要找包含连续100个a的行呢？上面的笨办法显然不科学，<strong>这时候就要用了基础正则中连续次数匹配条件—”{n}“了，里面n代表具体的数字，也就是连续多少个的意思。</strong>上面的示例，我们使用基础正则中的连续匹配条件可以改写成下面的格式：</p><p><img src="https://i.loli.net/2019/06/27/5d14551e6bbfa73556.jpg" alt></p><p>如上图，结果符合我们的预期。“{2}”表示连续出现2次的意思，a\{2\}就表示连续出现两次a的意思。现在，你肯定能举一反三。比如：如果我们要匹配连续100个a，可以写成如下匹配条件“a\{100\}”，<strong>也就是我们只需要修改其中的数字n即可。</strong></p><p>上面的用法刚才也提到了，它实际的意思的是：<strong>“包含至少2个连续a的行”。</strong>如果，我们只想找到<strong>“只包含连续两个a的行”</strong>时怎么办？这时候，就要用到我们上一篇提到的“位置规则”匹配条件。我们可以配合使用单词界定符”\&lt;,\&gt;”或者”\b”，具体命令如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1455b05f2b135997.jpg" alt></p><p>如上图，我们单词界定符“\b”（前后的\b表示单词界定符，中间的字母b表示要查找的内容，后面花括号中的数字2表示要匹配的次数），将测试文件test01中的<strong>“只包含连续两个b的行找了出来”。</strong>那么，我们现在再继续延伸一下，首先来验证下<strong>d\{2,3\}的匹配条件</strong>表示什么意思，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1456002280099045.jpg" alt></p><p>上图中的匹配条件将<strong>“包含至少2个连续的d，至多3个连续的d的行”</strong>查找了出来。也就是说<strong>“\{x,y\}”这类匹配条件，表示的是“至少连续出现x次，至多连续出现y次”的意思。</strong>举一反三，“\{x</p><p>,\}”表示“至少连续出现x次，至多不限次数”的意思，”\{,y\}”表示“至少出现0次，至多连续出现y次”的意思。</p><p><strong>我们现在用中学数学的方法小结一下：\{x,y\}表示“至少连续出现x次，至多连续出现y次”。如果y=0，表示“至少连续出现x次，至多次数不限”，且“\{x\}”和“\{x,\}”的意思是一样的。如果x=0，表示“至多出现y次，至少出现0次”的意思。</strong></p><p>现在，我们再来认识另一个匹配次数的<strong>正则符号“*”。其实，在通配符中也有符号“*”，表示匹配任意长度的任意字符。但是，在正则表达式中，符号“*”表示之前的字符连续出现任意次的意思。注意：千万不要和通配符中的“*”混淆了。</strong>具体使用方法如下：</p><p><img src="https://i.loli.net/2019/06/27/5d1456e50c72830042.jpg" alt></p><p>如上图，表示“搜索字母a前面出现任意个d的行”，任意个d当然也包含0个d。那么，如何在正则表达式中查找任意长度的任意字符呢？我们可以使用<strong>“.*”</strong>来实现，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d14571d813d021983.jpg" alt></p><p>上图中，表示匹配字母a后面接任意多个任意字符的行。其实，<strong>在正则表达式中“.”符号表示匹配任意单个字符。而在通配符中“?”表示单个字符，这一定同样要注意不要混淆。如下：</strong></p><p><img src="https://i.loli.net/2019/06/27/5d1463005ce9084092.jpg" alt></p><p>上图中，匹配条件“aa.”表示aa后面跟任意单个字符的行都会被匹配到，”aa..”表示aa后面跟任意两个字符的行都会被匹配到（空格也算字符）。理解了上述规则，回过头来我们再看“.<em>”就非常容易理解了，**”.\</em>“就表示任意单个字符连续出现多次的意思。**</p><p>接下来，我们再认识两个新的匹配条件：<strong>“\?”</strong>和<strong>“\+”</strong>。<strong>“\?”表示匹配其前面的字符出现0次或1次，也就说要么没有，要么最多出现1次。“\+”表示匹配前面的字符出现至少1次，也就是说前面的字符必须至少出现1次。</strong>我们再通过示例验证下：</p><p><img src="https://i.loli.net/2019/06/27/5d145810956e615136.jpg" alt></p><p>如上图，“a\?”表示a这个字符出现0次或1次的行，不仅匹配到了空行，也将包含连续多个a的也匹配到了。我们再看下面的例子：</p><p><img src="https://i.loli.net/2019/06/27/5d145836a590472950.jpg" alt></p><p>上图中，将a字符至少出现1次的行都匹配输出，那么空行也就自然不输出了。</p><p>到此，基础正则连续次数匹配规则就全部介绍完了，国际惯例，我们还是总结一下，便于后续用到了查询：</p><ul><li><strong>*符号：</strong>表示前面的字符连续出现任意次，包括0次。</li><li><strong>.符号：</strong>表示单个任意字符。</li><li><strong>.*组合：</strong>表示任意长度的任意字符。</li><li><strong>\?组合：</strong>表示匹配前面的的字符0次或1次。</li><li><strong>\+组合：</strong>表示匹配其前面的字符至少1次，或连续多次，连续次数上不封顶。</li><li><strong>\{n\}符号：</strong>表示前面的字符连续出现n次，将会被匹配到</li><li><strong>\{x,y\}组合：</strong>表示前面的字符至少连续出现x次，最多连续出现y次，都能被匹配到，换句话说就是之前的字符连续出现的次数在x和y之间，即可被匹配。</li><li><strong>\{,n\}组合：</strong>表示之前的字符连续出现至多n次，最少0次，即可被匹配。</li><li><strong>\{n,\}组合：</strong>表示之前的字符连续出现至少n次，才会被匹配到。</li></ul><h2 id="基础正则中的常用符号"><a href="#基础正则中的常用符号" class="headerlink" title="基础正则中的常用符号"></a><strong>基础正则中的常用符号</strong></h2><p>之前介绍的使用正则表达式去“匹配连续次数”中，我们使用过特殊符号“.”和”*”，分别用于匹配单个任意字符和连续出现任意次数。如果我们需要更精确的匹配呢？比如我们想从测试文本中找出a字母后接任意3个的字母的字串所在行，我们可以使用如下规则：</p><p><img src="https://i.loli.net/2019/06/27/5d1458c87910788625.jpg" alt></p><p>如上图，“[[:alpha:]]”就是我们现在需要认识的新符号。<strong>在正则表达式中，“[[:alpha:]]”表示“任意字母（不区分大小写）”。</strong>“[[:alpha:]]”这个符号看上去有些复杂，吐啊。。。吐啊。。。习惯就好了。其实，这个符号可以拆分成两部分去理解，所以也没那么可怕。<strong>“[[:alpha:]]”的第一部分是最外层的[ ]，表示指定范围内的任意单个字符，第二部分是最内层的[:alpha:]，表示不区分大小写的任意字母。所以，当两部分合起来就是表示任意单个字母（不区分大小写）。</strong></p><p>上面的[[:alpha:]]表示不区分大小写的任意字母，如果我们现在需要区分大小写呢？比如，上面的示例我们再细化一下，需要匹配a后面跟随的3个字母必须是小写字母，可以使用如下规则：</p><p><img src="https://i.loli.net/2019/06/27/5d14595a9f34d13289.jpg" alt></p><p>上图中，<strong>[[:lower:]]就是我们现在认识的第二个符号，它在正则表达式中表示任意单个小写字母。同样的，[[:upper:]]就表示任意单个大写字母。</strong></p><p><strong>聪明如你，一定发现了一些规律，那就是替换“[[: :]]”中的单词，就可表示不同的含义。在基础正则中，主要包含如下特殊符号：</strong></p><ul><li><strong>[[:alpha:]]：</strong>表示任意1个不区分大小写的字母。</li><li><strong>[[:lower:]]：</strong>表示任意1个小写字母。</li><li><strong>[[:upper:]]：</strong>表示任意1个大写字母。</li><li><strong>[[:digit:]]：</strong>表示0-9之间任意1个数字，包括0和9。</li><li><strong>[[:alnum:]]：</strong>表示任意1个数字或字母。</li><li><strong>[[:space:]]：</strong>表示任意1个空白字符，包括“空格”、“tab键”等。</li><li><strong>[[:punct:]]：</strong>表示任意一个标点符号。</li></ul><p>除了上面的特殊符号表示法外，还有其他办法实现 上述的匹配效果。还记得我们在正则表达式基础入门一文中举得一个[a-z]*的例子不？示例中，<strong>[a-z]其实就表示任意一个小写字母，效果与[[:lower:]]一样。同理，[A-Z]就与[[:upper:]]是等价的，表示任意一个大写字母；[0-9]与[[:digit:]]是等价的，表示任意一个0-9的数字。如下所示：</strong></p><p><img src="https://i.loli.net/2019/06/27/5d1459f80399818712.jpg" alt></p><p>那么，有了上面的基础，“[a-z][A-Z][0-9]”就表示任意一个不区分大小写的字母或0-9的数字，等价于[[:alnum:]]。</p><p><strong>上述两种表示的特殊符号使用没有强制性，看个人喜好而定。</strong>这里还需要强调下<strong>[ ]的作用，它表示匹配指定范围内的任意单个字符。</strong>还是先看下面的示例：</p><p><img src="https://i.loli.net/2019/06/27/5d145a7cbf8b763261.jpg" alt></p><p>如上图，字母a后面接一个方括号[]，里面有bcd三个字母。既然方括号[ ]表示匹配任意范围内的单字符，那么上面的匹配条件就是<strong>“匹配字母a后面紧跟b或c或d三个字母一次的行”。</strong>我们活学活用，<strong>[Bd#3]表示什么意思呢？它就表示字符是大写B、或者是小写d、或者是特殊符号#、再或者是数字3各一次的场景，都可以被匹配到。</strong>如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145acb223a362278.jpg" alt></p><p>上图中，表示匹配包含om或oe或oo或ob字符串的行。</p><p><strong>现在，我们理解了方括号[  ]的含义，那么我们再来认识一下它的性格迥异的亲兄弟[^  ]，它表示匹配指定范围外的任意一个字符，与方括号[  ]的含义正好相反。</strong>还是看下面示例：</p><p><img src="https://i.loli.net/2019/06/27/5d145b22aa7ea24329.jpg" alt></p><p>如上图，与前面示例的效果正好相反，将测试文件中不包含om，或不包含oe，或不包含oo，或不包含ob字符的行匹配输出。同理，[^0-9]就表示匹配单个非数字字符，[^a-z]就表示匹配单个非小写字母等等，这里就不再举例了，很好理解。这里有个疑问，比如前面说了[0-9]与[[:digit:]]是等价的，<strong>那么[^0-9]与[^[:digit:]]是否等价？</strong>试试就知道了，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145bae6ce1253207.jpg" alt></p><p><strong>针对这些常用符号，我们总结出一张表如下：</strong></p><table><thead><tr><th>符号</th><th>等价于</th><th>含义</th></tr></thead><tbody><tr><td>[a-z]</td><td>[[:lower:]]</td><td>任意一个小写字母</td></tr><tr><td>[^a-z]</td><td>[^[:lower:]]</td><td>任意一个非小写字母</td></tr><tr><td>[A-Z]</td><td>[[:upper:]]</td><td>任意一个大写字母</td></tr><tr><td>[^A-Z]</td><td>[^[:upper:]]</td><td>任意一个非大写字母</td></tr><tr><td>[0-9]</td><td>[[:digit:]]</td><td>任意一个0-9的数字</td></tr><tr><td>[^0-9]</td><td>[^[:digit:]]</td><td>任意一个非0-9的数字</td></tr><tr><td>[a-zA-Z]</td><td>[[:alpha:]]</td><td>任意一个不区分大小写的字母</td></tr><tr><td>[^a-zA-Z]</td><td>[^[:alpha:]]</td><td>任意一个非字母的字符</td></tr><tr><td>[a-zA-Z0-9]</td><td>[[:alnum:]]</td><td>任意一个数字或字母</td></tr><tr><td>[^a-zA-Z0-9]</td><td>[^[:alnum:]]</td><td>任意一个非数字或字母的字符，比如符号</td></tr></tbody></table><p>其实，不仅[0-9]、[[:digit:]]可以匹配数字，还有一种简写的格式符号也能表示数字，比如”\d”就是表示十进制数字0-9。但是，<strong>并不是所有的正则表达式解释器都能够识别这些简写格式。</strong>因此，建议大家还是采用上述非简写格式来编写规则。</p><h2 id="基础正则中的分组与后向引用"><a href="#基础正则中的分组与后向引用" class="headerlink" title="基础正则中的分组与后向引用"></a><strong>基础正则中的分组与后向引用</strong></h2><p>在正则表达式中除了使用“位置匹配”、“连续次数匹配”和“常用符号”外，还可以使用分组和后向引用。那么，什么是分组？我们还是通过示例来说明，如下先创建一个测试文件test02：</p><p><img src="https://i.loli.net/2019/06/27/5d145c61b70a939419.jpg" alt></p><p>我们先使用如下规则进行匹配：</p><p><img src="https://i.loli.net/2019/06/27/5d145c83109d716151.jpg" alt></p><p>如上图，后面的匹配次数2只影响前面单字母b，所以上例中kkutysllbb、kkutysllbbb都会被匹配到。如果，我们想将上面示例中匹配要求改成连续匹配两次kkutysllb怎么办？这个时候，我们就需要使用分组的匹配的规则，将kkutysllb当做一个整体group来匹配。命令规则如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145d00bb06e90017.jpg" alt></p><p>上图中，<strong>\(  \)就表示分组，它将其中的内容看做一个整体。</strong>分组还可以嵌套，什么意思呢？接着往下看：</p><p><img src="https://i.loli.net/2019/06/27/5d145d47db40a55740.jpg" alt></p><p>如上图，为了说明分组嵌套，我们将上例中存在重复字母的地方做了单字母分组，实际环境时没必要这样分组。在上图中，我们将字母k和字母l做了两个子分组，且指定每个子分组各连续出现两次。同时，将kkutysllb做了一个大分组，包括上述的两个子分组，且指定整个大分组也连续出现两次。</p><p>我想我把分组的概念应该是说清楚了，接下来我们再看看什么是后向引用？之所以先讲分组，后介绍后向引用，是因为<strong>后向引用是以分组为前提的。</strong>还是以示例切入介绍，我们再创建一个测试文件test03，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145dfc9085770478.jpg" alt></p><p>现在，我们通过正则表达式将上述文件中的各行进行匹配，如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145e2ff1cd886406.jpg" alt></p><p>H.\{4\}表示大写字母H的后面跟随了4个任意字符，其中”.”表示任意单个字符，前面已经说过了。通过上面的匹配规则，Hello和Hilll两个单词都会被匹配到，也就是上述测试文件三行都符合规则。我们现在有了新需求，需要找出kkutysllb单词前后两个词一致的行，通过上面的规则明显无法满足需求，这时候就需要用到后向引用了。如下：</p><p><img src="https://i.loli.net/2019/06/27/5d145e8181a2c65417.jpg" alt></p><p>上图中，<strong>后面那个\1就表示后向引用，它的意思是kkutysllb后面的单词必须与前面\(H.\{4\}\)完全一致才符合规则。</strong>这究竟是什么道理呢？其实，<strong>\1的意思是等价整个正则中第1个分组的正则匹配到的结果。</strong>大白话就是，上例中整个正则只有一个分组\(H.\{4\}\)，当它与测试文件的第一行文本匹配时，会匹配到Hello，这时\1也为Hello。当它与测试文件第二行文本匹配时，会匹配到Hilll，这时\1也为HIlll。换句话说，\1引用整个正则中第1个分组的正则，同理，\2就引用了整个正则中第2个分组的正则，以此类推。。。如果你还是不明白，那就看下图理解吧，如果仍然不明白，那我也没招了。。。</p><p><img src="https://i.loli.net/2019/06/27/5d145eee25fe838328.jpg" alt></p><p>以上就是后向引用，再次强调，<strong>使用后向引用的前提是将需要引用的部分进行分组。</strong></p><p>在前述内容的例子中，我们使用了分组嵌套，那么在对分组嵌套后向引用时，到底哪个是分组1，哪个是分组n呢？我们来看下图：</p><p><img src="https://i.loli.net/2019/06/27/5d145f2da865898868.jpg" alt></p><p>在上图中，红色部分是一对分组，蓝色部分是另一对分组，当我们需要后向引用时，外层分组也就是红色分组是第1个分组，内层分组也就是蓝色分组是第2个分组。<strong>这是因为分组的顺序取决于分组符号的左侧部分的顺序</strong>，由于红色分组的左侧部分排在最前面，所以红色分组是第1个。</p><p>至此，分组和后向引用我们掰扯完了，我们还是保持国际惯例，对分组和后向引用做个总结，便于我们后续查询，如下：</p><ul><li><strong>\( \)：</strong>表示分组，我们可以将其中的内容当做一个整体，分组是可以嵌套的。</li><li><strong>\(ab\)：</strong>表示将ab当做一个整体去处理。</li><li><strong>\1：</strong>表示引用整个正则表达式中第1个分组中的正则匹配到的结果。</li><li><strong>\2：</strong>表示引用整个正则表达式中第2个分组中的正则匹配到的结果。</li></ul><h2 id="基础正则中的转义符"><a href="#基础正则中的转义符" class="headerlink" title="基础正则中的转义符"></a><strong>基础正则中的转义符</strong></h2><p>现在，我们来认识下正则中的常用符合，就是<strong>反斜杠”\“。</strong>反斜杠有什么用？我们还是先不解释，通过示例来描述。如下，我们还是创建的一个测试文件test04，内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># cat test04</span></span><br><span class="line">base</span><br><span class="line">a1<span class="comment">#$</span></span><br><span class="line">ddd</span><br><span class="line">a-!@</span><br><span class="line">cccc</span><br><span class="line">a..</span><br></pre></td></tr></table></figure><p>如上，此时我们想匹配a..这行，利用我们前面了解的知识，规则可能会这样写。匹配条件如下：</p><p><img src="https://i.loli.net/2019/06/27/5d146343910bc63820.jpg" alt></p><p>上图中，虽然匹配结果包含了我们的需求，但是也多了其他三行，这让我们很不爽。主要是因为点符号”.”在基础正则中表示任意1个字符。因此，我们如果想要在匹配条件中匹配点点，就需要使用转移符号反斜杠“\”。<strong>转义符号“\”与正则中的符号结合起来就表示这个符号本身。</strong>因此，上面的需求我们可以使用如下的匹配条件：</p><p><img src="https://i.loli.net/2019/06/27/5d14607c70a2692622.jpg" alt></p><p>如上图，\.表示单个点符号，同理\*就表示单个星符号。在前面提到过，在基本正则中，\?表示其前面的字符出现0次或1次，\+表示前面的字符出现至少1次。那么，如果我们相匹配问号?和加号+呢？难道我们还要再在前面加一个反斜杠\\？或\\+，答案是否定的。<strong>在Linux中，如果想要在正则表达式中匹配问号?或加号+，只需要在匹配条件中直接输入?或+即可</strong>，大家可以自己尝试。但是，在某些时候，<strong>我们需要匹配反斜杠自身，这时需要在反斜杠前面再加上反斜杠就行。</strong></p><p>下面我们写个经常会使用的正则表达式—<strong>-匹配ifconfig输出中的各个网卡的ip地址。</strong>如下：</p><p><img src="https://i.loli.net/2019/06/27/5d14610017cd230432.jpg" alt></p><p>此时，我们需要首先对ipv4的地址做个分析，首先它是由4组三位数组成，且每组数字不大于255，前3组三位数后面都带一个点号”.”，那么我们的正则规则可以如下来写：</p><p><img src="https://i.loli.net/2019/06/27/5d14612e7203434521.jpg" alt></p><p>上图中，([0-2]{,1})([0-9]{1,2}).表示至少为1位数字，最多为3位数字，且首位数字不大于2，后面以点号“.”结尾的字段，然后将这部分作为一个分组连续匹配3次，最后这一段([0-2]{,1})([0-9]{1,2})也表示至少为1位数字，最多为3位数字，且首位数字不大于2。<strong>注意：上面的ip地址匹配并不精确，我们其实并没有限制每组数字范围在1-254。不过，没关系，当我们了解了扩展正则就能写一个更精确的ip地址匹配规则。</strong></p><p>至此，基础的正则的五朵金花也就介绍完了。在正则入门时我们说过，Linux中正则表达式分为基础正则表达式与扩展正则表达式，在下一篇文章中我们会介绍扩展正则，其实它们的用法都是相似的，而且写法也差不多，基础正则理解并掌握了，扩展正则几乎不费力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前一篇《正则表达式基础入门》中，我们总结了跟&lt;strong&gt;“位置匹配”&lt;/strong&gt;的相关的正则，它其实也是基础正则五朵金花之一，我们只是在入门介绍中单独拿出来进行科普。这篇文章我们就来掰扯掰扯基础正则的其它四朵金花。
    
    </summary>
    
      <category term="shell编程" scheme="https://kkutysllb.cn/categories/shell%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="DevOps" scheme="https://kkutysllb.cn/tags/DevOps/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-24-Linux原生网络虚拟化实践</title>
    <link href="https://kkutysllb.cn/2019/06/24/2019-06-24-Linux%E5%8E%9F%E7%94%9F%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96%E5%AE%9E%E8%B7%B5/"/>
    <id>https://kkutysllb.cn/2019/06/24/2019-06-24-Linux原生网络虚拟化实践/</id>
    <published>2019-06-24T14:35:56.000Z</published>
    <updated>2019-06-24T15:30:56.477Z</updated>
    
    <content type="html"><![CDATA[<p>本篇是虚拟化技术系列最后一片文章（容器技术作为专题单独介绍），介于后续OpenStack的Neutron在利用libvirt构建虚拟化网络服务时，利用了很多Linux虚拟网络功能（Linux内核中的虚拟网络设备以及其他网络功能）。因此，在网络虚拟化的收尾部分，特意安排一篇Linux虚拟网络的基础实践，一方面巩固大家对网络虚拟化的认知，另一方面也为后续学习OpenStack打好基础。下面，就来带大家了解一下Linux系统原生的与Neutron密切相关的虚拟网络设备。<a id="more"></a></p><h2 id="Tap"><a href="#Tap" class="headerlink" title="Tap"></a><strong>Tap</strong></h2><p>Tap这个概念大家应该不陌生，Tap交换机总听说过吧，信令平台采集探针数据收敛必备设备，是个纯二层设备。Linux中的tap属于虚拟网络设备，也是个二层虚拟设备。而在linux中所指的“设备”并不是我们实际生产或生活中常见路由器或交换机这类设备，它其实本质上往往是一个数据结构、内核模块或设备驱动这样含义。</p><p><strong>在linux中，tap和tun往往是会被并列讨论，tap位于二层，tun位于三层。</strong>为了说明这一点我们先看看下面linux用于描述tap和tun的数据结构内容：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Struct  tun_strruct  &#123;</span><br><span class="line">Char name [<span class="number">8</span>];      <span class="comment">//设备名称</span></span><br><span class="line">Unsigned  <span class="keyword">long</span>  flags；  <span class="comment">//区分tun和tap的标志位；</span></span><br><span class="line">Struct fasync_struct  *fasync;  <span class="comment">//文件异步调用通知接口</span></span><br><span class="line">Wait_queue_head_t read_wait;  <span class="comment">//消息队列读等待标志</span></span><br><span class="line">Struct net_device dev;   <span class="comment">//定义一个抽象网络设备</span></span><br><span class="line">Struct sk_buff_head txq;  <span class="comment">//定义一个缓存区</span></span><br><span class="line">Struct net_device_stats stats; <span class="comment">//定义一个网卡状态信息结构</span></span><br><span class="line">&#125;；</span><br></pre></td></tr></table></figure><p>从上面的数据结构可以看出，tap和tun的数据结构其实一样的，用来区别两者的其实就是flags。Tap从功能上说，位于二层也就是数据链路层，而二层的主要网络协议包括：</p><ul><li><strong>点对点协议（Point-to-Point Protocol）</strong></li><li><strong>以太网协议（Ethernet）</strong></li><li><strong>高级数据帧链路协议（High-Level Data Link Protocol）</strong></li><li><strong>帧中继（Frame Relay）</strong></li><li><strong>异步传输模式（Asynchronous Transfer Mode）</strong></li></ul><p><strong>而tap只是与以太网协议（Ethernet）对应，所以tap常被称为“虚拟以太网设备” 。</strong>要想使用Linux命令行操作一个tap，首先Linux得有tun模块（Linux使用tun模块实现了tun/tap），检查方法如下所示，输入modinfo tun命令如果有如下输出，就表示Linux内核具备tun模块。</p><p><img src="https://i.loli.net/2019/06/24/5d10e1d00f5a237937.jpg" alt></p><p>接下来，还要看看Linux内核是否加载tun模块。检查办法就是输入指令lsmod|grep tun，查看是否有如下图所示信息输出。如果有，就表示已加载，否则输入modprobe tun指令进行加载，然后再次输入lsmod|grep tun指令进行确认（<strong>这种加载-确认的操作方式是一种好的操作习惯，要养成！！！</strong>）</p><p><img src="https://i.loli.net/2019/06/24/5d10e2df2274078453.jpg" alt></p><p>当我们确认了Linux加载了tun模块后，我们还需要确认Linux是否有操作tun/tap的命令行工具tunctl。在Linux命令行输入tunctl help进行确认。</p><p><img src="https://i.loli.net/2019/06/24/5d10e2f95825263133.jpg" alt></p><p>如上图所示，如果提示command not found，在CentOS 6.x系统下直接输入yum install -y tunctl安装即可。但是，在CentOS 7.x系统下，需要先指定一个特定仓库，然后按照指定的仓库进行安装，否则会提示找不到tunctl rpm包。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先创建一个指定仓库</span></span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/yum.repos.d/nux-misc.repo</span><br><span class="line">&gt; [nux-misc]</span><br><span class="line">&gt; name=Nux Misc</span><br><span class="line">&gt; baseurl=http://li.nux.ro/download/nux/misc/el7/x86_64/</span><br><span class="line">&gt; enabled=0</span><br><span class="line">&gt; gpgcheck=1</span><br><span class="line">&gt; gpgkey=http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro</span><br><span class="line">&gt; EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照指定仓库进行安装</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># yum -y --enablerepo=nux-misc install tunctl</span></span><br></pre></td></tr></table></figure><p>安装完后，再次执行tunctl help命令后，有如下信息输出，表示安装成功。</p><p><img src="https://i.loli.net/2019/06/24/5d10e355ee04934514.jpg" alt></p><p>具备了tun和tunctl后，我们就亦可以创建一个tap设备了，创建命令使用-t选项指定创建的tap设备名称。</p><p><img src="https://i.loli.net/2019/06/24/5d10e3731b61a65536.jpg" alt></p><p>此时，我们输入ip link list或ifconfig命令可以查看到刚才创建的tap设备llb_tap1。</p><p><img src="https://i.loli.net/2019/06/24/5d10e3934dcba94992.jpg" alt></p><p>通过上面的命令输出，我们发现这个tap设备还没有绑定ip，可以通过执行ip addr或ifconfig命令为该tap设备绑定一个ip地址，我们给它规划一个192.168.1.1/24的地址。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过ip addr命令绑定ip地址</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip addr local 192.168.1.1/24 dev llb_tap1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者使用ifconfig命令绑定ip地址</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig llb_tap1 192.168.1.1/24</span></span><br></pre></td></tr></table></figure><p><strong>注意：上述操作方式，个人推荐第一种，毕竟我们要与时俱进。。。</strong></p><p><img src="https://i.loli.net/2019/06/24/5d10e3ff2808d82295.jpg" alt></p><p>到此为止，一个tap设备就创建完毕了，在OpenStack的Neutron中创建的虚拟网络，虚拟机的vNIC与虚拟交换机之间就是通过一个tap桥接，而这个tap设备就是俗称的<strong>端口组</strong>，所以也就有了同一个端口组内的port归属同一个network的说法。后续通过测试用例，我们会再次讲述tap的用法。</p><h2 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a><strong>Namespace</strong></h2><p><strong>namespace是Linux虚拟网络的一个重要概念。</strong>传统的Linux的许多资源是全局的，比如进程ID资源。而<strong>namespace的目的首先就是将这些资源进行隔离</strong>。容器技术的两大实现基石之一就是namespace，它主要负责容器虚拟化中资源隔离，另一个就是cgroups，主要负责容器虚拟化中资源的管理。</p><p>在Linux中，可以在一个Host内创建许多namespace，于是那些原本是Linux全局的资源，就<strong>变成了namespace范围内的“全局”资源</strong>，而且不同namespace的资源互相不可见、彼此透明。Linux中会被namespace隔离的全局资源包括：</p><ul><li><strong>uts_ns：</strong>UTS是Unix Timesharing System的简称，包含内核名称、版本、底层体系结构等信息。</li><li><strong>ipc_ns：</strong>所有与进程通信（IPC）有关的信息。</li><li><strong>mnt_ns：</strong>当前装载的文件系统；</li><li><strong>pid_ns：</strong>有关进程的id信息。</li><li><strong>user_ns：</strong>资源配额信息。</li><li><strong>net_ns：</strong>网络信息。</li></ul><p>至于为什么这些全局资源会被隔离，是由Linux全局资源的数据结构定义决定的，如下所示，Linux的全局资源数据结构定在在文件<strong>nsproxy.h</strong>中。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Struct nsproxy &#123;</span><br><span class="line">Atomic_t count;</span><br><span class="line">Struct uts_namespace *uts_ns;</span><br><span class="line">Struct ipc_namespace *ipc_ns;</span><br><span class="line">Struct mnt_namespace *mnt_ns;</span><br><span class="line">Struct pid_namespace *pid_ns;</span><br><span class="line">Struct user_namespace *user_ns;</span><br><span class="line">Struct net *net_ns;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>从资源的隔离的角度来说，Linux namespace的示意图如下所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e5fd7f09157174.jpg" alt></p><p>上图表明，每个namespace里面将原本是全局资源的进行了隔离，彼此互相不可见。同时在Linux的Host或者每一个VM中，都各自有一套相关的全局资源。借助虚拟化的概念，我们可以将Linux Host的namespace称为root namespace，其它虚拟机namespace称为guest namespace。</p><p><strong>单纯从网络的视角来看，一个namespace提供了一份独立的网络协议栈。包括：网络设备接口、IPv4、IPv6、IP路由、防火墙规则、sockets等。</strong>一个Linux Device，无论是虚拟设备还是物理设备，只能位于一个namespace中，不同namespace的设备之间通过<strong>veth pair</strong>设备进行连接，veth pair设备也是一个虚拟网络设备，可以暂时将其理解为<strong>虚拟网线</strong>，后面会详细介绍。</p><p>Linux中namespace的操作命令是<strong>ip netns</strong>，这个命令的帮助如下所示：</p><p><img src="https://i.loli.net/2019/06/24/5d10e6656350639989.jpg" alt></p><p>首先，我们通过ip netns list命令查看一下当前系统的namespace列表信息，由于当前系统没有创建namespace，所以没有任何信息返回。然后，我们通过ip netns add命令添加一个namespace（llb_ns1）。最后，再通过ip netns list命令进行查看，结果如下图所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e68d9034919520.jpg" alt></p><p>接下来，我们可以通过ip link set名ing把刚才创建的虚拟tap设备迁移到llb_ns1中去。这时候，我们在root namespace下执行ifconfig命令就找不到llb_tap1设备了。</p><p><img src="https://i.loli.net/2019/06/24/5d10e6af6c3bd92998.jpg" alt></p><p>我们可以通过ip netns exec [NAME] CMD的方式操作，namespace中的设备，其中NAME参数为namespace名称，CMD是要执行的指令。</p><p><img src="https://i.loli.net/2019/06/24/5d10e6cbb0ab748635.jpg" alt></p><p>在namespace中给llb_tap1重新绑定ip地址。因为，llb_tap1在root namesapce中绑定的ip归属root namesapce的资源，当llb_tap1更改归属namespace时，原来的资源自然无法使用，必须重新配置。</p><p><img src="https://i.loli.net/2019/06/24/5d10e6f14e8f678387.jpg" alt></p><p>到此为止，namespace的创建就讲述完了，后续还会根据实例继续讲解其它用法。</p><h2 id="Veth-pair"><a href="#Veth-pair" class="headerlink" title="Veth pair"></a><strong>Veth pair</strong></h2><p>veth pair不是一个设备，而是一对设备，以连接两个虚拟以太端口，类似网线。操作veth pair，需要跟namespace一起配合，不然就没有意义。其实，veth pair设备本质上有三个接口，一端连接linux内核，另两端连接两个tap设备，是一个Y型结构，实现上更像一个HUB。如下图所示：</p><p><img src="https://i.loli.net/2019/06/24/5d10e73ac8a6350129.jpg" alt></p><p>两个namespace llb_ns1/llb_ns2中各有一个tap设备，组成veth pair，两者的ip如上图所示，测试两个ip进行互ping。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap1 type veth peer name llb_tap2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建两个namespace：llb_ns1，llb_ns2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将veth pair设备两端的两个tap设备移动到对应的namespace中</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap1 netns llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap2 netns llb_ns2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置两个tap设备的ip地址，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ifconfig llb_tap1 192.168.1.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ifconfig llb_tap2 192.168.1.2/24 up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试两个tap设备互ping</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ping -c 5 192.168.1.2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ping -c 5 192.168.1.1</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/24/5d10e7a926e6297494.jpg" alt></p><p>通过veth pair我们可以连接两个namespace中的两个tap设备，但是veth pair一端只能连接两个tap，如果是三个或多个namespace内的tap要实现互联怎么办？这时候，就只能使用Linux Bridge来完成。</p><h2 id="Linux-Bridge-vSwitch"><a href="#Linux-Bridge-vSwitch" class="headerlink" title="Linux Bridge/vSwitch"></a><strong>Linux Bridge/vSwitch</strong></h2><p>在Linux的网络部分，Bridge和Switch是一个概念。所以，大家把Linux Bridge看做一个交换机来理解就行，也就是2层的一个汇聚设备。Linux实现Bridge功能的是<strong>brctl模块</strong>。在命令行里敲一下brctl，如果能显示相关内容，则表示有此模块，否则还需要安装。安装命令是：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@C7-Server01 ~]<span class="comment"># yum install -y bridge-utils</span></span><br></pre></td></tr></table></figure><p>Bridge本身的概念，我们通过一个综合测试用例来讲述Bridge的基本用法，同时也涵盖前面所述的几个概念：tap、namesapce、veth pair。实验拓扑图如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10e7ffbbd5e47582.jpg" alt></p><p>上图中，有4个namespace，每个namespace都有一个tap与交换机上一个tap口组成veth pair。这样4个namespace就通过veth pair及Bridge互联起来。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap1 type veth peer name tap1_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap2 type veth peer name tap2_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap3 type veth peer name tap3_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap4 type veth peer name tap4_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建namespace</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns3</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把tap设备移动到对应namespace中</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap1 netns llb_ns1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap2 netns llb_ns2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap3 netns llb_ns3</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap4 netns llb_ns4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Bridge</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addbr br1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把相应的tap设备添加到br1的端口上</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap1_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap2_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap3_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># brctl addif br1 tap4_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置相应tap设备的ip，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ifconfig llb_tap1 192.168.1.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ifconfig llb_tap2 192.168.1.2/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns3 ifconfig llb_tap3 192.168.1.3/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns4 ifconfig llb_tap4 192.168.1.4/24 up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将Bridge及所有tap设备状态设置为up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set br1 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap1_peer up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap2_peer up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap3_peer up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set tap4_peer up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行互ping测试</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns1 ping -c 3 192.168.1.4</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns3 ping -c 3 192.168.1.2</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns2 ping -c 3 192.168.1.1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns4 ping -c 3 192.168.1.2</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/24/5d10e86b3a0bf84209.jpg" alt></p><h2 id="Router"><a href="#Router" class="headerlink" title="Router"></a><strong>Router</strong></h2><p>Linux创建Router不像Bridge一样有一个直接命令，甚至连间接命令都没有。因为它自身就是一个路由器（Router）。不过Linux默认没有打开路由转发功能。可以用less /proc/sys/net/ipv4/ip_forward这个命令验证。这个命令就是查看一下这个文件（/proc/sys/net/ipv4/ip_forward）的内容。该内容是一个数字。如果是“0”，则表示没有打开路由功能。把“0”修改为“1”，就是打开了Linux的路由转发功能，修改命令为echo “1” &gt; /proc/sys/net/ipv4/ip_forward。这种打开方法，在机器重启以后就会失效了。一劳永逸的方法是修改配置文件“/etc/sysctl.conf”，将net.ipv4.ip_forward=0修改为1，保存后退出即可。</p><p><img src="https://i.loli.net/2019/06/24/5d10e8984235e46830.jpg" alt></p><p>下面，我们还是通过一个示例来直观感受一下Route的功能。实验拓扑图如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10e8b93b3d745185.jpg" alt></p><p>在上图中，llb_ns5/llb_tap5与llb_ns6/llb_tap6不在同一个网段中，中间需要经一个路由器进行转发才能互通。图中的Router是一个示意，其实就是Linux开通了路由转发功能。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap5 type veth peer name tap5_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap6 type veth peer name tap6_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建namespace：llb_ns5、llb_ns6</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns5</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将tap设备迁移到对应namespace中</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap5 netns llb_ns5</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap6 netns llb_ns6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置tap设备ip地址，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns5 ifconfig llb_tap5 192.168.100.5/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns6 ifconfig llb_tap6 192.168.200.5/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap5_peer 192.168.100.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap6_peer 192.168.200.1/24 up</span></span><br></pre></td></tr></table></figure><p>现在，我们先来做个ping测试，提示网络不可达，如下图所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e8f78316127513.jpg" alt></p><p>我们查看下llb_ns5的路由表信息，如下图所示，llb_ns5并没有到达192.168.200.0/24网段的路由表项，因此需要手工进行添加。</p><p><img src="https://i.loli.net/2019/06/24/5d10e914a452d47741.jpg" alt></p><p>在llb_ns5中添加到192.168.200.0/24网段静态路由信息，同时在llb_ns6中添加到192.168.100.0/24回程路由信息。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在llb_ns5中添加到192.168.200.0/24的静态路由</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns5 route add -net 192.168.200.0 netmask 255.255.255.0 gw 192.168.100.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同理，在llb_ns6中添加到192.168.100.0/24的回程路由</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns6 route add -net 192.168.100.0 netmask 255.255.255.0 gw 192.168.200.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看llb_ns5、llb_ns6的路由信息</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns5 route -ne</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns6 route -ne</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/06/24/5d10e957464e269449.jpg" alt></p><p>再次进行ping尝试，可以ping通，结果如下图所示。</p><p><img src="https://i.loli.net/2019/06/24/5d10e97fb5eb697934.jpg" alt></p><h2 id="Tun"><a href="#Tun" class="headerlink" title="Tun"></a><strong>Tun</strong></h2><p>在前面tap的时候就介绍过tun和tap其实同一数据结构，只是通过flags标志位来区分。Tap是二层虚拟以太网设备，那么<strong>tun就是三层的点对点的虚拟隧道设备</strong>。也就是说Linux原生支持三层隧道技术。至于什么是隧道技术？在上一篇数据中心的网络虚拟化技术有详细介绍，这里不再赘述。</p><p><strong>Linux一共原生支持5种三层隧道技术，分别是：</strong></p><ul><li><strong>ipip：</strong>IP in IP，在IPv4报文的基础上再封装一个IPv4报文头，属于IPv4 in IPv4。</li><li><strong>GRE：</strong>通用路由封装（Generic Routing Encapsulation），定义在任意一种网络层协议上封装任意一个其他网络层协议的协议，属于IPv4/IPv6 over IPv4。</li><li><strong>sit：</strong>与ipip类似，只不过用IPv4报文头封装一个IPv6报文，属于IPv6 over IPv4。</li><li><strong>isatap：</strong>站内自动隧道寻址协议，一般用于IPv4网络中的IPv6/IPv4节点间的通信。</li><li><strong>vti：</strong>全称是Virtual Tunnel Interface，为IPSec隧道提供一个可路由的接口类型。</li></ul><p>国际惯例，我们还是通过一个具体的测试实例来理解tun。实验拓扑图如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10e9a968d9b77681.jpg" alt></p><p>上图的tun1、tun2，如果我们先忽略的话，剩下的就是我们在前面讲述过的内容。测试用例的第一步，就是使图中的tap7与tap8配置能通，借助上面route的配置，这里我们不再重复。当tap7和tap8配通以后，如果我们不把图中的tun1和tun2暂时当做tun设备，而是当做两个“死”设备（比如当做是两个不做任何配置的网卡），那么这个时候tun1和tun2就像两个孤岛，不仅互相不通，而且跟tap7、tap8也没有关系。因此，我们就需要对tun1、tun2做相关配置，以使这两个两个孤岛能够互相通信。我们以ipip tunnel为例进行配置。</p><p>首先我们要加载ipip模块，Linux默认是没有加载这个模块的。通过命令行lsmod|grep ip进行查看，如果没有加载，可以通过命令modprobe ipip来加载ipip模块。具体过程参见tap部分，这里不再赘述。加载了ipip模块以后，我们就可以创建tun，并且给tun绑定一个ipip隧道。配置代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建veth pair设备</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap7 type veth peer name tap7_peer</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link add llb_tap8 type veth peer name tap8_peer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建namespace：llb_ns7，llb_ns8</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns7</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns add llb_ns8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将tap设备移动到对应的namespace</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap7 netns llb_ns7</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip link set llb_tap8 netns llb_ns8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置ip地址，并启动</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ifconfig llb_tap7 192.168.100.6/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ifconfig llb_tap8 192.168.200.6/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap7_peer 192.168.100.1/24 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ifconfig tap8_peer 192.168.200.1/24 up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置路由和回程路由</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 route add -net 192.168.200.0 netmask 255.255.255.0 gw 192.168.100.1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 route add -net 192.168.100.0 netmask 255.255.255.0 gw 192.168.200.1</span></span><br></pre></td></tr></table></figure><p>测试互通，结果如下，表明底层underlay网络可达。</p><p><img src="https://i.loli.net/2019/06/24/5d10ea08d124792954.jpg" alt></p><p>下来，我们创建隧道设备tun，构建overlay网络，代码如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在llb_ns7和llb_ns8中创建tun1、tun2和ipip tunnel</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ip tunnel add tun1 mode ipip remote 192.168.200.6 local 192.168.100.6</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ip tunnel add tun2 mode ipip remote 192.168.100.6 local 192.168.200.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活tun设备，并配置ip地址</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ip link set tun1 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ip link set tun2 up</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns7 ip addr add 10.10.10.50 peer 10.10.20.50 dev tun1</span></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ip netns exec llb_ns8 ip addr add 10.10.20.50 peer 10.10.10.50 dev tun2</span></span><br></pre></td></tr></table></figure><p>互通测试，结果如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10eaa6b8ab112350.jpg" alt></p><p><strong>把上面的命令行脚本中的ipip换成gre，其余不变，就创建了一个gre隧道的tun设备对。</strong>因为我们说tun是一个设备，那么我们可以通过ifconfig这个命令，来看看这个设备的信息，代码如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10eb1c828ed42461.jpg" alt></p><p>可以看到，tun1是一个ipip tunnel的一个端点，IP是10.10.10.50，其对端IP是10.10.20.50。 再看路由表信息，代码如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10eb4d248e961131.jpg" alt></p><p>图中的内容告诉我们，到达目的地10.10.10.50的路由是一个直连路由直接从tun1出去即可，其实tun1和tun2就是overlay网络的VTEP。</p><p>至此，Linux原生网络虚拟化介绍完毕。其实，还有个iptables，一般我们把它理解为Linux的包过滤防火墙，其实当其所属服务器处于网络报转发的中间节点时，它也是一个网络防火墙。它的“防火”机制就是通过一个个链结合策略表完成，而那一个个链其本质就是一个三层的虚拟网络设备。由于篇幅原因，iptables会放在Linux常用运维工具分类中介绍。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇是虚拟化技术系列最后一片文章（容器技术作为专题单独介绍），介于后续OpenStack的Neutron在利用libvirt构建虚拟化网络服务时，利用了很多Linux虚拟网络功能（Linux内核中的虚拟网络设备以及其他网络功能）。因此，在网络虚拟化的收尾部分，特意安排一篇Linux虚拟网络的基础实践，一方面巩固大家对网络虚拟化的认知，另一方面也为后续学习OpenStack打好基础。下面，就来带大家了解一下Linux系统原生的与Neutron密切相关的虚拟网络设备。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-24-打造文件实时同步架构之rsync篇</title>
    <link href="https://kkutysllb.cn/2019/06/24/2019-06-24-%E6%89%93%E9%80%A0%E6%96%87%E4%BB%B6%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E4%B9%8Brsync%E7%AF%87/"/>
    <id>https://kkutysllb.cn/2019/06/24/2019-06-24-打造文件实时同步架构之rsync篇/</id>
    <published>2019-06-24T10:19:24.000Z</published>
    <updated>2019-06-24T10:50:42.652Z</updated>
    
    <content type="html"><![CDATA[<p>rsync是可以实现增量备份的工具。配合任务计划，rsync能实现定时或间隔同步，配合inotify或sersync，可以实现触发式的实时同步。事实上，rsync有一套自己的算法，其算法原理以及rsync对算法实现的机制可能比想象中要复杂一些。平时使用rsync实现简单的备份、同步等功能足以，没有多大必要去深究这些原理性的内容。如果你对这些原理感兴趣，可以通过rsync命令的man文档、并且借助”-vvvv”分析rsync执行过程，结合rsync的算法原理去深入理解。本篇文章由于篇幅有限，只是介绍rsync的使用方法和它常用的功能，以及rsync和sersync如何配合打造文件实时同步架构。<a id="more"></a></p><p><img src="https://i.loli.net/2019/06/24/5d10a4fd1a40892780.jpg" alt></p><h2 id="rsync文件同步机制简介"><a href="#rsync文件同步机制简介" class="headerlink" title="rsync文件同步机制简介"></a><strong>rsync文件同步机制简介</strong></h2><p>rsync是一款开源的、快速的、多功能的、可实现全量及增量的本地或远程数据镜像同步备份的优秀工具。rsync适用于Unix/Linux/Windows等多种操作系统平台。</p><p><strong>rsync可以实现scp的远程拷贝(rsync不支持远程到远程的拷贝，但scp支持)、cp的本地拷贝、rm删除和”ls -l”显示文件列表等功能。</strong>但需要注意的是，r<strong>sync的最终目的或者说其原始目的是实现两端主机的文件同步，因此实现的scp/cp/rm等功能仅仅只是同步的辅助手段，且rsync实现这些功能的方式和这些命令是不一样的。</strong></p><p>rsync的目的是实现本地主机和远程主机上的文件同步(<strong>包括本地推到远程，远程拉到本地两种同步方式)</strong>，也可以实现本地不同路径下文件的同步，但不能实现远程路径1到远程路径2之间的同步，这个功能scp是可以实现的。</p><p>不考虑rsync的实现细节，就文件同步而言，涉及了<strong>源文件和目标文件的概念，还涉及了以哪边文件为同步基准。</strong>例如，想让目标主机上的文件和本地文件保持同步，则是以本地文件为同步基准，将本地文件作为源文件推送到目标主机上。反之，如果想让本地主机上的文件和目标主机上的文件保持同步，则目标主机上的文件为同步基准，实现方式是将目标主机上的文件作为源文件拉取到本地。当然，要保持本地的两个文件相互同步，rsync也一样能实现，这就像Linux中cp命令一样，以本地某文件作为源，另一文件作为目标文件，但是这两者实现的本质是完全不同的。</p><p>既然是文件同步，<strong>在同步过程中必然会涉及到源和目标两文件之间版本控制的问题</strong>，例如是否要删除源主机上没有但目标上多出来的文件，目标文件比源文件更新(newer than source)时是否仍要保持同步，遇到软链接时是拷贝软链接本身还是拷贝软链接所指向的文件，目标文件已存在时是否要先对其做个备份等等。</p><p><strong>rsync同步过程中由两部分模式组成：决定哪些文件需要同步的检查模式</strong>以及<strong>文件同步时的同步模式。</strong></p><p><strong>1）检查模式是指按照指定规则来检查哪些文件需要被同步</strong>，例如哪些文件是明确被排除不传输的。默认情况下，rsync使用<strong>“quick check”算法</strong>快速检查源文件和目标文件的大小、mtime(修改时间)是否一致，如果不一致则需要传输。当然，也可以通过在rsync命令行中指定某些选项来改变quick check的检查模式，比如”–size-only”选项表示”quick check”将仅检查文件大小不同的文件作为待传输文件。<strong>rsync支持非常多的选项，其中检查模式的自定义性是非常有弹性的。</strong></p><p><strong>2）同步模式是指在文件确定要被同步后，在同步过程发生之前要做哪些额外工作。</strong>例如：上文所说的是否要先删除源主机上没有但目标主机上有的文件，是否要先备份已存在的目标文件，是否要追踪链接文件等额外操作。<strong>rsync也提供非常多的选项使得同步模式变得更具弹性。</strong></p><p>相对来说，为rsync手动指定同步模式的选项更常见一些，只有在有特殊需求时才指定检查模式，因为大多数检查模式选项都可能会影响rsync的性能。</p><h2 id="rsync的三种工作模式"><a href="#rsync的三种工作模式" class="headerlink" title="rsync的三种工作模式"></a><strong>rsync的三种工作模式</strong></h2><p>rsync命令有三种常见模式，具体如下：</p><p><strong>1）本地模式：</strong>本地文件系统上实现同步，命令行语法格式如下：</p><ul><li><ul><li>rsync [option] [SRC] [DEST]</li><li>rsync [选项] [源文件] [目标文件]</li></ul></li></ul><p><strong>2）通过远程Shell访问模式：</strong>本地主机使用远程shell和远程主机通信，命令行语法格式如下：</p><ul><li><p><strong>拉取（Pull）:</strong></p></li><li><ul><li><ul><li>rsync [option] [USER@]HOST:SRC [DEST]</li><li>rsync [选项] 用户@主机:源文件 [目标文件]</li></ul></li></ul></li><li><p><strong>推送（Push）：</strong></p></li><li><ul><li><ul><li>rsync [option] [SRC] [USER@]HOST:DEST</li><li>rsync [选项] [源文件] 用户@主机:目标文件</li></ul></li></ul></li></ul><p><strong>3）rsync守护进程模式：</strong>本地主机通过网络套接字连接远程主机上的rsync daemon，命令行语法格式如下：</p><ul><li><p><strong>拉取（Pull）：</strong></p></li><li><ul><li><ul><li>rsync [option] [USER@]HOST::SRC [DEST]</li><li>rsync [选项] 用户@主机::源文件 [目标文件]</li><li>rsync [option] rsync://[USER@]HOST[:PORT]/SRC [DEST]</li><li>rsync [选项] rsync://用户@主机:端口/源文件 [目标文件]</li></ul></li></ul></li><li><p><strong>推送（Push）：</strong></p></li><li><ul><li><ul><li>rsync [option] SRC [USER@]HOST::DEST</li><li>rsync [选项] [源文件] 用户@主机::目标文件</li><li>rsync [option] SRC rsync://[USER@]HOST[:PORT]/DEST</li><li>rsync [选项] [源文件] rsync://用户@主机:端口/目标文件</li></ul></li></ul></li></ul><p>前两者的本质是通过管道通信，而第三种则是让远程主机上运行rsync服务，使其监听在一个端口上，等待客户端的连接。</p><p>其实，<strong>还有第四种工作方式：通过远程shell也能临时启动一个rsync daemon</strong>，它不同于方式3，它不要求远程主机上事先启动rsync服务，而是临时派生出rsync daemon，属于单用途的一次性daemon，仅用于临时读取daemon的配置文件，当此次rsync同步完成，远程shell启动的rsync daemon进程也会自动消逝。此通信方式的命令行语法格式同方式3，但<strong>要求options部分必须明确指定”–rsh”选项或其短选项”-e”。</strong></p><p><strong>上述语法格式中，如果仅有一个SRC或DEST参数，则将以类似于”ls -l”的方式列出源文件列表(只有一个路径参数，总会认为是源文件)，而不是复制文件。</strong></p><p>另外，使用rsync一定要注意的一点是，<strong>源路径如果是一个目录的话，带上尾随斜线和不带尾随斜线是不一样的，不带尾随斜线表示的是整个目录包括目录本身，带上尾随斜线表示的是目录中的文件，不包括目录本身。</strong></p><p>由于rsync支持一百多个选项，所以此处只介绍几个常用选项。完整的选项说明以及rsync的使用方法参见系统的man rsync帮助文档。<strong>其中，重要参数选项如下：</strong></p><ul><li><strong>-v：</strong>显示rsync过程中详细信息。可以使用”-vvvv”获取更详细信息。</li><li><strong>-P：</strong>显示文件传输的进度信息。(实际上”-P”=”–partial –progress”，其中的”–progress”才是显示进度信息的)。</li><li><strong>-n –dry-run：</strong>仅测试传输，而不实际传输。常和”-vvvv”配合使用来查看rsync是如何工作的。</li><li><strong>-a –archive：</strong>归档模式，表示递归传输并保持文件属性。等同于”-rtopgDl”。</li><li><strong>-r –recursive：</strong>递归到目录中去。</li><li><strong>-t –times：</strong>保持mtime属性。<strong>强烈建议任何时候都加上”-t”，否则目标文件mtime会设置为系统时间，导致下次更新检查出mtime不同从而导致增量传输无效。</strong></li><li><strong>-o –owner：</strong>保持owner属性(属主)。</li><li><strong>-g –group：</strong>保持group属性(属组)。</li><li><strong>-p –perms：</strong>保持perms属性(权限，不包括特殊权限)。</li><li><strong>-D：</strong>是”–device –specials”选项的组合，即也拷贝设备文件和特殊文件。</li><li><strong>-l –links：</strong>如果文件是软链接文件，则拷贝软链接本身而非软链接所指向的对象。</li><li><strong>-z：</strong>传输时进行压缩提高效率。</li><li><strong>-R –relative：</strong>使用相对路径。意味着将命令行中指定的全路径而非路径最尾部的文件名发送给服务端，包括它们的属性。</li><li><strong>–size-only：</strong>默认算法是检查文件大小和mtime不同的文件，使用此选项将只检查文件大小。</li><li><strong>-u –update：</strong>仅在源mtime比目标已存在文件的mtime新时才拷贝。注意，该选项是接收端判断的，不会影响删除行为。</li><li><strong>-d –dirs：</strong>以不递归的方式拷贝目录本身。默认递归时，如果源为”dir1/file1”，则不会拷贝dir1目录，使用该选项将拷贝dir1但不拷贝file1。</li><li><strong>–max-size：</strong>限制rsync传输的最大文件大小。可以使用单位后缀，还可以是一个小数值(例如：”–max-size=1.5m”)</li><li><strong>–min-size：</strong>限制rsync传输的最小文件大小。这可以用于禁止传输小文件或那些垃圾文件。</li><li><strong>–exclude：</strong>指定排除规则来排除不需要传输的文件。</li><li><strong>–delete：</strong>以SRC为主，对DEST进行同步。多则删之，少则补之。注意”–delete”是在接收端执行的，所以它是在exclude/include规则生效之后才执行的。</li><li><strong>-b –backup ：</strong>对目标上已存在的文件做一个备份，备份的文件名后默认使用”~”做后缀。</li><li><strong>–backup-dir：</strong>指定备份文件的保存路径。不指定时默认和待备份文件保存在同一目录下。</li><li><strong>-e ：</strong>指定所要使用的远程shell程序，默认为ssh。</li><li><strong>–port：</strong>连接daemon时使用的端口号，默认为873端口。</li><li><strong>–password-file：</strong>daemon模式时的密码文件，可以从中读取密码实现非交互式。注意，这不是远程shell认证的密码，而是rsync模块认证的密码。</li><li><strong>-W –whole-file：</strong>rsync将不再使用增量传输，而是全量传输。在网络带宽高于磁盘带宽时，该选项比增量传输更高效。</li><li><strong>–existing：</strong>要求只更新目标端已存在的文件，目标端还不存在的文件不传输。注意，使用相对路径时如果上层目录不存在也不会传输。</li><li><strong>–ignore-existing：</strong>要求只更新目标端不存在的文件。和”–existing”结合使用有特殊功能，见下文示例。</li><li><strong>–remove-source-files：</strong>要求删除源端已经成功传输的文件。</li></ul><p><strong>虽然选项非常多，但最常用的选项组合是”avz”，即压缩和显示部分信息，并以归档模式传输。</strong></p><h2 id="rsync本地和远程shell使用示例"><a href="#rsync本地和远程shell使用示例" class="headerlink" title="rsync本地和远程shell使用示例"></a><strong>rsync本地和远程shell使用示例</strong></h2><p>以下示例既可以通过shell远程链接的工作模式实现，也可以在本地模式实现，具体的实现方式不同，应用的实际场景就不同，这里只是作为示例讲解，在实际使用时要灵活变通。</p><p><strong>1）将/etc/hosts文件拷贝到本地/tmp目录下</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync /etc/hosts /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l /tmp/hosts</span></span><br><span class="line">-rw-r--r-- 1 root root 158 Jun  3 14:13 /tmp/hosts</span><br></pre></td></tr></table></figure><p><strong>2）将/etc/cron.d目录拷贝到/tmp目录下</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r /etc/cron.d /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -ld /tmp/cr*</span></span><br><span class="line">drwxr-xr-x 2 root root 4096 Jun  3 14:14 /tmp/cron.d</span><br></pre></td></tr></table></figure><p><strong>3）将/etc/cron.d目录拷贝到/tmp下，但要求在/tmp下也生成etc子目录</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用-R选项，利用相对路径拷贝机制</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -R -r /etc/cron.d /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree -l /tmp/etc/</span></span><br><span class="line">/tmp/etc/</span><br><span class="line">└── cron.d</span><br><span class="line">├── 0hourly</span><br><span class="line">├── raid-check</span><br><span class="line">└── sysstat</span><br><span class="line">1 directory, 3 files</span><br></pre></td></tr></table></figure><p>其中”-R”选项表示使用相对路径，<strong>此相对路径是以目标目录为根的。</strong>对于上面的示例，表示在目标上的/tmp下创建etc/cron.d目录，即/tmp/etc/cron.d，etc/cron.d的根”/“代表的就是目标/tmp。</p><p><strong>4）如果要拷贝的源路径较长，但只想在目标主机上保留一部分目录结构，例如，要拷贝/var/log/anaconda/*到/tmp下，但只想在/tmp下保留从log开始的目录，这时可以使用一个点代表相对路径的起始位置即可，也就是将长目录进行划分。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -R -r /var/./log/anaconda /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree -l /tmp/log/</span></span><br><span class="line">/tmp/<span class="built_in">log</span>/</span><br><span class="line">└── anaconda</span><br><span class="line">├── anaconda.log</span><br><span class="line">├── ifcfg.log</span><br><span class="line">├── journal.log</span><br><span class="line">├── ks-script-n7L3eP.log</span><br><span class="line">├── ks-script-v6NMcO.log</span><br><span class="line">├── packaging.log</span><br><span class="line">├── program.log</span><br><span class="line">├── storage.log</span><br><span class="line">└── syslog</span><br><span class="line">1 directory, 9 files</span><br></pre></td></tr></table></figure><p><strong>这种方式，从点开始的目录都是相对路径，其相对根目录为目标路径。</strong>所以对于上面的示例，将在目标上创建/tmp/log/anaconda/*。</p><p><strong>5）对远程目录下已存在的文件做一个备份</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]# rsync -R -r --backup /var/./log/anaconda /tmp</span><br><span class="line">[root@c7-test01 ~]# ls -l /tmp/log/anaconda/</span><br><span class="line">total 4880</span><br><span class="line">-rw------- 1 root root   10945 Jun  3 14:22 anaconda.log</span><br><span class="line">-rw------- 1 root root   10945 Jun  3 14:18 anaconda.log~</span><br><span class="line">-rw------- 1 root root   18216 Jun  3 14:22 ifcfg.log</span><br><span class="line">-rw------- 1 root root   18216 Jun  3 14:18 ifcfg.log~</span><br><span class="line">-rw------- 1 root root 1711915 Jun  3 14:22 journal.log</span><br><span class="line">-rw------- 1 root root 1711915 Jun  3 14:18 journal.log~</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-n7L3eP.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:18 ks-script-n7L3eP.log~</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-v6NMcO.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:18 ks-script-v6NMcO.log~</span><br><span class="line">-rw------- 1 root root  312693 Jun  3 14:22 packaging.log</span><br><span class="line">-rw------- 1 root root  312693 Jun  3 14:18 packaging.log~</span><br><span class="line">-rw------- 1 root root   29448 Jun  3 14:22 program.log</span><br><span class="line">-rw------- 1 root root   29448 Jun  3 14:18 program.log~</span><br><span class="line">-rw------- 1 root root   78942 Jun  3 14:22 storage.log</span><br><span class="line">-rw------- 1 root root   78942 Jun  3 14:18 storage.log~</span><br><span class="line">-rw------- 1 root root  320770 Jun  3 14:22 syslog</span><br><span class="line">-rw------- 1 root root  320770 Jun  3 14:18 syslog~</span><br></pre></td></tr></table></figure><p><strong>通过–backup参数可以在目标目录下，对已存在的文件就被做一个备份，备份文件默认使用”~”做后缀，可以使用”–suffix”指定备份后缀。</strong>同时，可以使用<strong>“–backup-dir”指定备份文件保存路径，但要求保存路径必须存在。指定备份路径后，默认将不会加备份后缀，除非使用”–suffix”显式指定后缀，如”–suffix=~”</strong>。代码如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -R -r --backup-dir=/tmp/backup --backup /var/./log/anaconda /tmp</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l /tmp/backup/log/anaconda/</span></span><br><span class="line">total 2440</span><br><span class="line">-rw------- 1 root root   10945 Jun  3 14:22 anaconda.log</span><br><span class="line">-rw------- 1 root root   18216 Jun  3 14:22 ifcfg.log</span><br><span class="line">-rw------- 1 root root 1711915 Jun  3 14:22 journal.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-n7L3eP.log</span><br><span class="line">-rw------- 1 root root       0 Jun  3 14:22 ks-script-v6NMcO.log</span><br><span class="line">-rw------- 1 root root  312693 Jun  3 14:22 packaging.log</span><br><span class="line">-rw------- 1 root root   29448 Jun  3 14:22 program.log</span><br><span class="line">-rw------- 1 root root   78942 Jun  3 14:22 storage.log</span><br><span class="line">-rw------- 1 root root  320770 Jun  3 14:22 syslog</span><br></pre></td></tr></table></figure><p><strong>6）源地址带与不带斜线（/）的区别的例子</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建实验环境</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># mkdir -p /data1/&#123;test1,test2&#125;/data2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果源目录的末尾有斜线，就会复制目录内的内容，而不是复制目录本身</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av /data1/ /data2/  # 本地同步</span></span><br><span class="line">sending incremental file list</span><br><span class="line">created directory /data2</span><br><span class="line">./</span><br><span class="line">test1/</span><br><span class="line">test1/data2/</span><br><span class="line">test2/</span><br><span class="line">test2/data2/</span><br><span class="line"></span><br><span class="line">sent 149 bytes  received 64 bytes  426.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果源目录没有斜线，则会复制目录本身及目录下的内容</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av /data1 /data2 # 本地同步</span></span><br><span class="line">sending incremental file list</span><br><span class="line">data1/</span><br><span class="line">data1/test1/</span><br><span class="line">data1/test1/data2/</span><br><span class="line">data1/test2/</span><br><span class="line">data1/test2/data2/</span><br><span class="line"></span><br><span class="line">sent 155 bytes  received 36 bytes  382.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看/data2目录下文件信息</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ls -l /data2</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 4 root root 32 Apr 28 01:27 data1</span><br><span class="line">drwxr-xr-x 3 root root 19 Apr 28 01:27 test1</span><br><span class="line">drwxr-xr-x 3 root root 19 Apr 28 01:27 test2</span><br></pre></td></tr></table></figure><p><strong>源路径如果是一个目录的话，带上尾随斜线和不带尾随斜线是不一样的，不带尾随斜线表示的是整个目录包括目录本身，带上尾随斜线表示的是目录中的文件，不包括目录本身。</strong></p><p><strong>7）删除文件的特殊例子</strong></p><p>当一个目录下有几十万或十几万个文件，使用rsync的–deleye选项可以很快进行删除。这里注意一点，是删除目录下的所有文件，而不是目录本身。</p><p>选项–delete使tmp目录内容和空目录保持一致，不同的文件及目录将会被删除，即null里有什么内容，tmp里就有什么内容，null里没有的，而tmp里有的就必须要删除，因为null目录为空，因此此命令会删除/tmp目录中的所有内容。<strong>使用”–delete”选项后，接收端的rsync会先删除目标目录下已经存在，但源端目录不存在的文件。也就是”多则删之，少则补之”。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看tmp目录下文件信息</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ls -l /tmp</span></span><br><span class="line">total 4</span><br><span class="line">drwxr-xr-x 3 root root  23 Apr 28 00:22 home</span><br><span class="line">-rw-r--r-- 1 root root 187 Apr 22 22:56 hosts</span><br><span class="line">drwx------ 3 root root  17 Apr 27 21:43 systemd-private-5b1137097c084a899dfd1557504d079d-chronyd.service-6Ra806</span><br><span class="line">。。。</span><br><span class="line">drwx------ 2 root root   6 Apr 23 18:24 vmware-root_9660-3101179142</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个空目录null</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># mkdir /null</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除tmp目录下所有文件和子目录</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av --delete /null/ /tmp/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting vmware-root_9660-3101179142/</span><br><span class="line">deleting vmware-root_9609-4121731445/</span><br><span class="line">deleting vmware-root_9604-3101310240/</span><br><span class="line">deleting vmware-root_9573-4146439782/</span><br><span class="line">deleting vmware-root_9458-2857896559/</span><br><span class="line">deleting vmware-root_9456-2866351085/</span><br><span class="line">。。。</span><br><span class="line">deleting hosts</span><br><span class="line">./</span><br><span class="line"></span><br><span class="line">sent 47 bytes  received 3,911 bytes  7,916.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次查看/tmp目录下文件和子目录信息</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># ls -l /tmp</span></span><br><span class="line">total 0</span><br></pre></td></tr></table></figure><p><strong>8）”–existing”和”–ignore-existing”使用示例</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建测试环境，测试环境的结构如下：</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree /tmp/&#123;a,b&#125;</span></span><br><span class="line">/tmp/a</span><br><span class="line">├── bashrc</span><br><span class="line">├── c</span><br><span class="line">│   └── find</span><br><span class="line">├── fstab</span><br><span class="line">├── profile</span><br><span class="line">└── rc.local</span><br><span class="line">/tmp/b</span><br><span class="line">├── crontab</span><br><span class="line">├── fstab</span><br><span class="line">├── profile</span><br><span class="line">└── rc.local</span><br><span class="line">1 directory, 9 files</span><br></pre></td></tr></table></figure><p><strong>“–existing”是只更新目标端已存在的文件。</strong>目前/tmp/{a,b}目录中内容如上，bashrc在a目录中，crontab在b目录中，且a目录中多了一个c子目录。如下结果只有3个目标上已存在的文件被更新了，由于目标上没有c目录，所以c目录中的文件也没有进行传输。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --existing /tmp/a/ /tmp/b </span></span><br><span class="line">sending incremental file list</span><br><span class="line">fstab</span><br><span class="line">profile</span><br><span class="line">rc.local</span><br><span class="line">sent 2972 bytes  received 70 bytes  6084.00 bytes/sec</span><br><span class="line">total size is 204755  speedup is 67.31</span><br></pre></td></tr></table></figure><p>而<strong>“–ignore-existing”是更新目标端不存在的文件。</strong>如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --ignore-existing /tmp/a/ /tmp/b</span></span><br><span class="line">sending incremental file list</span><br><span class="line">bashrc</span><br><span class="line">c/</span><br><span class="line">c/find</span><br><span class="line">sent 231 bytes  received 62 bytes  586.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br></pre></td></tr></table></figure><p><strong>“–existing”和”–ignore-existing”结合使用时，有个特殊功效，当它们结合”–delete”使用的时候，文件不会传输，但会删除receiver端额外多出的文件。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建实验环境，如下</span></span><br><span class="line"></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># tree &#123;a,b&#125;</span></span><br><span class="line">a</span><br><span class="line">├── stu01</span><br><span class="line">├── stu02</span><br><span class="line">├── stu03</span><br><span class="line">└── stu04</span><br><span class="line">b</span><br><span class="line">└── a.log</span><br><span class="line">0 directories, 5 files</span><br><span class="line"><span class="comment"># 使用-n选项测试传输，并没有实际效果</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -nrv --delete a/ b/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting a.log</span><br><span class="line">stu01</span><br><span class="line">stu02</span><br><span class="line">stu03</span><br><span class="line">stu04</span><br><span class="line"></span><br><span class="line">sent 104 bytes  received 33 bytes  274.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00 (DRY RUN)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --existing &amp;&amp; --ignore-existing &amp;&amp; --delete结合使用，是删除接收端（目的端）多余的文件，且不会传输文件</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -nrv --existing --ignore-existing --delete a/ b/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting a.log</span><br><span class="line"></span><br><span class="line">sent 92 bytes  received 21 bytes  226.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00 (DRY RUN)</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -nrv --existing --ignore-existing --delete b/ a/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">deleting stu04</span><br><span class="line">deleting stu03</span><br><span class="line">deleting stu02</span><br><span class="line">deleting stu01</span><br><span class="line"></span><br><span class="line">sent 58 bytes  received 48 bytes  212.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00 (DRY RUN)</span><br></pre></td></tr></table></figure><p>实际上，<strong>“–existing”和”–ingore-existing”是传输规则，只会影响receiver要求让sender传输的文件列表，属于传输模式。而receiver决定哪些文件在传输之前如何处理属于同步模式，</strong>所以各种同步规则，比如：”–delete”等操作都不会被这两个选项影响。</p><p><strong>9）”–remove-source-files”删除源端文件</strong></p><p><strong>使用该选项后，源端已经更新成功的文件都会被删除，源端所有未传输或未传输成功的文件都不会被移除。</strong>未传输成功的原因有多种，如exclude排除了，”quick check”未选则该文件，传输中断等等。总之，显示在”rsync -v”被传输列表中的文件都会被移除。如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --remove-source-files a/ b/ .</span></span><br><span class="line">sending incremental file list</span><br><span class="line">a.log</span><br><span class="line">stu01</span><br><span class="line">stu02</span><br><span class="line">stu03</span><br><span class="line">stu04</span><br><span class="line">sent 331 bytes  received 151 bytes  964.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls -l &#123;a/,b/&#125;</span></span><br><span class="line">a/:</span><br><span class="line">total 0</span><br><span class="line">b/:</span><br><span class="line">total 0</span><br></pre></td></tr></table></figure><p><strong>上述显示出来的文件在源端全部被删除。</strong></p><p><strong>10）”–exclude”排除规则</strong></p><p>使用”–exclude”选项指定排除规则，排除那些不需要传输的文件。如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -r -v --exclude=/tmp/a/* /tmp/a/ /tmp/b/* a/</span></span><br><span class="line">sending incremental file list</span><br><span class="line">c/</span><br><span class="line">sent 81 bytes  received 16 bytes  194.00 bytes/sec</span><br><span class="line">total size is 0  speedup is 0.00</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls /tmp/b</span></span><br><span class="line">c</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># ls /tmp/a</span></span><br><span class="line">c</span><br></pre></td></tr></table></figure><p>上面的代码意思是不传送/tmp/a下的所有文件，但是传送/tmp/b下的所有文件，因此在传送列表中只有一个c/目录准备传送。</p><p><strong>需要注意的是：一个”–exclude”只能指定一条规则，要指定多条排除规则，需要使用多个”–exclude”选项，或者将排除规则写入到文件中，然后使用”–exclude-from”选项读取该规则文件。</strong></p><p><strong>另外，除了”–exclude”排除规则，还有”–include”包含规则，顾名思义，它就是筛选出要进行传输的文件，所以include规则也称为传输规则。它的使用方法和”–exclude”一样。如果一个文件即能匹配排除规则，又能匹配包含规则，则先匹配到的立即生效，生效后就不再进行任何匹配。</strong></p><p>关于规则，最重要的一点是它的作用时间。<strong>当发送端敲出rsync命令后，rsync将立即扫描命令行中给定的文件和目录(扫描过程中还会按照目录进行排序，将同一个目录的文件放在相邻的位置)，这称为拷贝树(copy tree)，扫描完成后将待传输的文件或目录记录到文件列表中，然后将文件列表传输给接收端。</strong>而<strong>筛选规则的作用时刻是在扫描拷贝树时，会根据规则来匹配并决定文件是否记录到文件列表中(严格地说是所有文件都会记录到文件列表中的，只不过排除的文件会被标记为hide隐藏起来)，只有记录到了文件列表中的文件或目录才是真正需要传输的内容。</strong>换句话说，筛选规则的生效时间在rsync整个同步过程中是非常靠前的，它会影响很多选项的操作对象，最典型的如”–delete”。</p><p><strong>实际上，排除规则和包含规则都只是”–filter”筛选规则的两种特殊规则。</strong>“–filter”比较复杂，它有自己的规则语法和匹配模式，由于篇幅有限，以及考虑到本文实际应用定位，”–filter”规则不便在此多做解释，仅简单说明下规则类。</p><p><strong>以下是rsync中的规则种类，不解之处请结合下文的”–delete”分析：</strong></p><p><strong>1）exclude规则：</strong>即排除规则，只作用于发送端，被排除的文件不会进入文件列表(实际上是加上隐藏规则进行隐藏)。</p><p><strong>2）include规则：</strong>即包含规则，也称为传输规则，只作用于发送端，被包含的文件将明确记录到文件列表中。</p><p><strong>3）hide规则：</strong>即隐藏规则，只作用于发送端，隐藏后的文件对于接收端来说是看不见的，也就是说接收端会认为它不存在于源端。</p><p><strong>4）show规则：</strong>即显示规则，只作用于发送端，是隐藏规则的反向规则。</p><p><strong>5）protect规则：</strong>即保护规则，该规则只作用于接收端，被保护的文件不会被删除掉。</p><p><strong>6）risk规则：</strong>即取消保护规则。是protect的反向规则。</p><p><strong>除此之外，还有一种规则是”clear规则”，作用是删除include/exclude规则列表。</strong></p><p><strong>rsync在发送端将文件列表发送给接收端后，接收端的generato进程会扫描每个文件列表中的信息，然后对列表中的每个信息条目都计算数据块校验码，最后将数据库校验码发给发送端，发送端通过校验码来匹配哪些数据块是需要传输的，这样就实现了增量传输的功能：只传输改变的部分，不会传输整个文件。而delete删除的时间点是generator进程处理每个文件列表时、生成校验码之前进行的，</strong>先将目标上存在但源上不存在的多余文件删除，这样就无需为多余的文件生成校验码。</p><p>但是，<strong>如果exclude和delete规则同时使用时，却不会删除被exclude排除的文件</strong>。这是因为，<strong>delete动作是比”–exclude”规则更晚执行的，被”–exclude”规则排除的文件不会进入文件列表中，在执行了delete时会认为该文件不存在于源端，从而会导致目标端将这些文件删除。但这是想当然的</strong>，尽管理论上确实是这样的，但是rsync为了防止众多误删除情况，提供了两种规则：<strong>保护规则(protect)和取消保护规则(risk)。</strong>默认情况下，<strong>“–delete”和”–exclude”一起使用时，虽然发送端的exclude规则将文件标记为隐藏，使得接收端认为这些被排除文件在源端不存在，但rsync会将这些隐藏文件标记为保护文件，使得它们不受delete行为的影响，这样delete就删除不了这些被排除的文件。</strong>如果还是想要强行删除被exclude排除的文件，可以使用”–delete-excluded”选项强制取消保护，这样即使被排除的文件也会被删除。</p><p><strong>11）指定ssh连接参数，如端口、连接的用户、ssh选项等</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync -e "ssh -p 22 -o StrictHostKeyChecking=no" /etc/fstab 192.168.101.252:/tmp</span></span><br><span class="line">Warning: Permanently added <span class="string">'192.168.101.252'</span> (ECDSA) to the list of known hosts.</span><br><span class="line">root@192.168.101.252<span class="string">'s password: </span></span><br><span class="line"><span class="string">Permission denied, please try again.</span></span><br><span class="line"><span class="string">root@192.168.101.252'</span>s password:</span><br></pre></td></tr></table></figure><p>在要求保障数据安全的场景下，可以使用-e选项借助SSH隧道进行加密传输数据，-p是SSH命令的选项，指定SSH传输的端口号为22，-o是SSH的认证方式，上述示例表示不通过秘钥认证，可见直接指定ssh参数是生效的。</p><p><strong>12）拉取或推送文件及目录（类似scp命令）</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Server02上拉取/home/目录到本地/tmp目录下</span></span><br><span class="line"></span><br><span class="line">[root@C7-Server01 ~]<span class="comment"># rsync -av 192.168.101.82:/home/ /tmp/</span></span><br><span class="line">root@192.168.101.82<span class="string">'s password: </span></span><br><span class="line"><span class="string">receiving incremental file list</span></span><br><span class="line"><span class="string">./</span></span><br><span class="line"><span class="string">kkutysllb/</span></span><br><span class="line"><span class="string">kkutysllb/.bash_history</span></span><br><span class="line"><span class="string">kkutysllb/.bash_logout</span></span><br><span class="line"><span class="string">kkutysllb/.bash_profile</span></span><br><span class="line"><span class="string">kkutysllb/.bashrc</span></span><br><span class="line"><span class="string">kkutysllb/202012312234.55</span></span><br><span class="line"><span class="string">kkutysllb/data001</span></span><br><span class="line"><span class="string">kkutysllb/data002</span></span><br><span class="line"><span class="string">kkutysllb/data003</span></span><br><span class="line"><span class="string">kkutysllb/data004</span></span><br><span class="line"><span class="string">。。。</span></span><br><span class="line"><span class="string">sent 767 bytes  received 4,507 bytes  958.91 bytes/sec</span></span><br><span class="line"><span class="string">total size is 1,440  speedup is 0.27</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 查看本地/tmp目录下内容</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[root@C7-Server01 ~]# ls -l /tmp</span></span><br><span class="line"><span class="string">total 4</span></span><br><span class="line"><span class="string">drwx------ 7 root root 4096 Apr 28 00:00 kkutysllb</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 推送本地/etc/udev/目录下所有内容到Server02的/home/kkutysllb/目录下</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[root@C7-Server01 ~]# rsync -av /etc/udev 192.168.101.82:/home/kkutysllb/</span></span><br><span class="line"><span class="string">root@192.168.101.82'</span>s password: </span><br><span class="line">sending incremental file list</span><br><span class="line">udev/</span><br><span class="line">udev/hwdb.bin</span><br><span class="line">udev/udev.conf</span><br><span class="line">udev/rules.d/</span><br><span class="line"></span><br><span class="line">sent 7,944,769 bytes  received 66 bytes  1,765,518.89 bytes/sec</span><br><span class="line">total size is 7,942,619  speedup is 1.00</span><br></pre></td></tr></table></figure><p>与scp命令复制的结果进行对比可以发现，使用rsync复制时，重复执行复制直至目录下文件相同就不再进行复制了。</p><h2 id="rsync使用技巧"><a href="#rsync使用技巧" class="headerlink" title="rsync使用技巧"></a><strong>rsync使用技巧</strong></h2><p><strong>1）实际运维场景常用选项-avz，相当于-vzrtopg（这是网上文档常见的选项），但是此处建议大家使用-avz选项，更简单明了。如果在脚本中使用也可以省略-v选项。</strong></p><p><strong>2）关于z压缩选项的使用建议，如果为内网环境，且没有其他业务占用带宽，可以不使用z选项。不压缩传输，几乎可以满带宽传输（千M网络），压缩传输则网络发送速度就会骤降，压缩的速率赶不上传输的速度。</strong></p><p><strong>3）选项n是一个提高安全性的选项，它可以结合-v选项输出模拟的传输过程，如果没有错误，则可以去除n选项真正的传输文件。</strong></p><h2 id="rsync-daemon模式"><a href="#rsync-daemon模式" class="headerlink" title="rsync daemon模式"></a><strong>rsync daemon模式</strong></h2><p>既然rsync通过远程shell就能实现两端主机上的文件同步，还要使用rsync的服务干啥？试想下，你有的机器上有一堆文件需要时不时地同步到众多机器上去，比如目录a、b、c是专门传输到web服务器上的，d/e、f、g/h是专门传输到ftp服务器上的，还要对这些目录中的某些文件进行排除，如果通过远程shell连接方式，无论是使用排除规则还是包含规则，甚至一条一条rsync命令地传输，这都没问题，但太过繁琐且每次都要输入同样的命令显得太死板。使用rsync daemon就可以解决这种死板问题。而且，<strong>rsync daemon是向外提供服务的，这样只要告诉了别人rsync的url路径，外人就能向ftp服务器一样获取文件列表并进行选择性地下载，</strong>所以，你所制定的列表，所有人都可以获取到并使用。</p><p>在Linux内核官网<a href="http://www.kernel.org上，就提供rsync的下载方式，官方给出的地址是rsync://rsync.kernel.org/pub，可以根据这个地址找出你想下载的内核版本。例如：要找出linux-3.0.15版本的内核相关文件，可以在客户端执行如下命令：" target="_blank" rel="noopener">www.kernel.org上，就提供rsync的下载方式，官方给出的地址是rsync://rsync.kernel.org/pub，可以根据这个地址找出你想下载的内核版本。例如：要找出linux-3.0.15版本的内核相关文件，可以在客户端执行如下命令：</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --no-motd -r -v -f "+ */" -f "+ linux-3.0.15*" -f "- *" -m rsync://rsync.kernel.org/pub/</span></span><br><span class="line">receiving file list ... <span class="keyword">done</span></span><br><span class="line">drwxr-xr-x          4,096 2019/05/04 03:15:23 .</span><br><span class="line">drwxr-xr-x          4,096 2014/11/12 05:50:10 linux</span><br><span class="line">drwxr-xr-x          4,096 2019/03/12 23:06:47 linux/kernel</span><br><span class="line">drwxr-xr-x        258,048 2019/05/23 13:52:08 linux/kernel/v3.x</span><br><span class="line">-rw-r--r--     76,803,806 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.bz2</span><br><span class="line">-rw-r--r--     96,726,195 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.gz</span><br><span class="line">-rw-r--r--            836 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.sign</span><br><span class="line">-rw-r--r--     63,812,604 2012/01/04 03:00:31 linux/kernel/v3.x/linux-3.0.15.tar.xz</span><br><span class="line">sent 58 bytes  received 82,915 bytes  2,720.43 bytes/sec</span><br><span class="line">total size is 237,343,441  speedup is 2,860.49</span><br></pre></td></tr></table></figure><p>我们无需关注上面的规则代表什么意思，需要关注的重点是<strong>通过rsync可以向外提供文件列表并提供相应的下载。</strong>同样，我们还可以根据路径，将rsync daemon上的文件拉取到本地实现下载的功能。</p><p><img src="https://i.loli.net/2019/06/24/5d10a9d9a1b2597343.jpg" alt></p><p>rsync daemon是”rsync –daemon”或再加上其他一些选项启动的，它会读取配置文件，<strong>默认是/etc/rsyncd.conf，并默认监听在873端口上</strong>，当外界有客户端对此端口发起连接请求，通过这个网络套接字就可以完成连接，以后与该客户端通信的所有数据都通过该网络套接字传输。</p><p><strong>rsync daemon的通信方式和传输通道与远程shell不同。远程shell连接的两端是通过管道完成通信和数据传输的，当连接到目标端时，将在目标端上根据远程shell进程fork出rsync进程使其成为rsync server。而rsync daemon是事先在server端上运行好的rsync后台进程(根据启动选项，也可以设置为非后台进程)，它监听套接字等待client端的连接，连接建立后所有通信方式都是通过套接字完成的。</strong></p><p><strong>rsync中的server的概念从来就不代表是rsync daemon，server在rsync中只是一种通用称呼，只要不是发起rsync请求的client端，就是server端，可以认为rsync daemon是一种特殊的server，其实daemon更准确的称呼应该是service。</strong></p><p><strong>以下是rsync client连接rsync daemon时的命令语法：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST]</span><br><span class="line">rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST]</span><br><span class="line">Push: rsync [OPTION...] SRC... [USER@]HOST::DEST</span><br><span class="line">rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST</span><br></pre></td></tr></table></figure><p><strong>连接命令有两种类型，一种是rsync风格使用双冒号的”rsync user@host::src dest”，一种是url风格的”rsync://user@host:port/src dest”。对于rsync风格的连接命令，如果想要指定端口号，则需要使用选项”–port”。</strong></p><p>上述语法中，其中daemon端的路径，如user@host::src，它的src代表的是模块名，而不是真的文件系统中的路径。关于rsync中的模块就是其配置文件中定义的各个功能。</p><h2 id="daemon配置文件rsyncd-conf"><a href="#daemon配置文件rsyncd-conf" class="headerlink" title="daemon配置文件rsyncd.conf"></a><strong>daemon配置文件rsyncd.conf</strong></h2><p>默认”rsync –daemon”读取的配置文件为/etc/rsyncd.conf，有些版本的系统上可能该文件默认不存在，需要手动创建。rsyncd.conf默认配置内容如下：</p><p><img src="https://i.loli.net/2019/06/24/5d10aa16df0cd37237.jpg" alt></p><p>在上述示例配置文件中，先定义了一些全局选项，然后定义了[ftp1]，这个用中括号包围的”[ftp1]”就是rsync中所谓的模块，<strong>ftp1为模块ID，必须保证唯一，每个模块中必须定义一项”path”，path定义的是该模块代表的路径</strong>，此示例文件中，如果想请求ftp1模块，则在客户端使用”rsync user@host::ftp1”，这表示访问user@host上的/home/ftp目录，如果要访问/home/ftp目录下的子目录www，则”rsync user@host::ftp1/www”。</p><p>以下是我自用的配置项，也算是一个配置示例：</p><p>[</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">root@c7-server01 ~]<span class="comment"># cat /etc/rsyncd.conf </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局配置参数</span></span><br><span class="line"></span><br><span class="line"> port=888    <span class="comment"># 指定rsync端口。默认873</span></span><br><span class="line"> uid = 0 <span class="comment"># rsync服务的运行用户，默认是nobody，文件传输成功后属主将是这个uid</span></span><br><span class="line"> gid = 0 <span class="comment"># rsync服务的运行组，默认是nobody，文件传输成功后属组将是这个gid</span></span><br><span class="line"> use chroot = no <span class="comment"># rsync daemon在传输前是否切换到指定的path目录下，并将其监禁在内</span></span><br><span class="line"> max connections = 200 <span class="comment"># 指定最大连接数量，0表示没有限制</span></span><br><span class="line"> timeout = 300         <span class="comment"># 确保rsync服务器不会永远等待一个崩溃的客户端，0表示永远等待</span></span><br><span class="line"> motd file = /var/rsyncd/rsync.motd   <span class="comment"># 客户端连接过来显示的消息</span></span><br><span class="line"> pid file = /var/run/rsyncd.pid       <span class="comment"># 指定rsync daemon的pid文件</span></span><br><span class="line"> lock file = /var/run/rsync.lock      <span class="comment"># 指定锁文件</span></span><br><span class="line"> <span class="built_in">log</span> file = /var/<span class="built_in">log</span>/rsyncd.log       <span class="comment"># 指定rsync的日志文件，而不把日志发送给syslog</span></span><br><span class="line"> dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2  <span class="comment"># 指定哪些文件不用进行压缩传输</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定模块，并设定模块配置参数，可以创建多个模块</span></span><br><span class="line"></span><br><span class="line">[kksql]        <span class="comment"># 模块ID</span></span><br><span class="line">path = /kksql/ <span class="comment"># 指定该模块的路径，该参数必须指定。启动rsync服务前该目录必须存在。rsync请求访问模块本质就是访问该路径。</span></span><br><span class="line">ignore errors      <span class="comment"># 忽略某些IO错误信息</span></span><br><span class="line"><span class="built_in">read</span> only = <span class="literal">true</span>  <span class="comment"># 指定该模块是否可读写，即能否上传文件，false表示可读写，true表示可读不可写。所有模块默认不可上传</span></span><br><span class="line">write only = <span class="literal">false</span> <span class="comment"># 指定该模式是否支持下载，设置为true表示客户端不能下载。所有模块默认可下载</span></span><br><span class="line">list = <span class="literal">false</span>       <span class="comment"># 客户端请求显示模块列表时，该模块是否显示出来，设置为false则该模块为隐藏模块。默认true</span></span><br><span class="line">hosts allow = 10.0.5.0/24 <span class="comment"># 指定允许连接到该模块的机器，多个ip用空格隔开或者设置区间</span></span><br><span class="line">hosts deny = 0.0.0.0/32   <span class="comment"># 指定不允许连接到该模块的机器</span></span><br><span class="line">auth users = rsync_backup <span class="comment"># 指定连接到该模块的用户列表，只有列表里的用户才能连接到模块，用户名和对应密码保存在secrets file中，这里使用的不是系统用户，而是虚拟用户。不设置时，默认所有用户都能连接，但使用的是匿名连接</span></span><br><span class="line">secrets file = /etc/rsyncd.passwd <span class="comment"># 保存auth users用户列表的用户名和密码，每行包含一个username:passwd。由于"strict modes"默认为true，所以此文件要求非rsync daemon用户不可读写。只有启用了auth users该选项才有效。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">[kkweb]    <span class="comment"># 以下定义的是第二个模块</span></span><br><span class="line">path=/kkweb/</span><br><span class="line"><span class="built_in">read</span> only = <span class="literal">false</span></span><br><span class="line">ignore errors</span><br><span class="line">comment = anyone can access</span><br></pre></td></tr></table></figure><p>1）从客户端推到服务端时，文件的属主和属组是配置文件中指定的uid和gid。但是客户端从服务端拉的时候，文件的属主和属组是客户端正在操作rsync的用户身份，因为执行rsync程序的用户为当前用户。</p><p>2）auth users和secrets file这两行不是一定需要的，省略它们时将默认使用匿名连接。但是如果使用了它们，则secrets file的权限必须是600。客户端的密码文件也必须是600。</p><p>3）关于secrets file的权限，实际上并非一定是600，只要满足除了运行rsync daemon的用户可读即可。是否检查权限的设定是通过选项strict mode设置的，如果设置为false，则无需关注文件的权限。但默认是yes，即需要设置权限。</p><p>配置完后，再就是提供模块相关目录、身份验证文件等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># useradd -r -s /sbin/nologin rsync</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># mkdir /&#123;kksql,kkweb&#125;</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># chown -R rsync.rsync /&#123;kksql,kkweb&#125;</span></span><br></pre></td></tr></table></figure><p>提供模块kksql的身份验证文件，由于rsync daemon是以root身份运行的，所以要求身份验证文件对非root用户不可读写，所以设置为600权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># echo "rsync_backup:Oms_2600" &gt;&gt; /etc/rsyncd.passwd</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># chmod 600 /etc/rsyncd.passwd</span></span><br></pre></td></tr></table></figure><p>然后启动rsync daemon，启动方式很简单。如果是CentOS 7，则自带启动脚本：systemctl stard rsyncd。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@c7-server01 ~]<span class="comment"># rsync --daemon</span></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">[root@c7-server01 ~]<span class="comment"># systemctl enable rsyncd &amp;&amp; systemctl start rsyncd</span></span><br></pre></td></tr></table></figure><p>启动好rysnc daemon后，它就监听在指定的端口上，等待客户端的连接。由于上述示例中的模块kksql配置了身份验证功能，所以客户端连接时会询问密码。如果不想手动输入密码，则可以使用”–password-file”选项提供密码文件，密码文件中只有第一行才是传递的密码，其余所有的行都会被自动忽略。</p><p>在客户端上访问daemon上各模块的文件示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rsync模式</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only --port 888 root@192.168.101.11::kkweb/</span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only --port 888 rsync_bakup@192.168.101.11::kkweb/</span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># url模式</span></span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only rsync://root@192.168.101.11:888/kkweb/ </span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br><span class="line">[root@c7-test01 ~]<span class="comment"># rsync --list-only rsync://rsync@192.168.101.11:888/kkweb/ </span></span><br><span class="line">drwxr-xr-x          4,096 2019/06/03 17:51:49 .</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu01.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu02.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu03.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu04.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu05.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu06.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu07.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu08.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu09.img</span><br><span class="line">-rw-r--r--              0 2019/06/03 17:51:49 stu10.img</span><br></pre></td></tr></table></figure><p>晕吧？下一篇我们介绍sersync，继续晕。。。嘿嘿</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;rsync是可以实现增量备份的工具。配合任务计划，rsync能实现定时或间隔同步，配合inotify或sersync，可以实现触发式的实时同步。事实上，rsync有一套自己的算法，其算法原理以及rsync对算法实现的机制可能比想象中要复杂一些。平时使用rsync实现简单的备份、同步等功能足以，没有多大必要去深究这些原理性的内容。如果你对这些原理感兴趣，可以通过rsync命令的man文档、并且借助”-vvvv”分析rsync执行过程，结合rsync的算法原理去深入理解。本篇文章由于篇幅有限，只是介绍rsync的使用方法和它常用的功能，以及rsync和sersync如何配合打造文件实时同步架构。
    
    </summary>
    
      <category term="Linux常用运维工具" scheme="https://kkutysllb.cn/categories/Linux%E5%B8%B8%E7%94%A8%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="https://kkutysllb.cn/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-21-服务器外部交换网络虚拟化</title>
    <link href="https://kkutysllb.cn/2019/06/22/2019-06-21-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A4%96%E9%83%A8%E4%BA%A4%E6%8D%A2%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
    <id>https://kkutysllb.cn/2019/06/22/2019-06-21-服务器外部交换网络虚拟化/</id>
    <published>2019-06-22T03:54:39.000Z</published>
    <updated>2019-06-22T04:49:18.065Z</updated>
    
    <content type="html"><![CDATA[<p>数据中心服务器外部交换网络的虚拟化主要包括：<strong>数据中心内部二、三层交换设备虚拟化、大二层组网架构</strong>和<strong>软件定义网络SDN技术。</strong>其中，软件定义网络SDN技术在目前中国移动NovoNet战略中还处于现网试点阶段，预计2020年初会引入电信云数据中心。而数据中心内部二、三层交换设备虚拟化及大二层组网架构已完全成熟，并已在当前电信云数据中心中实际部署，本文也主要讲述这两种技术，其中大二层组网基石VxLAN需要大家重点掌握。<a id="more"></a></p><h2 id="数据中心内部二、三层交换设备虚拟化"><a href="#数据中心内部二、三层交换设备虚拟化" class="headerlink" title="数据中心内部二、三层交换设备虚拟化"></a><strong>数据中心内部二、三层交换设备虚拟化</strong></h2><p>现有数据中心内部的三层网络架构起源于园区网，传统数据中心。如下图所示，三层架构将网络分为<strong>接入(access)、汇聚(aggregation)</strong>和<strong>核心(core)</strong>三层。</p><p><img src="https://i.loli.net/2019/06/22/5d0da8867958c61533.jpg"></p><p><strong>接入层</strong>通常使用二层交换机，主要负责接入服务器、标记VLAN以及转发二层的流量。<strong>汇聚层</strong>通常使用三层交换机，主要负责同一POD（所谓POD就是划分范围就是汇聚交换机管理的范围，对应到上图就是底层的三个iStack堆叠组范围）内的路由，并实现ACL等安全策略。<strong>核心层</strong>通常使用业务路由器，主要负责实现跨POD的路由，并提供Internet接入以及VPN等服务。</p><p>在三层的网络规划中，接入层和汇聚层间通常是二层网络，接入交换机双上联到汇聚交换机，并运行STP来消除环路，汇聚交换机作为网关终结掉二层，和核心路由器起IGP来学习路由，少数情况下会选择BGP。</p><p>众所周知，MAC自学习是以太网的根基，它以泛洪帧为探针同步转发表，实现简单，无需复杂控制。但是，如果网络中存在环路，泛洪会在极短的时间内使网络瘫痪。为此，引入STP用来破除转发环路，不过STP的引入却带来了更多问题，如收敛慢、链路利用率低、规模受限、难以定位故障等。虽然，业界为STP的优化费劲了心思，但打补丁的方式只能治标而不能治本。随着数据中心I/O密集型业务的大发展，STP成了网络最为明显的一块短板，处理掉STP自然也就成了数据中心网络架构演进打响的第一枪。</p><h2 id="物理设备的“多虚一”：VRRP、iStack和虚拟机框"><a href="#物理设备的“多虚一”：VRRP、iStack和虚拟机框" class="headerlink" title="物理设备的“多虚一”：VRRP、iStack和虚拟机框"></a><strong>物理设备的“多虚一”：VRRP、iStack和虚拟机框</strong></h2><p>物理设备的“多虚一”功能，能够将多台设备中的控制平面进行整合，形成一台统一的逻辑设备，这台逻辑设备不但具有统一的管理IP，而且在各种L2和L3协议中也将表现为一个整体。因此在完成整合后，STP所看到的拓扑自然就是无环的了，这就间接地规避了STP的种种问题。</p><p>目前，普遍采用VRRP、堆叠iStack和虚拟机框技术完成物理设备的“多虚一”功能。其中，虚拟机框技术和传统的堆叠技术一样，不同的物理设备分享同一个控制平面，实际上就相当于为物理网络设备做了个集群，也有选主和倒换的过程。相比之下，虚拟机框在组网上的限制较少，而且在可用性方面的设计普遍要好于堆叠，因此可以看作对于堆叠技术的升级，我们只要了解堆叠技术即可。而且，虚拟机框技术以Cisco的VSS、Juniper的Virtual Chassis以及H3C的IRF 2为代表，在中国移动NFV电信云数据中心内华为交换机堆叠技术与此等价。</p><h3 id="VRRP技术"><a href="#VRRP技术" class="headerlink" title="VRRP技术"></a><strong>VRRP技术</strong></h3><p><strong>定义：</strong>虚拟路由冗余协议VRRP，通过把几台设备联合组成一台虚拟的路由设备，将虚拟路由设备的IP地址作为用户的默认网关实现与外部网络通信。</p><p><img src="https://i.loli.net/2019/06/22/5d0da901a17cc99156.jpg"></p><p>如上图所示，VRRP将局域网内的一组路由器划分在一起，形成一个VRRP备份组，它在功能上相当于一台虚拟路由器，使用虚拟路由器号进行标识。VRRP设备有自己的虚拟IP地址和虚拟MAC地址，它的外在表现形式和实际的物理路由器完全一样。局域网内的主机将VRRP设备的IP地址设置为默认网关，通过VRRP设备与外部网络进行通信。</p><p>VRRP设备是工作在实际的物理三层设备之上的。它由多个实际的三层设备组成，包括一个Master设备和多个Backup设备。Master设备正常工作时，局域网内的主机通过Master设备与外界通信。当Master设备出现故障时，Backup设备中的一台设备将成为新的Master设备，接替转发报文的工作。</p><p><strong>VRRP的工作过程为：</strong></p><p>1）VRRP设备中的物理设备根据优先级选举出Master。Master设备通过发送免费ARP报文，将自己的虚拟MAC地址通知给与它连接的设备或者主机，从而承担报文转发任务；</p><p>2）Master设备周期性发送VRRP报文，以公布其配置信息（优先级等）和工作状况；</p><p>3）如果Master设备出现故障，VRRP设备中的Backup设备将根据优先级重新选举新的Master；</p><p>4）VRRP设备状态切换时，Master设备由一台设备切换为另外一台设备，新的Master设备只是简单地发送一个携带VRRP设备的MAC地址和虚拟IP地址信息的免费ARP报文，这样就可以更新与它连接的主机或设备中的ARP相关信息。网络中的主机感知不到Maste设备已经切换为另外一台设备。</p><p>5）Backup设备的优先级高于Master设备时，由Backup设备的工作方式（抢占方式和非抢占方式）决定是否重新选举Master。</p><p><strong>VRRP根据优先级来确定VRRP设备中每台物理设备的角色（Master设备或Backup设备）。优先级越高，则越有可能成为Master设备。</strong></p><p>1）初始创建的设备工作在Backup状态，通过VRRP报文的交互获知VRRP设备中其他成员的优先级：</p><ol><li>如果VRRP报文中Master设备的优先级高于自己的优先级，则路由器保持在Backup状态；</li><li>如果VRRP报文中Master设备的优先级低于自己的优先级，采用抢占工作方式的路由器将抢占成为Master状态，周期性地发送VRRP报文，采用非抢占工作方式的路由器仍保持Backup状态；</li><li>如果在一定时间内没有收到VRRP报文，则本设备切换为Master状态。</li></ol><p>2）VRRP优先级的取值范围为0到255（数值越大表明优先级越高），可配置的范围是1到254，优先级0为系统保留给路由器放弃Master位置时候使用，255则是系统保留给IP地址拥有者使用。当设备为IP地址拥有者时，其优先级始终为255。因此，当VRRP设备内存在IP地址拥有者时，只要其工作正常，则为Master设备。</p><p><strong>VRRP的的工作方式</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0da9511990669228.jpg"></p><p>主备备份方式表示业务仅由Master设备承担。当Master设备出现故障时，才会由选举出来的Backup设备接替它工作。</p><p>如上图左边所示，初始情况下，Device A是Master路由器并承担转发任务，Device B和Device C是Backup路由器且都处于就绪监听状态。如果Device A发生故障，则虚拟路由器内处于Backup状态的Device B和Device C路由器将根据优先级选出一个新的Master路由器，这个新Master路由器继续为网络内的主机转发数据。</p><p>负载分担方式是指多台设备同时承担业务，因此负载分担方式需要两个或者两个以上的VRRP设备，每个VRRP设备都包括一个Master设备和若干个Backup设备，各VRRP设备中的Master设备可以各不相同。通过在一个设备的一个接口上可以创建多个VRRP设备，使得该设备可以在一个VRRP设备中作为Master设备，同时在其他的VRRP设备中作为Backup设备。</p><p>在上图右边中，有三个VRRP设备存在：</p><ul><li>VRRP1：Device A作为Master设备，Device B和Device C作为Backup设备。</li><li>VRRP2：Device B作为Master设备，Device A和Device C作为Backup设备。</li><li>VRRP3：Device C作为Master设备，Device A和Device B作为Backup设备。</li></ul><p>为了实现业务流量在Device A、Device B和Device C之间进行负载分担，需要将局域网内的主机的默认网关分别设置为VRRP1、2和3。在配置优先级时，需要确保三个VRRP设备中各路由器的VRRP优先级形成一定的交叉，使得一台路由器尽可能不同时充当2个Master路由器。</p><h3 id="iStack交换机堆叠技术"><a href="#iStack交换机堆叠技术" class="headerlink" title="iStack交换机堆叠技术"></a><strong>iStack交换机堆叠技术</strong></h3><p><strong>堆叠iStack（Intelligent Stack），是指将多台支持堆叠特性的交换机设备组合在一起，从逻辑上组合成一台交换设备。</strong>如下图所示，SwitchA与SwitchB通过堆叠线缆连接后组成堆叠iStack，对于上游和下游设备来说，它们就相当于一台交换机Switch。通过交换机堆叠，可以实现网络高可靠性和网络大数据量转发，同时简化网络管理。</p><p><img src="https://i.loli.net/2019/06/22/5d0da999163d217174.jpg"></p><blockquote><p><strong>1）高可靠性。</strong>堆叠系统多台成员交换机之间<strong>冗余备份</strong>；堆叠支持<strong>跨设备的链路聚合</strong>功能，实现<strong>跨设备的链路冗余备份</strong>。</p><p><strong>2）强大的网络扩展能力。</strong>通过增加成员交换机，可以轻松的扩展堆叠系统的<strong>端口数</strong>、<strong>带宽</strong>和<strong>处理能力</strong>；同时<strong>支持成员交换机热插拔</strong>，<strong>新加入的成员交换机自动同步主交换机的配置文件和系统软件版本。</strong></p><p><strong>3）简化配置和管理。</strong>一方面，用户可以<strong>通过任何一台成员交换机登录</strong>堆叠系统，对堆叠系统所有成员交换机进行<strong>统一配置和管理</strong>；另一方面，堆叠形成后，<strong>不需要配置复杂的二层破环协议和三层保护倒换协议</strong>，简化了网络配置。</p></blockquote><p><strong>堆叠涉及以下几个基本概念：</strong></p><p><strong>1）角色：</strong>堆叠中所有的单台交换机都称为成员交换机，按照功能不同，可以分为三种角色：</p><ul><li><strong>主交换机</strong>：主交换机（Master）负责<strong>管理整个堆叠</strong>。堆叠中<strong>只有一台</strong>主交换机。</li><li><strong>备交换机：</strong>备交换机（Standby）是<strong>主交换机的备份交换机</strong>。当主交换机故障时，备交换机会接替原主交换机的所有业务。堆叠中<strong>只有一台</strong>备交换机。</li><li><strong>从交换机：</strong>从交换机（Slave）主要<strong>用于业务转发</strong>，从交换机数量越多，堆叠系统的转发能力越强。除主交换机和备交换机外，堆叠中其他所有的成员交换机都是从交换机。</li></ul><p><strong>2）堆叠ID：</strong>即<strong>成员交换机的槽位号</strong>（Slot ID），用来<strong>标识和管理成员交换机</strong>，堆叠中所有成员交换机的<strong>堆叠ID都是唯一</strong>的。</p><p><strong>3）堆叠优先级</strong>：是成员交换机的<strong>一个属性</strong>，<strong>主要用于角色选举过程中确定成员交换机的角色，优先级值越大表示优先级越高，优先级越高当选为主交换机的可能性越大。</strong></p><p><strong>堆叠建立的过程包括以下四个阶段：</strong></p><p><strong>Step1：物理连接。</strong>如下图所示，根据网络需求，选择适当的连接方式和连接拓扑，组建堆叠网络。根据连接介质的不同，堆叠可分为<strong>堆叠卡堆叠</strong>和<strong>业务口堆叠</strong>。</p><p><img src="https://i.loli.net/2019/06/22/5d0da9d67084f96031.jpg"></p><p>如上图所示，每种连接方式都可组成<strong>链形</strong>和<strong>环形</strong>两种连接拓扑。下表从可靠性、链路带宽利用率和组网布线是否方便的角度对两种连接拓扑进行对比。</p><table><thead><tr><th style="text-align:left"><strong>连接拓扑</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td style="text-align:left">链形连接</td><td>首尾不需要有物理连接，<strong>适合长距离堆叠</strong>。</td><td><strong>可靠性低</strong>：其中一条堆叠链路出现故障，就会造成堆叠分裂。堆叠链路<strong>带宽利用率低</strong>：整个堆叠系统只有一条路径。</td><td>堆叠成员交换机距离较远时，组建环形连接比较困难，可以使用链形连接。</td></tr><tr><td style="text-align:left">环形连接</td><td><strong>可靠性高</strong>：其中一条堆叠链路出现故障，环形拓扑变成链形拓扑，不影响堆叠系统正常工作。堆叠链路<strong>带宽利用率高</strong>：数据能够按照最短路径转发。</td><td>首尾需要有物理连接，<strong>不适合长距离堆叠</strong>。</td><td>堆叠成员交换机距离较近时，从可靠性和堆叠链路利用率上考虑，建议使用环形连接。</td></tr></tbody></table><p><strong>Step2：主交换机选举。</strong>确定出堆叠的连接方式和连接拓扑，完成成员交换机之间的物理连接之后，所有成员交换机上电。此时，堆叠系统开始进行主交换机的选举。在堆叠系统中每台成员交换机都具有一个确定的角色，其中，主交换机负责管理整个堆叠系统。<strong>主交换机选举规则如下（依次从第一条开始判断，直至找到最优的交换机才停止比较）：</strong></p><ol><li><strong>运行状态比较，已经运行的交换机优先处于启动状态的交换机竞争为主交换机。</strong>堆叠主交换机选举超时时间为20s，堆叠成员交换机上电或重启时，由于不同成员交换机所需的启动时间可能差异比较大，因此不是所有成员交换机都有机会参与主交换机的选举：<strong>启动时间与启动最快的成员交换机相比，相差超过20s的成员交换机没有机会参与主交换机的选举，只能被动加入堆叠成为非主交换机，加入过程可参见堆叠成员加入与退出。因此，如果希望指定某一成员交换机成为主交换机，则可以先为其上电，待其启动完成后再给其他成员交换机上电。</strong></li><li><strong>堆叠优先级高的交换机优先竞争为主交换机。</strong></li><li><strong>叠优先级相同时，MAC地址小的交换机优先竞争为主交换机。</strong></li></ol><p><strong>Step3：拓扑收集和备交换机选举。</strong>主交换机选举完成后，会收集所有成员交换机的拓扑信息，根据拓扑信息计算出堆叠转发表项和破环点信息下发给堆叠中的所有成员交换机，并向所有成员交换机分配堆叠ID。之后进行备交换机的选举，作为主交换机的备份交换机。<strong>除主交换机外最先完成设备启动的交换机优先被选为备份交换机</strong>。<strong>当除主交换机外其它交换机同时完成启动时，备交换机的选举规则如下（依次从第一条开始判断，直至找到最优的交换机才停止比较）：</strong></p><ol><li><strong>堆叠优先级最高的设备成为备交换机。</strong></li><li><strong>堆叠优先级相同时，MAC地址最小的成为备交换机。</strong></li></ol><p>除主交换机和备交换机之外，剩下的其他成员交换机作为从交换机加入堆叠。</p><p><strong>Step4：稳定运行。</strong>角色选举、拓扑收集完成之后，所有成员交换机会自动同步主交换机的系统软件和配置文件。<strong>堆叠具有自动加载系统软件的功能，待组成堆叠的成员交换机不需要具有相同软件版本，只需要版本间兼容即可</strong>。当备交换机或从交换机与主交换机的软件版本不一致时，备交换机或从交换机会自动从主交换机下载系统软件，然后使用新系统软件重启，并重新加入堆叠。<strong>堆叠具有配置文件同步机制，</strong>备交换机或从交换机会将主交换机的配置文件同步到本设备并执行，以保证堆叠中的多台设备能够像一台设备一样在网络中工作，并且在主交换机出现故障之后，其余交换机仍能够正常执行各项功能。</p><p><strong>堆叠支持跨设备链路聚合技术，通过配置跨设备Eth-Trunk接口实现。</strong>用户可以将不同成员交换机上的物理以太网端口配置成一个聚合端口连接到上游或下游设备上，实现多台设备之间的链路聚合。当其中一条聚合链路故障或堆叠中某台成员交换机故障时，Eth-Trunk接口能够将流量重新分布到其他聚合链路上，实现了链路间和设备间的备份，保证了数据流量的可靠传输。</p><p><img src="https://i.loli.net/2019/06/22/5d0daa5681cf010026.jpg"></p><blockquote><p>如上图左边所示，流向网络核心的流量将均匀分布在聚合链路上，当某一条聚合链路失效时，Eth-Trunk接口将流量通过堆叠线缆重新分布到其他聚合链路上，实现了链路间的备份。</p><p>如上图右边所示，流向网络核心的流量将均匀分布在聚合链路上，当某台成员交换机故障时，Eth-Trunk接口将流量重新分布到其他聚合链路上，实现了设备间的备份。</p></blockquote><p>跨设备链路聚合实现了数据流量的可靠传输和堆叠成员交换机的相互备份。但是由于堆叠设备间堆叠线缆的带宽有限，跨设备转发流量增加了堆叠线缆的带宽承载压力，同时也降低了流量转发效率。<strong>为了提高转发效率，减少堆叠线缆上的转发流量，设备支持流量本地优先转发。</strong>即从本设备进入的流量，优先从本设备相应的接口转发出去；如果本设备相应的接口故障或者流量已经达到接口的线速，那么就从其它堆叠成员交换机的接口转发出去。</p><p><img src="https://i.loli.net/2019/06/22/5d0daa867bc0484968.jpg"></p><p>如上图所示，SwitchA与SwitchB组成堆叠，上下行都加入到Eth-Trunk。如果没有本地优先转发，则从SwitchA进入的流量，根据当前Eth-Trunk的负载分担方式，会有一部分经过堆叠线缆，从SwitchB的物理接口转发出去。设备支持本地优先转发之后，从SwitchA进入的流量，只会从SwitchA的接口转发，流量不经过堆叠线缆。</p><p><strong>设备堆叠ID缺省为0。</strong>堆叠时由堆叠主交换机对设备的堆叠ID进行管理，当堆叠系统有新成员加入时，如果新成员与已有成员堆叠ID冲突，则堆叠主交换机从0～最大的堆叠ID进行遍历，找到第一个空闲的ID分配给该新成员。新建堆叠或堆叠成员变化时，如果不在堆叠前手动指定各设备的堆叠ID，则由于启动顺序等原因，最终堆叠系统中各成员的堆叠ID是随机的。因此，在建立堆叠时，建议提前规划好设备的堆叠ID，或通过特定的操作顺序，使设备启动后的堆叠ID与规划的堆叠ID一致。<strong>修改堆叠ID设备需要重启。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0daabda7d4130313.jpg"></p><p>如上图所示，<strong>堆叠成员加入是指向已经稳定运行的堆叠系统添加一台新的交换机</strong>。堆叠成员加入分为<strong>新成员交换机带电加入和不带电加入</strong>，带电加入则需要采用堆叠合并的方式完成，此处堆叠成员加入是指不带电加入。新成员交换机加入堆叠时，<strong>建议采用不带电加入</strong>。</p><p>堆叠成员加入的过程如下：</p><p><strong>Step1：</strong>新加入的交换机连线上电启动后，进行角色选举，新加入的交换机会选举为从交换机，堆叠系统中原有主备从角色不变。</p><p><strong>Step2：</strong>角色选举结束后，主交换机更新堆叠拓扑信息，同步到其他成员交换机上，并向新加入的交换机分配堆叠ID（新加入的交换机没有配置堆叠ID或配置的堆叠ID与原堆叠系统的冲突时）。</p><p><strong>Step3：</strong>新加入的交换机更新堆叠ID，并同步主交换机的配置文件和系统软件，之后进入稳定运行状态。</p><p><strong>堆叠成员退出是指成员交换机从堆叠系统中离开</strong>。根据退出成员交换机角色的不同，对堆叠系统的影响也有所不同：当主交换机退出，备份交换机升级为主交换机，重新计算堆叠拓扑并同步到其他成员交换机，指定新的备交换机，之后进入稳定运行状态。当备交换机退出，主交换机重新指定备交换机，重新计算堆叠拓扑并同步到其他成员交换机，之后进入稳定运行状态。当从交换机退出，主交换机重新计算堆叠拓扑并同步到其他成员交换机，之后进入稳定运行状态。堆叠成员交换机退出的过程，主要就是拆除堆叠线缆和移除交换机的过程。</p><blockquote><p><strong>对于环形堆叠：</strong>成员交换机退出后，为保证网络的可靠性还需要把退出交换机连接的两个端口通过堆叠线缆进行连接。</p><p><strong>对于链形堆叠：</strong>拆除中间交换机会造成堆叠分裂。这时需要在拆除前进行业务分析，尽量减少对业务的影响。</p></blockquote><h3 id="高级STP欺骗：跨设备链路聚合M-LAG"><a href="#高级STP欺骗：跨设备链路聚合M-LAG" class="headerlink" title="高级STP欺骗：跨设备链路聚合M-LAG"></a><strong>高级STP欺骗：跨设备链路聚合M-LAG</strong></h3><p>STP会严重浪费链路资源，根源在于它会禁止冗余链路上的转发。如果通过一种办法来“欺骗”STP，让它认为物理拓扑中没有冗余链路，那么就可以解决上述问题。实际上，上面的VRRP、堆叠技术中就用到了跨设备链路聚合，不过在实现上过于复杂，因此又出现了一类技术，它们不再为控制平面做集群，只保留了跨设备链路聚合的能力，这类技术以M-LAG为代表。</p><p><strong>定义：M-LAG（Multichassis Link Aggregation Group）即跨设备链路聚合组，是一种实现跨设备链路聚合的机制，将一台设备与另外两台设备进行跨设备链路聚合，从而把链路可靠性从单板级提高到了设备级，组成双活系统。</strong>如下图左边所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0dab358e08a98162.jpg"></p><p>如上图右边所示，用户侧设备Switch（可以是交换机或主机）通过M-LAG机制与另外两台设备（SwitchA和SwitchB）进行跨设备链路聚合，共同组成一个双活系统。这样可以实现SwitchA和SwitchB共同进行流量的转发，保证网络的可靠性。</p><p>M-LAG作为一种跨设备链路聚合的技术，除了具备增加带宽、提高链路可靠性、负载分担的优势外，还具备以下优势：</p><ul><li><strong>更高的可靠性：</strong>把链路可靠性从单板级提高到了设备级。</li><li><strong>简化组网及配置：</strong>可以将M-LAG理解为一种横向虚拟化技术，将双归接入的两台设备在逻辑上虚拟成一台设备。M-LAG提供了一个没有环路的二层拓扑同时实现冗余备份，不再需要繁琐的生成树协议配置，极大的简化了组网及配置。</li><li><strong>独立升级：</strong>两台设备可以分别进行升级，保证有一台设备正常工作即可，对正在运行的业务几乎没有影响。</li></ul><p>下表对M-LAG涉及的相关概念做了个总结，如下：</p><table><thead><tr><th>概念</th><th>说明</th></tr></thead><tbody><tr><td>M-LAG主设备</td><td>部署M-LAG且状态为主的设备。</td></tr><tr><td>M-LAG备设备</td><td>部署M-LAG且状态为备的设备。<strong>说明：</strong>正常情况下，主设备和备设备同时进行业务流量的转发。</td></tr><tr><td>peer-link链路</td><td>peer-link链路是一条直连链路且必须做链路聚合，用于交换协商报文及传输部分流量。为了增加peer-link链路的可靠性，推荐采用多条链路做链路聚合。</td></tr><tr><td>peer-link接口</td><td>peer-link链路两端直连的接口均为peer-link接口。</td></tr><tr><td>M-LAG成员接口</td><td>M-LAG主备设备上连接用户侧主机（或交换设备）的Eth-Trunk接口。为了增加可靠性，推荐链路聚合配置为LACP模式。</td></tr></tbody></table><p><strong>基于M-LAG组成的双活系统提供了设备级的可靠性，如下图所示，M-LAG的建立过程有如下几个步骤：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0dab988404b57115.jpg"></p><p><strong>Step1：</strong>当M-LAG两台的设备完成配置后，两台的设备会通过peer-link链路定期发送M-LAG协商报文。当收到对端的M-LAG协商报文后，会判断M-LAG协商报文中的DFS Group编号是否和本端相同。如果两台的DFS Group编号相同，则这两台设备配对成功。</p><p><strong>Step2：</strong>配对成功后，两台设备会通过比较M-LAG协商报文中的DFS Group优先级确定出主备状态。以SwitchB为例，当SwitchB收到SwitchA发送的报文时，SwitchB会查看并记录对端信息，然后比较DFS Group的优先级，如果SwitchA的DFS Group优先级高于本端的DFS Group优先级，则确定SwitchA为M-LAG主设备，SwitchB为M-LAG从设备。如果SwitchA和SwitchB的DFS Group优先级相同，比较两台设备的MAC地址，确定MAC地址小的一端为M-LAG主设备。这里的主、备不影响正常的流量转发，只有在出现故障时才起作用。</p><p><strong>Step3：</strong>协商出主备后，两台设备之间会通过网络侧链路周期性地发送M-LAG心跳报文，也可以配置通过管理网络检测心跳，当两台设备均能够收到对端发送的报文时，双活系统即开始正常的工作。M-LAG心跳报文主要用于peer-link故障时的双主检测。</p><p><strong>Step4：</strong>正常工作后，两台设备之间会通过peer-link链路发送M-LAG同步报文实时同步对端的信息，M-LAG同步报文中包括MAC表项、ARP表项等，这样任意一台设备故障都不会影响流量的转发，保证正常的业务不会中断。</p><p>M-LAG双活系统建立成功后即进入正常的工作，M-LAG主备设备负载分担共同进行流量的转发。如果出现故障，无论是链路故障、设备故障还是peer-link故障，M-LAG都能够保证正常的业务不受影响。按照数据中心TOR与EOR之间连接要求，我们这里仅以交换机双归接入普通以太网络和IP网络为例进行介绍。</p><p>正常工作时，如下图所示，来自非M-LAG成员端口的单播流量，按照正常的转发流程进行转发，而来自M-LAG成员端口的单播流量按照M-LAG聚合设备负荷分担方式进行转发。</p><p><img src="https://i.loli.net/2019/06/22/5d0dabe4ab98052900.jpg"></p><p>而来自非M-LAG成员接口的组播/广播流量 ，在SwitchA收到组播流量后向各个下一跳转发，当流量到达SwitchB时，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量不会向S-2转发。而来自M-LAG成员接口的组播/广播流量 ，在SwitchA收到组播流量后向各个下一跳转发，当流量到达SwitchB时，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量也不会向S-2转发。如下图所示：</p><p><img src="https://i.loli.net/2019/06/22/5d0dac55927f483641.jpg"></p><p>对于网络侧发往M-LAG成员接口的单播流量，流量会负载分担到SwitchA和SwitchB，然后发送至双活接入的设备。对于网络侧发往非M-LAG成员接口的单播流量，以发往S-1为例，流量不会进行负载分担，而是直接发到SwitchA，由SwitchA发往S-1。而网络侧的组播/广播流量不会在SwitchA、SwitchB之间采用负载分担方式转发，此处以SwitchA转发为例进行说明。</p><p>以SwitchA为例，SwitchA会发送到每一个用户侧端口，由于peer-link与M-LAG成员接口存在单向隔离机制，到达SwitchB的流量不会向S-2转发。如下图：</p><p><img src="https://i.loli.net/2019/06/22/5d0dac78abf2e86468.jpg"></p><p>故障情况时，如下图所示，当peer-link链路故障时，按照M-LAG应用场景不同，M-LAG的设备表现也不同。</p><p><img src="https://i.loli.net/2019/06/22/5d0dac98e241381855.jpg"></p><p>当M-LAG应用于TRILL网络的双归接入时，peer-link故障但心跳状态正常会触发状态为备的设备上M-LAG接口处于Error-Down状态。当M-LAG应用于普通以太网络、VXLAN网络或IP网络的双归接入时，peer-link故障但心跳状态正常会触发状态为备的设备上除管理网口、peer-link接口和堆叠口以外的接口处于Error-Down状态。M-LAG主设备侧Eth-Trunk链路状态仍为Up，M-LAG备设备侧Eth-Trunk链路状态变为Down，双归场景变为单归场景。一旦peer-link故障恢复，处于Error-Down状态的M-LAG接口默认将在2分钟后自动恢复为Up状态，处于Error-Down状态的其它物理接口将自动恢复为Up状态。数据中心场景中接入交换机TOR上处于Error-Down状态的M-LAG接口默认在5分钟后自动恢复为Up状态。</p><p>当M-LAG主设备故障时，M-LAG备设备将升级为主，其设备侧Eth-Trunk链路状态仍为Up，流量转发状态不变，继续转发流量。</p><p><img src="https://i.loli.net/2019/06/22/5d0dace16c0f061750.jpg"></p><blockquote><p>M-LAG主设备侧Eth-Trunk链路状态变为Down，双归场景变为单归场景。M-LAG主备设备端口状态默认不回切，即当M-LAG主设备故障发生时，M-LAG由备状态升级为主状态的设备仍保持主状态，恢复故障的主设备成为M-LAG的备设备。</p><p>如果是M-LAG备设备发生故障，M-LAG的主备状态不会发生变化，M-LAG备设备侧Eth-Trunk链路状态变为Down。M-LAG主设备侧Eth-Trunk链路状态仍为Up，流量转发状态不变，继续转发流量，双归场景变为单归场景。</p></blockquote><p>当下行Eth-Trunk链路发生故障，M-LAG主备状态不会变化，流量切换到另一条链路上进行转发。发生故障的Eth-Trunk链路状态变为Down，双归场景变为单归场景。</p><p><img src="https://i.loli.net/2019/06/22/5d0dad0a9c43c76801.jpg"></p><p>当上行链路发生故障，由于双归接入普通以太网络时，心跳报文一般是走管理网络，故不影响M-LAG主设备的心跳检测，对于双活系统没有影响，M-LAG主备设备仍能够正常转发。如图中所示，由于M-LAG主设备的上行链路故障，通过M-LAG主设备的流量均经过peer-link链路进行转发。</p><p><img src="https://i.loli.net/2019/06/22/5d0dad348065a39648.jpg"></p><p><strong>双归接入IP网络的M-LAG设备在正常工作时，与双归接入普通以太网类似，唯一区别是在为了保证M-LAG上三层组播功能正常，需要在M-LAG主备设备间部署一条三层直连链路，部分组播流量可以通过该三层链路进行转发，这条链路一般称为C链。在故障场景下，双归接入IP网络的M-LAG设备与接入普通以太网也只在上行链路故障时，表现有所区别。</strong></p><p>双归接入IP网络的M-LAG设备，在上行链路故障时，M-LAG备设备由于收不到主设备的心跳报文而变为双主状态。</p><p><img src="https://i.loli.net/2019/06/22/5d0dad7dd2d6613666.jpg"></p><p>这时用户侧流量到达SwitchA时，不能经过peer-link链路转发且没有可用的上行出接口，SwitchA会将用户流量全部丢弃。此时，需要分别在M-LAG主备设备上配置Monitor Link关联上行接口和下行接口，当上行出接口Down时，下行接口状态也会变为Down，这样就可以防止用户侧流量被丢弃。</p><p><strong>M-LAG特性主要应用于将服务器或交换机双归接入普通以太网络、TRILL（Transparent Interconnection of Lots of Links）、VXLAN（Virtual eXtensible Local Area Network）和IP网络。</strong>一方面可以起到负载分担流量的作用，另一方面可以起到备份保护的作用。由于M-LAG支持多级互联，M-LAG的组网可以分为单级M-LAG和多级M-LAG。</p><p>在电信云数据中心中，为了保证可靠性，服务器一般采用链路聚合的方式接入网络，如果服务器接入的设备故障将导致业务的中断。为了避免这个问题的发生，服务器可以采用跨设备链路聚合的方式接入网络，在SwitchA与SwitchB之间部署M-LAG，实现服务器的双归接入。组成M-LAG的两台TOR形成负载分担，共同进行流量转发，当其中一台设备发生故障时，流量可以快速切换到另一台设备，保证业务的正常运行。服务器双归接入时的配置和一般的链路聚合配置没有差异，必须保证服务器侧和交换机侧的链路聚合模式一致，推荐两端均配置为LACP模式。除了接入交换机TOR组成M-LAG外，汇聚交换机EOR之间也需要部署M-LAG配置，与下层TOR的M-LAG进行级联，这样不仅可以简化组网，而且在保证可靠性的同时可以扩展双归接入服务器的数量。多级M-LAG互联必须基于V-STP方式进行配置，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0dadce3fbec15942.jpg"></p><p>V-STP方式下，组成M-LAG的设备在二层网络中可以不作为根桥，组网灵活且支持M-LAG的级联。V-STP功能还可以解决M-LAG中错误配置或错误连线导致的环路问题，所以推荐采用V-STP方式。</p><p><strong>VRRP技术、堆叠以及M-LAG数据中心二、层设备虚拟化技术优缺点对比及适用场景总结</strong></p><table><thead><tr><th></th><th><strong>优点</strong></th><th><strong>缺点</strong></th><th><strong>应用场景</strong></th></tr></thead><tbody><tr><td><strong>堆叠</strong></td><td>简化本地网络管理节点、易维护，同时增加系统端口密度和带宽，充分发挥设备性能。</td><td>缺点就是可靠性不高，堆叠系统分裂、系统升级都会影响业务中断</td><td>广泛应用于企业、教育</td></tr><tr><td><strong>VRRP</strong></td><td>网络设备冗余、可靠性高</td><td>配置复杂、网络建设投入成本高，不能充分发挥设备的网络性能</td><td>银行、证券、政府内网</td></tr><tr><td><strong>M-LAG</strong></td><td>网络可靠性非常高，设备控制层面独立，能单独设备升级且不影响业务，充分发挥设备性能</td><td>管理节点多（控制层面无法虚拟化）</td><td>银行、证券、数据中心（双规场景）</td></tr></tbody></table><h2 id="大二层网络的基石VxLAN"><a href="#大二层网络的基石VxLAN" class="headerlink" title="大二层网络的基石VxLAN"></a><strong>大二层网络的基石VxLAN</strong></h2><h3 id="大二层是个什么鬼？"><a href="#大二层是个什么鬼？" class="headerlink" title="大二层是个什么鬼？"></a><strong>大二层是个什么鬼？</strong></h3><p>虽然Leaf-Spine为无阻塞传输提供了拓扑的基础，但是还需有配套合适的转发协议才能完全发挥出拓扑的能力。STP的设计哲学与Leaf-Spine完全就是不相容的，冗余链路得不到利用，灵活性和扩展性极差，废除STP而转向“大二层”成了业界的基本共识。</p><p>说到大二层的“大”，首先体现在物理范围上。虚拟机迁移是云数据中心的刚性需求，由于一些License与MAC地址是绑定的，迁移前后虚拟机的MAC地址最好不变，同时为了保证业务连续，迁移前后虚拟机的IP地址也不可以变化。迁移的位置是由众多资源因素综合决定的，网络必须支持虚拟机迁移到任何位置，并能够保持位于同一个二层网络中。所以大二层要大到横贯整个数据中心网络，甚至是在多个数据中心之间。</p><p>大二层的“大”，还意味着业务支撑能力的提升。随着公有云的普及，“多租户”成了云数据中心网络的基础能力。而传统二层网络中，VLAN最多支持的租户数量为4096，当租户间IP地址重叠的时候，规划和配置起来也比较麻烦，因此VLAN不能很好地支撑公有云业务的飞速发展。同理，对于大规模私有云而言，VLAN也难以胜任其对于网络虚拟化提出的要求。</p><p>大二层网络的实现主要依赖于网络叠加技术。网络叠加技术指的是一种物理网络架构上叠加的虚拟化技术模式，其大体框架是对基础网络不进行大规模修改的条件下，实现应用在网络上的承载，并能与其他网络业务分离，以基于IP的基础网络技术为主。其实这种模式是以对传统技术的优化而形成的。早期就有标准支持的二层Overlay技术，如RFC3378（Ethernet in IP），并且基于Ethernet over GRE的技术。H3C与思科都在物理网络基础上分别发展了私有二层Overlay技术：<strong>EVI（Ethernet Virtual Interconnection）与OTV（Overlay Transport Virtualization）</strong>。主要用于解决数据中心之间的二层互联与业务扩展问题，并且对于承载网络的基本要求是IP可达，部署上简单且扩展方便。</p><p><strong>网络叠加技术可以从技术解决目前数据中心面临的三个主要问题：</strong></p><p><strong>1）解决了虚拟机迁移范围受到网络架构限制的问题。</strong>网络叠加是一种封装在IP报文之上的新的数据格式，因此，这种数据可以通过路由的方式在网络中分发，而路由网络本身并无特殊网络结构限制，具备良性大规模扩展能力，并且对设备本身无特殊要求，且具备很强的故障自愈能力、负载均衡能力。</p><p><strong>2）解决了虚拟机规模受网络规格限制的问题</strong>。虚拟机数据封装在IP数据包中后，对网络只表现为封装后的网络参数，即隧道端点的地址，因此，对于承载网络（特别是接入交换机），MAC地址规格需求极大降低，最低规格也就是几十个（每个端口一台物理服务器的隧道端点MAC）。但是，对于核心/网关处的设备表项（MAC/ARP）要求依然极高，当前的解决方案是采用分散方式，即通过多个核心/网关设备来分散表项的处理压力。</p><p><strong>3）解决了网络隔离/分离能力限制的问题。</strong>针对VLAN数量在4000以内的限制，在网络叠加技术中沿袭了云计算“<strong>租户</strong>”的概念，称之为Tenant ID（租户标识），用24或64比特表示，可以支持16M的虚拟隔离网络划分。针对VLAN技术下网络的TRUANK ALL（VLAN穿透所有设备）的问题，网络叠加对网络的VLAN配置无要求，可以避免网络本身的无效流量带宽浪费，同时网络叠加的二层连通基于虚拟机业务需求而创建，在云的环境中全局可控。</p><h3 id="Overlay网络常用技术协议"><a href="#Overlay网络常用技术协议" class="headerlink" title="Overlay网络常用技术协议"></a><strong>Overlay网络常用技术协议</strong></h3><p>目前，IETF在Overlay技术领域有如下三大技术路线正在讨论：<strong>VxLAN（Virtual eXtensible Local Area Network）</strong>，是由VMware、思科、Arista、Broadcom、Citrix和红帽共同提出的IETF草案，是一种将以太网报文封装在UDP传输层上的隧道转发模式，目的UDP端口号为4798。<strong>NVGRE（Network Virtualization using Generic Routing Encapsulation）</strong>，是微软、Dell等提出的草案，是将以太网报文封装在GRE内的一种隧道转发模式。<strong>STT（Stateless Transport Tunneling）</strong>，是Nicira公司提出的一种Tunnel技术，目前主要用于Nicira自己的NVP平台上。STT利用了TCP的数据封装形式，但改造了TCP的传输机制，数据传输遵循全新定义的无状态机制，无需三次握手，以太网数据封装在无状态TCP中。三种网络叠加技术的优缺点和数据封装格式如下：</p><p><img src="https://i.loli.net/2019/06/22/5d0dae365edc152866.jpg"></p><p><strong>这三种二层网络叠加技术，大体思路均是将以太网报文承载到某种隧道层面，其差异性在于选择和构造隧道的不同，而底层均是IP转发。</strong>VxLAN和STT对于现网设备的流量均衡要求较低，即负载链路负载分担适应性好，一般的网络设备都能对L2~L4的数据内容参数进行链路聚合或等价路由的流量均衡，而NVGRE则需要网络设备对GRE扩展头感知并对flow ID进行Hash，需要硬件升级；STT对于TCP有较大修改，隧道模式接近UDP性质，隧道构造技术具有革新性，且复杂度较高，而VxLAN利用了现有通用的UDP传输，成熟性极高。总体来说，<strong>VLXAN技术相对具有优势。</strong></p><h3 id="VxLAN的包封装格式"><a href="#VxLAN的包封装格式" class="headerlink" title="VxLAN的包封装格式"></a><strong>VxLAN的包封装格式</strong></h3><p>VXLAN报文是在原始的二层报文前面再封装一个新的报文，新的报文中和传统的以太网报文类似，拥有源目mac、源目ip等元组。当原始的二层报文来到vtep节点后会被封装上VXLAN包头（在VXLAN网络中把可以封装和解封装VXLAN报文的设备称为vtep，vtep可以是虚拟switch也可以是物理switch），打上VXLAN包头的报文到了目标的vtep后会将VXLAN包头解封装，并获取原始的二层报文。</p><p><img src="https://i.loli.net/2019/06/22/5d0dae68818e161615.jpg"></p><p><strong>1）VXLAN头部。</strong>共计8个字节，目前使用的是Flags中的一个8bit的标识位和24bit的VNI（Vxlan Network Identifier），其余部分没有定义，但是在使用的时候必须设置为0x0000。</p><p><strong>2）外层的UDP报头。目的端口使用4798，</strong>但是可以根据需要进行修改，同时UDP的校验和必须设置成全0。</p><p><strong>3）IP报文头。</strong>目的IP地址可以是单播地址，也可以是多播地址。单播情况下，目的IP地址是VTEP（Vxlan Tunnel End Point）的IP地址；多播情况下引入VxLAN管理层，利用VNI和IP多播组的映射来确定VTEPs。<strong>protocol设置值为0x11</strong>，说明这是UDP数据包。Source ip是源vTEP_IP。Destination ip是目的VTEP IP。</p><p><strong>4）Ethernet Header。</strong>Destination Address：目的VTEP的MAC地址，即为本地下一跳的地址（通常是网关MAC地址）。<strong>VLAN Type被设置为0x8100，并可以设置Vlan Id tag（这就是vxlan的vlan标签）</strong>。<strong>Ethertype设置值为0x8000，指明数据包为IPv4的</strong>。</p><p><strong>outer mac header以及outer ip header里面的相关元组信息都是vtep的信息，和原始的二层报文没有任何关系。</strong>所在数据包在源目vtep节点之间的传输和原始的二层报文是毫无关系的，依靠的是外层的包头完成。  除此之外还有几个字段需要关注：</p><p>1、隧道终端VTEP用于多VxLAN报文进行封装/解封装，包括MAC请求报文和正常VXLAN数据报文，在一端封装报文后通过隧道向另一端VTEP发送封装报文，另一端VTEP接收到封装的报文解封装后根据被封装的MAC地址进行转发，VTEP可由支持VXLAN的硬件设备或软件来实现。</p><p>2、在UDP header里面有一个source port的字段，用于VxLAN网络节点之间ECMP的hash；</p><p>3、在VXLAN Header里的reserved字段，作为保留字段，很多厂商都会加以运用来实现自己组网的一些特性。</p><h3 id="VxLAN的控制和转发平面"><a href="#VxLAN的控制和转发平面" class="headerlink" title="VxLAN的控制和转发平面"></a><strong>VxLAN的控制和转发平面</strong></h3><p><strong>1）数据平面—隧道机制</strong> </p><p>VTEP为虚拟机的数据包加上了层包头，这些新的报头只有在数据到达目的VTEP后才会被去掉。中间路径的网络设备只会根据外层包头内的目的地址进行数据转发，对于转发路径上的网络来说，一个Vxlan数据包跟一个普通IP包相比，除了个头大一点外没有区别。由于VxLAN的数据包在整个转发过程中保持了内部数据的完整，因此<strong>VxLAN的数据平面是一个基于隧道的数据平面</strong>。</p><p><strong>2）控制平面—改进的二层协议</strong></p><p> VxLAN不会在虚拟机之间维持一个长连接，所以VxLAN需要一个控制平面来记录对端地址可达情况。控制平面的表为（VNI，内层MAC，外层vtep_ip）。Vxlan学习地址的时候仍然保存着二层协议的特征，节点之间不会周期性的交换各自的路由表，对于不认识的MAC地址，VXLAN依靠组播来获取路径信息(如果有SDN Controller，可以向SDN单播获取)。</p><p>另一方面，VxLAN还有自学习的功能，当VTEP收到一个UDP数据报后，会检查自己是否收到过这个虚拟机的数据，如果没有，VTEP就会记录源vni/源外层ip/源内层mac对应关系，避免组播学习。</p><h3 id="VxLAN的报文转发"><a href="#VxLAN的报文转发" class="headerlink" title="VxLAN的报文转发"></a><strong>VxLAN的报文转发</strong></h3><p><strong>1）ARP报文转发，转发过程如下图：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0daed33215e73398.jpg"></p><p><strong>Step1：</strong>主机A向主机B发出ARP Request，Src MAC为MAC-A，Dst MAC为全F；</p><p><strong>Step2：</strong>ARP Request报文到达vtep-1后，vtep-1对其封装VxLAN包头，其中外层的Src MAC为vtep-1的MAC-1，Dst MAC为组播mac地址， Src ip为vtep-1的IP-1，Dst ip为组播ip地址，并且打上了VxLAN VNID：10。由于vtep之间是三层网络互联的，广播包无法穿越三层网络，所以只能借助组播来实现arp报文的泛洪。通常情况下一个组播地址对应一个VNID，同时可能会对应一个租户或者对应一个vrf网络，通过VNID进行租户之间的隔离。</p><p><strong>Step3：</strong>打了VxLAN头的报文转发到了其他的vtep上，进行VxLAN头解封装，原始的ARP Request报文被转发给了vtep下面的主机，并且在vtep上生成一条MAC-A（主机A的mac）、VxLAN ID、IP-1（vtep-1的ip）的对应表项；</p><p><strong>Step4：</strong>主机B收到ARP请求，回复ARP Response，Src MAC：MAC-B、Dst MAC：MAC-A；</p><p><strong>Step5：</strong>ARP Response报文到达vtep-2后，被打上VxLAN的包头，此时外层的源目mac和ip以及VxLAN ID是根据之前在vtep-2上的MAC-A、VXLAN ID、IP-1对应表项来封装的，所以ARP Response是以单播的方式回复给主机A；</p><p><strong>Step6：</strong>打了VxLAN头的报文转发到vtep-1后，进行VxLAN头的解封装，原始的ARP Response报文被转发给了主机A；</p><p><strong>Step7：</strong>主机A收到主机B返回的ARP Response报文，整个ARP请求完成。</p><p><strong>这种用组播泛洪ARP报文的方式是VxLAN技术早期的方式，这种方式也是有一些缺点，比如产生一些不可控的组播流量等，所以现在很多厂商已经使用了控制器结合南向协议（比如openflow或者一些私有南向协议）来解决ARP的报文转发问题。</strong></p><p><strong>2）单播报文转发（同一个Vxlan），转发过程如下：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0daf0abec5020399.jpg"></p><p>在经过arp报文后，vtep-1和vtep-2上都会形成一个VxLAN二层转发表，大致如下（不同厂商表项可能略有不同，但是最主要的是以下元素）：</p><p><strong>vtep-1：</strong></p><table><thead><tr><th><strong>MAC</strong></th><th><strong>VNI</strong></th><th><strong>vtep</strong></th></tr></thead><tbody><tr><td>MAC-A</td><td>10</td><td>e1/1</td></tr><tr><td>MAC-B</td><td>10</td><td>vtep-2 ip</td></tr></tbody></table><p><strong>vtep-2：</strong></p><table><thead><tr><th><strong>MAC</strong></th><th><strong>VNI</strong></th><th><strong>vtep</strong></th></tr></thead><tbody><tr><td>MAC-B</td><td>10</td><td>e1/1</td></tr><tr><td>MAC-A</td><td>10</td><td>vtep-1 ip</td></tr></tbody></table><p> <strong>Step1：</strong>host-A将原始报文上行送到vtep-1；</p><p><strong>Step2：</strong>根据目的mac和VNI的号（这里的VNI获取是vlan和vxlan的mapping查询出的结果），查找到外层的目的ip是vtep-2 ip，然后将外层的源目ip分别封装为vtep-1 ip和vtep-2 ip，源目mac为下一段链路的源目mac；</p><p><strong>Step3：</strong>数据包穿越ip网络；</p><p><strong>Step4：</strong>vtep-2根据VNI、外层的源目ip，进行解封装，通过VNI和目的mac查表，得到目的端口是e1/1；</p><p><strong>Step5：</strong>host-B接受此原始报文，并回复host-A，回复过程同上一致。</p><p><strong>3）不同Vxlan之间，VxLAN与VLAN之间转发</strong></p><p>不同VxLAN之间转发，VxLAN与VLAN之间转发，各个厂商的解决方案不大一致，一般情况下所以跨VxLAN的转发需要一个叫VxLAN网关的设备，这个VxLAN网关可以是物理交换机也可以是软件交换机。具体如下图：</p><p><img src="https://i.loli.net/2019/06/22/5d0daf467c21689706.jpg"></p><p>另外cisco对不同vxlan之间的转发又是另外一种模式，由于cisco是一家硬件设备厂商，所以它的所有vtep都是硬件交换机，但是通常物理交换机收到的都是报文已经是Hypervisor层打了vlan tag了，所以cisco的处理方式会比较复杂，大体上是通过一个L3的VNI来完成。</p><p>VxLAN和VLAN之间的转发这部分，说实话我也想不到太多实际的应用场景，如果是一个L3的网络更是没有意义，因为VLAN的网关终结在了leaf节点上，唯一想到的场景可能就是L2网络中部分设备不支持打VxLAN只能打VLAN Tag（硬件overlay一般不会出现这个情况，软件overlay情况下，会有一些物理服务器没有vswitch来打vxlan tag，只能依靠硬件交换机来打vlan tag）。这种情况下是需要通过VxLAN网关设备来完成一个VxLAN到VLAN或者VLAN到VxLAN的mapping关系。这其实很好理解，就是比如一个打了VxLAN包头的报文要去访问一个某一个VLAN，这时候报文需要被先送到一个叫VxLAN网关设备，这个设备查找VxLAN和VLAN的对应表后，将VxLAN包头拆去，再打上内层的VLAN Tag，然后将数据包送给VLAN的网络中。同理，VLAN到VxLAN网络反之处理类似。</p><h3 id="典型的VXLAN组网模式"><a href="#典型的VXLAN组网模式" class="headerlink" title="典型的VXLAN组网模式"></a><strong>典型的VXLAN组网模式</strong></h3><p><strong>1）软件模式：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0dafc75e03223461.jpg"></p><p>软件模式中vtep功能由vswitch实现，图中物理交换网络是一个L3的网络，实际软件overlay的场景下物理的交换网并不一定要是一个L3的网络，只要物理服务器的ip互相可达即可，当然L3的网络是一个比较好的选择，因为L3的网络扩展性比L2好，L2网络一大就会有各种二层问题的存在，比如广播泛洪、未知单播泛洪，比如TOR的mac问题，这个前文已经讲了很多，这里就不赘述了。</p><p><strong>优点：</strong></p><p>1）硬件交换机的转发平面和控制平面解耦，更加灵活，不受物理设备和厂商的限制；</p><p>2）现有的硬件网络设备无需进行替换。（如果现有物理设备是L2的互联网络也可以使用软件overlay的方式，当然L3互联更佳，overlay的网络还是建议L3的方式）</p><p><strong>缺点：</strong></p><p>1）vSwitch转发性能问题，硬件交换机转发基本都是由ASIC芯片来完成，ASIC芯片是专门为交换机转发而设计的芯片，而vSwitch的转发是由x86的CPU来完成，当vSwitch作为vtep后并且增加4-7层服务外加分布式路由的情况下，性能、可靠性上可能会存在问题；</p><p>2）无法实现虚拟网络和物理服务器网络的共同管理；</p><p>3）如果是用商业的解决方案，则软件层面被厂商绑死，如果选用开源或自研的vswitch，存在一定的难度和风险。</p><p>4）软件overlay容易造成管理上的混乱，特别对于传统行业来说，overlay网络到底是网络运维管还是云平台运维管，会有一些利益冲突。不过这个并不是一个技术问题，在云计算环境下，类似这种问题会越来越多。</p><p>案例：互联网公司用软件overlay的还挺多，下面附上几个的方案简介，设计一些商业解决方案和一些开源的解决方案：</p><p><a href="http://www.sdnap.com/sdn-study/5885.html" target="_blank" rel="noopener">UCloud的“公有云SDN网络实践”分享</a></p><p><a href="http://www.sdnlab.com/14855.html" target="_blank" rel="noopener">浙江电信云资源池引入VxLAN的部署初探（NSX）</a></p><p><strong>2）硬件模式：</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db03a3bcd890662.jpg"></p><p>硬件模式中vtep功能由物理交换机来实现，硬件overlay解决方案中物理网络<strong>一般都是L3的网络</strong>。</p><p><strong>优点：</strong></p><p>1）硬件交换机作为vtep转发性能比较有保证；</p><p>2）虚拟化网络和物理服务器网络可以统一管理；</p><p>3）向下兼容各种的虚拟化平台（商业或开源）。</p><p><strong>缺点：</strong></p><p>1）各硬件交换机厂商的解决方案难有兼容性，可能会出现被一个厂商锁定的情况；</p><p>2）老的硬件交换机基本上芯片都不支持VxLAN技术，所以要使用vxlan组网，会出现大规模的设备升级；</p><p><strong>案例：</strong>硬件overlay主要是Cisco、华为、H3C这种传统的硬件厂商提供解决方案，下面链接一个华为的的案例：</p><p><a href="http://www.sdnap.com/sdn-technology/6132.html" target="_blank" rel="noopener">美团云携手华为SDN解决方案</a></p><p><strong>3）软硬件混合模式</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db0632f6bc42809.jpg"></p><p>软硬件混合模式是一种比较折中的解决方案，虚拟化平台上依然使用vSwitch作为vtep，而对于物理服务器使其接入物理的vtep交换机。</p><p><strong>优点：</strong>整合了软、硬件模式的优缺点，是一种比较理想的overlay网络模型；</p><p><strong>缺点：</strong>落地起来架构比较复杂，统一性比较差，硬件交换机和软件交换机同时作为vtep，管理平台上兼容性是个问题。</p><p><strong>案例：</strong>这个解决方案目前一些硬件的交换机厂商会提供（比如华为），还有就是一些研发能力比较强的互联网公司会选择这个方案。</p><p>网络叠加技术作为网络虚拟化在数据层的实现手段，解决了虚拟机迁移范围受到网络架构限制、虚拟机规模受网络规格限制、网络隔离/分离能力限制的问题。同时，支持网络叠加的各种协议、技术正不断演进，VxLAN作为一种典型的叠加协议，最具有代表性。Linux内核3.7已经加入了对VxLAN协议的支持。另外，由IETF工作组提出的网络虚拟化叠加（NVO3）草案也在讨论之中，各大硬件厂商也都在积极参与标准的制定并研发支持网络叠加协议的网络产品，这些都在推动着软件定义网络SDN技术在云数据中心各个业务领域逐步落地。</p><h2 id="Spline-leaf架构下的SDN引入"><a href="#Spline-leaf架构下的SDN引入" class="headerlink" title="Spline-leaf架构下的SDN引入"></a><strong>Spline-leaf架构下的SDN引入</strong></h2><p>从 2003 年开始，随着虚拟化技术的引入，原来三层（three-tier）数据中心中，在二层以pod形式做了隔离的计算、网络和存储资源，形成一种池化的组网模式。这种技术产生了从接入层到核心层的大二层域的需求，如下图所示 。</p><p><img src="https://i.loli.net/2019/06/22/5d0db08b6b17627858.jpg"></p><p>虚拟机的引入，使得应用的部署方式越来越分布式，导致东西向流量越来越大。这些流量需要被高效地处理，并且还要保证低的、可预测的延迟。 然而，vPC只能提供两个并行上行链路，因此<strong>三层数据中心架构中的带宽成为了瓶颈</strong>。 三层架构的另一个问题是<strong>服务器到服务器延迟（server-to-server latency）随着流量路径的不同而不同。</strong></p><p>针对以上问题，提出了一种新的数据中心设计，称作基于Clos网络的Spine-and-Leaf架构（Clos network-based Spine-and-Leaf architecture），如下图所示。事实已经证明，这种架构可以提供高带宽、低延迟、非阻塞的服务器到服务器连接，<strong>成为超大规模网络首选。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db0c0c1a3f64646.jpg"></p><p>在以上两级Clos架构中，每个低层级的交换机（leaf）都会连接到每个高层级的交换机（spine），形成一个<strong>full-mesh拓扑</strong>。leaf 层由接入交换机组成，用于连接服务器等设备。spine层是网络的骨干，负责将所有的leaf连接起来。 fabric中的每个leaf都会连接到每个spine，如果一个spine挂了，数据中心的吞吐性能只会有轻微的下降。如果某个链路拥塞了，添加一个spine交换机就可以扩展每个leaf的上行链路，增大了leaf和spine之间的带宽，缓解了链路拥塞的问题。如果接入层的端口数量成为了瓶颈，那就直接添加一个新的leaf，然后将其连接到每个spine并做相应的配置即可。<strong>leaf层的接入端口和上行链路都没有瓶颈时，这个架构就实现了无阻塞。</strong></p><p><strong>在Spine-and-Leaf架构中，任意一个服务器到另一个服务器的连接，都会经过相同数量的设备（除非这两个服务器在同一leaf下面），这保证了延迟是可预测的，因为一个包只需要经过一个spine和另一个leaf就可以到达目的端。</strong></p><p>在数据中心的实际应用中，Spline-and-leaf架构又分为<strong>分层解耦架构</strong>和<strong>全融合架构</strong>两种，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db123413b967489.jpg"></p><p><strong>在分层解耦架构中，业务功能模块化，由不同的叶子节点分别实现。各节点可灵活合并和解耦，Pod分区具备灵活扩展能力。</strong>适用于东西流量预期有显著增长、数据中心规模大、未来高扩展的场景，比如：中国移动私有云部署场景。</p><p><strong>在全融合架构中，业务功能集中由一对核心交换机实现，属于接入+核心的二层架构。</strong>这种架构中，<strong>网络设备少，流量模型简单，维护简单，但扩展性较差。</strong>适用于中小型数据中心、DC规划可预见/扩展性低的场景，比如：各个中小企业的私有云部署场景。</p><p><strong><em>在数据中心中，到底使用大二层还是Spline-and-leaf解决方案，是由服务器的规模决定的，二者之间的区别只是服务器接入能力不同，并不存在谁比谁更先进一说。</em></strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db168c2f3385731.jpg"></p><blockquote><p>大二层架构是由核心层+接入层共两层组成，两台核心交换机做堆叠，作为网关，FW、LB等设备旁挂在网关边上。接入层TOR两两堆叠，可提供2000+台物理服务的接入能力。</p><p>Spline-and-leaf架构是由核心层+汇聚层+接入层共三层组成，两台核心交换机做堆叠，作为网关，FW、LB等设备旁挂在网关边上。Spine层交换机数量为2的倍数，相互独立，不做堆叠。接入层TOR两两堆叠，可提供2000~5000台物理服务的接入能力。</p></blockquote><p>中国移动私有云SDN解决方案技术规范中，对数据中内部的组网架构图要求如下（电信云的要求类似，多了网管CE的部署要求）。</p><p><img src="https://i.loli.net/2019/06/22/5d0db195edab866822.jpg"></p><ul><li>虚拟化/裸金属服务器混合场景优先采用混合SDN组网方式，即硬件接入交换机（SDN ToR）和VSW作为VTEP。</li><li>SDN控制器或控制器插件支持南向数据一致性校验和对账。</li><li>SDN控制器支持allowed-address-pairs功能。</li><li>控制器支持OpenFlow+Evpn流表和路由相互转换。</li><li>支持IPv4/IPv6双栈网络服务的自动化部署。</li><li>支持vFW上IPv6防火墙策略的自动化部署。</li><li>Server Leaf和Border leaf采用MC-LAG方式组网，并且由控制器下发MC-LAG相关业务。</li></ul><p>上述技术规范中，<strong>EVPN+OpenFlow双控制面是未来演进方向</strong>。相比OpenFlow单控制面来说，区别如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db1d1e9b4a86962.jpg"></p><p>在单控制面OpenFlow组网中，<strong>SDN控制器通过OpenFlow给虚拟交换机和物理交换机统一下发流表，虚拟交换机和物理交换机按照流表进行转发。</strong>优点就是OpenFlow灵活性高，SDN控制器和转发设备间可实现解耦。但是缺点很明显，也就是SDN控制器成为瓶颈，收敛性能和稳定性欠佳，不适合大规模资源池网络，最关键的是和已有IP网络的兼容性不好，如检测等特性。</p><p>在双控制面EVPN+OpenFlow组网中，<strong>SDN控制器通过OpenFlow给虚拟交换机下发流表，物理交换机之间通过BGP-EVPN协议建立转发表项。</strong>优点就是可靠性高，兼具OpenFlow的灵活性和EVPN的扩展性，符合未来演进方向，也减少现有设备投资改造费用。缺点就是对控制器要求较高，需要控制器支持EVPN协议，具备OpenFlow流表和EVPN转发表的翻译能力，而且这属于设备商的私有接口部分，只有少数几个厂家支持，比如，华为。</p><p>在实际多机房部署时，具体规划如下图所示</p><p><img src="https://i.loli.net/2019/06/22/5d0db20054c3737805.jpg"></p><ul><li>各个机房通过东西向互联交换机互联。</li><li>每个机房部署一套SDN控制器和云平台，采用硬件分布式或混合分布式SDN方案。</li><li>核心生产区、测试区和DMZ区的服务器统一挂在业务TOR下面。</li><li>所有的SDN控制器和云平台统一部署在管理网络区（POD1内）。</li><li>FC SAN分散部署在各个机房内，分布式存储集中部署在POD7。</li><li>在每个机房内部署东西向防火墙，Internet出口部署南北向防火墙。</li></ul><p>以上就是数据中心层面服务器外部交换网络虚拟化的全部内容，至此网络虚拟化技术理论方面的知识点全部介绍完毕，后续会用一篇Linux原生网络虚拟化实战作为全部虚拟化专题的结束篇。可能有人会提到SDN还没讲，一方面是因为目前还在试点，另一方面是因为SDN涉及面很广，有时间的话我会写个专题，从协议、控制器部署、实际解决方案等专门来介绍SDN。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据中心服务器外部交换网络的虚拟化主要包括：&lt;strong&gt;数据中心内部二、三层交换设备虚拟化、大二层组网架构&lt;/strong&gt;和&lt;strong&gt;软件定义网络SDN技术。&lt;/strong&gt;其中，软件定义网络SDN技术在目前中国移动NovoNet战略中还处于现网试点阶段，预计2020年初会引入电信云数据中心。而数据中心内部二、三层交换设备虚拟化及大二层组网架构已完全成熟，并已在当前电信云数据中心中实际部署，本文也主要讲述这两种技术，其中大二层组网基石VxLAN需要大家重点掌握。
    
    </summary>
    
      <category term="NFV关键技术" scheme="https://kkutysllb.cn/categories/NFV%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="电信云" scheme="https://kkutysllb.cn/tags/%E7%94%B5%E4%BF%A1%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-21-KVM到底是个啥？</title>
    <link href="https://kkutysllb.cn/2019/06/22/2019-06-21-KVM%E5%88%B0%E5%BA%95%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F/"/>
    <id>https://kkutysllb.cn/2019/06/22/2019-06-21-KVM到底是个啥？/</id>
    <published>2019-06-22T03:52:01.000Z</published>
    <updated>2019-06-22T05:02:04.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="KVM的现状"><a href="#KVM的现状" class="headerlink" title="KVM的现状"></a><strong>KVM的现状</strong></h2><p>KVM最初是由Qumranet公司的Avi Kivity开发的，作为他们的VDI产品的后台虚拟化解决方案。为了简化开发，Avi Kivity并没有选择从底层开始新写一个Hypervisor，而是选择了<strong>基于Linux kernel，通过加载模块使Linux kernel本身变成一个Hypervisor。</strong>2006年10月，在先后完成了基本功能、动态迁移以及主要的性能优化之后，Qumranet正式对外宣布了KVM的诞生。同月，<strong>KVM模块的源代码被正式纳入Linux kernel，成为内核源代码的一部分。</strong>2008年9月4日，Redhat公司以1.07亿美元收购了Qumranet公司，包括它的KVM开源项目和开发人员。自此，Redhat开始在其RHEL发行版中集成KVM，逐步取代Xen，并从RHEL7开始，正式不支持Xen。<a id="more"></a></p><p>在KVM出现之前，Xen虚拟化解决方案已经业界比较成熟的一款开源VMM，但是KVM出现之后，很快被Linux内核社区接受，就是因为Xen不是通过Linux内核去管理系统资源（硬件/软件），而是通过自身的管理系统去完成，仅这一点就让Linux内核社区很不爽。同时，Xen在当时设计上采用半虚方式，需要修改Guest OS的内核来满足I/O驱动性能要求，从而不支持商用OS（Windows、Mac OS）的虚拟化，使得Xen相比KVM来说，在硬件辅助虚拟化的支撑上包袱更重，转型困难重重。</p><p>目前，<strong>KVM已经成为OpenStack用户选择的事实上的Hypervisor标准。</strong>OpenStack自身调查数据显示，KVM占87%以上的部署份额，详细参见<a href="http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014/。可以说，KVM已经主导公有云部署的Hypervisor市场，同时在电信云部署方面也是一枝独秀。" target="_blank" rel="noopener">http://superuser.openstack.org/articles/openstack-user-survey-insights-november-2014/。可以说，KVM已经主导公有云部署的Hypervisor市场，同时在电信云部署方面也是一枝独秀。</a></p><p>功能上，虚拟化发展到今天，各个Hypervisor的主要功能都差不多。KVM由于其开源性，反而商业上的限制较少。性能上，KVM和Xen都能达到原生系统95%以上的效率（CPU、内存、网络、磁盘等benchmark衡量），KVM甚至还略微好过Xen一点点。微软虽然宣布其Hype-V的性能更好，但这只是微软一家之言，并没有公开的数据支撑。</p><p>在电信云NFV领域来说，由于通信网络设备的实时性要求很高，且NFV的开源平台OPNFV选择了OpenStack。为了更好实现网络功能虚拟化的愿景，其实时性要求责无旁贷的落到了KVM头上，NFV-KVM项目也就顺理成章地诞生了，它作为OPNFV的子项目主要解决KVM在实时性方面受到的挑战。详情请参见<a href="https://wiki.opnfv.org/display/kvm/Nfv-kvm。" target="_blank" rel="noopener">https://wiki.opnfv.org/display/kvm/Nfv-kvm。</a></p><p><strong>总的来说，虚拟化技术发展到今天已经非常成熟，再加上DPDK代码的开源化、KVM也好、其他Hypervisor也好，在转发性能的优化和硬件辅助虚拟化的支撑上都半斤八两，但由于KVM的开源性特性以及社区的热度，使得其在云计算领域解决方案一枝独秀。甚至，华为在其FusionSphere6.3版本也开始拥抱KVM而抛弃了Xen。要知道，华为在剑桥大学可是专门有一支团队在研究Xen虚拟化。</strong></p><h2 id="KVM虚拟化实现"><a href="#KVM虚拟化实现" class="headerlink" title="KVM虚拟化实现"></a><strong>KVM虚拟化实现</strong></h2><p><strong>KVM全称是Kernel-based Virtual Machine，即基于内核的虚拟机</strong>，是采用硬件辅助虚拟化技术的全虚拟化解决方案。对于I/O设备（如硬盘、网卡等），KVM即支持QEMU仿真的全虚，也支持virtio方式的半虚。<strong>KVM从诞生开始就定位于基于硬件虚拟化支持的全虚实现</strong>，由于其在Linux内核2.6版本后被集成，<strong>通过内核加载模式使得Linux内核变成一个事实上的Hypervisor</strong>，但是硬件管理还是由Linux Kernel来完成。因此，<strong>它是一个典型Type 2型虚拟化</strong>，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db4f46083634024.jpg"></p><p>如上图，<strong>一个KVM客户机就对应一个Linux进程，每个vCPU对应这个进程下的一个线程，还有单独处理I/O的线程，属于同一个进程组（忘了的，请回顾本站CPU虚拟化系列文章）。</strong>所以，宿主机上Linux Kernel可以像调度普通Linux进程一样调度KVM虚拟机，<strong>这种机制使得Linux Kernel的进程优化和调度功能优化等策略，都能用于KVM虚拟机。</strong>比如：通过进程权限限定功能可以限制KVM客户机的权限和优先级等。</p><p><strong>由于KVM嵌入Linux内核中，除了硬件辅助虚拟化（如VT-d）透传的硬件设备能被虚拟机看见外，其他的I/O设备都是QEMU模拟出来的，所以QEMU是KVM的天生好基友。而在内存管理方面，由于KVM本身就是Linux Kernel中一个模块，所以其内存管理完全依赖于Linux内核。</strong>Linux系统的所有内存管理机制，如大页、零页（重复页）共享KSM、NUMA、mmap共享内存等，都可以用于KVM虚拟机的内存管理上。</p><p><strong>在数据存储方面，由于KVM是Linux Kernel的一部分，它可以利用所有存储厂商的存储架构，支持Linux支持的任何存储和文件系统来存储数据。同时，还支持全局文件系统GFS2等共享文件系统上的虚拟机镜像</strong>，允许虚拟机在多个Host之间共享存储或使用逻辑卷共享存储。KVM虚拟机的原生磁盘格式为QCOW2，支持磁盘镜像、快照、多级快照和压缩加密等虚拟化特性。但是，有一点需要注意：<strong>基于KVM的镜像盘必须使用RAW格式（一种非精简格式盘，在华为FC解决方案中称为普通盘。它就像我们的物理硬盘一样，分配多大空间就是多大空间），否则利用镜像发布虚机会不成功。</strong></p><p><strong>在实时迁移方面，KVM虚拟机支持在多个Host之间热迁移，无论是OVS分布式虚拟交换机，还是Linux Bridge分布式虚拟交换机，KVM虚拟机都能完美兼容实现虚拟接入，且对用户（实际用户，APP等，也就是我们常说的租户这个概念）是透明的。同时，还支持将客户机当前状态，也就是快照保存到磁盘，并在以后恢复。</strong></p><p><strong>在设备驱动方面，KVM支持混合虚拟化，</strong>其中半虚拟化的驱动程序安装在虚拟机的OS中，允许虚拟机使用优化I/O接口而不用模拟设备。KVM使用的半虚拟化驱动程序是IBM和RedHat联合Linux社区开发的virtio标准，它是一个与Hypervisor独立的，构建设备驱动程序的接口，是KVM内核的另一个好基友，不仅支持KVM对其调用，还支持VMware、Hyper-V对其调用。同时，就像前面提到的，KVM也支持VT-d技术，两者如胶似漆，完美契合，通过将Host上PCI总线上的设备透传给虚拟机，让虚拟机可以直接使用原生的驱动程序来驱动这些物理设备。忘了啥是VT-d的，请回顾本站I/O虚拟化一文。同时，就像前面开篇提到的KVM在<strong>在性能方面能达到原生的95%以上，不差于Xen虚拟化，但在伸缩性方面支持拥有多达288个vCPU和4TB RAM的虚拟机，远超于Xen（Xen因为DM0的存在，其伸缩性受限，单机最大支持32个vCPU，1.5TB RAM）。</strong></p><p><strong>通过上面大致了解，可以说KVM就是在硬件辅助虚拟化技术之上构建起来的VMM。</strong>但并非要求所有硬件虚拟化技术都支持才能运行KVM虚拟化，<strong>KVM对硬件最低的依赖是CPU的硬件虚拟化支持</strong>（比如：Intel的VT-x技术和AMD的AMD-V技术），而其他的内存和I/O的硬件虚拟化支持，会让整个KVM虚拟化下的性能得到更多的提升。所以，我们在虚拟机部署KVM功能时，首先就是要查看宿主机Host（也就是我们实验环境的虚拟机，一般是在VMware Workstations的虚拟机打开CPU虚拟化功能。如果是真实环境，则需要物理服务器的BIOS中要开启VT-x功能，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db52ae861d10143.jpg"></p><p>然后，进入服务器后通过如下指令确认VT-x功能支持，如下图所示。</p><p><img src="https://i.loli.net/2019/06/22/5d0db5521d6ee20494.jpg"></p><p>如果什么输出都没有，那说明你的系统并没有支持虚拟化的处理 ，不能使用KVM。<strong>另外Linux发行版本必须在64bit环境中才能使用KVM。</strong></p><h2 id="KVM软件架构拆解"><a href="#KVM软件架构拆解" class="headerlink" title="KVM软件架构拆解"></a><strong>KVM软件架构拆解</strong></h2><p>KVM是在硬件虚拟化支持下的全虚拟化技术，所以它能在相应硬件上运行几乎所有的操作系统，如：Linux、Windows、FreeBSD、MacOS等。KVM虚拟化的核心主要由以下两个模块组成：</p><p><strong>1）KVM内核模块，它属于标准Linux内核的一部分，是一个专门提供虚拟化功能的模块，主要负责CPU和内存的虚拟化，</strong>包括：客户机的创建、虚拟内存的分配、CPU执行模式的切换、vCPU寄存器的访问、vCPU的执行。</p><p><strong>2）QEMU用户态工具，它是一个普通的Linux进程，为客户机提供设备模拟的功能，</strong>包括模拟BIOS、PCI/PCIE总线、磁盘、网卡、显卡、声卡、键盘、鼠标等。<strong>同时它通过ioctl系统调用与内核态的KVM模块进行交互。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db571ed3c212430.jpg"></p><p>上图中，<strong>在KVM虚拟化架构下，每个客户机就是一个QEMU进程</strong>，在一个宿主机上有多少个虚拟机就会有多少个QEMU进程。客户机中的每一个虚拟CPU对应QEMU进程中的一个执行线程，一个宿主机Host中只有一个KVM内核模块，所有虚拟机都与这个内核模块进行交互。</p><h3 id="KVM内核模块"><a href="#KVM内核模块" class="headerlink" title="KVM内核模块"></a><strong>KVM内核模块</strong></h3><p>KVM内核模块是KVM虚拟化的核心模块，它在内核中由两部分组成：<strong>一个是处理器架构无关的部分，用lsmod命令中可以看到，叫作kvm模块</strong>；<strong>另一个是处理器架构相关的部分，在Intel平台上就是kvm_intel这个内核模块。</strong>如下图所示，KVM的主要功能是<strong>初始化CPU硬件，打开虚拟化模式，然后将虚拟机运行在虚拟环境下，并对虚拟机的运行提供一定的支持。</strong></p><p><img src="https://i.loli.net/2019/06/22/5d0db59ae145834533.jpg"></p><p>KVM仅支持硬件辅助的虚拟化，所以，“<strong>打开并初始化系统硬件以支持虚拟机的运行”</strong>是KVM模块本职工作。以Intel CPU架构服务器为例，KVM打开并初始化硬件以支持虚拟机运行的过程如下：</p><p><strong>Step1：</strong>在被内核加载的时候，KVM模块会先初始化内部的数据结构。</p><p><strong>Step2：</strong>做好准备之后，KVM模块检测系统当前的CPU，然后打开CPU控制寄存器CR4中的虚拟化模式开关，并通过执行<strong>VMXON指令</strong>将宿主操作系统（包括KVM模块本身）置于CPU虚拟化模式中的<strong>根模式root operation</strong>，详细参见本站计算虚拟化之CPU虚拟化一文。</p><p><strong>Step3：</strong>KVM模块创建特殊设备文件/dev/kvm并等待来自用户空间的命令。</p><p><strong>Step4：</strong>后续虚拟机的创建和运行，其本质上就是是一个用户空间的QEMU和内核空间的KVM模块相互配合的过程。</p><p><strong>这里面的/dev/kvm这个设备比较关键，它可以被当作一个标准的字符设备，用来缓存用户空间与内核空间切换的上下文，也就是ioctl调用上下文，是KVM模块与用户空间QEMU的通信接口。</strong></p><p>针对/dev/kvm文件的最重要的loctl调用就是“<strong>创建虚拟机</strong>”。这里的创建虚拟机，<strong>简单理解就是KVM为了某个虚拟机创建对应的内核数据结构，并且返回一个文件句柄来代表所创建的虚拟机。</strong>针对该文件句柄的loctl调用可以对虚拟机做相应的管理，比如创建用户空间虚拟地址和客户机物理地址及真实内存物理地址的映射关系，再比如创建多个可供运行的vCPU。KVM模块同样会为每一个创建出来的vCPU生成对应的文件句柄，对其文件句柄进行相应的loctl调用，就可以对vCPU进行调度管理。</p><p>而针对vCPU的最重要的loctl调用就是“<strong>运行虚拟处理器</strong>”。通过它，虚拟机就可以在非根模式下运行，一旦执行敏感指令，就通过<strong>VMX Exit</strong>切入根模式，由KVM决定后续的操作并返回执行结果给虚拟机。</p><p>除了处理器虚拟化，<strong>内存虚拟化的实现也是由KVM内核模块完成，包括影子页表和EPT硬件辅助，均由KVM内核模块负责完成GVA—&gt;HPA的两级转换。</strong>处理器对设备的访问主要是通过<strong>I/O指令</strong>和<strong>MMIO</strong>，其中<strong>I/O指令会被处理器直接截获，MMIO会通过配置内存虚拟化来捕捉。一般情况下，除非对外设有极高性能要求，比如虚拟中断和虚拟时钟，外设均是由QEMU这个用户空间的进程来模拟实现。</strong></p><h3 id="QEMU用户态设备模拟"><a href="#QEMU用户态设备模拟" class="headerlink" title="QEMU用户态设备模拟"></a><strong>QEMU用户态设备模拟</strong></h3><p>QEMU原本就是一个著名的开源虚拟机软件项目，既是一个功能完整的虚拟机监控器，也在QEMU-KVM的软件栈中承担设备模拟的工作。它是由一个法国工程师独立编写的代码实现，<strong>并不是KVM虚拟化软件的一部分</strong>，但是从名字就能知道它和KVM是一辈子的好基友。</p><p>QEMU最初实现的虚拟机是一个纯软件的实现，也就是我们常说的通过二进制翻译来实现虚拟机的CPU指令模拟，所以性能比较低。但是，其优点是跨平台，甚至可以支持客户机与宿主机并不是同一个架构，比如在x86平台上运行ARM客户机。同时，QEMU能与主流的Hypervisor完美契合，包括：Xen、KVM、Hyper-v，以及VMware各种Hypervisor等，为上述这些Hypervisor提供虚拟化I/O设备。</p><p><strong>而QEMU与KVM密不可分的原因，就是我们常说的QEMU-KVM软件协议栈。</strong>虚拟机运行期间，QEMU会通过KVM内核模块提供的系统调用ioctl进入内核，由KVM内核模块负责将虚拟机置于处理器的特殊模式下运行。一旦遇到虚拟机进行I/O操作时，KVM内核模块会从上次的系统调用ioctl的出口处返回QEMU，由QEMU来负责解析和模拟这些设备。除此之外，虚拟机的配置和创建，虚拟机运行依赖的虚拟设备，虚拟机运行时的用户操作环境和交互，以及一些针对虚拟机的特殊技术，比如动态迁移，都是由QEMU自己实现的。</p><p><strong>QEMU除了提供完全模拟的设备以外，还支持virtio协议的设备模拟。</strong>在前端虚拟机中需要安装相应的virtio-blk、virtio-scsi、virtio-net等驱动，就能连接到QEMU实现的virtio的虚拟化后端。除此之外，QEMU还提供了virtio-blk-data-plane的高性能的块设备I/O方式，与传统virtio-blk相比，它为每个块设备单独分配一个线程用于I/O处理，不需要与原QEMU执行线程同步和竞争锁，而且使用ioeventfd/irqfd机制，利用宿主机Linux上的AIO（异步I/O）来处理客户机的I/O请求，使得块设备I/O效率进一步提高。</p><p><strong><em>总之，在KVM虚拟化的软件架构中，KVM内核模块与QEMU用户态程序是处于最核心的位置，有了它们就可通过qemu命令行操作实现完整的虚拟机功能。</em></strong></p><h2 id="KVM的各类上层管理APP"><a href="#KVM的各类上层管理APP" class="headerlink" title="KVM的各类上层管理APP"></a><strong>KVM的各类上层管理APP</strong></h2><p>KVM目前已经有libvirt API、virsh命令行工具、OpenStack云管理平台等一整套管理工具，与VMware提供的商业化管理工具相比虽然有所差距，但KVM这一整套管理工具都是API化的、开源的，可以灵活使用，且能二次定制开发。</p><p><img src="https://i.loli.net/2019/06/22/5d0db5f94cc3658528.jpg"></p><p><strong>1）libvirt</strong></p><p>libvirt是使用最广泛的对KVM虚拟化进行管理的工具和应用程序接口，已经是事实上的虚拟化接口标准。作为通用的虚拟化API，<strong>libvirt不但能管理KVM，还能管理VMware、Hyper-V、Xen、VirtualBox等其他虚拟化方案。但是，在通过Docker或Kolla部署OpenStack时，由于容器镜像中集成了libvirt功能，需要关闭Host的libvirt服务，否则会发生PID调用错误。</strong></p><p><strong>2）virsh</strong></p><p>virsh是一个常用的管理KVM虚拟化的命令行工具，用于在单个宿主机上进行运维操作。virsh是用C语言编写的一个调用libvirt API的虚拟化管理工具，其源代码也是同步公布在libvirt这个开源项目中的。我们常用的KVM虚拟机查看指令就是virsh list –all，如下图：</p><p><img src="https://i.loli.net/2019/06/22/5d0db616cb5c237088.jpg"></p><p><strong>3）virt-manager</strong></p><p>virt-manager是专门针对虚拟机的图形化管理软件，底层与虚拟化交互的部分仍然是调用libvirt API来操作的。virt-manager除了提供虚拟机生命周期管理的基本功能，还提供性能和资源使用率的监控，同时内置了VNC和SPICE客户端，方便图形化连接到虚拟客户机中。virt-manager在RHEL、CentOS、Fedora等操作系统上都非常流行，因其图形化操作的易用性，成为新手入门学习虚拟化操作的首选管理软件。但是，在真实服务器端由于需要安装图形化界面，所以并不常用（服务器环境实现可视化一般是通过VNC功能实现）。</p><p><strong>4）OpenStack</strong></p><p>OpenStack是目前业界使用最广泛的功能最强大的云管理平台，它不仅提供了管理虚拟机的丰富功能，还有非常多其他重要管理功能，如：对象存储、块存储、网络、镜像、身份验证、编排服务、控制面板等。OpenStack的Nova、Cinder和Neutron，也就是计算、存储和网络管理组件仍然使用libvirt API来完成对底层虚拟化的管理，其计算、存储和网络服务组件的配置文件conf中，均有[libvirt]分类引用配置项。</p><p>以上，就是KVM这个耳熟能详的Hypervisor的全貌，作为运维人员掌握以上知识点即可，也就是理解KVM虚拟机的真正工作原理即可。如果工作层面涉及性能或管理的二次开发，必须进一步了解并掌握其官方社区的源码以及各种热点技术或BUG解决方案，这就需要自己钻研了。后面，我们会从实战的角度来介绍如何用熟KVM虚拟机的各类操作。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;KVM的现状&quot;&gt;&lt;a href=&quot;#KVM的现状&quot; class=&quot;headerlink&quot; title=&quot;KVM的现状&quot;&gt;&lt;/a&gt;&lt;strong&gt;KVM的现状&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;KVM最初是由Qumranet公司的Avi Kivity开发的，作为他们的VDI产品的后台虚拟化解决方案。为了简化开发，Avi Kivity并没有选择从底层开始新写一个Hypervisor，而是选择了&lt;strong&gt;基于Linux kernel，通过加载模块使Linux kernel本身变成一个Hypervisor。&lt;/strong&gt;2006年10月，在先后完成了基本功能、动态迁移以及主要的性能优化之后，Qumranet正式对外宣布了KVM的诞生。同月，&lt;strong&gt;KVM模块的源代码被正式纳入Linux kernel，成为内核源代码的一部分。&lt;/strong&gt;2008年9月4日，Redhat公司以1.07亿美元收购了Qumranet公司，包括它的KVM开源项目和开发人员。自此，Redhat开始在其RHEL发行版中集成KVM，逐步取代Xen，并从RHEL7开始，正式不支持Xen。
    
    </summary>
    
      <category term="KVM" scheme="https://kkutysllb.cn/categories/KVM/"/>
    
    
      <category term="Hypervisor" scheme="https://kkutysllb.cn/tags/Hypervisor/"/>
    
  </entry>
  
  <entry>
    <title>2019-06-20-SBA架构下的核心网大一统</title>
    <link href="https://kkutysllb.cn/2019/06/20/2019-06-20-SBA%E6%9E%B6%E6%9E%84%E4%B8%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BD%91%E5%A4%A7%E4%B8%80%E7%BB%9F/"/>
    <id>https://kkutysllb.cn/2019/06/20/2019-06-20-SBA架构下的核心网大一统/</id>
    <published>2019-06-20T09:37:24.000Z</published>
    <updated>2019-06-21T05:47:39.266Z</updated>
    
    <content type="html"><![CDATA[<p>移动通信核心网络从1G/2G程控交换、电路交换时代的大一统，到2.5G/3G/4G软交换、GPRS、EPC和IMS的专业分家，再到5G时代SBA架构统一，完美印证了那句俗得不能再俗的俗话—“天下大势，合久必分、分久必合”。<a id="more"></a></p><p>现在，很多人都以为5G的语音业务无论是初期的VoLTE、后续EPS Fallback，还是最终的VoNR仍是由IMS网络来提供，再加上现在讲解5G SBA架构的课件资料等都是以4G EPC网络架构做参考对比讲解。所以，5G 时代的核心网仍然是分成PS和IMS两大领域。其实，从业务提供层面来讲可以这样认为，毕竟不同业务的信令协议不同。但是，从网元实现和组网架构的设计理念来讲，未必还需要上述的专业分家。</p><h2 id="业务角度的核心网专业划分"><a href="#业务角度的核心网专业划分" class="headerlink" title="业务角度的核心网专业划分"></a><strong>业务角度的核心网专业划分</strong></h2><p>移动通信网络的核心网从GSM时代的2.5G时代开始就分为电路交换域CS和分组交换域PS，分别负责移动语音业务和GPRS手机上网业务，到了3G时代，核心网在CS和PS的基础上又多了一个“小兄弟”IMS域，它的定义是“IP多媒体子系统”，在2008年的3GPP R5版本定义并冻结。在国内，只有中国移动在2009年6省市部署IMS域试点，2010年27省全面部署IMS域并全面商用，初期用于固网VOBB政企和家庭固话业务承载，而到了4G时代，由于其作为VoLTE高清语音/视频通话业务的核心网络，<strong>在移动通信核心网的地位一下由“小兄弟”蹿升为“老大哥”</strong>。</p><p><img src="https://i.loli.net/2019/06/20/5d0b54522a92c53216.jpg"></p><p>目前，4G时代，移动通信核心网主要细分为EPC、IMS和软交换三大专业，EPC仍然继承提供GPRS业务，只是带宽大了很多，速度快了许多，可以简单理解为GPRS业务的增强版。而IMS则从初期只提供固网业务，发展为开始为用户提供高清语音/视频通话业务，不仅业务种类多了手机终端侧的视频通话、视频彩铃、一号多终端（虚拟eSIM卡）等富媒体业务，且语音质量（清晰度、保真度）和接续时延等用户感知，相比传统电路交换域CS，都有质的提升。<strong>至于软交换专业，那只是2G/3G时代电路域CS的产物，最终会被时代抛弃。</strong>这一点从中国电信、中国联通纷纷宣布关闭GSM网络来看，就是最好的证明。至于中国移动，由于其组网和业务承载的复杂性（中移动目前网络是全球最复杂一张网），以及用户迁移率（VoLTE业务用户转化）等原因，暂时无法宣布关闭GSM网络时间表，但是2G网络的语音业务体验不如VoLTE，这是肯定的。如果你的终端支持VoLTE功能，且是中移动客户，那就赶紧开通VoLTE吧，资费不变，感知提升，何乐而不为呢？</p><p><img src="https://i.loli.net/2019/06/20/5d0b546f4124435466.jpg"></p><p>因此，在现网4G网络阶段，移动通信核心网专业从业务承载的角度来看，分为<strong>EPC专业（用户数据业务、彩信业务）、IMS专业（高清语音/视频业务、短信业务）和软交换专业（传统语音业务、短信业务）</strong>三大专业。所以，虽然大家都宣称自己是核心网专业从业人员，但是业务流程、信令协议、网元配置以及人员技能储备等方面都是不同的。</p><p>到了5G时代，由于SBA服务化核心网架构的提出（中移动首提），所有<strong>网元功能模块全部“软”化</strong>，从软件模块化设计理念来看，<strong>这必然会导致业务管理网元功能SMF不仅只继承EPC-C面功能，同时整个IMS-C面功能也是能够集成统一的。而用户面网元功能UPF不仅只继承EPC-U，同样也能集成IMS-U。</strong></p><p><img src="https://i.loli.net/2019/06/20/5d0b5492bf87456950.jpg"></p><p>而且，基于IT领域“生产者/消费者”理念，<strong>导致SBA架构下各网元功能模块之间通过一条逻辑总线互联</strong>，<strong>松耦合且无强依赖关系。</strong>比如：鉴权解耦（AUSF）、用户数据解耦（UDM/UDR）、接入控制解耦（AMF）、业务功能解耦且接口开放（NEF、AF）、数据转发解耦（UPF）、全网路由寻址统一（NRF）等等。<strong>这个理念（生产者/消费者）与现有IMS网络架构设计理念（控制、承载、业务三分离）是相符的，这也为统一提供语音/数据类业务，不再细分IMS/CS/PS等专业打下基础。</strong>不是很理解？那我们就来掰扯下IMS网络架构如何与SBA架构融合。。。</p><h2 id="IMS逻辑网元与SBA架构网元功能融合"><a href="#IMS逻辑网元与SBA架构网元功能融合" class="headerlink" title="IMS逻辑网元与SBA架构网元功能融合"></a><strong>IMS逻辑网元与SBA架构网元功能融合</strong></h2><p>IMS网络架构从2008年3GPP R5版本冻结以来，一直没有变化。从这一点来看，正好说明这个架构是非常成熟的。即使到了4G时代，为了给用户提供VoLTE业务，加入了EPC网络。但是，<strong>从VoLTE业务角度来看，EPC只是一个接入网，为用户建立一条承载隧道，使其接入IMS网络，从而享受VoLTE高清类、富媒体类等业务，整体VoLTE业务及其增值业务控制都在IMS网络内，所以，IMS网络才是真正的核心网。</strong></p><p><img src="https://i.loli.net/2019/06/20/5d0b54bbd6f3d87730.jpg"></p><p>整个IMS网络架构也是分层的，分为<strong>接入层，承载层、控制层</strong>和<strong>业务层</strong>共4层。从VoLTE业务角度来看，接入层就是上图的EPC部分，承载层就是上图的IP专网，控制层和业务层就是上图的IMS部分（控制层功能和业务层功能由不同的物理网元实现）。其整体全貌如下，我们就按照从接入层到业务层的层次顺序，来掰扯下面这张图。。。</p><p><img src="https://i.loli.net/2019/06/20/5d0b54e0c9ae354705.jpg"></p><p>首先，就是核心层与接入层之间的用作边界网关的SBC网元，它主要用于<strong>用户接入控制，业务代理鉴权和数据包的路由转发</strong>。用户的业务请求从接入层首先送达SBC，然后由它负责向核心侧进行消息转发，而核心侧的响应也由它抓发给接入侧。所以，<strong>SBC网元必然是一个业务信令数据包和业务媒体数据包合一转发的网元，从数通角度来说与路由器功能一致。</strong>那么，从网元兼具的具体功能角度来看，用户接入控制功能可以卸载到SBA架构的AMF，业务代理鉴权可以卸载到SBA的AUSF，业务路由控制可以卸载到SBA的SMF，媒体面数据包转发功能可以卸载到SBA的UPF上。</p><p>从SBC往上，就到了真正的核心控制层，主要就三类网元<strong>CSCF</strong>、<strong>ENUM/DNS和HSS</strong>。先说CSCF，CSCF在3GPP定义中又分为三个逻辑功能网元P-CSCF、I-CSCF和S-CSCF。<strong>P-CSCF网元也是用于业务的接入控制，本质上它才是IMS核心控制层的入口。</strong>而现网由于涉及跨网络层对接，从安全角度考虑设置边界网关SBC，正是因为SBC的存在，使得P-CSCF变成了一个纯信令控制面网元。<strong>在固网业务网内，P-CSCF是独立设置的，与边界网关SBC采用星型拓扑连接，目的就是对不同业务区域用户接入统一集中管理。在VoLTE业务网内，P-CSCF与SBC合设。</strong>所以，从网元功能角度来看，其接入控制功能同样可以卸载SBA架构的AMF上。</p><p>I-CSCF网元主要用于跨IMS核心控制层互通，所以它有拓扑隐藏的功能。也就是说，从其他IMS核心控制层来的业务请求消息只能找本端的I-CSCF，本端核心网的其他网元对外来的业务请求消息是不可见的。这个网元的功能有个学名—<strong>“用户归属域入口”</strong>，见名知意，<strong>它本质上也是一种接入控制类网元。</strong>所以，从网元功能的角度来看，也可以卸载到SBA架构的AMF。如果考虑安全风险，也可以卸载到SBA架构的SMF。</p><p>S-CSCF网元主要用于信令的路由控制和业务逻辑的触发，从这点来看是个<strong>典型的会话控制类网元</strong>。所以，必然可以卸载到SBA的SMF上。</p><p>ENUM/DNS网元用于IMS域内全网的路由寻址，但是它不做路由转发，而是将寻址到的对端地址发给本端的S-CSCF，由本端的S-CSCF负责路由转发。由于IMS网络目前主要用于通话类语音业务，因此就涉及电话号码的翻译问题。在2G/3G时代，用户拨打电话，是通过纯号码分析功能完成全网路由寻址的，在VoLTE时代，由于语言业务承载在IP上，就涉及将电话号码翻译成IP地址的需求，这就是ENUM/DNS网元存在的意义。</p><p><img src="https://i.loli.net/2019/06/20/5d0b55045351845286.jpg"></p><p>如上图，ENUM/DNS从名字上就知道它是由ENUM和DNS两个逻辑网元组成，其实它还有个小名，叫ENS（初期一提ENS，很多人都懵逼了。。。我也是懵逼er之一。。。）。ENUM的功能主要完成用户电话号码到归属域域名+传输&amp;应用协议+服务端口的NAPTR翻译，然后将翻译后的结果送给DNS完成SRV和A查询的两步翻译，从而得到被叫用户归属域入口地址（也就是I-CSCF地址），实现业务从主叫端到被叫端的E2E接续。从ENUM/DNS的网元功能来看，它与SBA架构下NRF何其相似，这也是为什么有人提出5G时代DNS功能弱化的原因。唯一的区别就是现有ENUM/DNS还负责用户数据的存储，而SBA架构的NRF可没这么牛X，但是可以拉着它的“表兄弟”—UDM/UDR一起帮忙啊。。。所以，这都不是问题。</p><p>而HSS网元用于用户签约数据存储，用户接入的合法性鉴权/授权等功能，它也有个学名叫“用户数据中心”。在SBA架构下也有个类似的网元功能单元UDM/UDR，其功能与HSS也类似，同样用于用户签约数据的存储，只是它没有用户接入合法性鉴权/授权功能，这部分功能解耦在SBA架构的AUSF上面。所以，HSS网元的用户签约数据存储功能可以卸载到UDM/UDR上，而鉴权/授权功能可以卸载到AUSF上，两个网元功能单元通过SBA服务总线接口交互消息，同样也不是问题。</p><p>从核心控制层往上，就是业务层了，主要是各类业务服务器的包括基础业务提供服务器、增值业务提供服务器、短信、彩信、彩铃、彩印等等。。。这些业务服务器通过标准接口与核心控制层对接，只用于业务逻辑的生成和下发（比如：互转、彩铃播放等等），并不涉及业务路由的控制，所以它们与核心控制层是一种解耦关系，也就是说任何一家业务服务网元厂家只要按照统一接口标准开发自己的产品就可以与核心控制层完成对接，并实现特色业务提供。同样，在SBA架构下也有类似的网元功能单元，那就是AF，如果有进一步能力开放需求，还可以拉着NEF一起搞个“大事情”。</p><p>以上，通过IMS网络各网元功能的解析，阐述了从网元功能层面，IMS网元与SBA架构网元功能融合的可能性。下面，我们从网元处理逻辑的角度继续掰扯。</p><p>现网的网元种类很多，按专业细分：有软件换的、EPC的、承载网的、IMS的、增值业务的等等；按照所处的网络位置分：有边界网关、接入端局、互联互通关口局、长途汇接局、信令转发点等等。。。同样，现网各类网元的提供厂商也很多，有华为、中兴、爱立信、诺基亚等知名厂商。但是，无论什么类型的网元，无论由谁来提供，<strong>网元内部处理逻辑绕不开三大块：消息接口和分发逻辑单元、业务处理逻辑单元和数据库逻辑单元。</strong></p><p><img src="https://i.loli.net/2019/06/20/5d0b5538b393840575.jpg"></p><p><strong>消息接口和分发逻辑主要用来接收各类信令/媒体消息，按照一定的过滤机制分发给内部的业务逻辑处理单元，业务逻辑处理完成处理后，将业务状态缓存到内部的数据库逻辑单元，并将处理结果转发给消息接口和分发逻辑单元，然后消息接口单元按照一定路由策略转发给外部其他网元的消息接口单元。</strong>上述各类型网元内部基本上都是类似的处理逻辑，这种逻辑处理机制同样符合软件模块化设计的思想。而5G SBA架构本身就采用软件模块化设计的思想，不仅将各类网元功能“软”化，同时将传统网络各网元的逻辑功能进行了拆分和重组，使得每个“软”化的网元功能单元能力更加清晰。因此，也就有了我们开篇提到的SMF不仅只继承EPC-C面功能，同样也能集成IMS-C面功能的观点，增加的逻辑功能点主要涉及消息接口和分发逻辑单元以及业务处理单元的开发，而接入控制功能卸载到AMF、鉴权功能卸载AUSF等，增加的逻辑功能点也主要涉及上面两处。</p><p>但是，现网网元这种处理逻辑其实是一种有状态的设计理念，涉及业务状态在本网元内部的存储，一旦本网元故障，而业务数据没有异地灾备机制的情况下，就会发生业务受损。而5G时代为了进一步提高业务可靠性，有些厂家提出了<strong>无状态的设计理念</strong>，这就需要将传统网元内部数据库逻辑单元统一进行集群化部署，而与各业务功能单元的业务处理逻辑单元采用高可靠、负载均衡对接架构，从而实现业务高可用，无损失特性。这些从软件模块设计理念来看，那都不是事儿。</p><p>了解了上面网元内部处理逻辑的概念，那么理解不同业务信令、流程在同一类SBA网元功能单元完成逻辑判断和业务处理，也就是水到渠成的事情。比如：在SMF只能处理PS业务流程基础上，在其内部业务处理逻辑单元中增加SIP信令处理单元、Diameter信令处理单元、HTTP信令处理单元，就能在SMF上同样实现CSCF的功能。同理，在AMF也能实现P-CSCF/I-CSCF的功能。而<strong>5G网络基于业务流的QoS策略，在一个PDU会话中通过识别不同业务流，从而建立不同5QI（类似现在QCI)等级的业务承载</strong>，更为这种大一统的核心网架构提供天然的基础。</p><p>以上，就是我对5G SBA架构下核心网大一统的粗浅理解。在我个人看来，结合SBA服务化网络架构理念和网元功能单元“软”化，未来核心网专业大一统是很有可能的，并且从技术角度来看，应该也不存在无法解决的问题。但是，很可能有设备厂商从网络可靠、安全的角度提出异议，或者是一些细节问题提出解决进展慢等困难，其实这都是背后的商业目的在作怪。至于人员的技能融合，那是必须要完成的，否则就算只是核心网专业，那也玩不转。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;移动通信核心网络从1G/2G程控交换、电路交换时代的大一统，到2.5G/3G/4G软交换、GPRS、EPC和IMS的专业分家，再到5G时代SBA架构统一，完美印证了那句俗得不能再俗的俗话—“天下大势，合久必分、分久必合”。
    
    </summary>
    
      <category term="5G网络架构" scheme="https://kkutysllb.cn/categories/5G%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/"/>
    
    
      <category term="5G" scheme="https://kkutysllb.cn/tags/5G/"/>
    
  </entry>
  
</feed>
